<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ciandcd</title><link>http://www.ciandcd.com/</link><description>continuous integration and continuous delivery</description><atom:link href="http://www.ciandcd.com/feeds/ciandcd.rss.xml" rel="self"></atom:link><lastBuildDate>Sun, 21 Jun 2015 00:38:11 +0800</lastBuildDate><item><title>On Antifragility in Systems and Organizational Architecture</title><link>http://www.ciandcd.com/on-antifragility-in-systems-and-organizational-architecture.html</link><description>&lt;div&gt;&lt;p class="post-headline"&gt;&lt;h1&gt;On Antifragility in Systems and Organizational Architecture&lt;/h1&gt;&lt;/p&gt;&lt;p&gt;In his new book, &lt;a href="http://www.amazon.com/dp/1400067820?tag=contindelive-20"&gt;Antifragile&lt;/a&gt;, Nassim Taleb discusses the behaviour of complex systems and distinguishes three kinds: those that are fragile, those that are robust or resilient, and those that are antifragile. These types of systems differ in how they respond to volatility: &amp;#8220;The fragile wants tranquility, the antifragile grows from disorder, and the robust doesn&amp;#8217;t care too much.&amp;#8221; (p20) Taleb argues that we want to create systems that are antifragile &amp;#8211; that are designed to take advantage of volatility. I think this concept is incredibly powerful when applied to systems and organizational architecture.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h3&gt;Why Continuous Delivery Works&lt;/h3&gt;
&lt;p&gt;Taleb shows why the traditional approach of operations &amp;#8211; making change hard, since change is risky &amp;#8211; is flawed: &amp;#8220;the problem with artificially suppressed volatility is not just that the system tends to become extremely fragile; it is that, at the same time, it exhibits no visible risks&amp;#8230; These artificially constrained systems become prone to Black Swans. Such environments eventually experience massive blowups&amp;#8230; catching everyone off guard and undoing years of stability or, in almost all cases, ending up far worse than they were in their initial volatile state&amp;#8221; (p105)&lt;a href="#1"&gt;1&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;This a great explanation of how many attempts to manage risk actually result in &lt;a href="http://continuousdelivery.com/2013/08/risk-management-theatre/"&gt;risk management theatre&lt;/a&gt; &amp;#8211; giving the appearance of effective risk management while actually making the system (and the organization) extremely fragile to unexpected events. It also explains why &lt;a href="http://www.amazon.com/dp/0321601912?tag=contindelive-20"&gt;continuous delivery&lt;/a&gt; works. The most important heuristic we describe in the book is &amp;#8220;if it hurts, do it more often, and bring the pain forward.&amp;#8221; The effect of following this principle is to exert a constant stress on your delivery and deployment process to reduce its fragility so that releasing becomes a boring, low-risk activity.&lt;/p&gt;
&lt;h3&gt;Antifragile Systems&lt;/h3&gt;
&lt;p&gt;Another of Taleb&amp;#8217;s key claims is that it is impossible to predict &amp;#8220;Black Swan&amp;#8221; events: &amp;#8220;you cannot say with any reliability that a certain remote event or shock is more likely than another&amp;#8230; but you can state with a lot more confidence that an object or a structure is more fragile than another should a certain event happen.&amp;#8221; (p8). Thus we need &amp;#8220;to switch the blame from the inability to see an event coming&amp;#8230; to the failure to understand (anti)fragility, namely, &amp;#8216;why did we build something so fragile to these types of events?&amp;#8217;&amp;#8221; (p136).&lt;/p&gt;
&lt;p&gt;Unlike risk, fragility is actually measurable. How do we measure the fragility of the systems we build? We try to break them, using techniques such as &lt;a href="http://queue.acm.org/detail.cfm?id=2371297"&gt;game days&lt;/a&gt; and systems like &lt;a href="http://techblog.netflix.com/2012/07/chaos-monkey-released-into-wild.html"&gt;chaos monkey&lt;/a&gt;. The systematic application of stress to your systems is essential &amp;#8211; not just to ensure your systems are antifragile, but to develop the muscles of the people who create and maintain them through constant practice. After all, it&amp;#8217;s the combination of the system and the people who build and run it that has the quality of antifragility.&lt;/p&gt;
&lt;p&gt;In this context, an important quality of legacy systems is their fragility. Legacy systems that aren&amp;#8217;t touched for a long time will turn into fragile &amp;#8220;works of art&amp;#8221;: changing them is considered risky, the number of people who understand the system decreases with time, and their knowledge atrophies from lack of exercise.&lt;/p&gt;
&lt;p&gt;How do we create antifragile systems? Apply stress to them continuously so we are forced to simplify, homogenise, and automate. &lt;/p&gt;
&lt;h3&gt;Antifragile Organizations&lt;/h3&gt;
&lt;p&gt;We can measure the fragility of an organization by how long it takes before it liquidates its assets. Deloitte&amp;#8217;s Shift Index shows that the average life expectancy of a Fortune 500 company has declined from around 75 years half a century ago to less than 15 years today.&lt;/p&gt;
&lt;p&gt;Start-ups are notoriously fragile. But the ones that survive and grow turn into something potentially more dangerous &amp;#8211; robust organizations. The problem with robust organizations is that they resist change. They aren&amp;#8217;t quickly killed by changes to their environment, but they don&amp;#8217;t adapt to them either &amp;#8211; they die slowly. We see this effect all the time &amp;#8211; changing the culture of an established organization is incredibly hard.&lt;/p&gt;
&lt;p&gt;Antifragile organizations are those that have a culture that enables them to learn fast from their environment and adapt to it so they can take advantage of volatility. Here are some characteristics of antifragile organizations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Systems thinking.&lt;/strong&gt; Everybody in the organization knows the goals of the organization and makes sure their work is directly contributing towards these goals.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Theory Y Management.&lt;/strong&gt; Management needs to assume employees are self-motivated and will be able to learn how to solve problems themselves. Organizations need to make sure they hire antifragile people who will thrive in this environment. As Daniel Pink&amp;#8217;s &lt;a href="http://www.amazon.com/dp/1594484805?tag=contindelive-20"&gt;Drive&lt;/a&gt; points out, giving your employees autonomy, purpose, and the opportunity to learn and master new skills is what stops them from quitting, thus increasing the antifragility of your organization.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Continuous experimentation.&lt;/strong&gt; As described in &lt;a href="http://www.amazon.com/dp/0071635238?tag=contindelive-20"&gt;Toyota Kata&lt;/a&gt;, good management knows that the best solutions come from the workers. They create an environment in which practitioners are able to run experiments to learn as rapidly as possible. The feedback loops in command and control organizations are too slow for them to adapt effectively.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Disruptive product development.&lt;/strong&gt; Antifragile organizations aren&amp;#8217;t content with stress generated by their environment. Like humans exercising, they also try and disrupt themselves (the organizational equivalent of a game day). For example, &lt;a href="http://blogs.hbr.org/ideacast/2013/01/jeff-bezos-on-leading-for-the.html"&gt;Amazon cannibalized its own business&lt;/a&gt;, creating the Amazon Marketplace and the Kindle. Apple is &lt;a href="http://blogs.hbr.org/cs/2011/10/steve_jobs_solved_the_innovato.html"&gt;cannibalizing its Mac business&lt;/a&gt; with the iPad. Fragile organizations resist disrupting their own product lines, as &lt;a href="http://spectrum.ieee.org/semiconductors/processors/25-microchips-that-shook-the-world/5"&gt;Toshiba did at first with flash memory&lt;/a&gt;. If you do a good job at this you never need to worry about the competition &amp;#8211; you&amp;#8217;ll always beat them to it.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Fragility and Agility&lt;/h3&gt;
&lt;p&gt;As Taleb points out, &amp;#8220;antifragility is desirable in general, but not always, as there are cases in which antifragility will be costly, extremely so. Further, it is hard to consider robustness as always desirable&amp;#8212;to quote Nietzsche, one can die from being immortal.&amp;#8221; (p22) Of course working out where on the spectrum you want your systems and your organization to lie is an art, and the great artists are those that know how to build systems, organizations, and products simply, quickly and cheaply so that they are antifragile with respect to our biggest enemy: time. How do they do that? Using the same heuristics described in &amp;#8220;antifragile organizations&amp;#8221;, above, which closely mirror the &lt;a href="http://itrevolution.com/the-three-ways-principles-underpinning-devops/"&gt;Three Ways of Devops&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As I read &lt;a href="http://www.amazon.com/dp/1400067820?tag=contindelive-20"&gt;Antifragile&lt;/a&gt;, it reminded me of something I read a number of years ago: Kent Beck and Cynthia Andres&amp;#8217; &lt;a href="http://www.amazon.com/dp/0321278658?tag=contindelive-20"&gt;Extreme Programming Explained&lt;/a&gt;. The subtitle? Embrace Change. It strikes me that the concept of antifragile is what we were aiming for with agile the whole time: building systems (including human systems &amp;#8211; organizations) that benefit from volatility.&lt;/p&gt;

&lt;h4&gt;Endnotes&lt;/h4&gt;
&lt;p&gt;Thanks to &lt;a href="https://twitter.com/badrij"&gt;Badrinath Janakiraman&lt;/a&gt; for feedback on an earlier draft of this post.&lt;/p&gt;
&lt;p&gt;&lt;a name="1"&gt;1&lt;/a&gt; He is talking about financial markets, which are rather less fragile than IT systems, hence his rather generous &amp;#8220;years of stability&amp;#8221;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:38:11 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:on-antifragility-in-systems-and-organizational-architecture.html</guid></item><item><title>Book Review: The Phoenix Project</title><link>http://www.ciandcd.com/book-review-the-phoenix-project.html</link><description>&lt;div&gt;&lt;p class="post-headline"&gt;&lt;h1&gt;Book Review: The Phoenix Project&lt;/h1&gt;&lt;/p&gt;&lt;p&gt;I am not going to do a ton of book reviews on this blog (I have one more planned for next month). I&amp;#8217;ll only bother posting reviews of books that I believe are both excellent and relevant to &lt;a href="http://continuousdelivery.com/"&gt;Continuous Delivery&lt;/a&gt;. This book easily satisfies both criteria. Full disclosure: Gene gave me a draft of this book for free for reviewing purposes.&lt;/p&gt;
&lt;p&gt;You&amp;#8217;ve probably heard of Gene Kim, Kevin Behr and George Spafford before. They are the three amigos responsible for &lt;a href="http://www.amazon.com/dp/0975568612?tag=contindelive-20"&gt;The Visible Ops Handbook&lt;/a&gt;, which can be found in the book pile of every good IT operator. Their new book, &lt;a href="http://www.amazon.com/dp/0988262592?tag=contindelive-20"&gt;The Phoenix Project:  A Novel About IT, DevOps, and Helping Your Business Win&lt;/a&gt;, follows the format of Eliyahu Goldratt&amp;#8217;s classic, &lt;a href="http://www.amazon.com/dp/0884271951?tag=contindelive-20"&gt;The Goal&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Told from the perspective of newly-minted VP of IT Operations Bill Palmer, it describes the turnaround of failing auto parts company Parts Unlimited. This is to be achieved through the delivery of the eponymous Phoenix Project, a classic &amp;#8220;too big to fail&amp;#8221; software project designed to build a system which will revive the fortunes of the company.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;To quote (p51):&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;
The plot is simple: First, you take an urgent date-driven project, where the shipment date cannot be delayed because of external commitments made to Wall Street or customers. Then you add a bunch of developers who use up all the time in the schedule, leaving no time for testing or operations deployment. And because no one is willing to slip the deployment date, everyone after Development has to take outrageous and unacceptable shortcuts to hit the date.&lt;/p&gt;
&lt;p&gt;The results are never pretty. Usually, the software product is so unstable and unusable that even the people who were screaming for it end up saying that it&amp;#8217;s not worth shipping. And it&amp;#8217;s always IT Operations who still has to stay up all night, rebooting servers hourly to compensate for crappy code, doing whatever heroics are required to hide from the rest of the world just how bad things really are.
&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Part One of the book describes in loving detail the enormous clusterfuck pie that is baked from these ingredients. The pie is spiced with an internal Sarbanes-Oxley audit which reveals 952 control deficiencies, an outage of the payroll processing system, and various other problems that conspire to deepen the woe of the operations group, all of which are clearly drawn from the deep well of the authors&amp;#8217; real-life experiences.&lt;/p&gt;
&lt;p&gt;Apart from the main characters &amp;#8211; our hero Bill, his boss Steve, and the evil villain Sarah &amp;#8211; The Phoenix Project features a delightful rogues&amp;#8217; gallery which anyone working in an enterprise will recognize:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Brent Geller, the boy wonder whose encyclopedic knowledge of the company&amp;#8217;s Byzantine IT systems means that his involvement is necessary to get anything done.&lt;/li&gt;
&lt;li&gt;Patty McKee, the Director of Support who runs a change management process so bureaucratic that everybody bypasses it.&lt;/li&gt;
&lt;li&gt;John Pesche, the black binder wielding Chief Information Security Officer whose constant meddling under the guise of improving security has turned him into a pariah.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The second part of the book details how the IT group is reborn from the ashes of the Phoenix Project into a high-performing organization that is a strategic partner to the business. This is achieved through the application of a heavy dose of lean thinking (including &lt;a href="http://www.amazon.com/dp/0321601912?tag=contindelive-20"&gt;continuous delivery&lt;/a&gt;) administered by Erik, a mercurial IT and manufacturing guru Steve is courting to join the board. The book does an excellent job of showing &amp;#8211; as well as telling &amp;#8211; how to apply the concepts (and the effect of doing so) in an enterprise with plenty of technical debt. Perhaps the most eyebrow-raising part of this section is the way in which John has his soul mercilessly crushed to the point where he goes on a multi-day drinking spree before he is rehabilitated towards the end of the book (he is a phoenix too).&lt;/p&gt;
&lt;p&gt;John&amp;#8217;s narrative arc is just one example of how the book also succeeds as a novel. It&amp;#8217;s gripping, with moments of drama and high emotion, as well as some great one-liners. There was even one point when I teared up (bear in mind that I also cried during Forrest Gump &amp;#8211; unlike the book&amp;#8217;s central characters, I did not serve in the armed forces). &lt;/p&gt;
&lt;p&gt;Nobody who has read The Goal will miss The Phoenix Project&amp;#8217;s similarity in terms of style and plot. Perhaps my favourite thing about the book&amp;#8217;s pedagogical style is the way Erik (like Jonah in The Goal) uses the Socratic Method to give Bill the tools to solve his problems by himself. Of course this learning process is fictional, but it means you get to see Bill struggling with the questions and trying things out.&lt;/p&gt;
&lt;p&gt;It remains to be seen whether readers of the book will be able to apply these techniques as successfully as Bill without a real Erik to guide them. But of course, this is a limitation of any book. If I had one criticism it&amp;#8217;s that unlike real life, there aren&amp;#8217;t many experiments in the book that end up making things worse, and it&amp;#8217;s this process of failing fast, learning from your failures, and coming up with new experiments that is instrumental to a real learning culture.&lt;/p&gt;
&lt;p&gt;One important point worth noting if you are working in an organization like Parts Unlimited is this: the IT department&amp;#8217;s rebirth is only possible because of the Titanic proportions of the disaster that unfolds in Part One. For management to truly embrace change, a compelling event or a teachable moment (i.e. a Charlie Foxtrot) is required. Unless your organization faces the same existential threat that Parts Unlimited does, you&amp;#8217;ll have a much harder time convincing people they should adopt the tools described in the book.&lt;/p&gt;
&lt;p&gt;Overall, The Phoenix Project is a fantastic read. It&amp;#8217;s entertaining, cathartic, inspirational and informative. If, like me, you have an enormous backlog of books (and more work in process than you&amp;#8217;d like) I suggest giving yourself a break and putting this one to the top of your list. It&amp;#8217;ll only take you a day or two, and despite its conceptual density it will leave you feeling refreshed and energized with a bunch of new ideas to try out. &lt;a href="http://www.amazon.com/dp/0988262592?tag=contindelive-20"&gt;The Phoenix Project&lt;/a&gt; deserves to be read by everyone who works in &amp;#8211; or with &amp;#8211; IT.&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:38:09 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:book-review-the-phoenix-project.html</guid></item><item><title>Announcing FlowCon</title><link>http://www.ciandcd.com/announcing-flowcon.html</link><description>&lt;div&gt;&lt;p class="post-headline"&gt;&lt;h1&gt;Announcing FlowCon&lt;/h1&gt;&lt;/p&gt;&lt;p&gt;I spend quite a lot of time at conferences, and it consistently bothers me that they are so often focused on one particular function: development, testing, UX, systems administration. The point of continuous delivery is to accelerate the rate at which we can learn from each other &amp;#8211; and from our customers. That requires everyone involved in the delivery process (including users, product owners and entrepreneurs) to collaborate throughout. So why isn&amp;#8217;t there a conference which focuses on flow &amp;#8211; the emergent property of great teams?&lt;/p&gt;
&lt;p&gt;So I got together with a bunch of like-minded folks &amp;#8211; &lt;a href="http://flowcon.org/flowcon-sanfran-2013/speaker/Elizabeth+Hendrickson"&gt;Elisabeth Hendrickson&lt;/a&gt;, &lt;a href="http://flowcon.org/flowcon-sanfran-2013/speaker/Gene+Kim"&gt;Gene Kim&lt;/a&gt;, &lt;a href="http://flowcon.org/flowcon-sanfran-2013/speaker/John+Esser"&gt;John Esser&lt;/a&gt; and &lt;a href="http://flowcon.org/flowcon-sanfran-2013/speaker/Lane+Halley"&gt;Lane Halley&lt;/a&gt; &amp;#8211; and now there is a conference about creating flow: &lt;a href="http://flowcon.org/"&gt;FlowCon&lt;/a&gt;. It&amp;#8217;s on &lt;strong&gt;Friday November 1 in San Francisco&lt;/strong&gt;, and it&amp;#8217;s produced by &lt;a href="http://www.thoughtworks.com/"&gt;ThoughtWorks&lt;/a&gt; and &lt;a href="http://www.trifork.com/"&gt;Trifork&lt;/a&gt; (creators of the &lt;a href="http://gotocon.com/"&gt;GOTO conferences&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;The conference is based around four values:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Learning&lt;/strong&gt;: Our goal is to provide the best possible conference forum for practitioners to learn from each other how to build great products and services.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Open Information&lt;/strong&gt;: We aim to uncover how great products and services are built in real life and make this information freely available to the widest audience possible.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Diversity&lt;/strong&gt;: We believe the technology community &amp;#8211; and thus the conference speakers and participants &amp;#8211; should reflect the demographics of our customers and the wider world.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Spanning boundaries&lt;/strong&gt;: We believe that the best products and services are created collaboratively by people with a range of skills and experiences.&lt;/p&gt;
&lt;p&gt;We have put together nearly half of the &lt;a href="http://flowcon.org/flowcon-sanfran-2013/schedule/index.jsp"&gt;program&lt;/a&gt;, and we&amp;#8217;re delighted to announce that &lt;a href="http://flowcon.org/flowcon-sanfran-2013/speaker/Adrian+Cockcroft"&gt;Adrian Cockcroft&lt;/a&gt;, &lt;a href="http://flowcon.org/flowcon-sanfran-2013/speaker/Catherine+Courage"&gt;Catherine Courage&lt;/a&gt;, &lt;a href="http://flowcon.org/flowcon-sanfran-2013/speaker/Jeff+Gothelf"&gt;Jeff Gothelf&lt;/a&gt; and &lt;a href="http://flowcon.org/flowcon-sanfran-2013/speaker/Linda+Rising"&gt;Linda Rising&lt;/a&gt; will be giving keynotes. &lt;a href="http://flowcon.org/flowcon-sanfran-2013/schedule/index.jsp"&gt;The program&lt;/a&gt; is still a work in process (a minimum viable product, if you will). In particular, the after lunch sessions are empty &amp;#8211; for a good reason: &lt;strong&gt;we want you to speak in those slots&lt;/strong&gt;. We&amp;#8217;re looking for people working to create flow in their organization &amp;#8211; especially those who:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Span multiple roles and work across organizational silos.&lt;/li&gt;
&lt;li&gt;Work in any of the following areas: a highly regulated environment; a large, traditional enterprise; in the pursuit of social and economic justice.&lt;/li&gt;
&lt;li&gt;Are willing to share obstacles encountered or mistakes made and how you overcame them &amp;#8211; whether cultural or technological.&lt;/li&gt;
&lt;li&gt;Offer actionable advice &amp;#8220;the rest of us&amp;#8221; can apply today (even if we don&amp;#8217;t have the resources of Etsy / Amazon / Google).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Your talk could be about culture, technology, design, process &amp;#8211; the only really important criterion is that it draws on what you&amp;#8217;ve learned about helping to create flow in your organization.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;If that sounds like you, please &lt;a href="http://flowcon.org/flowcon-sanfran-2013/submit"&gt;submit your proposal&lt;/a&gt;. If you know someone who would do a great job, please encourage them to submit. Our submission process is designed to be entirely merit-based, which means that the first round is anonymous. The deadline is midnight Pacific time, Sunday June 23, 2013.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://secure.trifork.com/flowcon-sanfran-2013/registration/"&gt;Tickets for the conference are now on sale&lt;/a&gt; &amp;#8211; at $350 if you register before July 31, or $500 if you register afterwards. Whatever your role or domain, you&amp;#8217;re sure to find inspirational, disruptive thinking that will make you better at creating great products and services. I hope to see you there!&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:38:07 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:announcing-flowcon.html</guid></item><item><title>Videos from the Continuous Delivery track at QCon SF 2012</title><link>http://www.ciandcd.com/videos-from-the-continuous-delivery-track-at-qcon-sf-2012.html</link><description>&lt;div&gt;&lt;p class="post-headline"&gt;&lt;h1&gt;Videos from the Continuous Delivery track at QCon SF 2012&lt;/h1&gt;&lt;/p&gt;&lt;p&gt;At last year&amp;#8217;s QCon San Francisco I got to curate a track on continuous delivery. One of the goals of the &lt;a href="http://www.qconferences.com/"&gt;QCon conferences&lt;/a&gt; is &amp;#8220;information Robin Hood&amp;#8221; &amp;#8211; finding ways to get out into public the secret sauce of high performing organizations. So I set out to find talks that would answer the questions I frequently get asked: can continuous integration, automated testing, and trunk-based development scale? How does continuous delivery affect the way we do product management? What&amp;#8217;s the business case for continuous delivery? How do you grow a culture that enables it?&lt;/p&gt;
&lt;p&gt;You&amp;#8217;ll find the all these questions answered in the talks below, from the leaders who have been at the forefront of continuous delivery at Amazon, Facebook, Google and Etsy. They also discuss the tools they built and the and practices they use to enable continuous delivery. Finally, you get me talking about how you can adopt continuous delivery at your organization.&lt;/p&gt;
&lt;p&gt;Thanks so much to Jesse Robbins, Frank Harris, Nell Thomas, John Penix and Chuck Rossi for these great talks, and to the folks behind &lt;a href="http://qconsf.com/"&gt;QCon SF&lt;/a&gt; for an awesome conference.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
 
&lt;p&gt;&lt;a href="http://twitter.com/jesserobbins"&gt;Jesse Robbins&lt;/a&gt; ran ops at Amazon before quitting to co-found Opscode (creators of &lt;a href="http://www.opscode.com/chef/"&gt;Chef&lt;/a&gt;). He is also co-founder of &lt;a href="velocityconf.com"&gt;Velocity&lt;/a&gt;. In his copious spare time, he&amp;#8217;s a volunteer firefighter. Basically, Jesse is an enormous over-achiever. This is a fabulous &amp;#8211; and hilarious &amp;#8211; talk that discusses the hardest part of implementing continuous delivery: cultural change. This talk features my favourite devops aphorism:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.infoq.com/presentations/Hacking-Culture"&gt;&lt;img src="http://continuousdelivery.com/wp-content/uploads/2013/05/Screen-Shot-2013-05-25-at-9.15.46-AM.png" alt="Don&amp;#x27;t fight stupid, make more awesome" width="400" class="alignleft size-full wp-image-967"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br clear="all"&gt;&lt;/p&gt;
 
&lt;p&gt;One of the main goals of continuous delivery is to get fast feedback on your &lt;a href="http://www.drdobbs.com/architecture-and-design/hypothesis-driven-development/229000656"&gt;hypotheses&lt;/a&gt; so you can build the right thing. In this talk &lt;a href="http://twitter.com/hirefrank"&gt;Frank Harris&lt;/a&gt; and &lt;a href="http://twitter.com/nellwyn"&gt;Nell Thomas&lt;/a&gt; of &lt;a href="http://codeascraft.com/"&gt;Etsy&lt;/a&gt; show off a bunch of their tools, including the A/B testing framework they built for running experiments (which uses &lt;a href="http://martinfowler.com/bliki/FeatureToggle.html"&gt;feature toggles&lt;/a&gt; under the hood). They give an example of an experiment they&amp;#8217;re running right now, and discuss how the ability to gather and analyze data on customer behaviour in real time (see screenshot below) affects the way they do product development.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.infoq.com/presentations/Etsy-Deployment"&gt;&lt;img src="http://continuousdelivery.com/wp-content/uploads/2013/05/Screen-Shot-2013-05-25-at-10.31.38-AM.png" alt="Etsy&amp;#x27;ss A/B testing tool, Atlas" width="400" class="alignleft size-full wp-image-971"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br clear="all"&gt;&lt;/p&gt;
 
&lt;p&gt;In this talk, &lt;a href="http://research.google.com/pubs/author2207.html"&gt;John Penix&lt;/a&gt; of Google shows off the awesome product he and his team built for continuous integration and cloud-based testing at Google. Teams at Google are free to choose their own development practices and toolchain, but this one has a pretty high uptake. When people ask me if trunk-based development and continuous integration can scale, I like to show them the following slide:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.infoq.com/presentations/Continuous-Testing-Build-Cloud"&gt;&lt;img src="http://continuousdelivery.com/wp-content/uploads/2013/05/Screen-Shot-2013-05-25-at-11.15.03-AM.png" alt="CI at scale at Google" width="400" class="alignleft size-full wp-image-979"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br clear="all"&gt;&lt;/p&gt;
 
&lt;p&gt;In addition to discussing the process he uses to release twice a day, Facebook&amp;#8217;s lead release engineer &lt;a href="http://twitter.com/chuckr"&gt;Chuck Rossi&lt;/a&gt; shows off the extensive toolchain they built to deploy at scale. Highlights include Gatekeeper (screenshot below), which manages who gets to see which features as part of their dark launching process, and their deploy tool which categorizes all proposed patches based on the size of the patch, the amount of discussion around it, and the &amp;#8220;push karma&amp;#8221; of the committers. &lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.infoq.com/presentations/Facebook-Release-Process"&gt;&lt;img src="http://continuousdelivery.com/wp-content/uploads/2013/05/Screen-Shot-2013-05-25-at-11.39.28-AM.png" alt="Gatekeeper" width="400" class="alignleft size-full wp-image-982"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;br clear="all"&gt;&lt;/p&gt;
 
&lt;p&gt;Amazon, Etsy, Google and Facebook are all primarily software development shops which command enormous amounts of resources. They are, to use &lt;a href="https://twitter.com/BMC_DevOps"&gt;Christopher Little&amp;#8217;s&lt;/a&gt; metaphor, unicorns. How can the rest of us adopt continuous delivery? That&amp;#8217;s the subject of my talk, which describes four case studies of organizations that adopted continuous delivery, with varying degrees of success.&lt;/p&gt;
&lt;p&gt;One of my favourites &amp;#8211; partly because it&amp;#8217;s embedded software, not a website &amp;#8211; is the story of HP&amp;#8217;s LaserJet Firmware team, who re-architected their software around the principles of continuous delivery. People always want to know the business case for continuous delivery: the FutureSmart team provide one in &lt;a href="http://www.amazon.com/dp/0321821726?tag=contindelive-20"&gt;the book they wrote&lt;/a&gt; that discusses how they did it:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://continuousdelivery.com/wp-content/uploads/2013/05/Screen-Shot-2013-05-25-at-11.51.39-AM.png"&gt;&lt;img src="http://continuousdelivery.com/wp-content/uploads/2013/05/Screen-Shot-2013-05-25-at-11.51.39-AM.png" alt="Economics of continuous delivery" width="400" class="alignleft size-full wp-image-984"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:38:06 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:videos-from-the-continuous-delivery-track-at-qcon-sf-2012.html</guid></item><item><title>Risk Management Theatre: On Show At An Organization Near You</title><link>http://www.ciandcd.com/risk-management-theatre-on-show-at-an-organization-near-you.html</link><description>&lt;div&gt;&amp;#13;
&amp;#13;
				 &amp;#13;
		 &amp;#13;
		 &amp;#13;
		&amp;#13;
				&amp;#13;
&amp;#13;
		 &amp;#13;
		&amp;#13;
&amp;#13;
		&amp;#13;
		&amp;#13;
				&lt;a href="http://continuousdelivery.com/2013/09/how-we-got-40-female-speakers-at-flowcon/" rel="next"&gt;How To Create A More Diverse Tech Conference&lt;/a&gt; &amp;#160;&lt;a href="http://continuousdelivery.com/2013/05/videos-from-the-continuous-delivery-track-at-qcon-sf-2012/" rel="prev"&gt;Videos from the Continuous Delivery track at QCon SF 2012&lt;/a&gt; &amp;#187;&lt;p class="post-headline"&gt;&lt;h1&gt;Risk Management Theatre: On Show At An Organization Near You&lt;/h1&gt;&lt;/p&gt;				&lt;p&gt;&lt;strong&gt;Translations:&lt;/strong&gt; &lt;a href="http://cdkr.egloos.com/1908527"&gt;&amp;#54620;&amp;#44397;&amp;#47568;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;One of the concepts that will feature in the &lt;a href="http://www.amazon.com/dp/1449368425?tag=contindelive-20"&gt;new book I am working on&lt;/a&gt; is &amp;#8220;risk management theatre&amp;#8221;. This is the name I coined for the commonly-encountered control apparatus, imposed in a top-down way, which makes life painful for the innocent but can be circumvented by the guilty (the name comes by analogy with &lt;a href="http://www.vanityfair.com/culture/features/2011/12/tsa-insanity-201112"&gt;security theatre&lt;/a&gt;.) Risk management theatre is the outcome of optimizing processes for the case that somebody will do something stupid or bad, because (to quote &lt;a href="http://www.amazon.com/dp/0470405163?tag=contindelive-20"&gt;Bjarte Bogsnes talking about management&lt;/a&gt;), &amp;#8220;there might be someone who who cannot be trusted. The strategy seems to be preventative control on everybody instead of damage control on those few.&amp;#8221;&lt;/p&gt;
&lt;p&gt;Unfortunately risk management theatre is everywhere in large organizations, and reflects the continuing dominance of the &lt;a href="http://en.wikipedia.org/wiki/Theory_X_and_Theory_Y"&gt;Theory X&lt;/a&gt; management paradigm. The alternative to the top-down control approach is what I have called adaptive risk management, informed by human-centred management theories (for example the work of &lt;a href="http://www.amazon.com/dp/0071808019?tag=contindelive-20"&gt;Ohno&lt;/a&gt;, &lt;a href="https://www.deming.org/theman/theories/fourteenpoints"&gt;Deming&lt;/a&gt;, Drucker, &lt;a href="http://www.forbes.com/sites/stevedenning/2013/06/28/the-financial-times-flubs-the-management-revolution/"&gt;Denning&lt;/a&gt; and &lt;a href="http://onedublin.org/2012/06/19/stanford-universitys-carol-dweck-on-the-growth-mindset-and-education/"&gt;Dweck&lt;/a&gt;) and the study of how complex systems behave, particularly when they &lt;a href="http://www.amazon.com/dp/1409422216?tag=contindelive-20"&gt;drift into failure&lt;/a&gt;. Adaptive risk management is based on systems thinking, transparency, experimentation, and fast feedback loops.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;Here are some examples of the differences between the two approaches.&lt;/p&gt;


&lt;strong&gt;Adaptive risk management&lt;/strong&gt; (people work to detect problems through improving transparency and feedback, and solve them through improvisation and experimentation)
&lt;strong&gt;Risk management theatre&lt;/strong&gt; (management imposes controls and processes which make life painful for the innocent but can be circumvented by the guilty)


&lt;strong&gt;Continuous code review&lt;/strong&gt; in which engineers ask a colleague to look over their changes before check-in, technical leads review all check-ins made by their team, and code review tools allow people to comment on each others&amp;#8217; work once it is in trunk.
&lt;strong&gt;Mandatory code review&lt;/strong&gt; enforced by check-in gates where a tool requires changes to be signed off by somebody else before they can be merged into trunk. This is inefficient and delays feedback on non-trivial regressions (including performance regressions).


&lt;strong&gt;Fast, automated unit and acceptance tests&lt;/strong&gt; which inform engineers within minutes (for unit tests) or tens of minutes (for acceptance tests) if they have introduced a known regression into trunk, and which can be run on workstations before commit.
&lt;strong&gt;Manual testing&lt;/strong&gt; as a precondition for integration, especially when performed by a different team or in a different location. Like mandatory code review, this delays feedback on the effect of the change on the system as a whole.


&lt;strong&gt;A &lt;a href="http://www.informit.com/articles/article.aspx?p=1621865"&gt;deployment pipeline&lt;/a&gt;&lt;/strong&gt; which provides complete traceability of all changes from check-in to release, and which detects and rejects risky changes automatically through a combination of automated tests and manual validations.
&lt;strong&gt;A comprehensive documentation trail&lt;/strong&gt; so that in the event of a failure we can discover the human error that is the root cause of failures in the mechanistic, Cartesian paradigm that applies in the domain of &lt;a href="http://en.wikipedia.org/wiki/Cynefin"&gt;systems that are not complex&lt;/a&gt;.


&lt;strong&gt;Situational awareness&lt;/strong&gt; created through tools which make it easy to monitor, analyze and correlate relevant data. This includes process, business and systems level metrics as well as the discussion threads around events.
&lt;strong&gt;Segregation of duties&lt;/strong&gt; which acts as a barrier to knowledge sharing, feedback and collaboration, and reduces the situational awareness which is essential to an effective response in the event of an incident.


&lt;p&gt;It&amp;#8217;s important to emphasize that there are circumstances in which the countermeasures on the right are appropriate. If your delivery and operational processes are chaotic and undisciplined, imposing controls can be an effective way to improve &amp;#8211; so long as we understand they are a temporary countermeasure rather than an end in themselves, and provided they are applied with the consent of the people who must work within them.&lt;/p&gt;
&lt;p&gt;Here are some differences between the two approaches in the field of IT:&lt;/p&gt;


Adaptive risk management (people work to detect problems through improving transparency and feedback, and solve them through improvisation and experimentation)
Risk management theatre (management imposes controls and processes which make life painful for the innocent but can be circumvented by the guilty)


&lt;strong&gt;Principle-based and dynamic:&lt;/strong&gt; principles can be applied to situations that were not envisaged when the principles were created.
&lt;strong&gt;Rule-based and static&lt;/strong&gt;: when we encounter new technologies and processes (for example, cloud computing) we need to rewrite the rules.


&lt;strong&gt;Uses transparency to prevent accidents and bad behaviour.&lt;/strong&gt; When it&amp;#8217;s easy for anybody to see what anybody else is doing, people are more careful. As Louis Brandeis said, &amp;#8220;Publicity is justly commended as a remedy for social and industrial diseases. Sunlight is said to be the best of disinfectants; electric light the most efficient policeman.&amp;#8221;
&lt;strong&gt;Uses controls to prevent accidents and bad behaviour.&lt;/strong&gt; This approach is the default for legislators as a way to prove they have taken action in response to a disaster. But controls limit our ability to adapt quickly to unexpected problems. This introduces a new class of risks, for example over-reliance on emergency change processes because the standard change process is too slow and bureaucratic.


&lt;strong&gt;Accepts that systems drift into failure.&lt;/strong&gt; Our systems and the environment are constantly changing, and there will never be sufficient information to make globally rational decisions. Humans solve our problems and we must rely on them to make judgement calls.
&lt;strong&gt;Assumes humans are the problem.&lt;/strong&gt; If people always follow the processes correctly, nothing bad can happen. Controls are put in place to manage &amp;#8220;bad apples&amp;#8221;. Ignores the fact that process specifications always require interpretation and adaptation in reality.


&lt;strong&gt;Rewards people for collaboration, experimentation, and system-level improvements.&lt;/strong&gt; People collaborate to improve system-level metrics such as lead time and time to restore service. No rewards for &amp;#8220;productivity&amp;#8221; on individual or function level. Accepts that locally rational decisions can lead to system level failures.
&lt;strong&gt;Rewards people based on personal &amp;#8220;productivity&amp;#8221; and local optimization&lt;/strong&gt;. For example operations people optimizing for stability at the expense of throughput, or developers optimizing for velocity at the expense of quality (even though these are false dichotomies.)


&lt;strong&gt;Creates a culture of continuous learning and experimentation&lt;/strong&gt;: People openly discuss mistakes to learn from them and conduct &lt;a href="http://codeascraft.com/2012/05/22/blameless-postmortems/"&gt;blameless post-mortems&lt;/a&gt; after outages or customer service problems with the goal of improving the system. People are encouraged to try things out and experiment (with the expectations that many hypotheses will be invalidated) in order to get better.
&lt;strong&gt;Creates a culture of fear and mistrust&lt;/strong&gt;. Encourages finger pointing and lack of ownership for errors, omissions and failure to get things done. As in: If I don&amp;#8217;t do anything unless someone tells me to, I won&amp;#8217;t be held responsible for any resulting failure.


&lt;strong&gt;Failures are a learning opportunity&lt;/strong&gt;. They occur in controlled circumstances, their effects are appropriately mitigated, and they are encouraged as an opportunity to learn how to improve.
&lt;strong&gt;Failures are caused by human error&lt;/strong&gt; (usually a failure to follow some process correctly), and the primary response is to find the person responsible and punish them, and then use further controls and processes as the main strategy to prevent future problems.


&lt;p&gt;Risk management theatre is not just painful and a barrier to the adoption of continuous delivery (and indeed to continuous improvement in general). It is actually dangerous, primarily because it creates a culture of fear and mistrust. As Bogsnes says, &amp;#8220;if the entire management model reeks of mistrust and control mechanisms against unwanted behavior, the result might actually be more, not less, of what we try to prevent. The more people are treated as criminals, the more we risk that they will behave as such.&amp;#8221;&lt;/p&gt;
&lt;p&gt;This kind of organizational culture is a major factor whenever we see people who are scared of losing their jobs, or engage in activities designed to protect themselves in the case that something goes wrong, or attempt to make themselves indispensable through hoarding information.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;m certainly not suggesting that controls, IT governance frameworks, and oversight are bad in and of themselves. Indeed, applied correctly, they are essential for effective risk management. ITIL for example allows for a &lt;a href="http://continuousdelivery.com/2010/11/continuous-delivery-and-itil-change-management/"&gt;lightweight change management&lt;/a&gt; process that is completely compatible with an adaptive approach to risk management. What&amp;#8217;s decisive is how these framework are implemented. The way such frameworks are used and applied is determined by&amp;#8212;and perpetuates&amp;#8212;&lt;a href="http://www.infoq.com/minibooks/agile-adoption-transformation"&gt;organizational culture&lt;/a&gt;.&lt;/p&gt;
&lt;p id="dsq-content"&gt;


             


        &lt;/p&gt;

    &amp;#13;
 &amp;#13;
&amp;#13;
 &amp;#13;
&amp;#13;
 &amp;#13;
 &amp;#13;
 &amp;#13;
&amp;#13;
&lt;/div&gt;&amp;#13;
 &amp;#13;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:38:05 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:risk-management-theatre-on-show-at-an-organization-near-you.html</guid></item><item><title>How To Create A More Diverse Tech Conference</title><link>http://www.ciandcd.com/how-to-create-a-more-diverse-tech-conference.html</link><description>&lt;div&gt;&lt;p class="post-headline"&gt;&lt;h1&gt;How To Create A More Diverse Tech Conference&lt;/h1&gt;&lt;/p&gt;&lt;p&gt;I have been advised by people I trust that it&amp;#8217;s not a good idea to talk about how you got serious female representation at your conference until after it&amp;#8217;s over. However the shameful RubyConf &lt;a href="https://twitter.com/shanley/status/380179040545406976"&gt;&amp;#8220;binders full of men&amp;#8221;&lt;/a&gt; debacle and the Neanderthal level of discussion around it has wound me up enough to write this account somewhat prematurely. So here is how we achieved &amp;gt;40% female representation on our &lt;a href="http://flowcon.org/flowcon-sanfran-2013/speakers/"&gt;speaker roster&lt;/a&gt; at &lt;a href="http://flowcon.org/"&gt;FlowCon&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 0. Care About The Outcome.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When John Esser approached me to put together a conference about continuous delivery, devops and lean product development, I thought carefully about it. I&amp;#8217;ve helped put together a conference program before (&lt;a href="http://qconsf.com/sf2012/sf2012/"&gt;QCon SF 2012&lt;/a&gt;), and that was pretty hard work, so I wanted to be sure I had the correct motivation.&lt;/p&gt;
&lt;p&gt;One of the things that I have always disliked about tech conferences is being surrounded by a bunch of other straight white guys (nothing personal, some of my best friends are straight white guys). It&amp;#8217;s a constant reminder of the fact that, due to a number of socioeconomic factors, &lt;a href="http://whatever.scalzi.com/2012/05/15/straight-white-male-the-lowest-difficulty-setting-there-is/"&gt;straight white guys have it easier than others&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I wanted to put together a conference which reflects my community as I would like it to look, not as it actually looks. So one of the &lt;a href="http://flowcon.org/"&gt;four values&lt;/a&gt; the FlowCon program committee came up with was this: &amp;#8220;Diversity: We believe the technology community &amp;#8211; and thus the conference speakers and participants &amp;#8211; should reflect the demographics of our customers and the wider world.&amp;#8221;&lt;/p&gt;
&lt;p&gt;There are two reasons for this. Firstly, we can&amp;#8217;t effectively change the world through technology without diversity. To find out why, come and see &lt;a href="http://ashedryden.com/"&gt;Ashe Dryden&lt;/a&gt; talk about how &lt;a href="http://flowcon.org/flowcon-sanfran-2013/presentation/Programming%20Diversity"&gt;&amp;#8220;diverse communities and workplaces create better products&amp;#8221;&lt;/a&gt;. Second, one of the main reasons I like working at &lt;a href="http://www.thoughtworks.com/"&gt;ThoughtWorks&lt;/a&gt; is that one of &lt;a href="http://www.thoughtworks.com/about-us"&gt;the three pillars of our mission&lt;/a&gt; is to &amp;#8220;advocate passionately for social and economic justice.&amp;#8221; The fact there are so few women in IT reflects social and economic injustice inherent in our world.&lt;/p&gt;
&lt;p&gt;Making sure you actually have a mission for your conference is something I learned from helping out with &lt;a href="qconsf.com"&gt;QCon SF&lt;/a&gt;. It is a constant reminder of why you&amp;#8217;re doing it and what&amp;#8217;s important about it. If you don&amp;#8217;t have a mission, you&amp;#8217;re at the mercy of the implicit biases of the organizers. As RubyConf shows, you can&amp;#8217;t just throw in the &lt;a href="https://twitter.com/shanley/status/380186471174393856"&gt;&amp;#8220;one weird trick&amp;#8221;&lt;/a&gt; of anonymous submissions and expect that it will somehow solve the problem. Everybody on the program committee actually has to care about the outcome, or they won&amp;#8217;t put in the right amount of work to make it happen.&lt;/p&gt;
&lt;p&gt;Once you do that, the rest of the steps aren&amp;#8217;t that hard.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 1. Make Sure Your Program Committee Is Aligned With Your Mission&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Once I had an idea about the mission of the conference, I reached out to some people whom I thought would share it. I was lucky enough that &lt;a href="http://flowcon.org/flowcon-sanfran-2013/speaker/Elisabeth+Hendrickson"&gt;Elizabeth Hendrickson&lt;/a&gt;, &lt;a href="http://flowcon.org/flowcon-sanfran-2013/speaker/Lane+Halley"&gt;Lane Halley&lt;/a&gt; and &lt;a href="http://flowcon.org/flowcon-sanfran-2013/speaker/Gene+Kim"&gt;Gene Kim&lt;/a&gt; agreed to join &lt;a href="http://flowcon.org/flowcon-sanfran-2013/speaker/John+Esser"&gt;John Esser&lt;/a&gt; and me on the program committee.&lt;/p&gt;
&lt;p&gt;One of the main reasons I asked those particular people, apart from being extremely competent and well-respected in their field, was another conference goal: &amp;#8220;Spanning boundaries: We believe that the best products are created collaboratively by people with a range of skills and experiences.&amp;#8221; The program committee has representation from the UX, testing, operations, product development and programming communities.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 2. Make Sure Your Invited Speakers Are Aligned With Your Mission.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We made the decision to have about half the program be invited speakers. Part of that was about ensuring that we had a solid core program. But it was also a chance for us to put our mission into practice, so that when we put out the call for proposals we had a bunch of confirmed speakers who demonstrated we were serious about our mission.&lt;/p&gt;
&lt;p&gt;Thus we made sure that the invited speakers were respected boundary spanners, and that 50% of them were women. This involved more work than we would have had to put in had we just invited our friends (a popular strategy for program committees). It was also telling that we got more refusals from women than we got from men due to schedule conflicts. The main factor here was that female speakers are actually in greater demand than men because there are relatively fewer of them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Step 3. The Anonymous Call For Proposals&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If you jump straight to step 3, it&amp;#8217;s likely you will suffer the fate of RubyConf and fail. If you use this as your only strategy for increasing representation it won&amp;#8217;t work. This strategy has been &lt;a href="http://geekfeminism.org/2012/05/21/how-i-got-50-women-speakers-at-my-tech-conference/"&gt;thoroughly&lt;/a&gt; &lt;a href="http://2012.jsconf.eu/2012/09/17/beating-the-odds-how-we-got-25-percent-women-speakers.html"&gt;discussed&lt;/a&gt; by &lt;a href="http://www.startuplessonslearned.com/2012/11/solving-pipeline-problem.html"&gt;others&lt;/a&gt; who have used this approach as part of increasing diversity at their conference.&lt;/p&gt;
&lt;p&gt;We created a form in Google Docs for people to propose talks. They had to enter their email address, but we mentioned in the form that they should use one that didn&amp;#8217;t identify them if they wanted their proposal to be more anonymous. Of the 82 people who submitted a talk proposal, 18 (21%) were women as far as we can work out (once the program was confirmed I used &lt;a href="http://rapportive.com/"&gt;Rapportive&lt;/a&gt; to reverse-engineer email addresses based on publicly available information). Ultimately, three of the eight people who made it into the final program based on submitted proposals were women.&lt;/p&gt;
&lt;p&gt;The low female representation through the CFP is the reason our program isn&amp;#8217;t 50% female. Even getting the 21% of submissions that we did involved reaching out through mailing lists, Twitter, and our networks to encourage women to submit. This step, along with making it clear that you actually care, is essential if you in fact expect women to submit through the anonymous CFP.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Observations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These four steps resulted in &lt;a href="http://flowcon.org/flowcon-sanfran-2013/speakers/"&gt;10 of our 24 speakers being women&lt;/a&gt;. I have three main observations coming out of this process:&lt;/p&gt;
&lt;p&gt;First, unlike &lt;a href="https://www.usenix.org/blog/my-daughters-high-school-programming-teacher"&gt;increasing the number of women who take programming classes in school&lt;/a&gt; or enter the IT industry and don&amp;#8217;t immediately quit in horror, creating a conference with reasonable female representation is not actually a hard problem. Yes, we put in more work to achieve this goal than we would have had we not cared. But it wasn&amp;#8217;t significantly more.&lt;/p&gt;
&lt;p&gt;Conference organizers who claim to care but fail to achieve good representation should quit whining and take real steps to achieve this goal. The community should hold them to higher standards. If the conference speakers are a bunch of straight white guys, the only reason is that the organizers didn&amp;#8217;t care enough.&lt;/p&gt;
&lt;p&gt;Second, in the wake of RubyConf, I have been angered but unsurprised to observe the usual chorus about how increasing representation somehow means lowering standards. Not only is this incredibly insulting to the many extraordinary women working in our industry, but it is just false. I dare anyone to look at &lt;a href="http://flowcon.org/flowcon-sanfran-2013/schedule/index.jsp"&gt;the kick-ass program&lt;/a&gt; we have put together for FlowCon and try and claim that we have somehow lowered standards to achieve great a barely acceptable level of representation.&lt;/p&gt;
&lt;p&gt;Another thing you will hear is that it is harder to find female speakers on &amp;#8220;hard&amp;#8221; topics such as programming than for &amp;#8220;soft&amp;#8221; ones. I find this claim baffling because in my experience changing organizational culture (considered a &amp;#8220;soft&amp;#8221; topic) is, in my experience, way way harder than knocking out lines of code (even well-factored unit-tested ones). But you&amp;#8217;ll see on our program that women are covering the whole gamut from &lt;a href="http://flowcon.org/flowcon-sanfran-2013/presentation/Organizational%20Change%20Myths%20and%20Patterns%20for%20Evangelists"&gt;organizational change&lt;/a&gt; to &lt;a href="http://flowcon.org/flowcon-sanfran-2013/presentation/Therapeutic%20Refactoring"&gt;refactoring&lt;/a&gt; to &lt;a href="http://flowcon.org/flowcon-sanfran-2013/presentation/Configuration%20Management:%20Stability%20in%20Your%20Pipeline"&gt;configuration management&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Third, it&amp;#8217;s not all good news. In particular, we have only one non-white speaker. I&amp;#8217;ll hold my hand up on this &amp;#8211; we didn&amp;#8217;t explicitly set non-white representation as a goal within the program committee, and by the time it became obvious it was a problem (Step 3) it was too late to do anything. This demonstrates why steps 0-2 are important. If we run FlowCon again, we will do better.&lt;/p&gt;
&lt;p&gt;Meanwhile &lt;a href="http://flowcon.org/flowcon-sanfran-2013/schedule/index.jsp"&gt;check out the program&lt;/a&gt;, and &lt;a href="https://secure.trifork.com/flowcon-sanfran-2013/registration/registration.jsp?promotionCode=humb50"&gt;follow this link&lt;/a&gt; to register with a 10% discount. If you need more than a one day conference to come to San Francisco, &lt;a href="http://lanyrd.com/2013/balancedteam/"&gt;Balanced Team are running their conference&lt;/a&gt; the following two days.&lt;/p&gt;

&lt;p&gt;&lt;img src="http://engl165gs.files.wordpress.com/2013/05/colorblind-thought.jpg" width="240"&gt; &lt;strong&gt;End notes&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Another popular &lt;a href="http://geekfeminism.wikia.com/wiki/Silencing"&gt;silencing tactic&lt;/a&gt; in this discussion is that bringing attention to the level of diversity in a conference is in itself a form of sexism or racism. There&amp;#8217;s a cartoon on the left which expresses nicely why this is in fact horribly misguided (or you could check out &lt;a href="http://www.sociologyinfocus.com/2012/01/30/im-not-racist-im-colorblind/"&gt;one of the many excellent articles on &amp;#8220;colourblindness&amp;#8221; and racism&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Check out the Geek Feminism &lt;a href="http://geekfeminism.org/"&gt;blog&lt;/a&gt; and &lt;a href="http://geekfeminism.wikia.com"&gt;wiki&lt;/a&gt; for tons of useful information and advice on making things better for women in tech. Also check out the &lt;a href="https://twitter.com/CallbackWomen"&gt;@CallbackWomen&lt;/a&gt; and &lt;a href="https://twitter.com/DevChix"&gt;@DevChix&lt;/a&gt; Twitter accounts to spread the word for your CFP. Ashe Dryden also wrote &lt;a href="http://ashedryden.com/blog/increasing-diversity-at-your-conference"&gt;an excellent post&lt;/a&gt; on creating more diverse conferences.&lt;/p&gt;
&lt;p&gt;Another important factor when designing a woman-friendly conference is to create an &lt;a href="http://gotocon.com/flowcon-sanfran-2013/anti-harassment-policy/"&gt;anti-harassment&lt;/a&gt; policy. Check out &lt;a href="http://www.maryrobinettekowal.com/journal/on-sexual-harassment-at-conventions-elise-matheson-speaks-out/"&gt;this account&lt;/a&gt; of a woman who actually needed to use the anti-harassment policy (trigger alert).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;UPDATE&lt;/strong&gt; Of course, this entry is now starting to receive the attention of anonymous trolls. I&amp;#8217;ve left the first one as an example of the idiocy that passes for dialogue in this debate (and from supposedly smart people at that). But forthwith I&amp;#8217;ll be deleting anonymous or otherwise uncivil posts.&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:38:04 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:how-to-create-a-more-diverse-tech-conference.html</guid></item><item><title>FlowCon 2013 Wrap-Up, With Some Hard Data on Gender Diversity in Tech Conferences.</title><link>http://www.ciandcd.com/flowcon-2013-wrap-up-with-some-hard-data-on-gender-diversity-in-tech-conferences.html</link><description>&lt;div&gt;&lt;p class="post-headline"&gt;&lt;h1&gt;FlowCon 2013 Wrap-Up, With Some Hard Data on Gender Diversity in Tech Conferences.&lt;/h1&gt;&lt;/p&gt;&lt;p&gt;Thanks to all of you who came along to FlowCon! If you weren&amp;#8217;t able to make it, you can &lt;a href="http://www.youtube.com/channel/UCMk1sRo1hnTLMA3kpn6BVKg"&gt;watch the videos for free&lt;/a&gt; thanks to &lt;a href="https://communities.bmc.com/community/bsm_initiatives/devops/blog"&gt;BMC&lt;/a&gt;&lt;a&gt;&lt;/a&gt; and &lt;a href="http://www.thoughtworks.com/studios"&gt;ThoughtWorks Studios&lt;/a&gt;. The &lt;a href="http://flowcon.org/flowcon-sanfran-2013/schedule/index.jsp"&gt;slides are also available&lt;/a&gt; for downloading.&lt;/p&gt;
&lt;p&gt;Let me first express my thanks to our producers: Geeta Schmidt and Niley Barros of &lt;a href="http://www.trifork.com/"&gt;Trifork&lt;/a&gt; and Rebecca Phillips of &lt;a href="http://www.thoughtworks.com/studios"&gt;ThoughtWorks Studios&lt;/a&gt;. I also want to thank my fellow PC members &lt;a href="https://twitter.com/thinknow"&gt;Lane Halley&lt;/a&gt;, &lt;a href="https://twitter.com/testobsessed"&gt;Elisabeth Hendrickson&lt;/a&gt;, &lt;a href="https://twitter.com/RealGeneKim"&gt;Gene Kim&lt;/a&gt; and &lt;a href="https://twitter.com/johndesser"&gt;John Esser&lt;/a&gt;; our fabulous &lt;a href="flowcon.org/flowcon-sanfran-2013/speakers/"&gt;speakers&lt;/a&gt;; our generous &lt;a href="http://flowcon.org/flowcon-sanfran-2013/sponsors/"&gt;sponsors&lt;/a&gt;; and everyone who came along.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h3&gt;The Program&lt;/h3&gt;
&lt;p&gt;The goal of the program committee was to create a conference that represents our industry as we want it to look, not as it is right now. That&amp;#8217;s an ambitious goal that involves changing the way we think about everything from &lt;a href="http://www.youtube.com/watch?v=Nc587c76Syg"&gt;leadership&lt;/a&gt; and &lt;a href="http://www.youtube.com/watch?v=PijCUJDb_hc"&gt;governance&lt;/a&gt; through &lt;a href="http://www.youtube.com/watch?v=5JF_QMIMBls"&gt;product&lt;/a&gt; &lt;a href="http://www.youtube.com/watch?v=vhFcux5UO4A"&gt;development&lt;/a&gt; and &lt;a href="http://www.youtube.com/watch?v=Socg4SIzAB4"&gt;design&lt;/a&gt;, to &lt;a href="http://www.youtube.com/watch?v=6jmZkV23TEs"&gt;IT&lt;/a&gt; &lt;a href="http://www.youtube.com/watch?v=7779Wrun5fo"&gt;operations&lt;/a&gt;. Not only did our speakers cover all these topics; they also provided real examples of how these changes, along with the cultural changes necessary to support them, have been achieved at enterprise scale.&lt;/p&gt;
&lt;p&gt;Thus we attacked one of the main objections we hear time and time again &amp;#8212; &amp;#8220;that sounds great, but it couldn&amp;#8217;t work here&amp;#8221;. Part of our vision was to provide a platform for people to speak about gnarly, real-life examples that demonstrate that, with sufficient hard work and ingenuity, ideas like continuous delivery, devops, and lean product development can provide significant competitive advantage through higher quality, cost savings, and happier customers, even in traditionally slow-moving and highly-regulated industries with large, complex, heterogeneous systems.&lt;/p&gt;
&lt;p&gt;Two talks that I am particularly happy to have on record are &lt;a href="http://www.youtube.com/watch?v=Trqjj3d3lhQ"&gt;Gary Gruver&amp;#8217;s talk&lt;/a&gt; on doing continuous delivery for printer firmware at HP, and &lt;a href="http://www.youtube.com/watch?v=eMS97X5ZTGc"&gt;John Kordyback&amp;#8217;s talk&lt;/a&gt; on doing continuous delivery with mainframes in the financial services industry. Alternatively, if you want a vision of the state of the art of continuous delivery, it would be hard to beat &lt;a href="http://www.youtube.com/watch?v=wyWI3gLpB8o"&gt;Adrian Cockcroft&amp;#8217;s opening keynote&lt;/a&gt; (the most highly rated talk of the conference) on how Netflix approach building and running systems.&lt;/p&gt;
&lt;p&gt;Overall, both the individual quality of the talks and the vision they present in concert was incredibly inspiring. Gene Kim comments, &amp;#8220;The FlowCon program was amazing. In my mind, what was presented at FlowCon is what every IT practitioner will be required to know in 10 years time.&amp;#8221; Thank you again to all of our speakers.&lt;/p&gt;
&lt;h3&gt;Data on Gender Diversity&lt;/h3&gt;
&lt;p&gt;Part of representing the industry as we want it to look is changing its composition. Thus another personal goal for me was to gather data to support my hypothesis that &lt;a href="http://continuousdelivery.com/2013/09/how-we-got-40-female-speakers-at-flowcon/"&gt;taking steps to increase diversity&lt;/a&gt; at conferences doesn&amp;#8217;t mean reducing quality. FlowCon, like the excellent &lt;a href="http://gotocon.com/"&gt;GOTO conferences&lt;/a&gt; that Trifork produces, records feedback from participants. Everybody leaving a session can give feedback on whether they thought the talk was good, mediocre or poor by tapping a red, amber or green rectangle on an iPhone on their way out. We then calculate overall satisfaction as follows: satisfaction = (green votes) / (total votes).&lt;/p&gt;
&lt;p&gt;When we got back all the data, the first thing I did is look at the average (mean) satisfaction for male speakers versus female speakers. It turns out that in both cases the average is between 71% and 72%. First of all, this demonstrates that there was no statistically significant difference in satisfaction between male and female speakers. This is important because it means our steps to increase diversity &amp;#8212; including reaching out to a wide network to ensure that 50% of our invited speakers were women &amp;#8212; didn&amp;#8217;t &amp;#8220;lower the bar&amp;#8221;.&lt;/p&gt;
&lt;p&gt;There is also a deeper implication: any claim that the all-white-male conference programs that are so depressingly common in the tech industry are the result of some meritocratic process is BS. They are, rather, the result of not putting in enough effort to seek out high quality speakers from &lt;a href="http://martinfowler.com/bliki/HistoricallyDiscriminatedAgainst.html"&gt;historically discriminated against&lt;/a&gt; groups.&lt;/p&gt;
&lt;p&gt;If our industry were truly meritocratic, the speaker line-up and attendees would resemble the wider population, because we know that there is &lt;a href="http://martinfowler.com/bliki/DiversityImbalance.html"&gt;no biological explanation&lt;/a&gt; for the overwhelming proportion of white dudes in our industry. So let&amp;#8217;s not fool ourselves any more with claims that taking steps to improve diversity is &amp;#8220;reverse discrimination&amp;#8221;. Any time we don&amp;#8217;t take concrete, systematic steps forward we are silently complicit in perpetuating the status quo &amp;#8212; which is why it&amp;#8217;s not good enough when leaders in the tech community ignore the problem. If you ignore the problem, you&amp;#8217;re part of the problem.&lt;/p&gt;
&lt;p&gt;Finally, I want to emphasize that what the program committee achieved was not very hard, once we spent some time thinking the problem through, and also that it was insufficient. We had a reasonable level of gender diversity, but the speakers were still overwhelmingly white. I don&amp;#8217;t have data for the diversity of our audience, but based on observation, there were more white guys than I would see if I walked out of the door onto the streets (and this is in San Francisco, which is far from being representative of the wider population).&lt;/p&gt;
&lt;p&gt;If you want to educate yourself further on these issues, I suggest watching &lt;a href="http://www.youtube.com/watch?v=VafA2stfTUM"&gt;Ashe Dryden&amp;#8217;s talk&lt;/a&gt; on programming diversity. And if you&amp;#8217;d like to become more effective at creating change, check out &lt;a href="http://www.youtube.com/watch?v=PJWKvkfbPo0"&gt;Linda Rising&amp;#8217;s closing keynote&lt;/a&gt;. Here&amp;#8217;s to &lt;a href="http://geekfeminism.wikia.com/wiki/Resources_for_allies"&gt;taking small steps every day&lt;/a&gt; to make 2014 a marginally, incrementally, &lt;a href="http://geekfeminism.wikia.com/wiki/Timeline_of_incidents#2013"&gt;better year than 2013&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:38:02 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:flowcon-2013-wrap-up-with-some-hard-data-on-gender-diversity-in-tech-conferences.html</guid></item><item><title>The Science Behind the 2013 Puppet Labs DevOps Survey Of Practice</title><link>http://www.ciandcd.com/the-science-behind-the-2013-puppet-labs-devops-survey-of-practice.html</link><description>&lt;div&gt;&lt;p class="post-headline"&gt;&lt;h1&gt;The Science Behind the 2013 Puppet Labs DevOps Survey Of Practice&lt;/h1&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;By &lt;a href="https://twitter.com/RealGeneKim"&gt;Gene Kim&lt;/a&gt; and Jez Humble&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Last year, we both had the privilege of working with Puppet Labs to develop the 2012 DevOps Survey Of Practice. It was especially exciting for Gene, because we were able to benchmark the performance of over 4000 IT organizations, and to gain an understanding what behaviors result in their incredible performance. This continues research that he has been doing of high performing IT organizations that started for him in 1999.&lt;/p&gt;
&lt;p&gt;In this blog post, Gene Kim and I will discuss the research hypotheses that we&amp;#8217;re setting out to test in the 2013 DevOps Survey Of Practice, explain the mechanics of how these types of cross-population studies actually work (so you help this research effort or even start your own), then describe the key findings that came out of the 2012 study.&lt;/p&gt;
&lt;p&gt;But first off, if you&amp;#8217;re even remotely interested in DevOps, &lt;a href="http://www.surveygizmo.com/s3/1483785/DevOps-Survey-2013"&gt;go take the 2013 Puppet Labs DevOps Survey here&lt;/a&gt;! The survey closes on January 15, 2014, so hurry! It only takes about ten minutes.&lt;/p&gt;
&lt;p&gt;&lt;/p&gt;
&lt;h3 id="2013-devops-survey-research-goals"&gt;2013 DevOps Survey Research Goals&lt;/h3&gt;
&lt;p&gt;Last year&amp;#8217;s study (which we&amp;#8217;ll describe in more detail below) found that high performing organizations that were employing DevOps practices were massively outperforming their peers: they were doing 30x more frequent code deploys, and had deployment lead times measured in minutes or hours (versus lower performers, who required weeks, months or quarters to complete their deployments).&lt;/p&gt;
&lt;p&gt;The high performers also had far better deployment outcomes: their changes and deployments had twice the change success rates, and when the changes failed, they could restore service 12x faster.&lt;/p&gt;
&lt;p&gt;The goal of the 2013 study is to gain a better understanding of exactly what practices are required to achieve this high performance. Our hypothesis is that the following are required, and we&amp;#8217;ll be looking to independently evaluate the effect of each of these practices on performance:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;small teams with high trust that span the entire value stream: Dev, QA, IT Operations and Infosec&lt;/li&gt;
&lt;li&gt;shared goals and shared pain that span the entire value stream&lt;/li&gt;
&lt;li&gt;small development batch sizes&lt;/li&gt;
&lt;li&gt;presence of continuous, automated integration and testing&lt;/li&gt;
&lt;li&gt;emphasis on creating a culture of learning, experimentation and innovation&lt;/li&gt;
&lt;li&gt;emphasis on creating resilient systems&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We are also testing two other hypotheses that one of us (Gene) is especially excited about, because it&amp;#8217;s something he&amp;#8217;s wanted to do ever since 1999!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lead time&lt;/strong&gt;: In plant manufacturing, lead time is the time required to turn raw materials into finished goods. There is a deeply held belief in the Lean community that lead time is the single best predictor of quality, customer satisfaction and employee happiness. We are testing this hypothesis for the DevOps value stream in the 2013 survey instrument.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Organizational performance&lt;/strong&gt;: Last year, we confirmed that DevOps practices correlate with substantially improved IT performance (e.g., deploy frequencies, lead times, change success rates, MTTR). This year, we will be testing whether improved IT performance correlates with improved business performance. In this year&amp;#8217;s study, we&amp;#8217;ve added inserted three questions that are known to correlate with organizational performance, which is known to correlate with business performance (e.g., competitiveness in the marketplace, return on assets, etc.).&lt;/p&gt;
&lt;p&gt;Our dream headline would be, &amp;#8220;high performing organizations not only do 30x more frequent code deployments than their peers, but they also outperform the S&amp;amp;P 500 by 3x as measured by shareholder return and return on assets.&amp;#8221;&lt;/p&gt;
&lt;p&gt;Obviously, there are many other variables that contribute to business performance besides Dev and Ops performance (e.g., profitability, market segment, market share, etc.). However, in our minds, the reliance upon IT performance is obvious: as Chris Little said, &amp;#8220;Every organization is an IT business, regardless of what business they think they&amp;#8217;re in.&amp;#8221;&lt;/p&gt;
&lt;p&gt;When IT does poorly, the business will do poorly. And when IT helps the organization win, those organizations will out-perform their competitors in the marketplace.&lt;/p&gt;
&lt;p&gt;(This hypothesis forms the basis of the hedge fund that Erik wants to create in the last chapter of &lt;a href="http://www.amazon.com/The-Phoenix-Project-Business-ebook/dp/B00AZRBLHO/"&gt;&amp;#8220;The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win&amp;#8221;&lt;/a&gt;, where they would make long or short bets, based on the known operating characteristics of the IT organization.)&lt;/p&gt;
&lt;h3 id="the-theory-behind-cross-population-studies-and-survey-instruments"&gt;The Theory Behind Cross-Population Studies and Survey Instruments&lt;/h3&gt;
&lt;p&gt;Like last year, this year&amp;#8217;s DevOps survey is a cross-population study, designed to explore the link between organizational performance and organizational practices and cultural norms.&lt;/p&gt;
&lt;p&gt;What is a cross-population study? It&amp;#8217;s a statistical research technique designed to uncover what factors (e.g., practices, cultural norms, etc.) correlate with outcomes (e.g., IT performance). Cross-population studies are often used in medical research to answer questions like, &amp;#8220;is cigarette smoking a significant factor in early mortality?&amp;#8221;&lt;/p&gt;
&lt;p&gt;Properly designed cross-population studies are considered a much more rigorous approach of testing efficacy of what practices work than say, interviewing people about what they think worked, ROI stories from vendors, or collecting &amp;#8220;known, best practices.&amp;#8221;&lt;/p&gt;
&lt;p&gt;&lt;img class="alignnone" alt="" src="https://draftin.com:443/images/6464?token=JXI7N7uDAMybn5lduEANKdQhwmXNNkJoBYsb2fE4jeFS9KsliXjnALaa33oHlCOP0_nW8tviUZ8zYBdwzPIOPtQ" width="426" height="289"&gt;&lt;/p&gt;
&lt;p&gt;When doing survey design, we might state our hypotheses in the following form: &amp;#8220;we believe that IT organizations which have high trust have higher IT performance.&amp;#8221; In other words, the higher the trust levels in the IT organization, the higher the performance.&lt;/p&gt;
&lt;p&gt;We then put this question in the survey instrument, and then analyze the results. If we were to plot the results on a graph, we would put the dependent variable (i.e., performance) on the Y-axis, and the independent variable (i.e., presence of high trust) on the X-axis.&lt;/p&gt;
&lt;p&gt;We would then test to see if there is a correlation between the two. Shown below is an example of what it looks like when the two variables have low or no correlation, and one that has a significant positive correlation.&lt;/p&gt;
&lt;p&gt;&lt;img class="alignnone" alt="" src="https://draftin.com:443/images/6465?token=EVRyxLU4nXU9O19I5TKk_64PCVLbwzGFy9P3kEQKdMUo4Ceg6Yn-cC5jFw2_N9IQwEPkrSUBLmUA_cJKWm5YP3g" width="463" height="162"&gt;&lt;/p&gt;
&lt;p&gt;If we were to find a significant correlation, such as displayed on the right, we could then assert that &amp;#8220;the higher your organization&amp;#8217;s trust levels, in general, the higher your IT performance.&amp;#8221;&lt;/p&gt;
&lt;p&gt;(Graph adapted from &lt;a href="http://en.wikipedia.org/wiki/Correlation_and_dependence"&gt;Wikipedia entry on Correlation and Dependence&lt;/a&gt;.)&lt;/p&gt;
&lt;h3 id="key-2012-devops-survey-findings"&gt;The 2012 DevOps Survey&lt;/h3&gt;
&lt;p&gt;In this section, we will describe the the key findings that came out of the 2012 DevOps Survey, as well as a brief discussion of the research hypotheses that went into the survey design.&lt;/p&gt;
&lt;p&gt;In the DevOps community, we have long asserted that certain practices enables organizations simultaneously deliver fast flow of features to market, while providing world-class stability, reliability and security.&lt;/p&gt;
&lt;p&gt;We designed the survey to validate this, and tested a series of technical practices to determine which of them correlated with high performance.&lt;/p&gt;
&lt;p&gt;The survey ran for 30 days, and we had 4,039 completed respondents. (This is an astonishingly high number, by the way. When Kurt Milne and Gene Kim did similar studies in 2006, each study typically required $200K to do the survey design, gather responses from a couple hundred people, and then perform survey analysis.)&lt;/p&gt;
&lt;p&gt;You can find the &lt;a href="http://www.slideshare.net/realgenekim/2013-velocity-devops-metrics-its-not-just-for-webops-any-more"&gt;slides that Gene Kim, Jez Humble and James Turnbull presented at the 2013 Velocity Conference here&lt;/a&gt;, and the &lt;a href="https://puppetlabs.com/2013-state-of-devops-infographic"&gt;full Puppet Labs infographics and results here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&amp;#160;&lt;/p&gt;
&lt;p&gt;The first surprise was how much the high performing organizations were outperforming their non-high-performing peers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Agility metrics&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;30x more frequent code deployments&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;8,000x faster lead time than their peers&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Reliability metrics&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;2x the change success rate&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;12x faster MTTR&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In other words, they were more agile: they were deploying code 30x more frequently, and the lead time required to go from &amp;#8220;code committed&amp;#8221; to &amp;#8220;successfully running in production&amp;#8221; was completed 8,000x faster &amp;#8212; high performers had lead times measured in minutes or hours, while lower performers had lead times measured in weeks, months or even quarters.&lt;/p&gt;
&lt;p&gt;Not only were the high performers doing more work, but they had far better outcomes: when the high performers deployed changes and code, they were twice as likely to be completed successfully (i.e., without causing a production outage or service impairment), and when the change failed and resulted in an incident, the time required to resolve the incident was 12x faster.&lt;/p&gt;
&lt;p&gt;We were astonished and delighted with this finding, as it showed not only that it was possible to break the core, chronic conflict, but that it seemed to confirm that just as in manufacturing, agility and reliability go hand in hand. In other words, lead time correlates with both both agility and reliability.&lt;/p&gt;
&lt;p&gt;(Gene will write more on his personal interpretations of the 2012 DevOps Survey Of Practice in a future post.)&lt;/p&gt;
&lt;h3 id="conclusion"&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;We hope this gives you a good idea of why we&amp;#8217;ve worked so hard on the 2012 and 2013 DevOps Survey, as well as how to conduct your own cross-population studies. Please let us know if you have any questions or if there&amp;#8217;s anything we can do for you.&lt;/p&gt;
&lt;p&gt;And of course, help us understand what in DevOps and Continuous Delivery work by &lt;a href="http://www.surveygizmo.com/s3/1483785/DevOps-Survey-2013"&gt;taking 10 minutes to participate in the 2013 Puppet Labs DevOps Survey here by January 15, 2014&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;Thank you! &amp;#8211;Gene Kim and Jez Humble&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:38:01 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:the-science-behind-the-2013-puppet-labs-devops-survey-of-practice.html</guid></item><item><title>Visualizations of Continuous Delivery</title><link>http://www.ciandcd.com/visualizations-of-continuous-delivery.html</link><description>&lt;div&gt;&lt;p class="post-headline"&gt;&lt;h1&gt;Visualizations of Continuous Delivery&lt;/h1&gt;&lt;/p&gt;&lt;p&gt;&lt;a href="http://www.linkedin.com/pub/nhan-ngo/11/938/ba5"&gt;Nhan Ngo&lt;/a&gt;, a QA engineer at &lt;a href="http://spotify.com"&gt;Spotify&lt;/a&gt;, made four fabulous visualizations while reading &lt;a href="http://www.amazon.com/dp/0321601912?tag=contindelive-20"&gt;Continuous Delivery&lt;/a&gt;. She has very kindly agreed to make them available under a Creative Commons license so feel free to share them, download them, and print them out (click to get a higher resolution version). Thank you Nhan!&lt;/p&gt;
&lt;p&gt;&lt;a href="http://continuousdelivery.com/wp-content/uploads/2014/02/01_CD_the_idea_low-res.jpg"&gt;&lt;img src="http://continuousdelivery.com/wp-content/uploads/2014/02/01_CD_the_idea_low-res.jpg" alt="01_CD_the_idea_low-res" width="550" class="alignleft wp-image-1155"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://continuousdelivery.com/wp-content/uploads/2014/02/02_CD_test_strategy_low-res.jpg"&gt;&lt;img src="http://continuousdelivery.com/wp-content/uploads/2014/02/02_CD_test_strategy_low-res.jpg" alt="02_CD_test_strategy_low-res" width="550" class="alignleft wp-image-1156"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://continuousdelivery.com/wp-content/uploads/2014/02/03_CD_automated_acceptance_test_low-res.jpg"&gt;&lt;img src="http://continuousdelivery.com/wp-content/uploads/2014/02/03_CD_automated_acceptance_test_low-res.jpg" alt="03_CD_automated_acceptance_test_low-res" width="550" class="alignleft wp-image-1157"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://continuousdelivery.com/wp-content/uploads/2014/02/04_CD_managing_data_low-res.jpg"&gt;&lt;img src="http://continuousdelivery.com/wp-content/uploads/2014/02/04_CD_managing_data_low-res.jpg" alt="04_CD_managing_data_low-res" width="550" class="alignleft wp-image-1158"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:37:59 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:visualizations-of-continuous-delivery.html</guid></item><item><title>The 2014 State of DevOps Report Is Here!</title><link>http://www.ciandcd.com/the-2014-state-of-devops-report-is-here.html</link><description>&lt;div&gt;&lt;p class="post-headline"&gt;&lt;h1&gt;The 2014 State of DevOps Report Is Here!&lt;/h1&gt;&lt;/p&gt;&lt;p&gt;DevOps, a movement of people who care about developing and operating reliable, secure, high performance systems at scale, has always &amp;#8212; intentionally &amp;#8212; lacked a definition or manifesto. However (and this is fascinating in its own right) that doesn&amp;#8217;t mean that we can&amp;#8217;t measure the impact of DevOps, or how good people are at doing it. The proof of this, and also of the startling impact of the DevOps movement, is now available in the form of the 2014 State of DevOps report (which you can &lt;a href="http://puppetlabs.com/2014-devops-report"&gt;download for free&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;The report, a collaboration between &lt;a href="http://nicolefv.com/"&gt;Nicole Forsgren Velasquez&lt;/a&gt;, &lt;a href="https://twitter.com/RealGeneKim"&gt;Gene Kim&lt;/a&gt;, &lt;a href="https://puppetlabs.com/"&gt;Puppet Labs&lt;/a&gt;, and &lt;a href="https://twitter.com/jezhumble"&gt;yours truly&lt;/a&gt;, surveyed over 9,200 people worldwide, covering a wide range of industries and types of organization. Our goal for the report was ambitious. We set out to measure IT performance, business performance, the impact of particular practices (such as continuous integration, test automation, and version control), and also culture, and then to discover to what extent they influenced each other. How, you might ask, do you measure these things like culture and organizational performance? Following Douglas Hubbard&amp;#8217;s definition of measurement as &amp;#8220;A quantitatively expressed reduction of uncertainty based on one or more observations,&amp;#8221; it turns out that you can &lt;a href="www.amazon.com/dp/B003MXI78Y?tag=contindelive-20"&gt;measure anything&lt;/a&gt; if you put your mind to it. The report describes both our methodology and the way we measured these apparent intangibles.&lt;/p&gt;
&lt;p&gt;Indeed we not only measured these things: we have sound, statistically significant data that shows that culture and DevOps practices impact both IT performance and organizational performance. In direct contradiction to a popular narrative of the last ten years, IT matters &amp;#8212; indeed, the results show it is a competitive advantage &amp;#8212; and DevOps culture and practices are instrumental in achieving both high IT performance and organizational performance. Readers of this blog will be especially interested to learn that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Trunk-based development, continuous integration, and automated testing measurably improve both IT performance and organizational performance.&lt;/li&gt;
&lt;li&gt;Having a high-trust culture has a strong impact on both IT performance and organizational performance.&lt;/li&gt;
&lt;li&gt;Using external change approval processes such as a change advisory board, as opposed to peer-based code review techniques, significantly impacts throughput while doing almost nothing to improve stability.&lt;/li&gt;
&lt;li&gt;Job satisfaction is the biggest predictor of organizational performance, and using DevOps practices are good predictors of job satisfaction.
&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;I&amp;#8217;m very excited by the report. We improved on &lt;a href="https://puppetlabs.com/2013-state-of-devops-infographic"&gt;last year&amp;#8217;s&lt;/a&gt; method for measuring IT performance. We showed how you can measure culture and organizational performance. Most important, the analysis of our enormous data set demonstrates definitively that the strategies championed by the DevOps movement work, and that they provide a competitive advantage to your business.&lt;/p&gt;
&lt;p&gt;Many thanks to my collaborators, the fabulous team at PuppetLabs, and to all of you who took the survey.&lt;/p&gt;
&lt;p&gt;You can &lt;a href="http://puppetlabs.com/2014-devops-report"&gt;download the 2014 State of Devops Report for free.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:37:58 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:the-2014-state-of-devops-report-is-here.html</guid></item><item><title>Bliki: DiversityMediocrityIllusion</title><link>http://www.ciandcd.com/bliki-diversitymediocrityillusion.html</link><description>&lt;div&gt; 

 

 

&lt;p class="tagLabel"&gt;tags:&lt;/p&gt;

&lt;p class="clear"&gt;&lt;/p&gt;

&lt;p&gt;I've often been involved in discussions about deliberately
  increasing the diversity of a group of people. The most common case
  in software is increasing the proportion of women. Two examples are
  in hiring and conference speaker rosters where we discuss trying to
  get the proportion of women to some level that's higher than usual.
  A common argument against pushing for greater diversity is that it
  will lower standards, raising the spectre of a diverse but mediocre
  group.&lt;/p&gt;

&lt;p&gt;To understand why this is an illusionary concern, I like to
  consider a little thought experiment. Imagine a giant bucket that
  contains a hundred thousand marbles. You know that 10% of these
  marbles have a special sparkle that you can see when you carefully
  examine them. You also know that 80% of these marbles are blue and
  20% pink, and that sparkles exist evenly across both colors &lt;a href="#footnote-sparkle"&gt;[1]&lt;/a&gt;. If you were
  asked to pick out ten sparkly marbles, you know you could
  confidently go through some and pick them out. So now imagine you're
  told to pick out ten marbles such that five were blue and five were
  pink.&lt;/p&gt;
&lt;img src="images/diversityMediocrityIllusion/sketch.png"&gt;
&lt;p&gt;I don't think you would react by saying &amp;#8220;that's impossible&amp;#8221;.
  After all there are two thousand pink sparkly marbles in there,
  getting five of them is not beyond the wit of even a man. Similarly
  in software, there may be less women in the software business, but
  there are still enough good women to fit the roles a company or a
  conference needs.&lt;/p&gt;

&lt;p&gt;The point of the marbles analogy, however, is to focus on the
  real consequence of the demand for 50:50 split. Yes it's possible to
  find the appropriate marbles, but the downside is that it takes
  longer. &lt;a href="#footnote-non-binary"&gt;[2]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;That notion applies to finding the right people too. Getting
  a better than base proportion of women isn't impossible, but it does
  require more work, often much more work. This extra effort
  reinforces the rarity, if people have difficulty finding good
  people as it is, it needs determined effort to spend the extra time
  to get a higher proportion of the minority group &amp;#8212; even if you are
  only trying to raise the proportion of women up to 30%, rather than
  a full 50%.&lt;/p&gt;

&lt;p&gt;In recent years we've made increasing our diversity a high
  priority at ThoughtWorks. This has led to a lot of effort trying to
  go to where we are more likely to run into the talented women we are
  seeking: women's colleges, women-in-IT groups and conferences. We
  encourage our women to speak at conferences, which helps let other
  women know we value a diverse workforce. &lt;/p&gt;

&lt;p&gt;When interviewing, we make a point of ensuring there are women
  involved. This gives women candidates someone to relate to, and
  someone to ask questions which are often difficult to ask men. It's
  also vital to have women interview men, since we've found that women
  often spot problematic behaviors that men miss as we just don't have
  the experiences of subtle discriminations. Getting a diverse group
  of people inside the company isn't just a matter of recruiting, it
  also means paying a lot of attention to the environment we have, to try to
  ensure we don't have the same &lt;a href="AlienatingAtmosphere.html"&gt;AlienatingAtmosphere&lt;/a&gt; that
  much of the industry exhibits. &lt;a href="#footnote-client-atmosphere"&gt;[3]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;One argument I've heard against this approach is that if everyone
  did this, then we would run out of pink, sparkly marbles. We'll know
  this is something to be worried about when women are paid
  significantly more than men for the same work.&lt;/p&gt;

&lt;p&gt;One anecdote that stuck in my memory was from a large,
  traditional company who wanted to improve the number of women in
  senior management positions. They didn't impose a quota on
  appointing women to those positions, but they did impose a quota for
  women on the list of candidates. (Something like: "there must be at
  least three credible women candidates for each post".) This
  candidate quota forced the company to actively seek out women
  candidates. The interesting point was that just doing this, with no
  mandate to actually appoint these women, correlated with an increased
  proportion of women in those positions.&lt;/p&gt;

&lt;p&gt;For conference planning it's a similar strategy:  just putting out a call for
  papers and saying you'd like a diverse speaker lineup isn't enough.
  Neither are such things as blind review of proposals (and I'm not
  sure that's a good idea anyway). The important thing is to seek out
  women and encourage them to submit ideas. Organizing conferences is
  hard enough work as it is, so I can sympathize with those that don't
  want to add to the workload, but those that do can get there. &lt;a href="http://continuousdelivery.com/2013/09/how-we-got-40-female-speakers-at-flowcon/"&gt;FlowCon
  is a good example&lt;/a&gt; of a conference that made this an explicit
  goal and did far better than the industry average (and in case you
  were wondering, there was &lt;a href="http://continuousdelivery.com/2013/12/flowcon-2013-wrap-up/"&gt;no
  difference between men's and women's evaluation scores&lt;/a&gt;). &lt;/p&gt;

&lt;p&gt;So now that we recognize that getting greater diversity is a
  matter of application and effort, we can ask ourselves whether the
  benefit is worth the cost. In a broad professional sense, I've
  argued that it is, because our &lt;a href="DiversityImbalance.html"&gt;DiversityImbalance&lt;/a&gt; is
  reducing our ability to bring the talent we need into our profession,
  and reducing the influence our profession needs to have on society.
  In addition I believe there is a moral argument to push back against
  long-standing wrongs faced by
  &lt;a href="HistoricallyDiscriminatedAgainst.html"&gt;HistoricallyDiscriminatedAgainst&lt;/a&gt; groups. &lt;/p&gt;

&lt;p&gt;Conferences have an important role to play in correcting this
  imbalance. The roster of speakers is, at least subconsciously, a
  statement of what the profession should look like. If it's all white
  guys like me, then that adds to the &lt;a href="AlienatingAtmosphere.html"&gt;AlienatingAtmosphere&lt;/a&gt;
  that pushes women out of the profession. Therefore I believe that
  conferences need to strive to get an increased proportion of
  historically-discriminated-against speakers. We, as a profession,
  need to push them to do this. It also means that women have an
  extra burden to become visible and act as part of that better
  direction for us. &lt;a href="#footnote-burden"&gt;[4]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For companies, the choice is more personal. For me,
  ThoughtWorks's efforts to improve its diversity are a major factor
  in why I've been an employee here for over a decade. I don't think
  it's a coincidence that ThoughtWorks is also a company that has a
  greater open-mindedness, and a lack of political maneuvering, than most of the
  companies I've consulted with over the years. I consider those
  attributes to be a considerable competitive advantage in attracting
  talented people, and providing an environment where we can
  collaborate effectively to do our work. &lt;/p&gt;

&lt;p&gt;But I'm not holding ThoughtWorks up as an example of perfection.
  We've made a lot of progress over the decade I've been here, but we
  still have a long way to go. In particular we are very short of
  senior technical women. We've introduced a number of programs around
  networks, and leadership development, to help grow women
  to fill those gaps. But these things take time - all you have to do
  is look at our &lt;a href="http://www.thoughtworks.com/radar/faq"&gt;Technical Advisory Board&lt;/a&gt;
  to see that we are a long way from the ratio we seek. &lt;/p&gt;

&lt;p&gt;Despite my knowledge of how far we still have to climb, I can
  glimpse the summit ahead. At a recent AwayDay in Atlanta I was
  delighted to see how many younger technical women we've managed to
  bring into the company. While struggling to keep my head above water
  as the sole male during a late night game of &lt;a href="/articles/eurogames/"&gt;Dominion&lt;/a&gt;, I enjoyed a
  great feeling of hope for our future.&lt;/p&gt;

 

&lt;p class="acknowledgements"&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;

  
 Camila Tartari, Carol Cintra, Dani Schufeldt, Derek Hammer, Isabella
  Degen, Korny Sietsma, Lindy Stephens, Mridula Jayaraman, Nikki
  Appleby, Rebecca Parsons, Sarah Taraporewalla, Stefanie Tinder, and Suzi
  Edwards-Alexander
  
  commented on
  drafts of this article.
&lt;/p&gt;

&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/bliki/DiversityMediocrityIllusion.html&amp;amp;text=Bliki:%20DiversityMediocrityIllusion" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/bliki/DiversityMediocrityIllusion.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/bliki/DiversityMediocrityIllusion.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:37:33 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:bliki-diversitymediocrityillusion.html</guid></item><item><title>Using oauth for a simple command line script to access Google's data</title><link>http://www.ciandcd.com/using-oauth-for-a-simple-command-line-script-to-access-googles-data.html</link><description>&lt;div&gt;&lt;p&gt;I recently needed to write a simple script to pull some data
    from a Google website. Since I was grabbing some private data, I
    needed authorize myself to do that. I found it much more work than
    I expected, not because it's hard, but because there wasn't much
    documentation there to guide me - I had to puzzle out what path
    to go based on lots of not particularly relevant documentation. So
    once I'd figured it out I decided to write a short account of what
    I'd done, partly in case I need to do this again, and partly to
    help anyone else who wants to do this.&lt;/p&gt;

&lt;p&gt;First a disclaimer. This is what I figured out, it works for
    me, at the moment. I haven't done extensive research of whether
    this is the best way to do what I want (although it sure felt like
    extensive research while I was doing it). So bear that in mind.
    (And if you have better ways do let me know.)&lt;/p&gt;

&lt;p&gt;I did all of this in Ruby, since that's my familiar scripting
    language. I also used Google's api library for Ruby. But much of
    the overall flow would be the same for other languages, so if
    you're operating outside of Ruby I think much of what I did
    would still be relevant. I'll try to describe what I'm doing
    in a language independent view as much as possible, in addition to
    the ruby examples.&lt;/p&gt;

&lt;p&gt;I need to access a private playlist of
    videos on YouTube and print the titles of the videos on that
    playlist. &lt;a href="#footnote-actual-need"&gt;[1]&lt;/a&gt; Since this is a private
    playlist, I need to authenticate to Google and set up the
    necessary authorization for the script so it can get at that
    private data. I want to run this script without any manual
    intervention, so I want whatever auth mechanism I use to be
    something that the script can access itself, at least once I've
    logged into my laptop.&lt;/p&gt;

&lt;p&gt;Before I describe the successful path I followed, I should
    mention a path I took to a dead end. One of the things that made
    this simple exercise so tricky is that most of the documentation I
    read assumed I wanted to write a web-app that was guiding a
    browser. But I wanted a simple command line app (I guess because
    I'm old-fashioned that way) that didn't involve a browser. Reading
    through the &lt;a href="https://developers.google.com/accounts/docs/GettingStarted"&gt;Google guide to
    authentication and authorization&lt;/a&gt; I decided to use OAuth
    2.0, as that seems to be where Google wants to go. Google then
    gives several scenarios for OAuth authorization, of which the
    natural (if complex) one to go for seemed to be &lt;a href="https://developers.google.com/accounts/docs/OAuth2ServiceAccount"&gt;&lt;b&gt;Service Accounts&lt;/b&gt;&lt;/a&gt;. These support
    server-to-server access with authentication done via
    public/private key pair. I spent a good bit of time fiddling to
    get this to work and eventually was able to access google with it
    successfully, at which point I ran into a wall. With a service
    account, you effectively create a new user on Google. You then
    need some mechanism to allow that user to access your personal
    data. If you are running a domain on Google, there is a way
    to authorize service accounts to access your domain's data.
    However I could find no such mechanism for accessing data from a
    direct google account such as mine. Documentation implied you
    could do for some properties (such as analytics) but there was no
    general mechanism, such as one that would work for youtube data.
    It's always frustrating to spend many hours working out a solution
    and running into a hard wall like that, if this article does
    nothing more than save a few people from that effort, then it's
    worth writing.&lt;/p&gt;


&lt;h2&gt;Outline flow for authorization&lt;/h2&gt;

&lt;p&gt;The path that did work for me is based on what Google calls the
    &lt;a href="https://developers.google.com/accounts/docs/OAuth2InstalledApp"&gt;Installed Application&lt;/a&gt; flow, but
    one that I needed to adapt to ensure I could (mostly) do it
    without having to manually intervene or use a browser.&lt;/p&gt;

&lt;p&gt;To best explain how this works, I'll begin with a simple
    request to get that youtube listing. Whenever a script makes a request to
    get google data, you need to include an &lt;b&gt;access token&lt;/b&gt; in your
    request. Google's docs show such an HTTP request like this.&lt;/p&gt;

&lt;pre&gt;GET /plus/v1/people/me HTTP/1.1
Authorization: Bearer 1/fFBGRNJru1FQd44AzqT3Zg
Host: googleapis.com
&lt;/pre&gt;

&lt;p&gt;The access token is just a random looking bunch of characters.
    It lasts for a short amount of time, the current documentation
    says it lasts for just an hour. The access token is what the
    script needs to do its work, but that just leads to the question - how
    do you get an access token in the first place?&lt;/p&gt;

&lt;p&gt;One way to get an access token is to have a different kind of
    token - a &lt;b&gt;refresh token&lt;/b&gt;. Unlike access tokens, refresh tokens last
    for a long time. They only expire when they are revoked, they are
    superseded by later refresh tokens, or when Google has a hissy fit. For
    our script's purpose a refresh token is just the job. Once I have
    a refresh token, I can store it in a moderately safe place that
    the script can get to without manual intervention. I can then
    access the refresh token when I run the script, and as a first
    step use the refresh token to get a brand new access token. I can
    then use the access token for the rest of the script run
    (providing my script doesn't run longer than the lifetime of an
    access token - and even Ruby isn't that slow). I don't mind if
    getting the refresh token involves a manual step, because I don't
    need to do it very often.&lt;/p&gt;

&lt;p&gt;Before I explain how to get the refresh token, there's one
    other thing about them. Each refresh token (and the access token
    they obtain) has a limited authorization scope - meaning you say
    what data they are allowed to access. I can create a refresh token
    that's only valid for reading my youtube data. If a bad guy were
    to get this token he could not read my calendar data, nor modify
    my youtube data. Having different tokens with different scopes
    helps me limit what I do with each token, which makes me a touch
    more secure (and less worried with how safely I store the tokens).&lt;/p&gt;

&lt;p&gt;To get the refresh token, I do have to get a browser to log
    into google and authenticate itself as me. Like most people I have
    browser instances permanently logged into Google on my laptop, so that's no
    big deal. What I do is go to a google URL that's constructed in
    such a way to specify the authorization scope that I want. If I
    do that, while logged into my Google account, google will give me
    a &lt;b&gt;one-time authorization code&lt;/b&gt;. I then take that code and visit
    another URL and google hands me the refresh token that I want.&lt;/p&gt;

&lt;p&gt;Before all of this, there's a further thing I need to do -
    setup google to use APIs and allow access to the apps I want API
    access to reach. This is a manual task, but I only need to do it
    once (unless Google has a really big hissy fit). &lt;/p&gt;

&lt;p&gt;So here's the steps I need to go through:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Set up Google for API access - a one-time manual action with
      logged in browser&lt;/li&gt;

&lt;li&gt;Get a one-time authorization code - needs logged in browser,
      done rarely&lt;/li&gt;

&lt;li&gt;Exchange the authorization code for a refresh token - 
       API, done rarely&lt;/li&gt;

&lt;li&gt;Use the refresh token to get a new access token - api only, done once
      each time I run the script&lt;/li&gt;

&lt;li&gt;Use the access token when calling google - api only, done every time I
      call a google api&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Setting up Google&lt;/h2&gt;

&lt;p&gt;To use APIs with a google account I need to go into Google
      and set things up. The place I need to be is the &lt;a href="https://console.developers.google.com"&gt;Google
      Developers Console&lt;/a&gt;. I already had a project defined in the
      console, but you'll need to do that if you don't have one
      already. I then clicked on the project and then the APIs tab (on
      the left). This shows me of APIs and allows me to enable
      whichever APIs I wish. I need to ensure the API I want to use is
      enabled (in this case the Youtube Data API).&lt;/p&gt;

&lt;p class="figureImage"&gt;&lt;a name="command-line-google_enable-api.png"&gt;&lt;/a&gt;&lt;img alt="Figure 1" src="command-line-google/enable-api.png"&gt;&lt;/p&gt;
&lt;p&gt;I also need to have a OAuth client ID using the
      "Credentials" tab. I already had one set up.&lt;/p&gt;

&lt;p class="figureImage"&gt;&lt;a name="command-line-google_client-id.png"&gt;&lt;/a&gt;&lt;img alt="Figure 2" src="command-line-google/client-id.png"&gt;&lt;/p&gt;

&lt;h2&gt;Getting the one-time authorization code&lt;/h2&gt;

&lt;p&gt;To get the one-time authorization code you need to hit a
      specially crafted google URL while logged into Google. Google
      will then return the authorization code. Google's documentation,
      and various samples I ran into, explain doing this via a web
      app. In the course of your normal flow, the web app realizes it
      needs auth, and sends you over to google. &lt;/p&gt;

&lt;p&gt;Google can return the authorization code directly to your web
      app. All you need to do is run a server on your local machine
      and tell google its URL - eg &lt;code&gt;localhost:1234&lt;/code&gt;. Google
      will then issue a GET to that URL and include the authorization
      code as an parameter in the URL. Your code can then easily pick
      off the parameter. You don't need much of a webserver on this
      port to pick this up, all it ever needs to do is respond to this
      one request. This level of simple server doesn't even need
      Sinatra (Ruby's light weight web server framework), I remember
      many years ago being in an introductory Ruby class with Prag
      Dave where we wrote a simple web server in a few minutes. But I
      was too lazy to do even that.&lt;/p&gt;

&lt;p&gt;What I did instead was let my program craft the necessary
      google URL and print this URL out on the console. I then
      copy and paste it into my browser. Google (after a little dance
      to check I know what I'm doing) responds with the
      authorization code on a web page. I then copy and paste this code back
      into my script. It's not as smooth as an automated mechanism,
      but I don't care since I only have to do it once every blue moon.&lt;/p&gt;

&lt;p&gt;Let's look at my code for this. I divide any non-trivial
      command line script into multiple classes, separating the class
      that handles the command line interaction from an "engine" class
      that does all the work behind the scenes - essentially a use of
      &lt;a href="/eaaDev/SeparatedPresentation.html"&gt;Separated Presentation&lt;/a&gt;. I do this
      because I find it easier to separate the command line from the
      core code when I'm working on them.&lt;/p&gt;

&lt;p class="figureImage"&gt;&lt;a name="command-line-google_get-refresh.png"&gt;&lt;/a&gt;&lt;img alt="Figure 3" src="command-line-google/get-refresh.png" width="900"&gt;&lt;/p&gt;

&lt;p class="figureCaption"&gt;Figure 3: 
        Sequence diagram for how my code examples get a refresh token.
      &lt;/p&gt;
&lt;p&gt;For the command-line, I'm using &lt;a href="https://github.com/erikhuda/thor"&gt;Thor&lt;/a&gt;, which is a simple framework for building
      command-line applications in ruby &lt;a href="#footnote-cli-ruby"&gt;[2]&lt;/a&gt;.
      &lt;/p&gt;

&lt;pre&gt;require 'thor'
require_relative 'engine'

class CLI &amp;lt; Thor
  include Thor::Actions

  def initialize *args
    super(*args)
    @engine = Engine.new
  end

  desc "auth", "re-authorize the script"
  def auth
    puts "Point browser to following URL:\n\n"
    puts @engine.authorization_url
    puts "\n\n"
    auth_code = ask "paste in the authorization code"
    @engine.renew_refresh_token auth_code
  end
&lt;/pre&gt;

&lt;pre&gt;end

CLI.start(ARGV)
&lt;/pre&gt;

&lt;p&gt;Thor maps methods in the CLI class to sub-commands, so if
      the filename of the script is &lt;code&gt;get-vid&lt;/code&gt; I can invoke
      the authorization logic with &lt;code&gt;get-vid auth&lt;/code&gt;. &lt;a href="#footnote-cli-details"&gt;[3]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This authorization logic makes two calls to the underlying
      engine, one to get the url to display, the second to take the
      resulting authorization code that I paste in and use it renew
      the refresh token.&lt;/p&gt;

&lt;p&gt;I wrote the engine to handle the logic for futzing with
      youtube, but much of this code is really about dealing with
      google authorization. So I separated the authorization code out
      into a separate object, the GoogleAuthorizer. The engine creates
      an authorizer on initialization and delegates both the URL and
      renewal requests to it.&lt;/p&gt;

&lt;p class="code-label"&gt;class Engine...
&lt;/p&gt;

&lt;pre&gt;  def initialize
    @auth = GoogleAuthorizer.new(
      token_key: 'api-youtube',
      application_name: 'Gateway Youtube Example',
      application_version: '0.1'
      )
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;
&lt;/p&gt;

&lt;pre&gt;  def authorization_url
    @auth.authorization_url 'https://www.googleapis.com/auth/youtube.readonly'
  end
  
  def renew_refresh_token auth_code
    @auth.renew_refresh_token auth_code
  end
&lt;/pre&gt;

&lt;p&gt;I initialize the authorizer with three bits of data, the
      application name and version are used in some later API calls,
      the token key I'll expand on shortly.&lt;/p&gt;

&lt;p class="code-label"&gt;class GoogleAuthorizer
&lt;/p&gt;

&lt;pre&gt;  def initialize application_name: nil, application_version: "unknown", token_key: nil
    @application_name = application_name
    @token_store = TokenStore.new(token_key)
    @application_version = application_version
  end
&lt;/pre&gt;

&lt;p&gt;To construct the URL I use ruby's URL manipulation library&lt;/p&gt;

&lt;p class="code-label"&gt;class GoogleAuthorizer
&lt;/p&gt;

&lt;pre&gt;  def authorization_url scope
    params = {
      scope: scope,
      redirect_uri: 'urn:ietf:wg:oauth:2.0:oob',
      response_type: 'code',
      client_id: @token_store.client_id
    }
    url = {
      host: 'accounts.google.com',
      path: '/o/oauth2/auth',
      query: URI.encode_www_form(params)
    }

    return URI::HTTPS.build(url)
  end
&lt;/pre&gt;

&lt;p&gt;This code constructs a URL that looks something like: &lt;/p&gt;

&lt;pre&gt;https://accounts.google.com/o/oauth2/auth?
  scope=https://www.googleapis.com/auth/youtube.readonly&amp;amp;
  redirect_uri=urn:ietf:wg:oauth:2.0:oob&amp;amp;
  response_type=code&amp;amp;
  client_id=123456789012.apps.googleusercontent.com
&lt;/pre&gt;

&lt;p class="code-remark"&gt;To make it easier to read, I've added
       newlines and whitespace and decoded the URL escapes. I've also
       made up the client_id.&lt;/p&gt;

&lt;p&gt;The parameters to the URL are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;i&gt;scope:&lt;/i&gt; how much api we want to access, in this
         case we want readonly access to the youtube data api&lt;/li&gt;

&lt;li&gt;&lt;i&gt;redirect_uri:&lt;/i&gt; in the usual flow of using this with
         a web app,
         google redirects the browser to another URL (typically a
         localhost post) and deposits its response there. Using this
         value tells google I want it displayed in the browser for me
         to copy and paste&lt;/li&gt;

&lt;li&gt;&lt;i&gt;response_type:&lt;/i&gt; I want a one-time
         authorization code back&lt;/li&gt;

&lt;li&gt;&lt;i&gt;client_id&lt;/i&gt; I get this from the earlier interaction
         with the Google Developers Console&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Pasting that URL into my browser will (eventually) lead me
       to a web page from Google that shows the glistening
       authorization code.&lt;/p&gt;

&lt;h2&gt;Exchanging the authorization code for a refresh token&lt;/h2&gt;

&lt;p&gt;Now I have the authorization code I can initiate the second
       operation, obtaining the refresh token. I do this by contacting
       the Google authorization resource again, this time supplying
       the authorization code I just got from them and blending it
       with my client-secret, a code that identifies me to the
       google API. I don't need to be logged into Google for this
       step, nor do I need to use a browser.&lt;/p&gt;

&lt;p&gt;At this point I have to
       face up to another question: where do I store the refresh token
       once I have it? Since this is a script that I'm the only one
       using, I could just store it in the source code with something
       like&lt;/p&gt;

&lt;pre&gt;def refresh_token
  '1234567890WOxNS_gTztCGW3OBTKcSoKfLXDPc5TA7xz4MEudVrK5jSpoR30zcRFq6'
end
&lt;/pre&gt;

&lt;p&gt;I don't like this as I like to keep my code in repositories
       which are widely copied and often shared with others. Another
       option is to just dump the token in a file. My hard drive is
       encrypted, so that's reasonably safe - particularly since all
       I'm protecting is the dark secrets of my Youtube viewing
       habits. If I were being a bit more paranoid I could encrypt
       that file, but then that only raises the question of where to
       store the encryption key for the file, as I don't want to type
       in a password every time I use the script.&lt;/p&gt;

&lt;p&gt;Since I'm running this on a mac, I decided to use the Mac's
       built in keychain. This automatically opens when I log in and I
       can access it with the &lt;code&gt;security&lt;/code&gt; command-line
       application. I'll have to think of something else should I want
       to run this on my Ubuntu box, but I'll deal with that if I need
       to do that one day.&lt;/p&gt;

&lt;p&gt;Whatever my decision on where to store the refresh token, it
       is a decision, and one of the signs you need encapsulation is
       to hide decisions. That's why I created a
       &lt;code&gt;TokenStore&lt;/code&gt; class - to hide the decision of how I
       store my refresh token. I can also use the same class to store
       a couple of other little things, such as the client_id that I
       used earlier. The client_id is something I left in the source
       code, I'm sure if someone else wants to use this code they can
       figure out how to take it out.&lt;/p&gt;

&lt;p class="code-label"&gt;class TokenStore&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def client_id
    '123456789012.apps.googleusercontent.com'
  end
&lt;/pre&gt;

&lt;p&gt;To renew the refresh token, I need to use the one-time
       authorization code I got earlier to request new tokens, dig out
       the refresh token, and put it into my token store. (I say
       &amp;#8220;tokens&amp;#8221;, because Google responds with both an access
       token and a refresh token.)&lt;/p&gt;

&lt;p class="code-label"&gt;class GoogleAuthorizer&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def renew_refresh_token auth_code
    client = get_client_with_new_tokens(auth_code)
    token = client.authorization.refresh_token
    puts "new token: #{token}"
    @token_store.save_refresh_token token
  end
&lt;/pre&gt;

&lt;p&gt;To request these tokens, I talk again to Google, but this time
       I find it best to use the &lt;a href="https://github.com/google/google-api-ruby-client"&gt;ruby client
       library for the Google api&lt;/a&gt;. It took me a bit of time to
       figure out how to use it, as the documentation is sorely
       lacking. But it works pretty well once I'd got
       the hang of it. Here's the code to get the tokens:&lt;/p&gt;

&lt;p class="code-label"&gt;class GoogleAuthorizer...
&lt;/p&gt;

&lt;pre&gt;  def get_client_with_new_tokens auth_code
    client = Google::APIClient.new(
      application_name:  @application_name,
      application_version: @application_version
      )

    client.authorization = Signet::OAuth2::Client.new(
      token_credential_uri: 'https://www.googleapis.com/oauth2/v3/token',
      code: auth_code,
      client_id: @token_store.client_id,
      client_secret: @token_store.client_secret,
      redirect_uri: 'urn:ietf:wg:oauth:2.0:oob', 
      grant_type: 'authorization_code'
      )
    client.authorization.fetch_access_token!
    return client
  end
&lt;/pre&gt;

&lt;p&gt;This code first instantiates a google api client
       object and then gives it an authorization object, which uses the
       &lt;a href="https://github.com/google/signet"&gt;Signet library&lt;/a&gt;, which I also found
       wasn't very well documented. This particular combination of
       attributes worked for authorizing tokens&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;i&gt;application_name:&lt;/i&gt; not sure how this is used by google,
         but you need a value here to avoid an error in the client library&lt;/li&gt;

&lt;li&gt;&lt;i&gt;application_version:&lt;/i&gt; similar to the application name.
         Missing this out didn't get an error for me, but I left it in
         anyway as the error message for the name told me I needed it.&lt;/li&gt;

&lt;li&gt;&lt;i&gt;token_credential_uri:&lt;/i&gt; the URL to talk to for 
         authorization.  &lt;/li&gt;

&lt;li&gt;&lt;i&gt;code:&lt;/i&gt; the one-time authorization code&lt;/li&gt;

&lt;li&gt;&lt;i&gt;client_id:&lt;/i&gt; the client id from the earlier interaction
         with the Google Developers Console&lt;/li&gt;

&lt;li&gt;&lt;i&gt;client_secret:&lt;/i&gt; You also get this from the earlier interaction
         with the Google Developers Console. The google documentation
         says this isn't really a secret for applications like this,
         but they still call it a secret. I think of it as an enhanced
         identifier.&lt;/li&gt;

&lt;li&gt;&lt;i&gt;redirect_uri:&lt;/i&gt; I don't know what role this is
         playing, but when I left it off the library spanked me for
         a missing grant_type &lt;a href="#footnote-missing-grant-type"&gt;[4]&lt;/a&gt;. The google documentation implies I
         should set it to the same value that I did when requesting
         the authorization code in the first place.&lt;/li&gt;

&lt;li&gt;&lt;i&gt;grant_type:&lt;/i&gt; tells google I have an authorization
         code that I'd like to redeem for tokens. &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I didn't tap the HTTPS link to Google, but based on the
       google documentation, the resulting HTTP call should look
       something like this&lt;/p&gt;

&lt;pre&gt;POST /oauth2/v3/token HTTP/1.1
Host: www.googleapis.com
Content-Type: application/x-www-form-urlencoded

code=4/v6xr77ewYqhvHSyW6UJ1w7jKwAzu&amp;amp;
client_id=123456789012.apps.googleusercontent.com&amp;amp;
client_secret=ABC1234567890&amp;amp;
redirect_uri=urn:ietf:wg:oauth:2.0:oob&amp;amp;
grant_type=authorization_code
&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;fetch_access_token!&lt;/code&gt; method talks to google
       to get the tokens. Google sends back some JSON
       which the client library stores in the Signet authorization
       object. I can then get at the refresh token and save it in my 
       token store.&lt;/p&gt;

&lt;p class="code-label"&gt;class GoogleAuthorizer&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def renew_refresh_token auth_code
    client = get_client_with_new_tokens(auth_code)
    token = &lt;p class="highlight"&gt;client.authorization.refresh_token&lt;/p&gt;
    puts "new token: #{token}"
    @token_store.save_refresh_token token
  end
&lt;/pre&gt;

&lt;p&gt;I can then use the token store to save the token into my
       Mac's keychain.&lt;/p&gt;

&lt;p class="code-label"&gt;class TokenStore&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def save_refresh_token arg
    cmd = "security add-generic-password -a '#{@keychain_account}' -s '#{@keychain_account}' -w '#{arg}'"
    system cmd
  end
&lt;/pre&gt;

&lt;p&gt;I set the account for the token store to use
       when I created the engine.&lt;/p&gt;

&lt;p class="code-label"&gt;class Engine...
&lt;/p&gt;

&lt;pre&gt;  def initialize
    @auth = GoogleAuthorizer.new(
&lt;p class="highlight"&gt;      token_key: 'api-youtube',&lt;/p&gt;
      application_name: 'Gateway Youtube Example',
      application_version: '0.1'
      )
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class GoogleAuthorizer...
&lt;/p&gt;

&lt;pre&gt;  def initialize application_name: nil, application_version: "unknown", token_key: nil
    @application_name = application_name
    @token_store = &lt;p class="highlight"&gt;TokenStore.new(token_key)&lt;/p&gt;
    @application_version = application_version
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class TokenStore&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def initialize keychain_account
    @keychain_account = keychain_account
  end
&lt;/pre&gt;

&lt;p&gt;So the operating system command that's issued by
     &lt;code&gt;save_refresh_token&lt;/code&gt; is &lt;/p&gt;

&lt;pre&gt;security add-generic-password -a 'api-youtube' -s 'api-youtube' -w 'ABC123456-other-chars'
&lt;/pre&gt;

&lt;p&gt;The system command needs both a service (&lt;code&gt;-s&lt;/code&gt;)
       and an account (&lt;code&gt;-a&lt;/code&gt;) when storing a value. I use
       the same value for each of them, as I really just want a
       key-value store.&lt;/p&gt;

&lt;h2&gt;Using the refresh token to get an access token&lt;/h2&gt;

&lt;p&gt;The authorization logic is unusual, I expect only to invoke
       it once every blue moon. Also I now hope that once I've written
       it, I won't have to futz with it again, and I pulled it out
       into the authorizer so I can use that class with any other
       command line script that grabs Google data. &lt;/p&gt;

&lt;p&gt;What I do want to do is use all of this setup each time I
       want to do something useful, or in this case print out the
       videos on a private playlist. I begin with the Thor CLI&lt;/p&gt;

&lt;p class="code-label"&gt;class CLI&amp;#8230; (in file list-vid)
&lt;/p&gt;

&lt;pre&gt;  default_task :list
  desc "list", "list items in my playlist"
  def list
    puts @engine.list_playlist
  end
&lt;/pre&gt;

&lt;p class="code-remark"&gt;The &lt;code&gt;default_task&lt;/code&gt; annotation
       allows me to run the command &lt;code&gt;list-vid&lt;/code&gt; without a
       sub-command and invoke the list method.&lt;/p&gt;

&lt;p&gt;As usual, the CLI just delegates all the work to the
       engine. The outline of the engine method is pretty simple. First I
       define the parameters to a request with a simple hash. Next I
       ask a client object to execute the request, returning some JSON
       that I parse back into a ruby data structure. Finally I pull
       what I want out of that data structure to send back to the CLI.
       At this point, however, I want to keep focus on exchanging the
       refresh token for an access token. All this work is hidden
       behind the simple &lt;code&gt;client&lt;/code&gt; method call.&lt;/p&gt;

&lt;p class="code-label"&gt;class Engine...
&lt;/p&gt;

&lt;pre&gt;  def list_playlist
    request = playlist_request
    response = exec_request(request)
    return response['items'].map{|i| i['snippet']['title']}
  end
  def exec_request requestHash
    JSON.parse(&lt;p class="highlight"&gt;client&lt;/p&gt;.execute!(requestHash).body)
  end
&lt;/pre&gt;

&lt;p&gt;The client object is an instance of the Google API client
       that we saw earlier, although it's configured 
       differently. When executing the particular request, the client
      object will use an access token, but first it has to use the
      refresh token to get that access token. I set things up so that
      the first time the script wants to use a client object, I create
      one going through the dance with google to get the new access
      token. Once I've done that I keep this client object in the
      engine to use again for further requests.&lt;/p&gt;

&lt;p class="code-label"&gt;class Engine...
&lt;/p&gt;

&lt;pre&gt;  def client
    @client ||= @auth.api_client
    return @client
  end
&lt;/pre&gt;

&lt;p&gt;All of the business of exchanging refresh tokens for access
      tokens is handled by the Google client library. I just have to
      create a Google API object initialized with the right data
      and get it to fetch an access token. &lt;/p&gt;

&lt;p class="code-label"&gt;class GoogleAuthorizer...
&lt;/p&gt;

&lt;pre&gt;  def api_client
   client = Google::APIClient.new(
      application_name: @application_name,
      application_version: @application_version
      )
    
    client.authorization = Signet::OAuth2::Client.new(
      token_credential_uri: 'https://www.googleapis.com/oauth2/v3/token',
      client_id: @token_store.client_id,
      client_secret: @token_store.client_secret,
      refresh_token: @token_store.refresh_token,
      grant_type: 'refresh_token'
      )
    client.authorization.fetch_access_token!
    return client
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class TokenStore&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def client_secret
    @client_secret ||= `security find-generic-password -wa google-client-secret`.chomp
    @client_secret
  end
  def refresh_token
    @refresh_token ||= `security find-generic-password -wa #{@keychain_account}`.chomp
    @refresh_token
  end
&lt;/pre&gt;

&lt;p&gt;I can now use the returned Google API client object for the
      last step.&lt;/p&gt;

&lt;h2&gt;Using the access token when calling a Google API&lt;/h2&gt;

&lt;p&gt;If I use the Google API object to make calls on Google, it
       ensures that access token is added to the request, as well as
       organizing the request data in the right way. All I have to do
       is use &lt;code&gt;execute!&lt;/code&gt; method on the client object,
       passing in a hash with the data for my request.&lt;/p&gt;

&lt;p class="code-label"&gt;class Engine...
&lt;/p&gt;

&lt;pre&gt;  def list_playlist
    request = playlist_request
    response = exec_request(request)
    return response['items'].map{|i| i['snippet']['title']}
  end
  def exec_request requestHash
    JSON.parse(&lt;p class="highlight"&gt;client.execute!(requestHash)&lt;/p&gt;.body)
  end
&lt;/pre&gt;

&lt;p&gt;Since this article is really about using Oauth with Google,
        I can end it right here. However since I'm also futzing with
        the Google client library and trying to figure out how to use
        it to grab some data, I might as well continue with just
        enough to show how I pull out the titles of the videos in that
        private playlist.&lt;/p&gt;

&lt;h2&gt;Getting a list of videos from the Google API&lt;/h2&gt;

&lt;p&gt;To get data out of youtube (or any Google API) I have to
        decide which resource I want to talk to, what method I wish to
        invoke on that resource (they don't talk in terms of HTTP
        verbs) and use parameters to parameterize the call. &lt;/p&gt;

&lt;p&gt;The Google
        client library provides some affordances for determining the
        right api_method.  I can ask the client object to return me
        an API object which knows about a particular
        API.&lt;/p&gt;

&lt;p class="code-label"&gt;class Engine...
&lt;/p&gt;

&lt;pre&gt;  def youtube
    client.discovered_api('youtube', 'v3')
  end
&lt;/pre&gt;

&lt;p&gt;This call to &lt;code&gt;discovered_api&lt;/code&gt; does two things.
        Firstly it tells the client object to contact Google and
        download a description of Google's online API which it then
        caches. Secondly it returns a new Google API object configured
        with this information. Since the client caches the API
        description, I don't have to worry about caching the youtube
        API object myself. (If I need to talk to more than one Google
        API, I can use the &lt;code&gt;discovered_apis&lt;/code&gt; method, which
        will get multiple API's data from Google in a single
        call.)&lt;/p&gt;

&lt;p&gt;So what can I do with this Google API object? Sadly it's not
        immediately obvious. There's only one Google API class, and
        this one class operates with all the various Google APIs. So
        if I peruse its documentation (which I had to do from my local
        gem browser since the online version 404d on me) it doesn't
        tell me anything about methods to talk to youtube. What it does
        list are various methods that I can invoke to find out more,
        so I interrogated it with some ruby calls. First I invoked pry
        to allow me to interrogate the runtime object.&lt;/p&gt;

&lt;p class="code-label"&gt;...
&lt;/p&gt;

&lt;pre&gt;  def list_playlist
&lt;p class="highlight"&gt;    binding.pry&lt;/p&gt;
    request = playlist_request
    resp = exec_request(request)
    return resp['items'].map{|i| PlaylistTitle.new(i).title}    
  end
&lt;/pre&gt;

&lt;p&gt;That drops me into the pry REPL. I don't use this much, so
        I'm not that skilled with it, but I know it allows me to
        invoke methods on objects.&lt;/p&gt;

&lt;pre&gt;[1] pry(#&amp;lt;Engine&amp;gt;)&amp;gt; youtube.description
=&amp;gt; "Programmatic access to YouTube features."
[2] pry(#&amp;lt;Engine&amp;gt;)&amp;gt; youtube.discovered_methods
=&amp;gt; []
&lt;/pre&gt;

&lt;p&gt;OK - that's not helpful. But I suspect some clever
        metaprogramming is going on.&lt;/p&gt;

&lt;pre&gt;[3] pry(#&amp;lt;Engine&amp;gt;)&amp;gt; youtube.methods
=&amp;gt; [:activities,
 :channel_banners,
 :channel_sections,
 :channels,
 :guide_categories,
 :i18n_languages,
 :i18n_regions,
 :live_broadcasts,
 :live_streams,
 :playlist_items,
 :playlists,
&amp;#8230; lots more          
&lt;/pre&gt;

&lt;p&gt;So that's it &amp;#8212; the Google API does runtime code generation
        to add suitable methods to the api object. This way a single
        api class can have support different APIs, and each API can be
        generated at run time to be up to date with exactly what Google
        supports. Should Google change its API at run time, I don't
        need to update my client library, I'll just get a different
        set of runtime generated methods &amp;#8212; very clever.&lt;/p&gt;

&lt;p&gt;If you know me well, you'll know that when I use "clever"
        to describe some programming, it isn't usually a compliment.
        The downside of doing runtime code generation is that it's a
        pain in the neck to find out what methods are available to me.
        Usually when I want to know what an object does, I can look at
        some documentation. Even with minimal documentation I can at
        least see what methods are available. But in this case I can't
        even do that - I have to futz around with pry just to get a
        list of available method calls. And these method calls don't
        come with documentation to explain how to use them.&lt;/p&gt;

&lt;p&gt;My hope here is that these runtime-generated methods
        correspond to the Google API that's described in more generic
        terms. I see a method here called "playlists" - does this
        corresponds to the &lt;a href="https://developers.google.com/youtube/v3/docs/playlists"&gt;resource in
        the online API?&lt;/a&gt; Thankfully this correspondence holds
        pretty well. It seems that if I see a Google resource named
        something like &lt;code&gt;channels&lt;/code&gt; I can expect a
        runtime-generated method on the google api object called
        &lt;code&gt;channels&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;This is all enlightening, but how do I list the contents of
        a playlist? Each playlist on Google has an ID, so how do I see
        the what items are on playlist "1234"? Here it's important to
        understand how the Google youtube data API is organized (other
        Google APIs may be similar, I haven't looked enough to see).
        When we talk about an HTTP API using resources, I tend to
        imagine organizing the API in a URL style similar to that used
        on the web. So if I want the details about playlist 1234, I
        would expect a document at a URL that looks something like
        &lt;code&gt;youtube/api/playlists/1234&lt;/code&gt;. I can then acquire
        that document with an HTTP GET.&lt;/p&gt;

&lt;p&gt;But the Youtube Data API isn't structured like this. It has
        resources, but these resources have their own methods on them,
        and these methods are some subset of "list", "insert",
        "update", and "delete". What we have here isn't the usual HTTP
        verbs of "get", "post", "put" and "delete" but a CRUD style
        set of verbs &lt;a href="#footnote-crud"&gt;[5]&lt;/a&gt;. The key thing for me to
        realize was to think of this API like a set of relational
        tables. &lt;/p&gt;

&lt;p&gt;So to find the contents of a playlist, I have to know that
        a playlist consists of playlist items, and then formulate the
         equivalent of &lt;code&gt;select * from playlist_items where
        playlistId = '1234'&lt;/code&gt;&lt;/p&gt;

&lt;p class="code-label"&gt;class Engine...
&lt;/p&gt;

&lt;pre&gt;  def playlist_request
    {
&lt;p class="highlight"&gt;      api_method: youtube.playlist_items.list,&lt;/p&gt;
      parameters: {
&lt;p class="highlight"&gt;        playlistId: "PLJb2p0qX8R_ojWB5Bx4Q6TzaKcLMvsAim",&lt;/p&gt;
        part: 'snippet',
      }
    }    
  end
&lt;/pre&gt;

&lt;p&gt;A second thing to understand in the Google APIs is the
        notion of &lt;i&gt;parts&lt;/i&gt; of a resource. The full data referenced by a
        google resource like this can be large, so to reduce bandwidth
        Google breaks up resources into parts and requires me to
        specify which parts I want. In this case I'm just asking for
        the snippet part, since that will give the me the video titles
        that I'm looking for.&lt;/p&gt;

&lt;p class="code-label"&gt;class Engine...
&lt;/p&gt;

&lt;pre&gt;  def playlist_request
    {
      api_method: youtube.playlist_items.list,
      parameters: {
        playlistId: "PLJb2p0qX8R_ojWB5Bx4Q6TzaKcLMvsAim",
&lt;p class="highlight"&gt;        part: 'snippet',&lt;/p&gt;
      }
    }    
  end
&lt;/pre&gt;

&lt;p&gt;The &lt;a href="https://developers.google.com/youtube/v3/docs/playlistItems"&gt;documentation for
        playlist items&lt;/a&gt; shows me the structure of the playlist
        items.  &lt;/p&gt;

&lt;p&gt;When the API client executes the request, it returns a
        result object which contains lots of details about the
        interaction. To get to the meat of the result I can invoke the
        &lt;code&gt;body&lt;/code&gt; method on the result object and I'll get back the
        JSON which I can then parse into a ruby data structure:&lt;/p&gt;

&lt;p class="code-label"&gt;class Engine...
&lt;/p&gt;

&lt;pre&gt;  def exec_request requestHash
    JSON.parse(client.execute!(requestHash).body)
  end
&lt;/pre&gt;

&lt;pre&gt;pp @engine.exec_request(playlist_request)
&lt;/pre&gt;

&lt;pre&gt;# =&amp;gt;
{"kind"=&amp;gt;"youtube#playlistItemListResponse",
 "etag"=&amp;gt;"\"F9iA7pnxqNgrkOutjQAa9F2k8HY/bLvU5-1d6Q4rcW60TlK-JTVNovM\"",
 "pageInfo"=&amp;gt;{"totalResults"=&amp;gt;3, "resultsPerPage"=&amp;gt;5},
 "items"=&amp;gt;
  [{"kind"=&amp;gt;"youtube#playlistItem",
    "etag"=&amp;gt;"\"F9iA7pnxqNgrkOutjQAa9F2k8HY/9bdaIAc39A9cxUknU5cr7i2jzLg\"",
    "id"=&amp;gt;"PL_LBOgO7Mf7ZUfWrfLUl5YJuDy41Yo1j50eP8nIG35Wo",
    "snippet"=&amp;gt;
     {"publishedAt"=&amp;gt;"2014-01-14T23:09:37.000Z",
      "channelId"=&amp;gt;"UC0EbszLD1ceZeAkZ3JEI2GA",
&amp;#8230; lots more
&lt;/pre&gt;

&lt;p&gt;The snippet part contains a title element, so I can print
        out the titles in the returned list by digging a little way
        into the structure.&lt;/p&gt;

&lt;p class="code-label"&gt;class Engine...
&lt;/p&gt;

&lt;pre&gt;  def list_playlist
    request = playlist_request
    response = exec_request(request)
&lt;p class="highlight"&gt;    return response['items'].map{|i| i['snippet']['title']}&lt;/p&gt;
  end
  def exec_request requestHash
    JSON.parse(client.execute!(requestHash).body)
  end
&lt;/pre&gt;


&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:37:31 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:using-oauth-for-a-simple-command-line-script-to-access-googles-data.html</guid></item><item><title>Bliki: DataLake</title><link>http://www.ciandcd.com/bliki-datalake.html</link><description>&lt;div&gt; 

 

 

&lt;p class="tagLabel"&gt;tags:&lt;/p&gt;

&lt;p class="clear"&gt;&lt;/p&gt;

&lt;p&gt;Data Lake is a term that's appeared in this decade to describe
  an important component of the data analytics pipeline in the world of
  &lt;a href="/articles/bigData/"&gt;Big Data&lt;/a&gt;. The idea is to
  have a single store for all of the raw data that anyone in an
  organization might need to analyze. Commonly people use
  Hadoop to work on the data in the lake, but the concept is broader
  than just Hadoop.&lt;/p&gt;

&lt;p&gt;When I hear about a single point to pull together all the data
  an organization wants to analyze, I immediately think of the notion
  of the data warehouse (and data
  mart &lt;a href="#footnote-mart-v-warehouse"&gt;[1]&lt;/a&gt;). But there is a vital distinction between the
  data lake and the data warehouse. The data lake stores &lt;i&gt;raw&lt;/i&gt;
  data, in whatever form the data source provides. There is no
  assumptions about the schema of the data, each data source can use
  whatever schema it likes. It's up to
  the consumers of that data to make sense of that data for their own
  purposes.&lt;/p&gt;

&lt;img src="images/dataLake/contrast.png" width="600px"&gt;
&lt;p&gt;&lt;i&gt;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;This is an important step, many data warehouse initiatives didn't
  get very far because of schema problems. Data warehouses tend to
  go with the notion of a single schema for all analytics needs, but
  I've taken the view that a single unified data model is impractical
  for anything but the smallest organizations. To model even a
  slightly complex domain you need multiple
  &lt;a href="BoundedContext.html"&gt;BoundedContexts&lt;/a&gt;, each with its own data model. In
  analytics terms, you need each analytics user to use a model that
  makes sense for the analysis they are doing. By shifting to storing
  raw data only, this firmly puts the responsibility on the data
  analyst.&lt;/p&gt;

&lt;p&gt;Another source of problems for data warehouse initiatives is
  ensuring data quality. Trying to get an authoritative single source
  for data requires lots of analysis of how the data is acquired and
  used by different systems. System A may be good for some data, and
  system B for another. You run into rules where system A is better
  for more recent orders but system B is better for orders of a month
  or more ago, unless returns are involved. On top of this, data
  quality is often a subjective issue, different analysis has
  different tolerances for data quality issues, or even a different
  notion of what is good quality.&lt;/p&gt;

&lt;p&gt;This leads to a common criticism of the data lake - that it's just a
  dumping ground for data of widely varying quality, better named a
  data swamp. The criticism is both valid and irrelevant. The hot
  title of the New Analytics is "Data Scientist". Although it's a
  much-abused title, many of these folks do have a solid background in
  science. And any serious scientist knows all about data quality
  problems. Consider what you might think is the simple matter of analyzing temperature readings
  over time. You have to take into account that some weather stations
  are relocated in ways that may subtly affect the readings, anomalies due to problems
  in equipment, missing periods when the sensors aren't working. Many
  of the sophisticated statistical techniques out there are created
  to sort out data quality problems. Scientists are always skeptical about
  data quality and are used to dealing with questionable data. So for
  them the lake is important because they get to work with raw data
  and can be deliberate about applying techniques to make sense of it,
  rather than some opaque data cleansing mechanism that probably does
  more harm that good.&lt;/p&gt;

&lt;p&gt;Data warehouses usually would not just cleanse but also aggregate
  the data into a form that made it easier to analyze. But scientists
  tend to object to this too, because aggregation implies throwing
  away data. The data lake should contain all the data because you
  don't know what people will find valuable, either today or in a
  couple of years time. &lt;/p&gt;

&lt;p&gt;One of my colleagues illustrated this thinking with a recent
  example:

  "We were trying to compare our automated predictive models versus
  manual forecasts made by the company's contract managers. To do this
  we decided to train our models on year old data and compare our
  predictions to the ones made by managers at that time. Since we now
  know the correct results, this should be a fair test of accuracy.
  When we started to do this, it appeared that the manager's
  predictions were horrible and that even our simple models, made in
  just two weeks, were crushing them. We suspected that this
  out-performance was too good to be true. After a lot of testing and
  digging we discovered that the time stamps associated with those
  manager predictions were incorrect. They were being modified by some
  end-of-month processing report. So in short, these values in the
  data warehouse were useless; we feared that we would have no way of
  performing this comparison. After more digging we found that these
  reports had been stored and so we could extract the real forecasts
  made at that time. (We're crushing them again but it's taken many
  months to get there)."
  
&lt;/p&gt;

&lt;p&gt;The complexity of this raw data means that there is room for
  something that curates the data into a more manageable structure (as
  well as reducing the considerable volume of data.) The data lake shouldn't be
  accessed directly very much. Because the data is raw, you need a lot
  of skill to make any sense of it. You have relatively few people who
  work in the data lake, as they uncover generally useful views of
  data in the lake, they can create a number of data marts each of which
  has a specific model for a single bounded context. A larger number
  of downstream users can then treat these lakeshore marts as an
  authoritative source for that context. &lt;/p&gt;

&lt;img src="images/dataLake/context.png" width="600px"&gt;
&lt;p&gt;&lt;i&gt;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;So far I've described the data lake as singular point for
  integrating data across an enterprise, but I should mention that
  isn't how it was originally intended. The term &lt;a href="https://jamesdixon.wordpress.com/2010/10/14/pentaho-hadoop-and-data-lakes/"&gt;was
  coined by James Dixon in 2010&lt;/a&gt;, when he did that he intended a
  data lake to be used for a single data source, multiple data sources
  would instead form a "water garden". Despite its original formulation
  the prevalent usage now is to treat a data lake as combining many
  sources. &lt;a href="#footnote-usage"&gt;[2]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You should use a data lake for analytic purposes, not for
  collaboration between operational systems. When operational systems
  collaborate they should do this through services designed for the
  purpose, such as RESTful HTTP calls, or asynchronous messaging. The
  lake is too complex to trawl for operational communication. It may
  be that analysis of the lake can lead to new operational
  communication routes, but these should be built directly rather than
  through the lake.&lt;/p&gt;

&lt;p&gt;It is important that all data put in the lake should have a clear
  provenance in place and time. Every data item should have a clear
  trace to what system it came from and when the data was produced.
  The data lake thus contains a historical record. This might come
  from feeding &lt;a href="/eaaDev/DomainEvent.html"&gt;Domain Events&lt;/a&gt;
  into the lake, a natural fit with &lt;a href="/eaaDev/EventSourcing.html"&gt;Event Sourced&lt;/a&gt; systems. But it could
  also come from systems doing a regular dump of current state into the
  lake - an approach that's valuable when the source system doesn't
  have any temporal capabilities but you want a temporal
  analysis of its data. A consequence of this is that data put into
  the lake is immutable, an observation once stated cannot be removed
  (although it may be refuted later), you should also expect
  &lt;a href="ContradictoryObservations.html"&gt;ContradictoryObservations&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The data lake is schemaless, it's up to the source systems to
  decide what schema to use and for consumers to work out how to deal
  with the resulting chaos. Furthermore the source
  systems are free to change their inflow data schemas at will, and
  again the consumers have to cope. Obviously we prefer such changes
  to be as minimally disruptive as possible, but scientists prefer messy
  data to losing data.&lt;/p&gt;

&lt;p&gt;Data lakes are going to be very large, and much of the storage is
  oriented around the notion of a large schemaless structure - which
  is why Hadoop and HDFS are usually the technologies people use for
  data lakes. One of the vital tasks of the lakeshore marts is to
  reduce the amount of data you need to deal with, so that big data analytics
  doesn't have to deal with large amounts of data.&lt;/p&gt;

&lt;p&gt;The Data Lake's appetite for a deluge of raw data raises awkward
  questions about privacy and security. The principle of
  &lt;a href="Datensparsamkeit.html"&gt;Datensparsamkeit&lt;/a&gt; is very much in tension with the data
  scientists' desire to capture all data now. A data lake makes a
  tempting target for crackers, who might love to siphon choice bits
  into the public oceans. Restricting direct lake access to a small
  data science group may reduce this threat, but doesn't avoid the
  question of how that group is kept accountable for the privacy of
  the data they sail on. &lt;/p&gt;

 

&lt;p class="acknowledgements"&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;


    My thanks to 

    Anand Krishnaswamy, Danilo Sato, David Johnston, Derek Hammer, Duncan Cragg, Jonny Leroy, Ken
    Collier, Shripad Agashe, and Steven Lowe

    for discussing drafts of this post on our internal mailing lists
    
  &lt;/p&gt;

&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/bliki/DataLake.html&amp;amp;text=Bliki:%20DataLake" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/bliki/DataLake.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/bliki/DataLake.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:37:26 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:bliki-datalake.html</guid></item><item><title>Refactoring code that accesses external services</title><link>http://www.ciandcd.com/refactoring-code-that-accesses-external-services.html</link><description>&lt;div&gt;&lt;p class="abstract"&gt;&lt;i&gt;
    When I write code that deals with external services, I find it
    valuable to separate that access code into separate objects. Here
    I show how I would refactor some congealed code into a common
    pattern of this separation.
  &lt;/i&gt;&lt;/p&gt;&lt;p&gt;One of the characteristics of software systems is that they
    don't live on their own. In order to do something useful, they
    usually need to talk to other bits of software, written by
    different people, people that we don't know and who neither know
    or care about the software that we're writing.&lt;/p&gt;

&lt;p&gt;When we're writing software that does this kind of external
    collaboration, I think it's particularly useful to apply good
    modularity and encapsulation. There are common patterns which I
    see and have found valuable in doing this. &lt;/p&gt;

&lt;p&gt;In this article I'll take a simple example, and walk through
    the refactorings that introduce the kind of modularity I'm looking
    for.&lt;/p&gt;


&lt;h2&gt;The starting code&lt;/h2&gt;

&lt;p&gt;The example code's job is to read some data about videos from
      a JSON file, enrich it with data from YouTube, calculate some simple
      further data, and then return the data in JSON.&lt;/p&gt;

&lt;p&gt;Here is the starting code.&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json'))
    ids = @video_list.map{|v| v['youtubeID']}
    client = GoogleAuthorizer.new(
      token_key: 'api-youtube',
      application_name: 'Gateway Youtube Example',
      application_version: '0.1'
      ).api_client
    youtube = client.discovered_api('youtube', 'v3')
    request = {
      api_method: youtube.videos.list,
      parameters: {
        id: ids.join(","),
        part: 'snippet, contentDetails, statistics',
      }
    }
    response = JSON.parse(client.execute!(request).body)
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
      youtube_record = response['items'].find{|v| id == v['id']}
      video['views'] = youtube_record['statistics']['viewCount'].to_i
      days_available = Date.today - Date.parse(youtube_record['snippet']['publishedAt'])
      video['monthlyViews'] = video['views'] * 365.0 / days_available / 12      
    end
    return JSON.dump(@video_list)
  end
&lt;/pre&gt;

&lt;p class="code-remark"&gt;The language for this example is Ruby&lt;/p&gt;

&lt;p&gt;The first thing for me to say here is that there isn't much
      code in this example. If the entire codebase is just this
      script, then you don't have to worry as much about
      modularity. I need a small example, but any reader's eyes
      would glaze over if we looked at a real system. So I have to ask
      you to imagine this code as typical code in a system of tens of
      thousands of lines.&lt;/p&gt;

&lt;p class="skippable"&gt;The access to the YouTube API is mediated through a
      GoogleAuthorizer object which, for this article's purposes, I'm
      going to treat as an external API. It handles the messy
      details of connecting to a Google service (such as YouTube) and
      in particular handles authorization issues. If you want to
      understand how it works, take a look at &lt;a href="/articles/command-line-google.html"&gt;an article I wrote
      recently about accessing Google APIs&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;What's up with this code? You may not understand everything
      this code is doing, but you should be able to see that it mixes
      different concerns, which I've suggested by coloring the code
      example below. In order to make any
      changes you have to comprehend
       
how to access to YouTube's
      API,

how YouTube
      structures its data
, and 
 some domain logic.
&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json'))
    ids = @video_list.map{|v| v['youtubeID']}
&lt;p class="frag-api-access"&gt;    client = GoogleAuthorizer.new(
      token_key: 'api-youtube',
      application_name: 'Gateway Youtube Example',
      application_version: '0.1'
      ).api_client
    youtube = client.discovered_api('youtube', 'v3')
    request = {
      api_method: youtube.videos.list,
      parameters: {
        id: ids.join(","),
        part: &lt;p class="frag-youtube-data"&gt;'snippet, contentDetails, statistics',&lt;/p&gt;
      }
    }
    response = JSON.parse(client.execute!(request).body)&lt;/p&gt;
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
&lt;p class="frag-youtube-data"&gt;      youtube_record = response['items'].find{|v| id == v['id']}&lt;/p&gt;
&lt;p class="frag-youtube-data"&gt;      video['views'] = youtube_record['statistics']['viewCount'].to_i&lt;/p&gt;
      &lt;p class="frag-domain"&gt;days_available = Date.today - &lt;/p&gt;&lt;p class="frag-youtube-data"&gt;Date.parse(youtube_record['snippet']['publishedAt'])&lt;/p&gt;
&lt;p class="frag-domain"&gt;      video['monthlyViews'] = video['views'] * 365.0 / days_available / 12      &lt;/p&gt;
    end
    return JSON.dump(@video_list)
  end
&lt;/pre&gt;

&lt;p&gt;It's common for software mavens like me to talk about
      "separation of concerns" - which basically means different
      topics should be in separate modules. My primary reason for this
      is comprehension: in a well-modularized program each
      module should be about one topic, so I can remain ignorant of
      anything I don't need to understand. Should YouTube's data
      formats change, I shouldn't have to understand the domain logic
      of the application to rearrange the access code. Even if I'm
      making  a change that takes some new data from YouTube and uses
      it in some domain logic, I should be able to split my task into
      those parts and deal with each one separately, minimizing how
      many lines of code I need to keep spinning in my head.&lt;/p&gt;

&lt;p&gt;My refactoring mission is to split these concerns out into
      separate modules. When I'm done the only code in the Video
      Service should be the uncolored code - the code that coordinates
      these other responsibilities.&lt;/p&gt;

&lt;h2&gt;Putting the code under test&lt;/h2&gt;

&lt;p&gt;The first step in refactoring is always the same. You need to
      be confident that you aren't going to inadvertently break
      anything. Refactoring is all about stringing together a large
      set of small steps, all of which are behavior preserving. By
      keeping the steps small, we increase the chances that we don't
      screw up. But I know myself well enough to know I can screw up
      even the simplest change, so to get the confidence I need I have
      to have tests to catch my mistakes.&lt;/p&gt;

&lt;p&gt;But code like this isn't straightforward to test. It would be
      nice to write a test that asserts on the calculated monthly views
      field. After all if anything else goes wrong, this is going to
      give an incorrect answer. But the trouble is that I'm accessing
      live YouTube data, and people have a habit of watching videos.
      The view count field from YouTube will change regularly, causing
      my tests to go red &lt;a href="/articles/nonDeterminism.html"&gt;non-deterministically&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So my first task is to remove that piece of flakiness. I can
      do that by introducing a &lt;a href="http://martinfowler.com/bliki/TestDouble.html"&gt;Test Double&lt;/a&gt;, an object that looks like
      YouTube but instead responds in a deterministic fashion.
      Unfortunately here I run into The Legacy Code Dilemma. &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The Legacy Code Dilemma: When we change code, we should
        have tests in place. To put tests in place, we often have to
        change code.&lt;/p&gt;

&lt;p class="quote-attribution"&gt;&lt;a href="http://www.amazon.com/gp/product/0131177052?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=0131177052"&gt;-- Michael Feathers&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Given that I have to make this change without tests, I need
      to make the smallest and simplest changes I can think of that
      will get the interaction with YouTube behind a seam where I can
      introduce a test double. So my first step is to use &lt;a href="http://refactoring.com/catalog/extractMethod.html"&gt;Extract Method&lt;/a&gt; to get the
      interaction with YouTube separated from the rest of the routine
      by into its own method.&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json'))
    ids = @video_list.map{|v| v['youtubeID']}
&lt;p class="highlight"&gt;    response = call_youtube ids&lt;/p&gt;
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
      youtube_record = response['items'].find{|v| id == v['id']}
      video['views'] = youtube_record['statistics']['viewCount'].to_i
      days_available = Date.today - Date.parse(youtube_record['snippet']['publishedAt'])
      video['monthlyViews'] = video['views'] * 365.0 / days_available / 12      
    end
    return JSON.dump(@video_list)
  end

  def call_youtube ids
    client = GoogleAuthorizer.new(
      token_key: 'api-youtube',
      application_name: 'Gateway Youtube Example',
      application_version: '0.1'
      ).api_client
    youtube = client.discovered_api('youtube', 'v3')
    request = {
      api_method: youtube.videos.list,
      parameters: {
        id: ids.join(","),
        part: 'snippet, contentDetails, statistics',
      }
    }
    return JSON.parse(client.execute!(request).body)
  end
&lt;/pre&gt;

&lt;p&gt;Doing this achieves two things. Firstly it nicely pulls out
      the google API manipulation code into its own method (mostly)
      isolating it from any other kind of code. This on its own is
      worthwhile. Secondly, and more urgently, it sets up a seam which
      I can use to substitute test behavior. Ruby's built in minitest
      library allows me to easily stub individual methods on an object.&lt;/p&gt;

&lt;pre&gt;class VideoServiceTester &amp;lt; Minitest::Test
  def setup
    vs = VideoService.new
    vs.stub(:call_youtube, stub_call_youtube) do
      @videos = JSON.parse(vs.video_list)
      @&amp;#181;S  =  @videos.detect{|v| 'wgdBVIX9ifA' == v['youtubeID']}
      @evo =  @videos.detect{|v| 'ZIsgHs0w44Y' == v['youtubeID']}
    end
  end
  def stub_call_youtube
    JSON.parse(File.read('test/data/youtube-video-list.json'))
  end
  def test_microservices_monthly_json
    assert_in_delta 5880, @&amp;#181;S ['monthlyViews'], 1
    assert_in_delta   20, @evo['monthlyViews'], 1
  end
  # further tests as needed&amp;#8230;
&lt;/pre&gt;

&lt;p&gt;By separating out the YouTube call, and stubbing it, I can
      make this test behave deterministically. Well at least for
      today, for it to work tomorrow I need to do the same thing with
      the call to &lt;code&gt;Date.today&lt;/code&gt;. &lt;/p&gt;

&lt;p class="code-label"&gt;class VideoServiceTester&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def setup
&lt;p class="highlight"&gt;    Date.stub(:today, Date.new(2015, 2, 2)) do&lt;/p&gt;
      vs = VideoService.new
      vs.stub(:call_youtube, stub_call_youtube) do
        @videos = JSON.parse(vs.video_list)
        @&amp;#181;S  =  @videos.detect{|v| 'wgdBVIX9ifA' == v['youtubeID']}
        @evo =  @videos.detect{|v| 'ZIsgHs0w44Y' == v['youtubeID']}
      end
    end
  end
&lt;/pre&gt;

&lt;h2&gt;Separating the remote call into a connection object&lt;/h2&gt;

&lt;p&gt;Separating concerns by putting code into different functions
      is a first level of separation. But when the concerns are as
      different as domain logic and dealing with an external data
      provider, I prefer to increase the level of separation into
      different classes. &lt;/p&gt;

&lt;p class="figureImage"&gt;&lt;a name="no-sep.png"&gt;&lt;/a&gt;&lt;img alt="Figure 1" src="refactoring-external-service/no-sep.png" width="400"&gt;&lt;/p&gt;

&lt;p class="figureCaption"&gt;Figure 1: 
        At the beginning, the video service class contains four responsibilities
      &lt;/p&gt;
&lt;p&gt;My first step is therefore to create a new
      class and use &lt;a href="http://refactoring.com/catalog/moveMethod.html"&gt;Move Method&lt;/a&gt;.&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def call_youtube ids
    YoutubeConnection.new.list_videos ids
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class YoutubeConnection&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def list_videos ids
    client = GoogleAuthorizer.new(
      token_key: 'api-youtube',
      application_name: 'Gateway Youtube Example',
      application_version: '0.1'
      ).api_client
    youtube = client.discovered_api('youtube', 'v3')
    request = {
      api_method: youtube.videos.list,
      parameters: {
        id: ids.join(","),
        part: 'snippet, contentDetails, statistics',
      }
    }
    return JSON.parse(client.execute!(request).body)
  end
&lt;/pre&gt;

&lt;p&gt;With that I can also change the stub so it returns a test
      double rather than simply stubbing the method.&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoServiceTester&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def setup
    Date.stub(:today, Date.new(2015, 2, 2)) do
&lt;p class="highlight"&gt;      YoutubeConnection.stub(:new, YoutubeConnectionStub.new) do&lt;/p&gt;
        @videos = JSON.parse(VideoService.new.video_list)
        @&amp;#181;S  =  @videos.detect{|v| 'wgdBVIX9ifA' == v['youtubeID']}
        @evo =  @videos.detect{|v| 'ZIsgHs0w44Y' == v['youtubeID']}
      end
    end
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class YoutubeConnectionStub&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def list_videos ids
    JSON.parse(File.read('test/data/youtube-video-list.json'))
  end
&lt;/pre&gt;

&lt;p&gt;When doing this refactoring, I have to be wary that my shiny
      new tests
      won't catch any mistakes I make behind the stub, so I have to
      manually ensure that the production code still works. (And yes,
      since you asked, I did make a mistake while doing this (leaving
      off the argument to list-videos). There's a reason I need to
      test so much.)&lt;/p&gt;

&lt;p&gt;The greater separation of concerns you get with a separate
      class also gives you a better seam for testing - I can wrap
      everything that needs to be stubbed into a single object
      creation, which is particularly handy if we need to make
      multiple calls to the same service object during the test.&lt;/p&gt;

&lt;p&gt;With the call to YouTube moved to the connection object, the
      method on the video service isn't worth having any more so I
      subject it to
      &lt;a href="http://refactoring.com/catalog/inlineMethod.html"&gt;Inline Method&lt;/a&gt;.&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json'))
    ids = @video_list.map{|v| v['youtubeID']}
&lt;p class="highlight"&gt;    response = YoutubeConnection.new.list_videos ids&lt;/p&gt;
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
      youtube_record = response['items'].find{|v| id == v['id']}
      video['views'] = youtube_record['statistics']['viewCount'].to_i
      days_available = Date.today - Date.parse(youtube_record['snippet']['publishedAt'])
      video['monthlyViews'] = video['views'] * 365.0 / days_available / 12      
    end
    return JSON.dump(@video_list)
  end

&lt;p class="deleted"&gt;  def call_youtube ids
    YoutubeConnection.new.list_videos ids
  end&lt;/p&gt;
&lt;/pre&gt;

&lt;p&gt;I don't like that my stub has to parse the json string. On
      the whole I like to keep connection objects as &lt;a href="http://xunitpatterns.com/Humble%20Object.html"&gt;Humble Objects&lt;/a&gt;, because any behavior they do isn't tested. So I prefer
      to pull the parsing out into the callers.
      &lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json'))
    ids = @video_list.map{|v| v['youtubeID']}
    response = &lt;p class="highlight"&gt;JSON.parse&lt;/p&gt;(YoutubeConnection.new.list_videos(ids))
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
      youtube_record = response['items'].find{|v| id == v['id']}
      video['views'] = youtube_record['statistics']['viewCount'].to_i
      days_available = Date.today - Date.parse(youtube_record['snippet']['publishedAt'])
      video['monthlyViews'] = video['views'] * 365.0 / days_available / 12      
    end
    return JSON.dump(@video_list)
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class YoutubeConnection&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def list_videos ids
    client = GoogleAuthorizer.new(
      token_key: 'api-youtube',
      application_name: 'Gateway Youtube Example',
      application_version: '0.1'
      ).api_client
    youtube = client.discovered_api('youtube', 'v3')
    request = {
      api_method: youtube.videos.list,
      parameters: {
        id: ids.join(","),
        part: 'snippet, contentDetails, statistics',
      }
    }
    return &lt;p class="deleted"&gt;JSON.parse&lt;/p&gt;(client.execute!(request).body)
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class YoutubeConnectionStub&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def list_videos ids
    &lt;p class="deleted"&gt;JSON.parse&lt;/p&gt;(File.read('test/data/youtube-video-list.json'))
  end
&lt;/pre&gt;

&lt;p class="figureImage"&gt;&lt;a name="sep-connection.png"&gt;&lt;/a&gt;&lt;img alt="Figure 2" src="refactoring-external-service/sep-connection.png"&gt;&lt;/p&gt;

&lt;p class="figureCaption"&gt;Figure 2: 
        The first major step separates the youtube connection code
        into a &lt;b&gt;connection&lt;/b&gt; object.
      &lt;/p&gt;

&lt;h2&gt;Separating the youtube data structure into a Gateway&lt;/h2&gt;

&lt;p&gt;Now that I have the basic connection to YouTube separated and
      stubbable, I can work on the code that delves through the
      YouTube data structures. The problem here is that a bunch of
      code needs to know that to get the view count data, you have to
      look into the "statistics" part of the result, but to get the
      published date you need to delve into "snippet" section instead.
      Such delving is common with data from remote sources, it's
      organized the way that makes sense for them, not for me. This is
      entirely reasonable behavior, they don't have insights into my
      needs, I have enough trouble doing that on my own.
      &lt;/p&gt;

&lt;p&gt;I find that a good way to think about this is Eric Evans's notion of
      &lt;a href="http://martinfowler.com/bliki/BoundedContext.html"&gt;Bounded Context&lt;/a&gt;. YouTube organizes its data
      according to its context, while I want to organize mine
      according to a different one. Code that combines two bounded
      contexts gets convoluted because it's mixing two separate
      vocabularies together. I need to separate them with what Eric
      calls an &lt;i&gt;anti-corruption layer&lt;/i&gt;, a clear boundary between them.
      His illustration of an anti-corruption layer is of the Great
      Wall of China, and as with any wall like this, we need gateways
      that allow some things to pass between them. In software terms a
      gateway allows me to reach through the wall to get the data I
      need from the YouTube bounded context. But the gateway should be
      expressed in a way that makes sense within my context rather
      than theirs.&lt;/p&gt;

&lt;p class="figureImage"&gt;&lt;a name="gateway-sketch.png"&gt;&lt;/a&gt;&lt;img alt="Figure 3" src="refactoring-external-service/gateway-sketch.png" width="400"&gt;&lt;/p&gt;
&lt;p&gt;In this, simple, example, that means a gateway object that
      can give me the published date and view counts without the
      client needing to know how that's stored in the YouTube data
      structure. The gateway object translates from YouTube's context
      into mine.&lt;/p&gt;

&lt;p&gt;I begin by creating a gateway object that I initialize with
      response I got from the connection.&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json'))
    ids = @video_list.map{|v| v['youtubeID']}
&lt;p class="highlight"&gt;    youtube = YoutubeGateway.new(YoutubeConnection.new.list_videos(ids))&lt;/p&gt;
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
      youtube_record &lt;p class="highlight"&gt;= youtube.record(id)&lt;/p&gt;
      video['views'] = youtube_record['statistics']['viewCount'].to_i
      days_available = Date.today - Date.parse(youtube_record['snippet']['publishedAt'])
      video['monthlyViews'] = video['views'] * 365.0 / days_available / 12      
    end
    return JSON.dump(@video_list)
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class YoutubeGateway&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def initialize responseJson
    @data = JSON.parse(responseJson)
  end
  def record id
    @data['items'].find{|v| id == v['id']}
  end
&lt;/pre&gt;

&lt;p&gt;I created the simplest behavior I could at this point, even
      though I don't intend to use the gateway's record method
      eventually, indeed unless I stop for a cup of tea I don't think
      it will last for half-an-hour.&lt;/p&gt;

&lt;p&gt;Now I'll move the delving logic for the views from the
      service into the gateway, creating a separate gateway item class
      to represent each video record.&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json'))
    ids = @video_list.map{|v| v['youtubeID']}
    youtube = YoutubeGateway.new(YoutubeConnection.new.list_videos(ids))
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
      youtube_record = youtube.record(id)
&lt;p class="highlight"&gt;      video['views'] = youtube.item(id)['views']&lt;/p&gt;
      days_available = Date.today - Date.parse(youtube_record['snippet']['publishedAt'])
      video['monthlyViews'] = video['views'] * 365.0 / days_available / 12      
    end
    return JSON.dump(@video_list)
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class YoutubeGateway&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def item id
    {
      'views' =&amp;gt; record(id)['statistics']['viewCount'].to_i
    }
  end
&lt;/pre&gt;

&lt;p&gt;I do the same for the published date&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json'))
    ids = @video_list.map{|v| v['youtubeID']}
    youtube = YoutubeGateway.new(YoutubeConnection.new.list_videos(ids))
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
&lt;p class="deleted"&gt;      youtube_record = youtube.record(id)&lt;/p&gt;
      video['views'] = youtube.item(id)['views']
      days_available = Date.today - &lt;p class="highlight"&gt;youtube.item(id)['published']&lt;/p&gt;
      video['monthlyViews'] = video['views'] * 365.0 / days_available / 12      
    end
    return JSON.dump(@video_list)
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class YoutubeGateway&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def item id
    {
      'views'     =&amp;gt; record(id)['statistics']['viewCount'].to_i,
&lt;p class="highlight"&gt;      'published' =&amp;gt; Date.parse(record(id)['snippet']['publishedAt'])&lt;/p&gt;
    }
  end
&lt;/pre&gt;

&lt;p&gt;Since I'm using the records in the gateway looked up by key,
      I'd like to reflect that usage better in the internal data
      structure, which I can do by replacing the list with a hash&lt;/p&gt;

&lt;p class="code-label"&gt;class                                                                  YoutubeGateway&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def initialize responseJson
&lt;p class="highlight"&gt;    @data = JSON.parse(responseJson)['items']
      .map{|i| [ i['id'], i ] }
      .to_h&lt;/p&gt;
  end
  def item id
    {
      'views' =&amp;gt;  &lt;p class="highlight"&gt;@data[id]&lt;/p&gt;['statistics']['viewCount'].to_i,
      'published' =&amp;gt; Date.parse(&lt;p class="highlight"&gt;@data[id]&lt;/p&gt;['snippet']['publishedAt'])
    }
  end
&lt;p class="deleted"&gt;  def record id
    @data['items'].find{|v| id == v['id']}
  end&lt;/p&gt;

&lt;/pre&gt;

&lt;p class="figureImage"&gt;&lt;a name="sep-gateway.png"&gt;&lt;/a&gt;&lt;img alt="Figure 4" src="refactoring-external-service/sep-gateway.png"&gt;&lt;/p&gt;

&lt;p class="figureCaption"&gt;Figure 4: 
        Separating data handling into a &lt;b&gt;gateway&lt;/b&gt; object
      &lt;/p&gt;
&lt;p&gt;With that done, I've done the key separation that I wanted to
      do. The YouTube connection object handles the calls to YouTube,
      returning a data structure that it gives to the YouTube gateway
      object. The service code is now all about how I want to see the
      data rather than how it's stored in different services. &lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json'))
    ids = @video_list.map{|v| v['youtubeID']}
    youtube = YoutubeGateway.new(YoutubeConnection.new.list_videos(ids))
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
      video['views'] = youtube.item(id)['views']
      days_available = Date.today - youtube.item(id)['published']
      video['monthlyViews'] = video['views'] * 365.0 / days_available / 12      
    end
    return JSON.dump(@video_list)
  end
&lt;/pre&gt;

&lt;h2&gt;Separating the domain logic into a Domain Object&lt;/h2&gt;

&lt;p&gt;Although all the interaction of YouTube is now parceled out
      to separate objects, the video service still mixes its domain
      logic (how to calculate monthly views) from choreographing the
      relationship between the data stored locally and the data in the
      service. If I introduce a domain object for the video, I can
      separate that out.&lt;/p&gt;

&lt;p&gt;My first step is to simply wrap the hash of video data in an
      object.&lt;/p&gt;

&lt;p class="code-label"&gt;class Video&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def initialize aHash
    @data = aHash
  end
  def [] key
    @data[key]
  end
  def []= key, value
    @data[key] = value
  end
  def to_h
    @data
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json'))&lt;p class="highlight"&gt;.map{|h| Video.new(h)}&lt;/p&gt;
    ids = @video_list.map{|v| v['youtubeID']}
    youtube = YoutubeGateway.new(YoutubeConnection.new.list_videos(ids))
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
      video['views'] = youtube.item(id)['views']
      days_available = Date.today - youtube.item(id)['published']
      video['monthlyViews'] = video['views'] * 365.0 / days_available / 12      
    end
    return JSON.dump(@video_list&lt;p class="highlight"&gt;.map{|v| v.to_h}&lt;/p&gt;)
  end
&lt;/pre&gt;

&lt;p&gt;To move the calculation logic into the new video object, I
      first need to get it into the right shape for the move - which I
      can do by parcelling it all into a single method on video service with the video domain
      object and the YouTube gateway item as arguments. The first step
      to that is to use &lt;a href="http://refactoring.com/catalog/extractVariable.html"&gt;Extract Variable&lt;/a&gt; on the
      gateway item.&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json')).map{|h| Video.new(h)}
    ids = @video_list.map{|v| v['youtubeID']}
    youtube = YoutubeGateway.new(YoutubeConnection.new.list_videos(ids))
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
&lt;p class="highlight"&gt;      &lt;p class="highlight"&gt;youtube_item&lt;/p&gt; = youtube.item(id)&lt;/p&gt;
      video['views'] = &lt;p class="highlight"&gt;youtube_item&lt;/p&gt;['views']
      days_available = Date.today - &lt;p class="highlight"&gt;youtube_item&lt;/p&gt;['published']
      video['monthlyViews'] = video['views'] * 365.0 / days_available / 12      
    end
    return JSON.dump(@video_list.map{|v| v.to_h})
  end
&lt;/pre&gt;

&lt;p&gt;With that done I can easily extract the calculation logic into its own method.&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json')).map{|h| Video.new(h)}
    ids = @video_list.map{|v| v['youtubeID']}
    youtube = YoutubeGateway.new(YoutubeConnection.new.list_videos(ids))
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
      youtube_item = youtube.item(id)
&lt;p class="highlight"&gt;      enrich_video video, youtube_item&lt;/p&gt;
    end
    return JSON.dump(@video_list.map{|v| v.to_h})
  end

&lt;p class="highlight"&gt;  def enrich_video video, youtube_item&lt;/p&gt;
      video['views'] = youtube_item['views']
      days_available = Date.today - youtube_item['published']
      video['monthlyViews'] = video['views'] * 365.0 / days_available / 12          
  end
&lt;/pre&gt;

&lt;p&gt;And then it's easy to apply &lt;a href="http://refactoring.com/catalog/moveMethod.html"&gt;Move Method&lt;/a&gt;
       to move it into the video.&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json')).map{|h| Video.new(h)}
    ids = @video_list.map{|v| v['youtubeID']}
    youtube = YoutubeGateway.new(YoutubeConnection.new.list_videos(ids))
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
      youtube_item = youtube.item(id)
&lt;p class="highlight"&gt;      video.enrich_with_youtube youtube_item&lt;/p&gt;
    end
    return JSON.dump(@video_list.map{|v| v.to_h})
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class Video&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;&lt;p class="highlight"&gt;  def enrich_with_youtube youtube_item&lt;/p&gt;
    &lt;p class="highlight"&gt;@data&lt;/p&gt;['views'] = youtube_item['views']
    days_available = Date.today - youtube_item['published']
    &lt;p class="highlight"&gt;@data&lt;/p&gt;['monthlyViews'] = @data['views'] * 365.0 / days_available / 12          
  end

&lt;/pre&gt;

&lt;p&gt;With that done, I can remove the updates to video's hash.&lt;/p&gt;

&lt;p class="code-label"&gt;class Video&amp;#8230;
&lt;/p&gt;

&lt;pre class="deleted"&gt;  def []= key, value
    @data[key] = value
  end
&lt;/pre&gt;

&lt;p&gt;Now I have proper objects, I can simplify the choreography
       with ids in the service method. I start by using &lt;a href="http://refactoring.com/catalog/inlineTemp.html"&gt;Inline Temp&lt;/a&gt; on &lt;code&gt;youtube_item&lt;/code&gt; and then
       replace the reference to the enumeration index with a method
       call on the video object.&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json')).map{|h| Video.new(h)}
    ids = @video_list.map{|v| v['youtubeID']}
    youtube = YoutubeGateway.new(YoutubeConnection.new.list_videos(ids))
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
&lt;p class="deleted"&gt;      youtube_item = youtube.item(id)&lt;/p&gt;
      video.enrich_with_youtube(&lt;p class="highlight"&gt;youtube.item(video.youtube_id))&lt;/p&gt;
    end
    return JSON.dump(@video_list.map{|v| v.to_h})
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class Video&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;&lt;p class="highlight"&gt;  def youtube_id&lt;/p&gt;
    @data['youtubeID']
  end
&lt;/pre&gt;

&lt;p&gt;That allows me to just use the objects directly for the
       enumeration.&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json')).map{|h| Video.new(h)}
    ids = @video_list.map{|v| v['youtubeID']}
    youtube = YoutubeGateway.new(YoutubeConnection.new.list_videos(ids))
&lt;p class="highlight"&gt;    @video_list.each {|v| v.enrich_with_youtube(youtube.item(v.youtube_id))}&lt;/p&gt;
    return JSON.dump(@video_list.map{|v| v.to_h})
  end
&lt;/pre&gt;

&lt;p&gt;And remove the accessor for the hash in the video&lt;/p&gt;

&lt;p class="code-label"&gt;class Video&amp;#8230;
&lt;/p&gt;

&lt;pre class="deleted"&gt;  def [] key
    @data[key]
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json')).map{|h| Video.new(h)}
    ids = @video_list.map{|v| &lt;p class="highlight"&gt;v.youtube_id&lt;/p&gt;}
    youtube = YoutubeGateway.new(YoutubeConnection.new.list_videos(ids))
    @video_list.each {|v| v.enrich_with_youtube(youtube.item(v.youtube_id))}
    return JSON.dump(@video_list.map{|v| v.to_h})
  end
&lt;/pre&gt;

&lt;p&gt;I could replace the video object's internal hash with
       fields, but I don't think it's worth it as it's primarily
       loaded with a hash and its final output is a jsonified hash.
       An &lt;a href="http://martinfowler.com/bliki/EmbeddedDocument.html"&gt;Embedded Document&lt;/a&gt; is a perfectly
       reasonable form of domain object.&lt;/p&gt;

&lt;h2&gt;Musings on the final objects&lt;/h2&gt;

&lt;p class="figureImage"&gt;&lt;a name="separated-dependencies.png"&gt;&lt;/a&gt;&lt;img alt="Figure 5" src="refactoring-external-service/separated-dependencies.png"&gt;&lt;/p&gt;

&lt;p class="figureCaption"&gt;Figure 5: The objects created
       through this refactoring and their dependencies&lt;/p&gt;
&lt;p class="code-label"&gt;class VideoService...
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json')).map{|h| Video.new(h)}
    ids = @video_list.map{|v| v.youtube_id}
    youtube = YoutubeGateway.new(YoutubeConnection.new.list_videos(ids))
    @video_list.each {|v| v.enrich_with_youtube(youtube.item(v.youtube_id))}
    return JSON.dump(@video_list.map{|v| v.to_h})
  end
&lt;/pre&gt;

&lt;pre&gt;&lt;p class="highlight"&gt;class YoutubeConnection&lt;/p&gt;
  def list_videos ids
    client = GoogleAuthorizer.new(
      token_key: 'api-youtube',
      application_name: 'Gateway Youtube Example',
      application_version: '0.1'
      ).api_client
    youtube = client.discovered_api('youtube', 'v3')
    request = {
      api_method: youtube.videos.list,
      parameters: {
        id: ids.join(","),
        part: 'snippet, contentDetails, statistics',
      }
    }
    return client.execute!(request).body
  end
end
&lt;/pre&gt;

&lt;pre&gt;&lt;p class="highlight"&gt;class YoutubeGateway&lt;/p&gt;
  def initialize responseJson
    @data = JSON.parse(responseJson)['items']
      .map{|i| [ i['id'], i ] }
      .to_h
  end
  def item id
    {
      'views' =&amp;gt;  @data[id]['statistics']['viewCount'].to_i,
      'published' =&amp;gt; Date.parse(@data[id]['snippet']['publishedAt'])
    }
  end
end
&lt;/pre&gt;

&lt;pre&gt;&lt;p class="highlight"&gt;class Video&lt;/p&gt;
  def initialize aHash
    @data = aHash
  end
  def to_h
    @data
  end
  def youtube_id
    @data['youtubeID']
  end
  def enrich_with_youtube youtube_item
    @data['views'] = youtube_item['views']
    days_available = Date.today - youtube_item['published']
    @data['monthlyViews'] = @data['views'] * 365.0 / days_available / 12          
  end
end
&lt;/pre&gt;

&lt;p&gt;So what have I achieved? Refactoring often reduces code
       size, but in this case it's almost doubled from 26 to 54 lines.
       All else being equal, less code is better. But here I think the
       better modularity you get by separating the concerns is usually
       worth the size increase. This is also where the size of a pedagogical (i.e.
       toy) example can obscure the point. 26 lines of code isn't much
       to comprehend, but if we are talking about 2600 lines written
       in this style, then the modularity becomes well worth any code
       size increase. And usually any such increase is much smaller
       when you do this kind of thing with a larger code base since  
       you uncover more opportunities to reduce code size by
       eliminating duplication.&lt;/p&gt;

&lt;p&gt;You'll notice I've finished with four kinds of object here:
       coordinator, domain object, gateway, and connection. This is a
       common arrangement of responsibilities, although different cases
       see reasonable variations in how the dependencies are laid out.
       The best arrangement of responsibilities and dependencies
       differs due to particular needs. Code that needs to change
       frequently should be separated from code that changes rarely,
       or simply changes for different reasons. Code that is widely
       reused should not depend on code that is used only for a
       particular case. These drivers differ from circumstance to
       circumstance, and dicate the dependency patterns.&lt;/p&gt;

&lt;p&gt;One common change is to reverse the dependency between
       domain object and gateway - turning the gateway into a &lt;a href="http://martinfowler.com/eaaCatalog/mapper.html"&gt;Mapper&lt;/a&gt;.
       This allows the domain object to be independent of how it's
       populated, at the cost of the mapper knowing about the domain
       object and getting some access to its guts. If the domain
       object is used in many contexts, then this can be a valuable arrangement.&lt;/p&gt;

&lt;p&gt;Another change might be to shift the code for calling the
       connection from the coordinator to the gateway. This simplifies
       the coordinator but makes the gateway a bit more complex.
       Whether this is a good idea depends on whether the coordinator
       is getting too complex, or if many coordinators use the same
       gateway leading to duplicate code in setting up the connection.&lt;/p&gt;

&lt;p&gt;I also think it's likely I'd move some of the behavior of
       the connection out to the caller, particularly if the caller is
       a gateway object. The gateway knows what data it needs, so
       should supply the list of parts in the parameters of the call.
       But that's really only an issue once we have other clients
       calling &lt;code&gt;list_videos&lt;/code&gt;, so I'd be inclined to wait until that day. &lt;/p&gt;

&lt;p&gt;One thing that I feel is important, whatever the details of
       your case, is to have a consistent naming policy for the roles
       of the objects involved. I sometimes hear people say that you
       shouldn't put pattern names into your code, but I don't agree.
       Often pattern names help to communicate roles different
       elements play, so it's silly to spurn the opportunity.
       Certainly within a team your code will show common patterns and
       the naming should reflect that. I use "gateway" following my
       coining of the &lt;a href="http://martinfowler.com/eaaCatalog/gateway.html"&gt;Gateway&lt;/a&gt; pattern in P of
       EAA. I've used "connection" here to show the raw link to an
       external system, and intend to use that convention in my
       future writing. This naming convention isn't universal and while my
       pride would be gratifyingly inflated by you using my naming
       conventions, the important point isn't which naming convention
       you should use but that you should pick some convention.&lt;/p&gt;

&lt;p&gt;When I break a method into a group of objects like this,
       there's a natural question about the consequences for testing.
       I had a unit test for the original method in the video service,
       should I now write tests for the three new classes? My
       inclination is that, providing the existing tests cover the
       behavior sufficiently, there isn't a need to add any more right
       away. As we add more behavior, then we should add more
       tests, and if this behavior is added to the new objects then
       the new tests will focus on them. In time that may mean that
       some of tests currently targeting the video service will look
       out of place and should move. But all of this in the future and
       should be dealt with in the future.&lt;/p&gt;

&lt;p&gt;A particular concern I'd be watching for in the tests is the
       use of the stubs I put in over the YouTube connection. It's
       very easy for stubs like this to get out of hand, then they can
       actually slow down changes because a simple production code
       change leads to updating many tests. The essence here is to pay
       attention to duplication in the test code and address it as
       seriously as you do with duplication in production code.&lt;/p&gt;

&lt;p&gt;Such thinking about organizing test doubles naturally leads
       into the broader question of assembling service
       objects. Now that I have split a behavior from a single service
       object into three service objects and a domain entity (using
       the &lt;a href="http://martinfowler.com/bliki/EvansClassification.html"&gt;Evans Classification&lt;/a&gt;) there's a natural
       question about how the service objects should be instantiated,
       configured, and assembled. Currently the video service
       does this directly for its dependents, but this can easily get
       out of hand with larger systems. To handle this complexity it's
       common to use techniques such as &lt;a href="/articles/injection.html"&gt;Service Locators and
       Dependency Injection&lt;/a&gt;. I'm not going to talk about that
       right now, but that may be the topic for a follow-on article.&lt;/p&gt;

&lt;p&gt;This example uses objects, in large part because I'm far
       more familiar with object-oriented style than functional
       styles. But I would expect the fundamental division of
       responsibilities to be the same, but with boundaries set by
       function (or perhaps namespace) rather than classes and
       methods. Some other details would change, the video object
       would be a data structure and enriching it would create new
       data structures rather than modifying in place. Looking at this
       in a functional style would be an interesting article.&lt;/p&gt;

&lt;p&gt;Finally I want to re-stress an important general point about
       refactoring. Refactoring isn't a term you should use to any
       restructuring of a code base. It specifically means the
       approach that applies a series of very small
       behavior-preserving changes. We saw a couple of examples here
       where I deliberately introduced code that I knew I was going to
       remove shortly afterwards, just so I can take small steps that
       preserve behavior. This is the essence of refactoring, as I
       recently tweeted:&lt;/p&gt;

&lt;blockquote class="twitter-tweet" lang="en"&gt;Refactoring is a specific way to
           change code by a series of tiny behavior-preserving
       transformations. It is not just moving code around.&lt;a href="https://twitter.com/martinfowler/status/564813269102493696"&gt;-- Martin Fowler&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;The point here is that by taking small steps, you end up
       going faster because you don't break anything and thus avoid
       debugging. Most people find this counter-intuitive, I certainly
       did when Kent Beck first showed me how he refactored. But I quickly
       discovered how effective it is.&lt;/p&gt;


&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/articles/refactoring-external-service.html&amp;amp;text=Refactoring%20code%20that%20accesses%20external%20services" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/articles/refactoring-external-service.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/articles/refactoring-external-service.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;For articles on similar topics&amp;#8230;&lt;/h2&gt;

&lt;p&gt;&amp;#8230;take a look at the following tags:&lt;/p&gt;

 
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:37:24 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:refactoring-code-that-accesses-external-services.html</guid></item><item><title>Retreaded: ConversationalStories</title><link>http://www.ciandcd.com/retreaded-conversationalstories.html</link><description>&lt;div&gt; 

 

 

&lt;p class="tagLabel"&gt;tags:&lt;/p&gt;

&lt;p class="clear"&gt;&lt;/p&gt;

&lt;p&gt;Here's a common misconception about agile methods. It centers on
  the way user stories are created and flow through the development
  activity. The misconception is that the product owner (or business
  analysts) creates user stories and then put them in front of
  developers to implement. The notion is that this is a flow from
  product owner to development, with the product owner responsible for
  determining &lt;i&gt;what&lt;/i&gt; needs to be done and the developers
  &lt;i&gt;how&lt;/i&gt; to do it.&lt;/p&gt;
&lt;img src="images/conversationalStories/decreed.png"&gt;
&lt;p&gt;A justification for this approach is that this separates the
  responsibilities along the lines of competence. The product owner
  knows the business, what the software is for, and thus what needs to
  be done. The developers know technology and know how to do things,
  so they can figure out how to realize the demands of the product
  owner.&lt;/p&gt;

&lt;p&gt;This notion of product owners coming up with
  &lt;a href="DecreedStories.html"&gt;DecreedStories&lt;/a&gt; is a profound misunderstanding of the way
  agile development should work. When we were brainstorming names at
  &lt;a href="http://martinfowler.com/articles/agileStory.html"&gt;Snowbird&lt;/a&gt;, I
  remember Kent suggesting "conversational". This emphasized the fact
  that the heart of our thinking was of an on-going conversation
  between customers and developers about how a development project
  should proceed.&lt;/p&gt;
&lt;img src="images/conversationalStories/conversation.png"&gt;
&lt;p&gt;In terms of coming up with stories, what this means is that they
  are always something to be refined through conversation - and that
  developers should play an active role in helping that
  definition.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;spotting inconsistencies and gaps between the stories&lt;/li&gt;

&lt;li&gt;using technical knowledge to come up with new stories that
    seem to fit the product owner's vision&lt;/li&gt;

&lt;li&gt;seeing alternative stories that would be cheaper to build
    given the technological landscape&lt;/li&gt;

&lt;li&gt;split stories to make them easier to plan or implement&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is the Negotiable principle in Bill Wake's &lt;a href="http://xp123.com/xplor/xp0308"&gt;INVEST&lt;/a&gt; test for
  stories. Any member of an agile team can create stories and suggest
  modifications. It may be that just a few members of a team gravitate
  to writing most of the stories. That's up to the team's
  self-organization as to how they want that to happen. But everyone
  should be engaged in coming up and refining stories. (This
  involvement is in addition to the develpers' responsibility to
  estimate stories.)&lt;/p&gt;

&lt;p&gt;The product owner does have a special responsibility. In the end
  the product owner is the final decider on stories, particularly
  their prioritization. This reflects the fact that the product owner
  should be the best person to judge that slippery attribute of
  business value. But having a final decision maker should never stop
  others from participating, and should not lead people astray into a
  decreed model of stories.&lt;/p&gt;

&lt;p class="repost"&gt;reposted on 19 Feb 2015&lt;/p&gt;

&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/bliki/ConversationalStories.html&amp;amp;text=Bliki:%20ConversationalStories" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/bliki/ConversationalStories.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/bliki/ConversationalStories.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:37:22 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:retreaded-conversationalstories.html</guid></item><item><title>Bliki: BeckDesignRules</title><link>http://www.ciandcd.com/bliki-beckdesignrules.html</link><description>&lt;div&gt; 

 

 

&lt;p class="tagLabel"&gt;tags:&lt;/p&gt;

&lt;p class="clear"&gt;&lt;/p&gt;

&lt;p&gt;Kent Beck came up with his four rules of simple design while he
  was developing &lt;a href="ExtremeProgramming.html"&gt;ExtremeProgramming&lt;/a&gt; in the late 1990's. I express
  them like this. &lt;a href="#footnote-xp-formulation"&gt;[1]&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Passes the tests&lt;/li&gt;

&lt;li&gt;Reveals intention&lt;/li&gt;

&lt;li&gt;No duplication&lt;/li&gt;

&lt;li&gt;Fewest elements&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The rules are in priority order, so "passes the tests" takes
  priority over "reveals intention"&lt;/p&gt;

&lt;img src="images/beckDesignRules/kent.jpg" width="200px"&gt;
&lt;p&gt;&lt;i&gt;
    Kent Beck developed Extreme Programming, Test Driven Development,
    and can always be relied on for good Victorian facial hair for his
    local ballet.
  &lt;/i&gt;&lt;/p&gt;
&lt;p&gt;The most important of the rules is "passes the tests". XP was
  revolutionary in how it raised testing to a first-class activity in
  software development, so it's natural that testing should play a
  prominent role in these rules. The point is that whatever else you
  do with the software, the primary aim is that it works as
  intended and tests are there to ensure that happens.&lt;/p&gt;

&lt;p&gt;"Reveals intention" is Kent's way of saying the code
  should be easy to understand. Communication is a core value of
  Extreme Programing, and many programmers like to stress that
  programs are there to be read by people. Kent's form of expressing
  this rule implies that the key to enabling understanding is to express your
  intention in the code, so that your readers can understand what your
  purpose was when writing it.&lt;/p&gt;

&lt;p&gt;The "no duplication" is perhaps the most powerfully subtle of
  these rules. It's a notion expressed elsewhere as DRY or SPOT &lt;a href="#footnote-dry"&gt;[2]&lt;/a&gt;, Kent
  expressed it as saying everything should be said "Once and only Once."
  Many programmers have observed that the exercise of eliminating
  duplication is a powerful way to drive out good designs. &lt;a href="#footnote-dup-column"&gt;[3]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The last rule tells us that anything that doesn't serve the three
  prior rules should be removed. At the time these rules were
  formulated there was a lot of design advice around adding elements to
  an architecture in order to increase flexibility for future requirements.
  Ironically the extra complexity of all of these elements usually
  made the system harder to modify and thus less flexible in practice.&lt;/p&gt;

&lt;p&gt;People often find there is some tension between "no duplication"
  and "reveals intention", leading to arguments about which order
  those rules should appear. I've always seen their order as
  unimportant, since they feed off each other in refining the code. Such things
  as adding duplication to increase clarity is often papering over a problem,
  when it would be better to solve it. &lt;a href="#footnote-kent-empathy"&gt;[4]&lt;/a&gt;&lt;/p&gt;

&lt;img src="images/beckDesignRules/sketch.png" width="500px"&gt;
&lt;p&gt;&lt;i&gt;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;What I like about these rules is that they are very simple to
  remember, yet following them improves code in any language or
  programming paradigm that I've worked with. They are an example of
  Kent's skill in finding principles that are generally applicable and
  yet concrete enough to shape my actions.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;At the time there was a lot of &amp;#8220;design is subjective&amp;#8221;, &amp;#8220;design is
  a matter of taste&amp;#8221; bullshit going around. I disagreed. There are
  better and worse designs. These criteria aren&amp;#8217;t perfect, but they
  serve to sort out some of the obvious crap and (importantly) you can
  evaluate them right now. The real criteria for quality of design,
  &amp;#8220;minimizes cost (including the cost of delay) and maximizes benefit
  over the lifetime of the software,&amp;#8221; can only be evaluated post hoc,
  and even then any evaluation will be subject to a large bag full of
  cognitive biases. The four rules are generally predictive.&lt;/p&gt;

&lt;p class="quote-attribution"&gt;-- Kent Beck&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2&gt;Further Reading&lt;/h2&gt;

&lt;p&gt;There are many expressions of these rules out there, here are a
    few that I think are worth exploring:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.jbrains.ca/permalink/the-four-elements-of-simple-design"&gt;J.B.
      Rainsberger's summary&lt;/a&gt;. He also has a good discussion of &lt;a href="http://blog.thecodewhisperer.com/2013/12/07/putting-an-age-old-battle-to-rest/"&gt;the
      interplay between the rules 2&amp;amp;3.&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href="http://xprogramming.com/classics/expemergentdesign/"&gt;Ron Jeffries&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;These rules, like much else of Extreme Programming, were
      originally discussed and refined &lt;a href="http://c2.com/cgi/wiki?XpSimplicityRules"&gt;on Ward's Wiki&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;Kent reviewed this post and sent me some very helpful feedback,
    much of which I appropriated into the text.&lt;/p&gt;
&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/bliki/BeckDesignRules.html&amp;amp;text=Bliki:%20BeckDesignRules" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/bliki/BeckDesignRules.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/bliki/BeckDesignRules.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:37:21 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:bliki-beckdesignrules.html</guid></item><item><title>Retreaded: CodeAsDocumentation</title><link>http://www.ciandcd.com/retreaded-codeasdocumentation.html</link><description>&lt;div&gt; 

 

 

&lt;p class="tagLabel"&gt;tags:&lt;/p&gt;

&lt;p class="clear"&gt;&lt;/p&gt;

&lt;p&gt;One of the common elements of agile methods is that they raise
	 programming to a central role in software
	development - one much greater than the software engineering
	community usually does. Part of this is classifying the code as a
	major, if not the primary documentation of a software system.&lt;/p&gt;

&lt;p&gt;Almost immediately I feel the need to rebut a common
	misunderstanding. Such a principle is not saying that code is the
	only documentation. Although I've often heard this said of Extreme
	Programming - I've never heard the leaders of the Extreme
	Programming movement say this. Usually there is a need for
	further documentation to act as a supplement to the code. &lt;/p&gt;

&lt;p&gt;The rationale for the code being the primary source of
	documentation is that it is the only one that is sufficiently
	detailed and precise to act in that role - a point made so
	eloquently by Jack Reeves's famous essay &lt;a href="http://www.developerdotstar.com/mag/articles/reeves_design_main.html"&gt;"What
  is Software Design?"&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This principle comes with a important consequence - that it's
	important that programmers put in the effort to make sure that this
	code is clear and readable. Saying that code is documentation isn't
	saying that a particular code base is good documentation. Like any
	documentation, code can be clear or it can be gibberish. Code is no
	more inherently clear than any other form of documentation. (And
	other forms of documentation can be hopelessly unclear too - I've
	seen plenty of gibberish UML diagrams, to flog a popular horse.)&lt;/p&gt;

&lt;p&gt;Certainly it seems that most code bases aren't very good
	documentation. But just as it's a fallacy to conclude that declaring
	code to be documentation excludes other forms, it's a fallacy to say
	that because code is often poor documentation means that it's
	&lt;i&gt;necessarily&lt;/i&gt; poor. It is possible to write clear code, indeed I'm
	convinced that most code bases can be made much more clear.&lt;/p&gt;

&lt;p&gt;I think part of the reason that code is often so hard to read is
	because people aren't taking it seriously as documentation. If
	there's no will to make code clear, then there's little chance it
	will spring into clarity all by itself. So the first step to clear
	code is to accept that code is documentation, and then put the
	effort in to make it be clear. I think this comes down to what was
	taught to most programmers when they began to program. My teachers didn't put much emphasis on
	making code clear, they didn't seem to value it and certainly didn't
	talk about how to do it. We as a whole industry need to put much
	more emphasis on valuing the clarity of code.&lt;/p&gt;

&lt;p&gt;The next step is to learn how, and here let me offer you the
	advice of a best selling technical author - there's nothing like
	review. I would never think of publishing a book without having many
	people read it and give me feedback. Similarly there's nothing more
	important to clear code than getting feedback from others about
	what is or isn't easy to understand. So take every opportunity to
	find ways to get other people to read your code. Find out what they
	find easy to understand, and what things confuse them. (Yes, pair
	programming is a great way to do this.)&lt;/p&gt;

&lt;p&gt;For more concrete advice - well I suggest reading good books on
	programming style. &lt;a href="http://www.amazon.com/gp/product/0735619670?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=0735619670"&gt;Code Complete&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt; is the first place to look. I'll
	naturally suggest &lt;a href="http://martinfowler.com/books/refactoring.html"&gt;Refactoring&lt;/a&gt; - after all much of refactoring is
	about making code clearer. After Refactoring, &lt;a href="http://martinfowler.com/books/r2p.html"&gt;Refactoring to
	Patterns&lt;/a&gt; is an obvious suggestion. &lt;/p&gt;

&lt;p&gt;You'll always find people will disagree on various
	points. Remember that a code base is owned primarily by a team (even
	if you practice individual code ownership over bits of it). A
	professional programmer is prepared to bend her personal style to
	reflect the needs of the team. So even if you like ternary operators
	don't use them if your team doesn't find them easy to
	understand. You can program in your own style on your personal
	projects, but anything you do in a team should follow the needs of
	that team. &lt;/p&gt;

&lt;p class="repost"&gt;reposted on 25 Mar 2015&lt;/p&gt;

&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/bliki/CodeAsDocumentation.html&amp;amp;text=Bliki:%20CodeAsDocumentation" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/bliki/CodeAsDocumentation.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/bliki/CodeAsDocumentation.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:37:17 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:retreaded-codeasdocumentation.html</guid></item><item><title>Bliki: MicroservicePremium</title><link>http://www.ciandcd.com/bliki-microservicepremium.html</link><description>&lt;div&gt; 

 

 

&lt;p class="tagLabel"&gt;tags:&lt;/p&gt;

&lt;p class="clear"&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href="/articles/microservices.html"&gt;microservices
  architectural style&lt;/a&gt; has been the hot topic over the last year.
  At the recent &lt;a href="http://softwarearchitecturecon.com/sa2015"&gt;O'Reilly software
  architecture conference&lt;/a&gt;, it seemed like every session talked
  about microservices. Enough to get everyone's over-hyped-bullshit
  detector up and flashing. One of the consequences of this is that
  we've seen teams be too eager to embrace microservices, &lt;a href="#footnote-envy"&gt;[1]&lt;/a&gt; not
  realizing that microservices introduce complexity on their own
  account. This adds a premium to a project's cost and risk - one that
  often gets projects into serious trouble. &lt;/p&gt;

&lt;p&gt;While this hype around microservices is annoying, I do think it's a
  useful bit of terminology for a style of architecture which has been
  around for a while, but needed a name to make it easier to talk
  about. The important thing here is not how annoyed you feel about the
  hype, but the architectural question it raises: &lt;b&gt;is a microservice
  architecture a good choice for the system
  you're working on?&lt;/b&gt;&lt;/p&gt;

&lt;blockquote class="twitter-tweet" lang="en"&gt;
    any decent answer to an interesting question begins, "it depends..."
  &lt;a href="https://twitter.com/KentBeck/status/596007846887628801"&gt;-- Kent Beck&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;"It depends" must start my answer, but then I must shift the
  focus to what factors it depends &lt;i&gt;on&lt;/i&gt;. The fulcrum of whether
  or not to use microservices is the complexity of the system you're
  contemplating. The microservices approach is all about handling a
  complex system, but in order to do so the approach introduces its
  own set of complexities. When you use microservices you have to work
  on automated deployment, monitoring, dealing with failure, eventual
  consistency, and other factors that a distributed system introduces.
  There are well-known ways to cope with all this, but it's extra
  effort, and nobody I know in software development seems to have
  acres of free time.&lt;/p&gt;
&lt;img src="images/microservice-verdict/productivity.png"&gt;
&lt;p&gt;So my primary guideline would be &lt;b&gt;don't even consider
  microservices unless you have a system that's too complex to manage
  as a monolith&lt;/b&gt;. The majority of software systems should be built
  as a single monolithic application. Do pay attention to good
  modularity within that monolith, but don't try to separate it into
  separate services.&lt;/p&gt;

&lt;p&gt;The complexity that drives us to microservices can come from many
  sources including
  dealing with large teams &lt;a href="#footnote-conway"&gt;[2]&lt;/a&gt;, &lt;a href="http://samnewman.io/blog/2015/05/05/single-tenancy-vs-multi-tenancy/"&gt;multi-tenancy&lt;/a&gt;,
  supporting many
  user interaction models, allowing different business functions to
  evolve independently, and scaling. But the biggest factor
  is that of sheer size - people finding they have a monolith that's too big
  to modify and deploy.&lt;/p&gt;

&lt;p&gt;At this point I feel a certain frustration. Many of the problems
  ascribed to monoliths aren't essential to that style. I've heard people say that
  you need to use microservices because it's impossible to do
  &lt;a href="ContinuousDelivery.html"&gt;ContinuousDelivery&lt;/a&gt; with monoliths - yet there are plenty of
  organizations that succeed with a &lt;a href="http://paulhammant.com/2011/11/29/cookie-cutter-scaling/"&gt;cookie-cutter
  deployment&lt;/a&gt; approach: Facebook and Etsy are two well-known
  examples.&lt;/p&gt;

&lt;p&gt;I've also heard arguments that say that as a system increases in
  size, you have to use microservices in order to have parts that are
  easy to modify and replace. Yet there's no reason why you can't make
  a single monolith with well defined module boundaries. At least
  there's no reason &lt;i&gt;in theory&lt;/i&gt;, in practice it seems too easy for
  module boundaries to be breached and monoliths to get tangled as
  well as large.&lt;/p&gt;

&lt;p&gt;We should also remember that there's a substantial variation in
  service-size between different microservice systems. I've seen
  microservice systems vary from a team of 60 with 20 services to a
  team of 4 with 200 services. It's not clear to what degree service
  size affects the premium.&lt;/p&gt;

&lt;p&gt;As size and other complexity boosters kick into a project I've
  seen many teams find that microservices are a better place to be.
  But unless you're faced with that complexity, remember that the
  microservices approach brings a high premium, one that can slow down
  your development considerably. So if you can keep your system simple
  enough to avoid the need for microservices: do.&lt;/p&gt;

 

&lt;p class="acknowledgements"&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;

    I stole much of this thinking from my colleagues: James Lewis, Sam
    Newman, Thiyagu Palanisamy, and Evan Bottcher. Stefan Tilkov's
    comments on an earlier draft were instrumental in sharpening this post. Rob
    Miles, David Nelson, Brian Mason, and Scott Robinson discussed
    drafts of this article on our internal mailing list.
  &lt;/p&gt;

&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/bliki/MicroservicePremium.html&amp;amp;text=Bliki:%20MicroservicePremium" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/bliki/MicroservicePremium.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/bliki/MicroservicePremium.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:37:16 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:bliki-microservicepremium.html</guid></item><item><title>Bliki: Yagni</title><link>http://www.ciandcd.com/bliki-yagni.html</link><description>&lt;div&gt; 

 

 

&lt;p class="tagLabel"&gt;tags:&lt;/p&gt;

&lt;p class="clear"&gt;&lt;/p&gt;

&lt;p&gt;Yagni originally is an acronym that stands for "You Aren't Gonna
  Need It". It is a mantra from &lt;a href="ExtremeProgramming.html"&gt;ExtremeProgramming&lt;/a&gt;
  that's often used generally in agile software teams. It's a
  statement that some capability we presume our software needs in the future
  should not be built now because "you aren't gonna need it". &lt;/p&gt;

&lt;p&gt;Yagni is a way to refer to the XP practice of Simple Design (from
  the first edition of &lt;a href="http://www.amazon.com/gp/product/0321278658?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=0321278658"&gt;The White Book&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt;, the second edition refers to
  the related notion of "incremental design"). &lt;a href="#footnote-origin"&gt;[1]&lt;/a&gt; Like many elements of XP, it's a sharp contrast to
  elements of the widely held principles of software engineering in
  the late 90s. At that time there was a big push for careful up-front
  planning of software development.&lt;/p&gt;

&lt;p&gt;Let's imagine I'm working with a startup in Minas Tirith selling
  insurance for the shipping business. Their software system is broken
  into two main components: one for pricing, and one for sales. The
  dependencies are such that they can't usefully build
  sales software until the relevant pricing software is completed.&lt;/p&gt;

&lt;p&gt;At the moment, the team is working on updating the pricing
  component to add support for risks from storms. They know that in six
  months time, they will need to also support pricing for piracy
  risks. Since they are currently working on the pricing engine they consider
  building the presumptive feature &lt;a href="#footnote-presumptive-feature"&gt;[2]&lt;/a&gt; for piracy pricing now, since that way the pricing
  service will be complete before they start working on the sales
  software.&lt;/p&gt;

&lt;p&gt;Yagni argues against this, it says that since you won't need
  piracy pricing for six months you shouldn't build it until it's
  necessary. So if you think it will take two months to build this
  software, then you shouldn't start for another four months
  (neglecting any buffer time for schedule risk and updating the sales
  component). &lt;/p&gt;

&lt;p&gt;The first argument for yagni is that while we may now think we
  need this presumptive feature, it's likely that we will be wrong.
  After all the context of agile methods is an acceptance that we
  welcome changing requirements. A plan-driven requirements guru might
  counter argue that this is because we didn't do a good-enough job of
  our requirements analysis, we should have put more time and effort
  into it. I counter that by pointing out how difficult and costly it is to
  figure out your needs in advance, but even if you can, you can still
  be blind-sided when the Gondor Navy wipes out the pirates, thus
  undermining the entire business model.&lt;/p&gt;

&lt;p&gt;In this case, there's an obvious cost of the presumptive feature
  - the &lt;b&gt;cost of build&lt;/b&gt;: all the effort spent on analyzing, programming,
  and testing this now useless feature.&lt;/p&gt;

&lt;p&gt;But let's consider that we were completely correct with our
  understanding of our needs, and the Gondor Navy didn't wipe out the
  pirates. Even in this happy case, building the
  presumptive feature incurs two
  serious costs. The first cost is the cost of delayed value. By
  expending our effort on the piracy pricing software we didn't build
  some other feature.  If we'd instead put our energy into building
  the sales software for weather risks, we could have put a full
  storm risks feature into production and be generating revenue two
  months earlier. This &lt;b&gt;cost of delay&lt;/b&gt; due to the presumptive feature is
  two months revenue from storm insurance.&lt;/p&gt;

&lt;p&gt;The common reason why people build presumptive features is
  because they think it will be cheaper to build it now rather than
  build it later. But that cost comparison has to be made at least
  against the cost of delay,  preferably factoring in the
  probability that you're building an unnecessary feature, for which
  your odds are at least &amp;#8532;. &lt;a href="#footnote-kohavi"&gt;[3]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Often people don't think through the comparative cost of building
  now to building later. One approach I use when mentoring developers
  in this situation is to ask them to imagine any refactoring they
  would have to do later to introduce the capability when it's needed.
  Often that thought experiment is enough to convince them that it
  won't be significantly more expensive to add it later. Another
  result from such an imagining is to add something that's easy to do
  now, adds minimal complexity, yet significantly reduces the later
  cost. Using lookup tables for error messages rather than inline
  literals are an example that are simple yet
  make later translations easier to support.&lt;/p&gt;

&lt;blockquote class="twitter-tweet" lang="en"&gt;
    Reminder, any extensibility point that&amp;#8217;s never used isn&amp;#8217;t just
    wasted effort, it&amp;#8217;s likely to also get in your way as well
  &lt;a href="https://twitter.com/jeremydmiller/status/568797862441586688"&gt;-- Jeremy Miller&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;The cost of delay is one cost that a successful presumptive
  feature imposes, but another is the &lt;b&gt;cost of carry&lt;/b&gt;. The code for the
  presumptive feature adds some complexity to the software, this
  complexity makes it harder to modify and debug that software, thus
  increasing the cost of other features. The extra complexity from
  having the piracy-pricing feature in the software might add a couple
  of weeks to how long it takes to build the storm insurance sales
  component. That two weeks hits two ways: the additional cost to
  build the feature, plus the additional cost of delay since it look
  longer to put it into production. We'll incur a cost of carry on every
  feature built between now and the time the piracy insurance software
  starts being useful. Should we never need the piracy-pricing
  software, we'll incur a cost of carry on every feature built until
  we remove the piracy-pricing feature (assuming we do), together with
  the cost of removing it.&lt;/p&gt;

&lt;p&gt;So far I've divided presumptive features in two categories:
  successful and unsuccessful. Naturally there's really a spectrum
  there, and with one point on that spectrum that's worth highlighting: the right
  feature built wrong. Development teams are always learning, both
  about their users and about their code base. They learn about the
  tools they're using and these tools go through regular upgrades. They
  also learn about how their code works together. All this means that
  you often realize that a feature coded six months ago wasn't done
  the way you now realize it should be done. In that case you have
  accumulated &lt;a href="TechnicalDebt.html"&gt;TechnicalDebt&lt;/a&gt; and have to
  consider the &lt;b&gt;cost of repair&lt;/b&gt; for that feature or the on-going
  costs of working around its difficulties.&lt;/p&gt;

&lt;p&gt;So we end up with three classes of presumptive features, and four
  kinds of costs that occur when you neglect yagni for them.&lt;/p&gt;
&lt;img src="images/yagni/sketch.png"&gt;
&lt;p&gt;My insurance example talks about relatively user-visible
  functionality, but the same argument applies for abstractions to
  support future flexibility. When building the storm risk calculator,
  you may consider putting in abstractions and parameterizations now
  to support piracy and other risks later. Yagni says not to do this,
  because you may not need the other pricing functions, or if you do your
  current ideas of what abstractions you'll need will not match what
  you learn when you do actually need them. This doesn't mean to
  forego all abstractions, but it does mean any abstraction that makes
  it harder to understand the code for current requirements is
  presumed guilty.&lt;/p&gt;

&lt;p&gt;Yagni is at its most visible with larger features, but you see it
  more frequently with small things. Recently I wrote some code that
  allows me to highlight part of a line of code. For this, I allow the
  highlighted code to be specified using a regular expression. One
  problem I see with this is that since the whole regular expression
  is highlighted, I'm unable to deal with the case where I need the
  regex to match a larger section than what I'd like to highlight. I
  expect I can solve that by using a group within the regex and
  letting my code only highlight the group if a group is present. But I
  haven't needed to use a regex that matches more than what I'm
  highlighting yet, so I haven't extended my highlighting code to
  handle this case - and won't until I actually need it. For similar
  reasons I don't add fields or methods until I'm actually ready to
  use them.&lt;/p&gt;

&lt;p&gt;Small yagni decisions like this fly under the radar of project
  planning. As a developer it's easy to spend an hour adding an abstraction
  that we're sure will soon be needed. Yet all the arguments above
  still apply, and a lot of small yagni decisions add up to
  significant reductions in complexity to a code base, while speeding
  up delivery of features that are needed more urgently.&lt;/p&gt;

&lt;p&gt;Now we understand why yagni is important we can dig into a common
  confusion about yagni. &lt;b&gt;Yagni only applies to capabilities built
  into the software to support a presumptive feature, it does not
  apply to effort to make the software easier to modify.&lt;/b&gt; Yagni is
  only a viable strategy if the code is easy to change, so expending
  effort on refactoring isn't a violation of yagni because refactoring
  makes the code more malleable. Similar reasoning applies for
  practices like &lt;a href="SelfTestingCode.html"&gt;SelfTestingCode&lt;/a&gt; and
  &lt;a href="ContinuousDelivery.html"&gt;ContinuousDelivery&lt;/a&gt;. These are &lt;a href="/articles/designDead.html"&gt;enabling practices for evolutionary
  design&lt;/a&gt;, without them yagni turns from a beneficial practice into
  a curse. But if you do have a malleable code base, then yagni
  reinforces that flexibility. Yagni has the curious property that it
  is both enabled by and enables evolutionary design.
  &lt;/p&gt;

&lt;p&gt;
    Yagni is not a justification for neglecting the health of your
    code base. Yagni requires (and enables) malleable code.
  &lt;/p&gt;
&lt;p&gt;I also argue that yagni only applies when you introduce extra
  complexity now that you won't take advantage of until later. If you
  do something for a future need that doesn't actually increase the
  complexity of the software, then there's no reason to invoke
  yagni.&lt;/p&gt;

&lt;p&gt;Having said all this, there are times when applying yagni does cause
  a problem, and you are faced with an expensive change when an
  earlier change would have been much cheaper. The tricky thing here
  is that these cases are hard to spot in advance, and much easier to
  remember than the cases where yagni saved effort &lt;a href="#footnote-availability"&gt;[4]&lt;/a&gt;. My sense is that yagni-failures are relatively
  rare and their costs are easily outweighed by when yagni
  succeeds.&lt;/p&gt;

&lt;h2&gt;Further Reading&lt;/h2&gt;

&lt;p&gt;My essay &lt;a href="/articles/designDead.html"&gt;Is Design
    Dead&lt;/a&gt; talks in more detail about the role of design and
    architecture in agile projects, and thus role yagni plays as an
    enabling practice.&lt;/p&gt;

&lt;p&gt;This principle was first discussed and fleshed out on &lt;a href="http://c2.com/cgi/wiki?YouArentGonnaNeedIt"&gt;Ward's Wiki&lt;/a&gt;.&lt;/p&gt;
&lt;p class="acknowledgements"&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;

    Rachel Laycock talked through this post with me and played a
    critical role in its final organization. Chet Hendrickson and
    Steven Lowe reminded
    me to discuss small-scale yagni decisions.

    Rebecca Parsons, Alvaro Cavalcanti, Mark Taylor, Aman King, Rouan
    Wilsenach, Peter Gillard-Moss, Kief Morris, Ian Cartwright, James
    Lewis, Kornelis Sietsma, and Brian Mason participated in an insightful
    discussion about drafts of this article on our internal mailing list.
    
  &lt;/p&gt;

&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/bliki/Yagni.html&amp;amp;text=Bliki:%20Yagni" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/bliki/Yagni.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/bliki/Yagni.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:37:14 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:bliki-yagni.html</guid></item><item><title>Bliki: MonolithFirst</title><link>http://www.ciandcd.com/bliki-monolithfirst.html</link><description>&lt;div&gt; 

 

 

&lt;p class="tagLabel"&gt;tags:&lt;/p&gt;

&lt;p class="clear"&gt;&lt;/p&gt;

&lt;p&gt;As I hear stories about teams using a &lt;a href="/articles/microservices.html"&gt;microservices architecture&lt;/a&gt;, I've
    noticed a common pattern.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Almost all the successful microservice stories have started with a
      monolith that got too big and was broken up&lt;/li&gt;

&lt;li&gt;Almost all the cases where I've heard of a system that was built as a
      microservice system from scratch, it has ended up in serious trouble.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This pattern has led many of my colleagues to argue that &lt;b&gt;you
    shouldn't start a new project with microservices, even if you're
    sure your application will be big enough to make it worthwhile.
    &lt;/b&gt;.&lt;/p&gt;
&lt;img src="images/microservice-verdict/path.png"&gt;
&lt;p&gt;Microservices are a useful architecture, but even their advocates
  say that using them incurs a significant
  &lt;a href="MicroservicePremium.html"&gt;MicroservicePremium&lt;/a&gt;, which means they are only useful
  with more complex systems. This premium, essentially the cost of
  managing a suite of services, will slow down a team, favoring a
  monolith for simpler applications. This leads to a powerful argument
  for a monolith-first strategy, where you should build a new
  application as a monolith initially, even if you think it's likely
  that it will benefit from a microservices architecture later on.&lt;/p&gt;

&lt;p&gt;The first reason for this is classic &lt;a href="Yagni.html"&gt;Yagni&lt;/a&gt;. When you begin a new
  application, how sure are you that it will be useful to your users?
  It may be hard to scale a poorly designed but successful software
  system, but that's still a better place to be than its inverse. As
  we're now recognizing, often the best way to find out if a software
  idea is useful is to build a simplistic version of it and see how
  well it works out. During this first phase you need to prioritize
  speed (and thus cycle time for feedback), so the premium of
  microservices is a drag you should do without.&lt;/p&gt;

&lt;p&gt;The second issue with starting with microservices is that they
  only work well if you come up with good, stable boundaries between
  the services - which is essentially the task of drawing up the right
  set of &lt;a href="BoundedContext.html"&gt;BoundedContexts&lt;/a&gt;. Any refactoring of functionality
  between services is much harder than it is in a monolith. But even
  experienced architects working in familiar domains have great
  difficulty getting boundaries right at the beginning. By building a
  monolith first, you can figure out what the right boundaries are,
  before a microservices design brushes a layer of treacle over them.
  It also gives you time to develop the
  &lt;a href="MicroservicePrerequisites.html"&gt;MicroservicePrerequisites&lt;/a&gt; you need for finer-grained
  services. &lt;/p&gt;

&lt;p&gt;I've heard different ways to execute a monolith-first strategy.
  The logical way is to design a monolith carefully,
  paying attention to modularity within the software, both at the API
  boundaries and how the data is stored. Do this well, and it's a
  relatively simple matter to make the shift to microservices. However
  I'd feel much more comfortable with this approach if I'd heard a
  decent number of stories where it worked out that way. &lt;a href="#footnote-typical-monolith"&gt;[1]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A more common approach is to start with a monolith and gradually
  peel off microservices at the edges. Such an approach can leave a
  substantial monolith at the heart of the microservices
  architecture, but with most new development occurring in the
  microservices while the monolith is relatively quiescent. &lt;/p&gt;

&lt;p&gt;Another common approach is to just replace the monolith entirely.
  Few people look at this as an approach to be proud of, yet there are
  advantages to building a monolith as a
  &lt;a href="SacrificialArchitecture.html"&gt;SacrificialArchitecture&lt;/a&gt;. Don't be afraid of building a
  monolith that you will discard, particularly if a monolith can get
  you to market quickly.&lt;/p&gt;

&lt;p&gt;Another route I've run into is to start with just a couple of
  coarse-grained services, larger than those you expect to end up
  with. Use these coarse-grained services to get used to working with
  multiple services, while enjoying the fact that such coarse granularity
  reduces the amount of inter-service refactoring you have to do. Then
  as boundaries stabilize, break down into finer-grained services. &lt;a href="#footnote-duolith"&gt;[2]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;While the bulk of my contacts lean toward the monolith-first
  approach, it is &lt;a href="/articles/dont-start-monolith.html"&gt;by no
  means unanimous&lt;/a&gt;. The counter argument says that starting with
  microservices allows you to get used to the rhythm of developing in
  a microservice environment. It takes a lot, perhaps too much,
  discipline to build a monolith in a sufficiently modular way that it
  can be broken down into microservices easily. By starting with
  microservices you get everyone used to developing in separate small
  teams from the beginning, and having teams separated by service
  boundaries makes it much easier to scale up the development effort
  when you need to. This is especially viable for system replacements
  where you have a better chance of coming up with stable-enough
  boundaries early. Although the evidence is sparse, I feel that you
  shouldn't start with microservices unless you have reasonable
  experience of building a microservices system in the team.&lt;/p&gt;

&lt;p&gt;I don't feel I have enough anecdotes yet to get a firm handle on
  how to decide whether to use a monolith-first strategy. These are
  early days in microservices, and there are relatively few anecdotes
  to learn from. So anybody's advice on these topics must be seen as
  tentative, however confidently they argue.&lt;/p&gt;

&lt;h2&gt;Further Reading&lt;/h2&gt;

&lt;p&gt;Sam Newman &lt;a href="http://samnewman.io/blog/2015/04/07/microservices-for-greenfield/"&gt;describes a case study&lt;/a&gt; of a team considering using
    microservices on a greenfield project.&lt;/p&gt;
&lt;p class="acknowledgements"&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;

    I stole much of this thinking from my coleagues: James Lewis, Sam
    Newman, Thiyagu Palanisamy, and Evan Bottcher. Stefan Tilkov's
    comments on an earlier draft played a pivotal role in clarifying
    my thoughts. Chad Currie created the lovely glyphy
    dragons. Steven Lowe, Patrick Kua, Jean Robert D'amore, Chelsea
    Komlo, Ashok Subramanian, Dan Siwiec, Prasanna Pendse, Kief
    Morris, Chris Ford, and Florian Sellmayr discussed drafts on our
    internal mailing list.
  &lt;/p&gt;

&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/bliki/MonolithFirst.html&amp;amp;text=Bliki:%20MonolithFirst" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/bliki/MonolithFirst.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/bliki/MonolithFirst.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:37:11 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:bliki-monolithfirst.html</guid></item><item><title>Don't start with a Monolith</title><link>http://www.ciandcd.com/dont-start-with-a-monolith.html</link><description>&lt;div&gt;&lt;p class="subtitle"&gt;&amp;#8230; when your goal is a microservices
  architecture&lt;/p&gt;&lt;p&gt;Stefan Tilkov is a co-founder and principal consultant at innoQ , a technology consulting company with offices in Germany and Switzerland. He has been involved in the design of large-scale, distributed systems for more than two decades, using a variety of technologies and tools ranging from C++ and CORBA over J2EE/Java EE and Web Services to REST and Ruby on Rails. He has authored numerous articles and a book (&amp;#8220; REST und HTTP &amp;#8221;, German), and is a frequent speaker at conferences around the world.&lt;/p&gt;&lt;p&gt; In the last few months, I&amp;#8217;ve heard repeatedly that the only way to
get to a successful microservices architecture is by starting with a
monolith first. &lt;a href="http://www.codingthearchitecture.com/2014/07/06/distributed_big_balls_of_mud.html"&gt;To
paraphrase Simon Brown&lt;/a&gt;: If you can&amp;#8217;t build a well-structured
monolith, what makes you think you can build a well-structured set of
microservices? The most recent &amp;#8211; and, as usual, very convincing &amp;#8211;
rendering of this argument comes from &lt;a href="/bliki/MonolithFirst.html"&gt;Martin Fowler&lt;/a&gt; on this very site. As I had
a chance to comment on an earlier draft, I had some time to think
about this. And I did, especially because I usually find myself in
agreement with him, and some others whose views I typically share
seemed to agree with him, too.
&lt;/p&gt;

&lt;p&gt;I&amp;#8217;m firmly convinced that starting with a monolith is usually exactly the
wrong thing to do. 
&lt;/p&gt;

&lt;p&gt;Starting to build a new system is exactly the time when you should
be thinking about carving it up into pieces. I strongly disagree with
the idea that you can postpone this, as expressed by Sam Newman, again
someone I agree with 95% of the time:
&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I remain convinced that it is much easier to partition an existing,
  "brownfield" system than to do so up front with a new, greenfield
  system. You have more to work with. You have code you can examine,
  you can speak to people who use and maintain the system. You also
  know what 'good' looks like - you have a working system to change,
  making it easier for you to know when you may have got something
  wrong or been too aggressive in your decision making process.&lt;/p&gt;

&lt;p class="quote-attribution"&gt;&lt;a href="http://samnewman.io/blog/2015/04/07/microservices-for-greenfield"&gt;-- Sam Newman&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the majority of cases, it will be awfully hard, if not outright
impossible, to cut up an existing monolith this way. (That doesn&amp;#8217;t
mean it&amp;#8217;s always impossible, but that&amp;#8217;s a topic for a future post.)
There is some common ground in that I agree you should know the domain
you&amp;#8217;re building a system for very well before trying to partition
it, though: In my view, the ideal scenario is one where you&amp;#8217;re building a
&lt;i&gt;second version&lt;/i&gt; of an existing system.
&lt;/p&gt;

&lt;p&gt;If you are actually able to build a well-structured monolith, you
probably don&amp;#8217;t need microservices in the first place. Which is OK! I
definitely agree with &lt;a href="/bliki/MicroservicePremium.html"&gt;Martin&lt;/a&gt;: You shouldn&amp;#8217;t introduce the
complexity of additional distribution into your system if you don&amp;#8217;t
have a very good reason for doing so.
&lt;/p&gt;

&lt;p&gt;(So what would be a good reason? There are many, but to me the most
important one is to allow for fast, independent delivery of individual parts within a larger
system. Microservices&amp;#8217; main benefit, in my view, is enabling parallel
development by establishing a hard-to-cross boundary between different
parts of your system. By doing this, you make it hard &amp;#8211; or at least
harder &amp;#8211; to do the wrong thing: Namely, connecting parts that shouldn&amp;#8217;t
be connected, and coupling those that need to be connected too
tightly. In theory, you don&amp;#8217;t need microservices for this if you
simply have the discipline to follow clear rules and establish clear
boundaries within your monolithic application; in practice, I&amp;#8217;ve found
this to be the case only very rarely.)&lt;/p&gt;

&lt;img src="dont-start-monolith/monolith-theory-practice.svg" width="100%"&gt;
&lt;p class="photoCaption"&gt;
  You might be tempted to assume there are a number of nicely
  separated microservices hiding in your monolith, just waiting to be
  extracted. In reality, though, it&amp;#8217;s extremely hard to avoid creating
  lots of connections, planned and unplanned. In fact the whole point
  of the microservices approach is to make it hard to create something
  like this.
&lt;/p&gt;
&lt;p&gt;
But if you start with a monolith, the parts will become extremely
tightly coupled to each other. &lt;i&gt;That&amp;#8217;s the very definition of a
monolith&lt;/i&gt;. The parts will rely on features of the platform they all
use. They&amp;#8217;ll communicate based on abstractions that are shared because
they all use the same libraries. They&amp;#8217;ll communicate using means that
are only available when they are hosted in the same process. And these
are only the technical aspects! Far worse than that, the parts will
(almost) freely share domain objects, rely on the same, shared
persistence model, assume database transactions are readily available
so that there&amp;#8217;s no need for compensation &amp;#8230; Even the very fact that
it&amp;#8217;s easy to refactor things and move them around &amp;#8211; all in the
convenience of your IDE&amp;#8217;s view of a single project &amp;#8211; is what makes it
extremely hard to cut things apart again. It&amp;#8217;s &lt;i&gt;extremely&lt;/i&gt; hard to
split up an existing monolith into separate pieces.
&lt;/p&gt;

&lt;p&gt;
I strongly believe &amp;#8211; and experience from a number of our recent projects
confirms this &amp;#8211; that when you start out, you should think about
the subsystems you build, &lt;i&gt;and build them as independently of each
other as possible&lt;/i&gt;. Of course you should only do this if you believe
your system is large enough to warrant this. If it&amp;#8217;s just you and one
of your co-workers building something over the course of a few weeks,
it&amp;#8217;s entirely possible that you don&amp;#8217;t.
&lt;/p&gt;

&lt;p&gt;But starting with an approach where you carve up your system into
smaller parts, and treat each of them as a clearly separated, individual
system with its own development, deployment, and delivery cycle, and (the
possibility of) its own internal architecture, is a very powerful
concept that can help you to deliver a system in the first place.
&lt;/p&gt;

&lt;p&gt;So is there any actual experience to back this up? Yes, there are a few systems we&amp;#8217;ve been
involved with recently that showed this concept to work &amp;#8211; provided you
tolerate the fact that what I&amp;#8217;m talking about is more likely bigger
than your typical microservice. The most prominent
one is Otto.de, about which I did
&lt;a href="http://www.infoq.com/presentations/modular-ecommerce-website"&gt;a talk together with their tech lead&lt;/a&gt;
 (and which you can read about (in German) in
 &lt;a href="http://www.informatik-aktuell.de/entwicklung/methoden/von-monolithen-und-microservices.html"&gt;this nice write-up by one of their lead architects&lt;/a&gt;).
 But there are quite a few others as well, and I remain convinced it&amp;#8217;s
 a good idea to start building a system using this approach &amp;#8211;
 given you know the domain you&amp;#8217;re building for really, really well.
&lt;/p&gt;

&lt;p&gt;But there is another lesson to be learned from this discussion, in my
view &amp;#8211; and it&amp;#8217;s a more general one: Beware of architectural recipes
that are too simple and too obvious. This one &amp;#8211; start by carving up
your domain into separate, independent parts &amp;#8211; is no
exception. Sometimes a monolith &lt;i&gt;is&lt;/i&gt; preferable, sometime it&amp;#8217;s
not. If you decide to build things using a microservices approach, you
need to be aware that while it will be a lot easier to make localized
decisions in each individual part, it will be much harder to change
the very boundaries that enable this. Refactoring in the small becomes
easier, refactoring in the large becomes much harder. 
&lt;/p&gt;

&lt;p&gt;As is always the case with architecture discussions, there is no way
to get around the fact that you need to make that decision on your
own, in each and every individual case.
&lt;/p&gt;



&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/articles/dont-start-monolith.html&amp;amp;text=Don%E2%80%99t%20start%20with%20a%20monolith" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/articles/dont-start-monolith.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/articles/dont-start-monolith.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;For articles on similar topics&amp;#8230;&lt;/h2&gt;

&lt;p&gt;&amp;#8230;take a look at the tag: &lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:37:05 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:dont-start-with-a-monolith.html</guid></item><item><title>Tor for Technologists</title><link>http://www.ciandcd.com/tor-for-technologists.html</link><description>&lt;div&gt;&lt;p&gt;&lt;a href="https://torproject.org"&gt;Tor&lt;/a&gt; is a technology that is cropping up in
    news articles quite often nowadays. However, there exists a lot of
    misunderstanding about it. Even many technologists don't see past its
    use for negative purposes, but Tor is much more than that. It is an
    important tool for democracy and freedom of speech - but it's also
    something that is very useful in the day-to-day life of a
    technologist. Tor is also an interesting case study in how to design a
    system that has very specific security requirements.&lt;/p&gt;

&lt;p&gt;The Internet is currently a quite hostile place. There are threats
    of all kinds, ranging from &lt;a href="http://www.catb.org/jargon/html/S/script-kiddies.html"&gt;script
    kiddies&lt;/a&gt; and drive-by &lt;a href="https://www.owasp.org/index.php/Phishing"&gt;phishing&lt;/a&gt;
    attacks to pervasive &lt;a href="https://www.eff.org/nsa-spying"&gt;dragnet
    surveillance&lt;/a&gt; by many of the major intelligence services in the
    world. The extent of these problems have only recently become clear to
    us. In this context, a tool like Tor fills a very important niche. You
    could argue that it's a sign of the times that even a company like
    Facebook encourages &lt;a href="https://www.facebook.com/notes/protect-the-graph/making-connections-to-facebook-more-secure/1526085754298237"&gt;the use of Tor&lt;/a&gt;
    to access their services. The time is right to add Tor to your tool
    belt.&lt;/p&gt;


&lt;h2&gt;How does Tor work?&lt;/h2&gt;

&lt;p&gt;The goal of Tor is to enable anonymous network traffic. The word
      Tor originally stood for The Onion Router (although Tor should not be
      capitalized like an acronym). Onion routing is the method that Tor
      uses to hide the IP address of the client. This also has the added
      benefit of making it extremely hard for anyone in between to see who
      is asking for what information on the Internet. In practice, this
      means that if someone is intercepting your traffic close to your
      computer, Tor will hide what servers you are talking to. If someone is
      intercepting traffic close to the server, they will not be able to see
      who is visiting that server.&lt;/p&gt;

&lt;p&gt;Tor works by having thousands of volunteers hosting something
      called Tor Relays. These are fundamentally servers that spend all
      their time forwarding traffic. So when you want to access a service,
      the Tor client program that is running on your computer will choose
      three different relays. It will then take the packets of information
      and wrap them in encryption using public key cryptography. It will
      encrypt the packet three times over, to each one of the public keys of
      the relays that were chosen for this communication session. It will
      then send the triple-encrypted package to the first relay on the list.
      That relay will unwrap the encryption meant for itself - and what is
      left is a packet with two layers of encryption. The first relay will
      send the packet to the second relay, which will unwrap another layer
      of encryption. It sends this to the final relay which will unwrap the
      last piece of encryption. Since there is no encryption left, the final
      relay knows the real destination of the packet - so it contacts the
      original service requested. It then receives the answer and sends the
      information back, using the same process in reverse.&lt;/p&gt;

&lt;p class="figureImage"&gt;&lt;a name="sketch.png"&gt;&lt;/a&gt;&lt;img alt="Figure 1" src="tor-for-technologists/sketch.png"&gt;&lt;/p&gt;
&lt;p&gt;The final relay is usually called an exit relay. The first relay
      is usually called an entry relay. The way this works only the entry
      relay knows the real IP address of your machine - and only the exit
      relay knows which service you actually wanted to contact. And the
      service will only see the IP address of the exit relay. All of these
      things come together to protect the anonymity of the client quite
      well.&lt;/p&gt;

&lt;p&gt;One important point is that you don't have to trust all the Tor
      relays in order to be able to trust the Tor network. Since anyone can
      set up their own relay, there is nothing that stops bad actors from
      setting up relays. Thus, Tor is designed to not be compromised even
      if this happens.&lt;/p&gt;

&lt;p&gt;Tor is explicitly designed to be a &lt;a href="https://en.wikipedia.org/wiki/Low_latency"&gt;low-latency network&lt;/a&gt; - that means it
      is possible to use it for things like Instant Messaging, audio
      conferencing or even video conferencing under good circumstances.&lt;/p&gt;

&lt;h3&gt;The Tor client&lt;/h3&gt;

&lt;p&gt;If you want to use Tor, there are two options: the
        first one is to run the client program, and the second is to
        download and run something called the &lt;a href="https://www.torproject.org/projects/torbrowser.html.en"&gt;Tor Browser
        Bundle&lt;/a&gt;. If you want to use Tor for other kinds of traffic
        than web, it's useful to run the Tor client software
        directly. Installing it is usually done through the regular package
        management systems for your operating system - and once installed
        it should be running on your machine in the background, without you
        having to do anything at all. Since Tor works as a proxy, you don't
        interact with Tor directly. Instead you ask your regular programs
        to tunnel traffic through Tor.&lt;/p&gt;

&lt;p&gt;The Tor client program by default will listen to port 9050 using
        a protocol called &lt;a href="https://www.ietf.org/rfc/rfc1928.txt"&gt;SOCKS&lt;/a&gt; - this is a
        standard proxy protocol that many applications can be configured to
        use. For example, I use a program called &lt;a href="http://gajim.org/"&gt;Gajim&lt;/a&gt; for instant messaging. In this program I
        have configured all my accounts to connect over Tor by specifying
        the correct SOCKS proxy settings.&lt;/p&gt;

&lt;p&gt;If you are using command line tools, it is usually possible to
        use a program called &lt;i&gt;usewithtor&lt;/i&gt; or &lt;i&gt;torsocks&lt;/i&gt; in order to
        transparently make the program use Tor. There are other ways of
        using Tor as well - &lt;a href="https://www.mozilla.org/en-US/thunderbird/"&gt;Thunderbird&lt;/a&gt; can
        be configured to run all its traffic over Tor if you want to make
        your email harder to track and intercept - however, there are some
        risks associated with the manual configuration of Tor in
        Thunderbird. There are many possible types of information leakage, such as DNS
        lookups, geo-location information in headers and even IP addresses
        in mail headers in some cases. Fortunately, there exists a plugin called &lt;a href="https://addons.mozilla.org/En-us/thunderbird/addon/torbirdy/"&gt;TorBirdy&lt;/a&gt; that automates many of the steps
        and tries to stop such unsafe information leakage from happening. &lt;/p&gt;
&lt;h3&gt;The Tor Browser Bundle&lt;/h3&gt;

&lt;p&gt;By far the easiest way of using Tor is to download the Tor Browser
        Bundle which doesn't even have to be installed - it can be run directly after download
        (and you can have it on a USB stick or a DVD and run it from there as
        well). The Tor Browser Bundle is basically the Tor client software
        combined with a very customized browser. Since browsers have many ways
        they can leak your anonymity, and there are many other things that can
        threaten your security, the Tor Browser Bundle is much safer than just
        configuring your regular browser to use Tor.&lt;/p&gt;

&lt;p&gt;The Tor Browser Bundle comes with several plugins that will improve
        your privacy, and it has also been configured to not leak information
        through side channels such as font configuration, Flash plugins and
        other features.&lt;/p&gt;
&lt;h3&gt;Hidden Services&lt;/h3&gt;

&lt;p&gt;Tor also has another interesting feature that has gotten a lot of
        attention lately. Tor hidden services give a service the ability to
        hide its IP address, just as the regular Tor operation allows a client
        to hide its IP address. It works by using something called a
        rendezvous protocol, which is a bit complicated. Basically, you can
        imagine that both the client and the server establishes their own full
        circuits of three relays. They then meet in the middle, somewhere in
        the Tor network - and from that point they can establish a connection.
        In general, a hidden service is found and contacted using a
        .onion-URL, which consists of 16 alphanumeric characters followed by
        .onion. The 16 characters is a representation of the public key of the
        hidden service, which means that routing of the connection is
        self-verifying. No-one can fake being a hidden service without
        stealing their private key.&lt;/p&gt;

&lt;h3&gt;Facebook and Hidden Services&lt;/h3&gt;

&lt;p&gt;There are several aspects of Facebook &lt;a href="https://www.facebook.com/notes/protect-the-graph/making-connections-to-facebook-more-secure/1526085754298237"&gt;setting up a hidden service&lt;/a&gt;
          that are interesting. It is definitely not the standard usage to set
          up a hidden service for something like Facebook where the real
          location of the service is already well known. And since Facebook
          requires logins and they have a real name policy that also makes it
          harder to put it together. However, there are several reasons why
          Facebook decided to set up a hidden service - the first is that it
          makes it easy for them to block Tor exit relays to their regular sites
          without stopping access for valid users. The second reason is that a
          hidden service still ensures that Facebook doesn&amp;#8217;t see the clients IP
          address, and enforces end-to-end encryption. So overall, the Facebook
          hidden service doesn&amp;#8217;t fill all the same functions as hidden services
          usually do, but it&amp;#8217;s still a valuable thing to have. &lt;/p&gt;
&lt;p&gt;Hidden services are extremely powerful for several reasons. First,
        they allow the service provider to be anonymous. This has of course
        been used for a lot of good and bad reasons. Secondly, if you only
        expose a service as a hidden service, you can force your clients to
        use Tor - which makes it much harder for them to accidentally expose
        their information. Third, hidden services are implemented in such a
        way that the traffic will never leave the Tor network. There are no
        exit relays for hidden services. What this means is that communication
        to a hidden service will always be encrypted all the way to the
        service. This isn't necessarily true for regular Tor connections.
        Finally, you can expose a hidden service without having a publicly
        routable IP address. A very practical example of how to use this is to
        expose SSH as a hidden service. This allows you to SSH to your own
        server wherever it may be, without having to open firewalls or having
        a public IP. I run several servers at home that are not publicly
        reachable, and being able to SSH into them and manage them anyway is
        extremely powerful.&lt;/p&gt;

&lt;p&gt;Facebook recently made a splash by exposing their servers as
        a hidden service. This is a great step forward. So you might ask
        whether this is something you should do for the projects you
        work on. In general, I would say yes - although it does require
        some extra work and testing. Making this available to your users
        is something that in general is good for everyone.&lt;/p&gt;
&lt;h3&gt;Bridges and obfuscated protocols&lt;/h3&gt;

&lt;p&gt;In some regions there is enough censorship going on that using Tor
        in the regular manner doesn't work. Most of the time this is because
        they simply block traffic to all the Tor relays. And since the Tor
        relays have to be public, it's easy for a censor to simple download
        the list of IPs to be blocked.&lt;/p&gt;

&lt;p&gt;Because of this situation, the Tor software supports something
        called bridges. These are entry relays that are not publicly known -
        if you need one you can request one on the fly. The other solution
        that is sometimes necessary is to use obfuscated protocols. Since the
        most advanced censors use deep packet inspection it's important that
        Tor traffic can masquerade as other kinds of traffic. There are
        several different plugins implemented in Tor right now for solving
        this problem - but it's an ongoing arms race to beat censorship.&lt;/p&gt;

&lt;h2&gt;Anonymity is hard - what doesn't Tor protect against?&lt;/h2&gt;

&lt;p&gt;Tor is a tool, and just like any tool it has things it does well,
      and things it doesn't do so well. It is important to know that Tor
      can't protect against every threat out there. Even when you use Tor
      correctly, there are things to keep in mind.&lt;/p&gt;

&lt;p&gt;The biggest day-to-day problem when it comes to anonymity and
      security with Tor is that if the traffic you are sending to a server
      isn't encrypted, then Tor will not help with that. Specifically, the
      traffic between you and the first two relays will be encrypted and
      hard to attack. However, the exit relay will be able to see all
      traffic that passes through. The way to protect against this is to
      either use a protocol that encrypt your information all the way to
      the endpoint (such as HTTPS or SMTP with &lt;a href="https://en.wikipedia.org/wiki/STARTTLS/"&gt;STARTTLS&lt;/a&gt;). The other way is to use services
      that expose a hidden service - which ensures you get end-to-end
      encryption of all traffic.&lt;/p&gt;

&lt;h2&gt;Browser fingerprinting&lt;/h2&gt;

&lt;p&gt;There are many different ways a user can be identified outside
        of looking at their IP address. Browser fingerprinting is a
        surprisingly effective way to uniquely identify people. It uses
        several aspects, such as what plugins are available, what fonts are
        installed and many other features that are available to the
        server. The EFF have a test service called &lt;a href="https://panopticlick.eff.org/"&gt;Panopticlick&lt;/a&gt; which shows how easy it is
        to uniquely identify someone through their browser. &lt;/p&gt;
&lt;p&gt;Tor is usually classified as a low-latency network. This has a
      consequence for anonymity. Basically, Tor is designed in such a way
      that if an attacker can observe a large percentage of the network
      traffic between relays, it is possible for them to de-anonymize much
      of that traffic. The reason is that since Tor traffic is supposed to
      go out and come back in realtime, it can be possible to do
      correlation of packet times. This could lead to a privacy
      break. However, this is the way Tor was designed. Current research
      seems to imply that it is basically impossible to get total anonymity
      if you want something that is close to real time traffic. If you are
      fine with waiting a few hours for each packet to arrive, there are
      &lt;a href="https://en.wikipedia.org/wiki/Mix_network"&gt;other kinds of techniques&lt;/a&gt; that can be
      used, but Tor is not designed for those use cases.&lt;/p&gt;

&lt;p&gt;Tor can also not protect you if you make a mistake and expose your
      IP address in any way. A typical example of something that is a bad
      idea is to use Tor for peer-to-peer file sharing - not only does it
      destroy the Tor network - it also doesn't work to hide your address,
      since most file-sharing protocols expose your real IP address. If you
      want to be sure to not expose your IP address you also need to avoid
      running plugins that can expose it. This is one of the reasons why the
      Tor Browser Bundle doesn't ship with a Flash plugin - it is just too
      dangerous from a privacy-standpoint.&lt;/p&gt;

&lt;p&gt;The same caveat is true if you expose your server as a hidden
      service. If you really want to keep it hidden you need to make sure
      that none of your server components expose your IP address in any way.
      This can be easier said than done, as the next section will talk
      about.&lt;/p&gt;

&lt;h2&gt;Is Tor broken?&lt;/h2&gt;

&lt;p&gt;When I talk to technologists about Tor, one of the more common
      questions I will get is whether I think Tor is broken or whether we
      really can trust it, whether the government has put a backdoor in it,
      etc. There are many rumors and theories about Tor out there, and in
      general they are just that - rumors and theories. That said, Tor
      doesn't protect against everything, as I mentioned above. However, I
      would be extremely surprised if Tor contains a backdoor. The code is
      open source and many, many experts have looked at it over the years.
      The protocol is also open and has been implemented more than once in
      other open source projects. It is of course not impossible that there
      exists a backdoor that no-one has noticed, but personally I wouldn't
      bet on it. I do recommend any technologist that is worried about this
      eventuality to spend a few hours and go through the source code. The
      project can always do with more eyeballs. &lt;/p&gt;

&lt;h3&gt;Research&lt;/h3&gt;

&lt;p&gt;All that said, Tor has been broken many times. The main reason for
        this is that anonymity is extremely hard, and Tor is pushing the
        boundaries of what's been done before. Thus, Tor is a favorite subject
        for researchers to poke at and see if they can figure out ways of
        getting it to do unexpected things. It's also the main way we have of
        getting an idea of where the hard limits of anonymity lie. That Tor is
        the subject of lots of research is ultimately good for the community,
        since every time a new paper comes out that talks about a weakness in
        Tor, the Tor developers fix it. Nothing is perfect at the start, so
        this way of iterating and fixing is how we get to a state where we can
        trust a tool. &lt;/p&gt;
&lt;h3&gt;Events in 2014&lt;/h3&gt;

&lt;p&gt;One reason that people are worried about the degree of security of
        Tor, is because there has been a string of suspicious events over the
        last few years. Specifically, both users of Tor and people that have
        used hidden services have been arrested under various circumstances.
        However, we have at this point no indication that any of these take
        downs were actually because of flaws in the Tor software or protocol.
        In the cases we know of, it seems that the reasons can be chalked down
        to mistakes of various kinds. In one case, there was a bug in the
        Firefox version that the Tor Browser Bundle was based on, so after the
        police took over a server, they could attack this browser and
        de-anonymize the users in question (but only those that visited that
        specific hidden service). In the case of the first Silk Road, it seems
        clear that investigators managed to hack in to the administrator
        interface and convinced it to send back its real IP address.&lt;/p&gt;

&lt;p&gt;And then we have &lt;a href="https://en.wikipedia.org/wiki/Operation_Onymous"&gt;Operation
        Onymous&lt;/a&gt;. This operation was a collaboration between several
        different law enforcement agencies with the aim to take down a
        large number of illegal online marketplaces.  What is interesting
        about this operation is all the propaganda that surrounds
        it. Initially they were claiming to have taken down over 400 hidden
        services. But after the dust settled it seems only 27 sites were
        taken down, and only 17 arrests were made. You can still argue that
        this is a lot, since they were supposed to have been protected by
        Tor and hidden services. But the truth is that apparently the
        person hosting Silk Road 2 had his name written down somewhere -
        the law enforcement agencies took him in and started questioning
        him.  Apparently he named 16 other people that were promptly
        arrested and had their computers seized. So no Tor break was
        involved in this - even though one UK law enforcement agency
        actually tweeted a taunt about Tor being broken. It was all
        propaganda. More info can be found in &lt;a href="https://www.youtube.com/watch?v=pRrFWwA-47U"&gt;this video&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Further, looking at the &lt;a href="http://www.theguardian.com/world/interactive/2013/oct/04/tor-stinks-nsa-presentation-document"&gt;documents&lt;/a&gt;
        leaked by Edward Snowden, it is still clear that the strongest
        attackers out there don't seem to have a good way of breaking
        Tor. I find this to be very encouraging.&lt;/p&gt;

&lt;h2&gt;Everyday usage of Tor&lt;/h2&gt;

&lt;p&gt;Now that you have a better idea of what Tor is, it is time for us
      to quickly talk about how you can use Tor in your day-to-day job. As
      you can probably guess from the information above, Tor can be used for
      a lot of different purposes. But for the kind of work a developer is
      doing, the first and easiest thing you can do using Tor is to test
      your system in different ways. In many cases it's hard to see what a
      system looks like without cookies and personalization. Tor makes it
      very easy.&lt;/p&gt;

&lt;p&gt;The other kind of testing you can do is to make it possible to very
      easily try a system in such a way that it looks like it comes from
      many different countries. It is possible to control which relays Tor
      will use in such a way that you can control which country your traffic
      will come out from.&lt;/p&gt;

&lt;p&gt;As I mentioned before, putting SSH behind a Tor hidden service is a
      very useful way of getting access to your systems from anywhere on the
      planet, in a way that puts at least two layers of encryption on your
      connection. However, even for a setup like this, it's important to
      have good passwords (or only use public-key logins for SSH) and to
      make it impossible to login to the root account remotely.&lt;/p&gt;

&lt;h2&gt;Tails&lt;/h2&gt;

&lt;p&gt;If you happen to be in the kind of situation where Tor is
      absolutely necessary for you and you expect to be attacked by strong
      adversaries, you are in a tricky situation. Just installing Tor on
      your regular machine isn't necessarily going to be enough. Thus,
      there is a project called &lt;a href="https://tails.boum.org/"&gt;Tails&lt;/a&gt;, which is a
      Linux distribution that runs from a CD/DVD or USB drive. It is
      preconfigured to &lt;i&gt;only&lt;/i&gt; send network traffic over Tor and also
      has many other privacy and anonymity features. It comes bundled with
      several different tools that will allow you to easily communicate
      privately, send encrypted email and also working with documents in a
      safe way.&lt;/p&gt;

&lt;p&gt;The interesting thing with Tails is that it's completely amnesiac -
      it doesn't leave a trace on your computer after you have run it. It is
      possible to have it save a small amount of data, but by default no
      traces will be left.&lt;/p&gt;

&lt;p&gt;Tails is not something you will need in your regular day-job, but
      there are certain cases where something like Tails can be extremely
      powerful. Every journalist should know how to use it - and so should
      every lawyer. It's a powerful tool and it's a good idea to be prepared
      to use it before it becomes necessary. &lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Tor is a fantastic tool that can do a lot of amazing things. It is
      one of the strongest examples of useful cryptography deployed in a
      safe way we have right now. As we have seen though, no tool is
      perfect. And in order to use Tor correctly, it's a good idea to know
      how it works. As a next step I would recommend you to download Tor and
      try it out. One of the interesting things about anonymity is that it
      loves company - even if you don't need the anonymity you are
      protecting others who do need it by using Tor. One day you might go
      forward and run your own Tor relay as well - something that improves
      the Tor network for everyone.&lt;/p&gt;


&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/articles/tor-for-technologists.html&amp;amp;text=Tor%20for%20Technologists" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/articles/tor-for-technologists.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/articles/tor-for-technologists.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;For articles on similar topics&amp;#8230;&lt;/h2&gt;

&lt;p&gt;&amp;#8230;take a look at the tag: &lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:37:02 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:tor-for-technologists.html</guid></item><item><title>A Cherry Picker's Guide to Doctor Who</title><link>http://www.ciandcd.com/a-cherry-pickers-guide-to-doctor-who.html</link><description>&lt;div&gt;&lt;p&gt;Doctor Who is a British TV series with a long history. Its
    first broadcast was in 1963, and its current incarnation has run
    for eight seasons. Most TV series require you to start at the
    beginning and watch every episode, but although that has its
    delights, you don't have to do that with Doctor Who, since many of
    its best episodes are written so that you can enjoy them as a
    self-contained film. This is my personal suggestions of how to
    watch Doctor Who by cherry picking individual episodes.&lt;/p&gt;

&lt;p&gt;For most episodes of Doctor Who, all you need to know is the
    rough premise. The Doctor is a Time Lord, a human looking alien,
    who travels through time and space with various companions
    (usually human, young, and female). This setup allows writers to
    set self-contained stories in any setting: historical, futuristic,
    and current. His spaceship is called the TARDIS, from the outside
    it looks like a blue British Police Box from the 1960s, its inside
    is a large trans-dimensional space. Each episode usually sees the
    Tardis appear and the Doctor and his companion find some sticky
    predicament focused on some malevolent alien. They resolve the
    situation, often with a notable lack of violence. The Doctor is
    strikingly not like the usual action hero, he rarely uses weapons:
    relying on his wits to defeat any threats.&lt;/p&gt;

&lt;p&gt;Doctor Who is one of the longest running TV series, it began in
    November 1963. You can divide its history so far into two broad
    periods: Classic Who runs from its beginning until 1989 when
    the series was cancelled. New Who runs from its reboot in 2005 until
    today, where it's still going strong. Classic Who was a children's
    show that appealed to many adults. The writers of New Who mostly
    fell in love with Classic Who as children (as I did) and went on
    to write a show that should appeal equally to adults and children.
    I saw my first episode of New Who with two friends my own age and
    their pre-teen daughters, who hadn't seen Classic Who as we elders
    had.&lt;/p&gt;

&lt;p&gt;If you want to explore Doctor Who from scratch, I would start
    with New Who. Classic Who has its charms, and some remarkably good
    serials. But the quality of Classic Who varies from very good to
    truly awful, and comes with special effects that vary from cheesy to
    truly awful. So although I'll suggest a couple of Classic Who serials later
    on, most new viewers should start at New Who. Although there is some
    references to Classic Who, you can appreciate New Who without
    knowing anything about Classic Who.&lt;/p&gt;

&lt;p&gt;I'm focusing on cherry picking here, but plenty of people enjoy
    starting from the beginning of New Who and taking a
    completist approach through every episode. There are certainly rewards for the
    completist approach, with some good internal references and many great
    episodes that you can't appreciate by cherry picking. You don't
    need any advice to be a completist, but I will say that you should
    give it until at least the sixth episode (&lt;a href="https://en.wikipedia.org/wiki/Dalek_(Doctor_Who_episode)"&gt;Dalek&lt;/a&gt;)
    before you decide it's not for you. In particular the fourth and
    fifth episodes (&lt;a href="https://en.wikipedia.org/wiki/Aliens_of_London"&gt;Aliens of
    London / World War Three&lt;/a&gt;) are two of the weakest
    episodes in New Who, with some annoyingly juvenile humor.&lt;/p&gt;

&lt;p&gt;For my picks I'm selecting episodes which are both my favorite
    episodes, but also episodes that don't rely on any surronding
    story arc. If it is valuable to see other picked episodes first,
    I'll mention that. There's also an argument for watching some
    individual series completely, and I'll mention which of those I
    think are worth considering for that.&lt;/p&gt;

&lt;p&gt;The first question is where does a cherry picker begin, and
    I generally advise starting with &lt;a href="https://en.wikipedia.org/wiki/Blink_(Doctor_Who)"&gt;Blink&lt;/a&gt;, which is on most
    people's short list for greatest Who episode ever. It actually
    doesn't feature the Doctor that much, and mostly ignores the
    companion of that series. But it's a clever plot, superbly acted
    by a young Carey Mulligan, and lots of wit in the script.
    Deservedly it won a BAFTA drama award, a rare event for a sci-fi
    story. &lt;a href="#footnote-purpose"&gt;[1]&lt;/a&gt; The episode was written by &lt;a href="http://en.wikipedia.org/wiki/Steven_Moffat"&gt;Stevan
    Moffat&lt;/a&gt;, who since went on to be the showrunner for Doctor Who.
    He's also the co-showrunner for &lt;a href="http://en.wikipedia.org/wiki/Sherlock_(TV_series)"&gt;Sherlock&lt;/a&gt;,
    and the writer of &lt;a href="http://en.wikipedia.org/wiki/Jekyll_(TV_series)"&gt;Jekyll&lt;/a&gt; (a
    superb six-episode miniseries).
     I rate him with Joss Whedon as one of the best writers of our
    time.&lt;/p&gt;

&lt;p&gt;So start with &lt;a href="https://en.wikipedia.org/wiki/Blink_(Doctor_Who)"&gt;Blink&lt;/a&gt;, but after that you can mostly
    pick and mix as you like. I'm going to list my picks
    chronologically series by series, but you don't have to do them in
    that order. If any episodes require you to see some others first,
    I'll point that out. You should get used to different actors
    playing the Doctor - there is a clever techno-babble reason why
    multiple actors can play the same character, which doesn't affect
    a cherry picking watcher. Each actor emphasizes different aspects
    of the same character. Companions are different people, but again
    the cherry picking choices I've made don't rely too much on their
    ongoing story.&lt;/p&gt;

&lt;p&gt;The first series was the reboot of Doctor Who, making it return
    to the screen after a silence of fifteen years. The driving force
    for the reboot was &lt;a href="http://en.wikipedia.org/wiki/Russell_T_Davies"&gt;Russel T Davies&lt;/a&gt;, who already had garnered a
    fine reputation as a TV writer. Playing the Doctor in this series
    is Christopher Eccleston, sadly in his only series as the Doctor,
    with Rose (Billie Piper) as his companion. The standout episode of
    the first series is Moffat's &lt;a href="https://en.wikipedia.org/wiki/The_Empty_Child"&gt;The Empty Child / The Doctor
    Dances&lt;/a&gt;. Most movies
    aren't this good, and this double episode introduced the Moffat
    approach of combining fright and wit. The other cherry pick I'd
    make from the first series is &lt;a href="https://en.wikipedia.org/wiki/Dalek_(Doctor_Who_episode)"&gt;Dalek&lt;/a&gt;, which reintroduces the
    Doctor's iconic enemy in a story for them that still hasn't been
    surpassed.&lt;/p&gt;

&lt;p&gt;The first series also has one of the better story arcs, so may
    be worth doing the full series, if only to really enjoy the final
    two-parter which isn't worth watching without that context. (A tip
    if you do watch the whole series: don't watch the trailer for the
    next episode at the
    end of &lt;a href="https://en.wikipedia.org/wiki/Boom_Town_(Doctor_Who)"&gt;Boom Town&lt;/a&gt;, as
    it gives away an important part of the plot of &lt;a href="https://en.wikipedia.org/wiki/Bad_Wolf"&gt;Bad Wolf&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;The second
    series has David Tennant playing the tenth Doctor &lt;a href="#footnote-doctor-numbers"&gt;[2]&lt;/a&gt;. My main cherry pick here
    is again the Moffat episode:&lt;a href="https://en.wikipedia.org/wiki/The_Girl_in_the_Fireplace"&gt;The Girl in the Fireplace&lt;/a&gt;. I also think
    the two parter &lt;a href="https://en.wikipedia.org/wiki/The_Impossible_Planet"&gt;The Impossible Planet / The Satan
    Pit&lt;/a&gt; is worth
    watching.&lt;/p&gt;

&lt;p&gt;The third series continued with David Tenant but brought in a new
    companion: Martha. &lt;a href="https://en.wikipedia.org/wiki/Blink_(Doctor_Who)"&gt;Blink&lt;/a&gt; comes from this series, but another
    outstanding highlight of series three is &lt;a href="https://en.wikipedia.org/wiki/Human_Nature_(Doctor_Who_episode)"&gt;Human Nature / The Family
    of Blood&lt;/a&gt;. You do need to have watched a few Whos to really get into
    the Doctor's nature and character to appreciate this one, (I'd
    suggest watching the series 4 picks first). It
    was the first episode for me to reach the same heights as &lt;a href="https://en.wikipedia.org/wiki/The_Empty_Child"&gt;The Empty Child / The Doctor
    Dances&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For the fourth series Tenant was joined by the already well-known
    comedienne Catharine Tate as the companion Donna. Again Moffat came
    up with an outstanding thriller &lt;a href="https://en.wikipedia.org/wiki/Silence_in_the_Library"&gt;Silence in the Library /
    Forest of the Dead&lt;/a&gt;. But there are also other great highlights here.
    &lt;a href="https://en.wikipedia.org/wiki/The_Unicorn_and_the_Wasp"&gt;The Unicorn and the Wasp&lt;/a&gt; is probably the most out-and-out comedy in
    Who, a wonderful send-up of the Agatha Christie country house murder
    mystery (featuring Agatha herself, as only Who can). Russel Davis also
    came up with a taut character-driven thriller &lt;a href="https://en.wikipedia.org/wiki/Midnight_(Doctor_Who)"&gt;Midnight&lt;/a&gt;. I also
    really enjoyed &lt;a href="https://en.wikipedia.org/wiki/Turn_Left_(Doctor_Who)"&gt;Turn Left&lt;/a&gt;, but am not sure whether to recommend it
    here as it makes many references to non-cherry-picked episodes.
    However I think you can still enjoy it without  following
    those references, if only for some great acting from Tate and
    Bernard Cribbins. &lt;/p&gt;

&lt;p&gt;After the fourth season there was a year of specials, which
    generally isn't counted as a series. From this set, I'd pick &lt;a href="https://en.wikipedia.org/wiki/The_Waters_of_Mars"&gt;The Waters of Mars&lt;/a&gt;, which was a fine take on the "base under seige"
    style of Who plot.&lt;/p&gt;

&lt;p&gt;With series 5 there was a wholesale change. Russel Davies gave up
    the role of showrunner, handing over to Moffat. Tenant also gave up
    the role of the Doctor. Matt Smith became the eleventh Doctor, and
    we got a new companion in Amy, later joined by her husband Rory. The
    first two episodes of the Moffat era: &lt;a href="https://en.wikipedia.org/wiki/The_Eleventh_Hour_(Doctor_Who)"&gt;The
    Eleventh Hour&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/The_Beast_Below"&gt;The Beast Below&lt;/a&gt; are both
    worth picking. I also would pick the two parter &lt;a href="https://en.wikipedia.org/wiki/The_Time_of_Angels"&gt;The Time of Angels /
    Flesh and Stone&lt;/a&gt;,
    although for this one it's important to have seen &lt;a href="https://en.wikipedia.org/wiki/Blink_(Doctor_Who)"&gt;Blink&lt;/a&gt; first (to
    know about The Weeping Angels) and &lt;a href="https://en.wikipedia.org/wiki/Silence_in_the_Library"&gt;Silence in the Library /
    Forest of the Dead&lt;/a&gt; to know about River Song. River Song becomes an
    important character in various episodes in series 5 and 6 after
    this. Series 5 is also another good series to watch all the way
    through, with good development up to the finale. But if you want to
    stay with cherry picking, do watch &lt;a href="https://en.wikipedia.org/wiki/Amy%27s_Choice_(Doctor_Who)"&gt;Amy's Choice&lt;/a&gt;, and perhaps &lt;a href="https://en.wikipedia.org/wiki/Vincent_and_the_Doctor"&gt;Vincent and the Doctor&lt;/a&gt;&lt;a href="#footnote-vincent"&gt;[3]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;One of the traditions of New Who is to have a Christmas special
    episode for broadcast on Christmas Day. Davis did the earlier
    christmas specials, and I don't put any of them on my pick list. But
    I do pick Moffat's first special &lt;a href="https://en.wikipedia.org/wiki/A_Christmas_Carol_(Doctor_Who)"&gt;A
    Christmas Carol&lt;/a&gt;, as a clever remix of the Scrooge
    story starring Michael Gambon.&lt;/p&gt;

&lt;p&gt;With series 6, Moffat decided to break the pattern of earlier
    series and start the series with a
    two-parter &lt;a href="https://en.wikipedia.org/wiki/The_Impossible_Astronaut"&gt;The
    Impossible Astronaut / Day of the Moon"&lt;/a&gt;, which opens
    the series with a bang. The unresolved question from this episode
    may entice you to watch this whole series, and that's not a bad choice.
    If you would rather cherry-pick, you shouldn't miss &lt;a href="https://en.wikipedia.org/wiki/The_Doctor%27s_Wife"&gt;The Doctor's Wife&lt;/a&gt; (written by Neil
    Gaiman) and &lt;a href="https://en.wikipedia.org/wiki/The_Girl_Who_Waited"&gt;The Girl Who Waited&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Series 7 is a series of two distinct parts, with a halfway
    split seeing the departure of Amy and Rory and the arrival of the new companion
    Clara. My cherry-picking suggestion here is to see &lt;a href="https://en.wikipedia.org/wiki/Asylum_of_the_Daleks"&gt;The Asylum of the Daleks&lt;/a&gt; first (which sort-of introduces Clara). Then see
    &lt;a href="https://en.wikipedia.org/wiki/The_Snowmen"&gt;The Snowmen&lt;/a&gt; which picks up the Doctor after he is sadly
    separated from Amy and Rory and sort-of introduces him to Clara.
    Moffat does some clever meta-textual stuff here, which you can
    only appreciate fully if you're a completist but those two
    episodes still stand strong even without fully getting the
    meta-text in the background. I would also pick out &lt;a href="https://en.wikipedia.org/wiki/The_Crimson_Horror"&gt;The Crimson Horror&lt;/a&gt;, which is a comedy with a fun performance from
    Diana Rigg.&lt;/p&gt;

&lt;p&gt;Series 7 finishes with the Big Event episode of Who so far, the
    50th anniversary special (&lt;a href="https://en.wikipedia.org/wiki/The_Day_of_the_Doctor"&gt;The Day of the Doctor&lt;/a&gt;), broadcast 50 years after the first
    episode. Naturally there's a huge amount of references for the fans
    in this episode, which unites both Tenant and Smith's Doctor,
    together with John Hurt's sort-of Doctor. It still stands alone for
    pickers, so don't let the lack of background stop you watching it.
    Before you do, however, catch the minisode: the &lt;a href="https://www.youtube.com/watch?v=-U3jrS-uhuo"&gt;The Night of the Doctor&lt;/a&gt;
    on youtube. It's a remarkable coup of story-writing to get so much
    into a seven minute episode (although it does help to know that Paul
    McGann played the eighth Doctor, the one that immediately preceded the New Who
    period).&lt;/p&gt;

&lt;p&gt;With series 8, Peter Capaldi takes on the role of the twelfth Doctor,
    and brings a darker, less charming Doctor to the scene. Not everyone
    liked this take on the character, but I do as it reminds me of My
    Doctor (the 3rd - Jon Pertwee). It also is my favorite full-series
    arc, with some great character development and interplay between the Doctor and
    Clara. For cherry pickers, however, I'd pick out 
    &lt;a href="https://en.wikipedia.org/wiki/Into_the_Dalek"&gt;Into the Dalek&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Listen_(Doctor_Who)"&gt;Listen&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Kill_the_Moon"&gt;Kill the Moon&lt;/a&gt;, and &lt;a href="https://en.wikipedia.org/wiki/Mummy_on_the_Orient_Express"&gt;Mummy on the Orient Express&lt;/a&gt;, but ensure you see the
    others before you see &lt;a href="https://en.wikipedia.org/wiki/Mummy_on_the_Orient_Express"&gt;Mummy on the Orient Express&lt;/a&gt; since that last episode
    gains a lot from getting familiar with Capaldi's Doctor.&lt;/p&gt;

&lt;p&gt;As I write this, the latest Doctor Who episode was &lt;a href="https://en.wikipedia.org/wiki/Last_Christmas_(Doctor_Who)"&gt;Last Christmas&lt;/a&gt;, the latest
    Christmas Special, which is also a pick for its wonderful mashup of
    Alien, Santa Claus, and another movie you'll recognize.&lt;/p&gt;

&lt;p&gt;So how about Classic Who? There's lots of Classic Who, but if
    you're going to explore it, I should pick out a couple of places to start.
    Unlike New Who which goes for single or double episodes, Classic Who
    had short serials of 4-6 half hour episodes. I would start with &lt;a href="https://en.wikipedia.org/wiki/City_of_Death"&gt;City of Death&lt;/a&gt;, which has the iconic Tom Baker as the fourth Doctor,
    Romana as a Time Lord companion, and a script that clearly shows it
    was part-written by Douglas Adams. After that I have to point you to
    some 3rd Doctor (since he's My Doctor), and pick out
    &lt;a href="https://en.wikipedia.org/wiki/Carnival_of_Monsters"&gt;Carnival of
    Monsters&lt;/a&gt;, written by Robert Holmes, generally rated as the
    greatest of the Classic Who writers, and features Jo Grant who was
    the companion I best remember. &lt;/p&gt;



&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/articles/doctor-who.html&amp;amp;text=A%20Cherry%20Picker&amp;#x27;s%20Guide%20to%20Doctor%20Who" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/articles/doctor-who.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/articles/doctor-who.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;For articles on similar topics&amp;#8230;&lt;/h2&gt;

&lt;p&gt;&amp;#8230;take a look at the tag: &lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:37:00 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:a-cherry-pickers-guide-to-doctor-who.html</guid></item><item><title>Contract and End-to-End tests for Microservices</title><link>http://www.ciandcd.com/contract-and-end-to-end-tests-for-microservices.html</link><description>&lt;div&gt;&lt;h1&gt;Testing Strategies in a Microservice Architecture&lt;/h1&gt;

&lt;p class="abstract"&gt;
        There has been a shift in service based architectures over the last few
        years towards smaller, more focussed "micro" services. There are many
        benefits with this approach such as the ability to independently
        deploy, scale and maintain each component and
        parallelize development across multiple teams. However,
        once these additional network partitions have been introduced, the
        testing strategies that applied for monolithic in process applications
        need to be reconsidered.
      &lt;/p&gt;

&lt;p class="author"&gt;&lt;a href="http://github.com/tobyclemson" rel="author"&gt;Toby Clemson&lt;/a&gt;&lt;/p&gt;

 
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sat, 20 Jun 2015 23:46:55 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-20:contract-and-end-to-end-tests-for-microservices.html</guid></item><item><title>Videos from XConf Manchester</title><link>http://www.ciandcd.com/videos-from-xconf-manchester.html</link><description>&lt;div&gt;&lt;p&gt;I'm an author, speaker&amp;#8230; essentially a loud-mouthed pundit on the topic of software development. I've been working in the software industry since the mid-'80s where I got into the then-new world of object-oriented software. I got a call from ThoughtWorks in 1999 to consult on one of their projects and they rapidly became my favorite client. Within a year I was an employee.&lt;/p&gt;

&lt;p&gt;My main interest is to understand how to design software systems, so as to maximize the productivity of development teams. In doing this I&amp;#8217;ve looked to understand the patterns of good software design, and also the processes that support software design. I learn a lot from listening to the experiences of my fellow ThoughtWorkers: digging for useful ideas and communicating them to the wider world.&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sat, 20 Jun 2015 23:46:51 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-20:videos-from-xconf-manchester.html</guid></item><item><title>Sony a6000 with 16-70mm lens</title><link>http://www.ciandcd.com/sony-a6000-with-16-70mm-lens.html</link><description>&lt;div&gt;&lt;p&gt;There's been a steady rise of mirrorless cameras in the last
    few years. It seems all the cool kids have been getting into the
    Fuji X series. Although I'm not that concerned with camera gear, I
    have been keeping an eye on this trend myself - although I'm happy
    with most aspects of my current Canon DSLR system, there is one area where
    it comes up short - or rather comes up heavy, since the
    problematic aspect is weight.&lt;/p&gt;

&lt;img src="images/sony-a6000/20140818-DSC00172.jpg"&gt;
&lt;p class="photoCaption"&gt;
      Cadillac Mountain, Acadia National Park, ME
    &lt;/p&gt;
&lt;p&gt;My first serious digital camera was the &lt;a href="http://www.imaging-resource.com/PRODS/A1/A1A.HTM"&gt;Minolta A1&lt;/a&gt;.
    It was primitive by modern standards, small sensor, fixed lens,
    but it weighed in at 650g. That meant that it was easy to carry
    around, usually in a case that I threaded onto my belt. When it
    died and I got my first Canon DLSR, the increased weight meant
    that I now needed a special camera belt to carry it around on.
    Although the Canon was a much better camera, I missed the weight
    of the A1.&lt;/p&gt;

&lt;p&gt;So I've been watching the mirrorless scene, keeping an eye out
    for my neo-A1. The Fujis have got very close, but a vital element
    in the camera body is a tilting LCD display. Although I use the
    viewfinder most of the time, I do find it valuable to use a
    tilting display when I want to shoot at different angles: low
    down, overhead, or to get the right angle on a flower. The
    &lt;a href="http://www.amazon.com/gp/product/B00FPKDRVY?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B00FPKDRVY"&gt;rangefinder format Fujis&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt; didn't have a tilting screen (or lacked a
    viewfinder). The &lt;a href="http://www.amazon.com/gp/product/B00HYAL88W?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B00HYAL88W"&gt;XT-1&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt; does, but it then broke my weight limit.&lt;/p&gt;

&lt;p&gt;Although a lot of photo-blog concentrates on the Fujis, they
    aren't the only mirrorless game in town. The micro 4/3 cameras
    have been around for a while, but I didn't fancy reducing the
    sensor size. About a year ago a friend showed me his &lt;a href="http://www.amazon.com/gp/product/B0096W1OKS?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B0096W1OKS"&gt;Sony NEX-6&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt;. It seemed to hit all the
    right buttons for me, but it didn't have a lens that was right for
    me. I didn't like the power zoom on the kit lens - I wanted a good
    quality normal range zoom, something cose to the &lt;a href="http://www.amazon.com/gp/product/B002NEGTTM?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B002NEGTTM"&gt;15-85mm&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt; that spends most time on &lt;a href="/bliki/Canon60D.html"&gt;my
    Canon 60D&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So the appearance of the &lt;a href="http://www.amazon.com/gp/product/B00ENZRPG0?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B00ENZRPG0"&gt;Sony-Zeiss
    16-70mm&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt; lens, really made the Sony route pretty
    attractive. I waited for the &lt;a href="http://www.amazon.com/gp/product/B00I8BICCG?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B00I8BICCG"&gt;Sony a6000
    camera&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt; (the new version of the NEX-6) to be supported by
    Apple's raw processing and then went and bought the package. I've
    now used the combo for a few months, so can make an informal
    review.&lt;/p&gt;

&lt;img src="images/sony-a6000/sony-a6000.jpg"&gt;
&lt;p class="photoCaption"&gt;
      My &lt;a href="http://www.amazon.com/gp/product/B00I8BICCG?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B00I8BICCG"&gt;Sony a6000&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt; with the &lt;a href="http://www.amazon.com/gp/product/B00ENZRPG0?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B00ENZRPG0"&gt;Sony Zeiss 16-70mm lens&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt;. (While the
      other photos in the article were taken with this camera, I used
      my iPhone and &lt;a href="http://www.amazon.com/gp/product/B00JNRW950?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B00JNRW950"&gt;nova&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt; for this one.)&lt;/p&gt;
&lt;p&gt;Number one question for me, is the ergonomics - how much do I
    enjoy using it? My answer is "a lot". The weight is back down to
    the A1 levels, so I got a case that will thread onto my belt and a
    wrist strap. The whole setup is much more fun and convenient, the
    camera is out of the way until I need it, and I can get to it
    quickly when I do. The lens zooms smoothly and covers my common
    shooting range, and the lens balances really well with the camera.
    The camera controls are very customizable, you can assign most
    functions to any button, and I've found a setup that works well
    for me. The electronic viewfinder is big and bright, indeed my
    Canon's viewfinder now feels small when I use it. I particularly
    like the fact that I can set the electronic viewfinder to show
    subtle zebra stripes on over-exposed highlights, which means I
    don't need to chimp to check highlights any more.&lt;/p&gt;

&lt;p&gt;A lot of people like the Fuji's retro controls: aperture dial
    on the lens, dedicated shutter speed and ISO dials. Although they
    appeal to my nostalgia, I suspect the mode dial approach of the
    Sony works better for me. There are times when I like being able to switch quickly
    from shooting a landscape (aperture priority at f8) to shooting
    people (shutter speed at 1/250), and being able to do that with a
    single click of the mode dial is very handy. (The Sony also has the
    ability to save settings in presets, but I haven't felt the need
    to figure that out yet.)&lt;/p&gt;

&lt;p&gt;So how about image quality? I worry less about a camera's image
    quality then many people seem to. I consider that the quality of
    your camera is, at best, third on the list of factors of getting a
    good photograph. &lt;a href="#footnote-quality"&gt;[1]&lt;/a&gt; And realistically, any
    camera I'm likely to buy is going to be a better camera than I am
    a photographer. Having said that I'm not noticing any reduction in
    quality compared to my Canon setup. In fact I've noticed a couple
    of things looking better. The rendering of clouds in some
    situations is strikingly nice. I also think I'm able to dig out
    more detail in over and under exposed areas of the RAW files,
    giving me more flexibility when tweaking the exposure in
    post-processing.&lt;/p&gt;

&lt;img src="images/sony-a6000/20140915-DSC00362.jpg"&gt;
&lt;p class="photoCaption"&gt;
      Bergh&amp;#252;tte Fluhalp, Zermatt, Switzerland
    &lt;/p&gt;
&lt;p&gt;So all in all, I'm very happy with the purchase. It's now my
    principal serious camera, and will be my usual choice for future trips.&lt;/p&gt;

&lt;p&gt;I'm not yet at the point of getting more lenses for the Sony.
    There are a few that have some appeal. There is a &lt;a href="http://www.amazon.com/gp/product/B00B20OYUO?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B00B20OYUO"&gt;nice prime pancake lens&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt;, which would
    allow me to pop the camera in a jacket pocket. &lt;a href="http://www.amazon.com/gp/product/B0096W1ONK?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B0096W1ONK"&gt;Super-wide&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt; and &lt;a href="http://www.amazon.com/gp/product/B00HNJWSDS?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B00HNJWSDS"&gt;tele
    zooms&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt; would replace those ranges on my Canon. But I'm not sure I'm
    so committed to the Sony system yet that I want to invest in many
    more lenses.&lt;/p&gt;

&lt;p&gt;One lens I would most want is the equivalent to my &lt;a href="http://www.amazon.com/gp/product/B00009USVW?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B00009USVW"&gt;Canon 100mm f2&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt;, which I find really
    handy for my informal portraiture. If I could really stretch out
    my wish list, I'd like some way to link the camera to my &lt;a href="http://www.amazon.com/gp/product/B00JNRW950?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B00JNRW950"&gt;Nova flash unit&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt;. The Nova is such a
    convenient flash to carry around, it would be lovely to use it with
    cameras other than the iPhone.&lt;/p&gt;

&lt;img src="images/sony-a6000/20141030-DSC01260.jpg" width="400"&gt;
&lt;p class="photoCaption"&gt;Xu Hao,
    ThoughtWorks Head of Technology for China&lt;/p&gt;
&lt;p&gt;Most of the time, I prefer using the Sony, but there are cases
    where the Canon still has an edge. I enjoy taking informal
    portraits of my colleagues during meetings, here I find the Sony
    is a little slower to be ready to shoot, only a second or so, but
    that's significant when you want to get someone laughing. I also
    think the Canon would be better for wildlife or sports - although
    I rarely do those. I still have flash gear and more lenses for the
    Canon, and if I'm not concerned about weight I'm happy to keep
    using it.&lt;/p&gt;

&lt;p&gt;But generally I would recommend this combination. If you have
    an SLR and are looking for less weight, then it certainly fit the
    bill for me. If you're looking to get into serious cameras then,
    unless you are expecting to do wildlife or sports, I would
    recommend going mirrorless, and this is a fine option.&lt;/p&gt;



&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/articles/sony-a6000.html&amp;amp;text=Sony%20a6000%20with%2016-70mm%20lens" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/articles/sony-a6000.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/articles/sony-a6000.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;For articles on similar topics&amp;#8230;&lt;/h2&gt;

&lt;p&gt;&amp;#8230;take a look at the following tags:&lt;/p&gt;

 
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sat, 20 Jun 2015 23:46:30 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-20:sony-a6000-with-16-70mm-lens.html</guid></item><item><title>Garmin Oregon 600</title><link>http://www.ciandcd.com/garmin-oregon-600.html</link><description>&lt;div&gt;&lt;p&gt;Not long ago, a GPS unit (I rather like the British word
    "satnav") that you carry around when hiking, was a geeky item for
    gadget freaks. These days, most fairly well-off people carry a
    smart phone with that capability and take it for granted, so much
    so that it's reasonable to wonder if there's any value left in a
    dedicated handheld satnav. I still rather like having one, I use
    it either when hiking, or mounted on my handlebars when cycling. I
    prefer it because it's more rugged in poor weather, and also using
    the GPS won't drain my phone battery. For the last few years I've
    used the &lt;a href="http://www.amazon.com/gp/product/B000CSOXTO?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B000CSOXTO"&gt;Garmin 60CSx&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt;. It's a
    nice unit, but my device has a worsening fault with it freezing on
    startup. &lt;a href="#footnote-60-fault"&gt;[1]&lt;/a&gt;&lt;/p&gt;

&lt;img src="images/garmin-oregon-600/garmin-oregon-600.jpg" width="300"&gt;
&lt;p class="photoCaption"&gt;&lt;/p&gt;
&lt;p&gt;So I fancied getting something new, and I settled on the
    &lt;a href="http://www.amazon.com/gp/product/B00AXUXRUC?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B00AXUXRUC"&gt;Garmin
    Oregon 600&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt;. So far I haven't had a chance to use it on the bike,
    and the winter means I won't get such a chance for several more
    months. But I did take it for a few days hiking in Switzerland,
    and thought I'd share my experiences.&lt;/p&gt;

&lt;p&gt;The Oregon has a touch screen, rather than the dedicated
    buttons of the 60CSx. I was a bit concerned about that, since when I
    bought the 60CSx I also looked at the Oregon of the time and
    concluded that the screen wasn't bright enough. But things have
    improved a lot over the few years since, and the screen looks
    lovely and bright now. The touch screen generally makes it easier
    to operate. I have some concern about what I'll do in the winter
    when I have gloves on, but we don't do much winter hiking, so I
    think we'll not be too worried about it.&lt;/p&gt;

&lt;p&gt;The Oregon comes in several variants. I bought the basic 600
    model. There's also a 650, which adds a built in camera. Since
    I'll have my smart phone with me, and usually a more serious
    camera, I didn't feel that was worthwhile. The other variation in
    the line is the 600t (and 650t) which adds a complete set of topo
    maps for the USA, but these are worthless as I'll elaborate on
    later.&lt;/p&gt;

&lt;p&gt;For our hiking trip I found the Oregon 600 worked out really
    well. The unit fits nicely in the hand, and the display is bright
    and clear. The touch screen makes using the map display much
    easier, with the usual pan and pinch operations that we're used to
    with smartphones. I found the battery life to be excellent, a pair
    of rechargeable batteries easily lasted for a couple of days
    hiking.&lt;/p&gt;

&lt;p&gt;The unit is extremely customizable, with every slot on the UI
    customizable to a particular application or screen within an
    application. This can be a bit of a problem, as you can invest a
    lot of time setting up the unit to way you'd like it to work (and
    it's confusing should you accidentally remove a key application
    from the main screen, can't figure out how to get the unit to find
    locations, and have to resort to googling the web to find you need
    to dig the application out from setup menus). A nice feature is
    that you can save your customizations for different modes of use -
    so I can have one set of customizations for hiking and another for
    biking. This looks very useful, although I haven't tried switching
    modes yet. &lt;/p&gt;

&lt;p&gt;The accuracy of the unit seems pretty good. I tend to treat a
    satnav as unit to use in combination with map and compass, and I
    don't do things like geocaching, so I'm more tolerant of lower
    accuracy than some would be. But it seemed to track our hiking
    well. That said, we tended to be in open country, not in woods, or
    in canyons that are a challenge for a satnav.&lt;/p&gt;

&lt;p&gt;The great plus on modern satnavs is how they work with maps,
    and when it comes to maps and Garmin there is both bad news and
    good news. For biking with the 60CSx I bought a set of Garmin's
    &lt;a href="http://www.amazon.com/gp/product/B003FXWX54?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B003FXWX54"&gt;routable
    street maps for North America&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt;. This allowed me to plot a route out on my
    computer before the ride and then follow it, an approach which
    works really well for me. But the Garmin maps are locked to a
    single physical device, so I can't transfer them from my now
    defunct 60CSx to my new Oregon. This is ludicrous, it actually acts
    as a disincentive for upgrading a Garmin unit, and means there's
    little incentive to stay within the Garmin brand. If you've
    invested money in maps, it's very frustrating to not be able to
    use them with another device, particularly when maps are so easily
    available for free on your smartphone &lt;a href="#footnote-maps-provider"&gt;[2]&lt;/a&gt;. Indeed this, plus my annoyance with the startup
    fault on the 60CSx, were reasons to move away from Garmin when I
    wanted to replace the unit.&lt;/p&gt;

&lt;p&gt;But there's good news on the maps front - a vibrant ecosystem
    of open-source maps (see below for links). I had discovered
    these with the 60CSx, which is why I'd never bothered with buying
    topo maps (and why the "t" units are a waste of money). I'd
    downloaded excellent topo maps for hiking trips in the US and
    Europe. There are also routable street maps based on open street
    maps, although I don't yet know how well they will work for me
    until spring gets us back on our saddles. But at least for the
    swiss trip, I found &lt;a href="http://download.freizeitkarte-osm.de/garmin/latest/CHE_de_gmapsupp.img.zip"&gt;excellent
    open source maps&lt;/a&gt; that I could easily install and use. And I
    appreciate that, unlike the 60CSx, the Oregon allows me to easily
    switch between different map images without faffing around with
    the file system on the micro SD card. Garmin's &lt;a href="http://www.garmin.com/en-US/shop/downloads/basecamp"&gt;BaseCamp
    application&lt;/a&gt;  &amp;#8212; for working with maps, waypoints, and tracks &amp;#8212; has
    steadily improved over the years on the mac. I use it both for
    pre-planning routes and waypoints and also for looking at tracks
    when we're done.&lt;/p&gt;

&lt;p&gt;All in all, I like my upgrade. The Oregon 600 is all in all a
    better experience to work with than the old one. If you're
    considering a dedicated hand-held satnav, then this is a good
    choice (with the proviso that I'm only basing it on a single
    hiking trip so far.)&lt;/p&gt;



&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/articles/garmin-oregon-600.html&amp;amp;text=Garmin%20Oregon%20600%20-%20a%20brief%20review" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/articles/garmin-oregon-600.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/articles/garmin-oregon-600.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;For articles on similar topics&amp;#8230;&lt;/h2&gt;

&lt;p&gt;&amp;#8230;take a look at the tag: &lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sat, 20 Jun 2015 23:46:27 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-20:garmin-oregon-600.html</guid></item><item><title>Replacing Throwing Exceptions with Notification in Validations</title><link>http://www.ciandcd.com/replacing-throwing-exceptions-with-notification-in-validations.html</link><description>&lt;div&gt;&lt;p class="abstract"&gt;&lt;i&gt;
    If you're validating some data, you usually shouldn't be using
    exceptions to signal validation failures. Here I describe how I'd
    refactor such code into using the Notification pattern.
  &lt;/i&gt;&lt;/p&gt;&lt;p&gt;I was recently looking at some code to do some basic
    validation of some incoming JSON messages. It looked something like this. &lt;/p&gt;

&lt;pre&gt;public void check() {
   if (date == null) throw new IllegalArgumentException("date is missing");
   LocalDate parsedDate;
   try {
     parsedDate = LocalDate.parse(date);
   }
   catch (DateTimeParseException e) {
     throw new IllegalArgumentException("Invalid format for date", e);
   }
   if (parsedDate.isBefore(LocalDate.now())) throw new IllegalArgumentException("date cannot be before today");
   if (numberOfSeats == null) throw new IllegalArgumentException("number of seats cannot be null");
   if (numberOfSeats &amp;lt; 1) throw new IllegalArgumentException("number of seats must be positive");
 }&lt;/pre&gt;

&lt;p class="code-remark"&gt;The code for this example is Java&lt;/p&gt;

&lt;p&gt;This is a common way to approach validation. You run a series
    of checks on some data (here just some fields within the class in
    question). If any of these checks fails, you throw an exception
    with an error message.&lt;/p&gt;

&lt;p&gt;I have a couple of problems with this approach. Firstly I'm not
    happy with using exceptions for something like this. Exceptions
    signal something outside the expected bounds of behavior of the
    code in question. But if you're running some checks on outside input,
    this is because you expect some messages to fail - and if a
    failure is expected behavior, then you shouldn't be
    using exceptions.&lt;/p&gt;

&lt;p&gt; if a failure is expected behavior, then you shouldn't
    be using exceptions &lt;/p&gt;
&lt;p&gt;The second problem with code like this is that it fails with
    the first error it detects, but usually it's better to report all
    errors with the incoming data, not just the first. That way a
    client can choose to display all errors for the user to fix in a
    single interaction rather than give her the impression she's
    playing a game of whack-a-mole with the computer.&lt;/p&gt;

&lt;p&gt;My preferred way to deal with reporting validation issues like
    this is the &lt;a href="http://martinfowler.com/eaaDev/Notification.html"&gt;Notification
    pattern&lt;/a&gt;. A notification is an object that collects errors,
     each validation failure adds an error to the notification. A
    validation method returns a notification, which you can then
    interrogate to get more information. A simple usage looks
    has code like this for the checks.&lt;/p&gt;

&lt;pre&gt;private void validateNumberOfSeats(Notification note) {
  if (numberOfSeats &amp;lt; 1) note.addError("number of seats must be positive");
  // more checks like this
}&lt;/pre&gt;

&lt;p&gt;We can then have a simple call such as
    &lt;code&gt;aNotification.hasErrors()&lt;/code&gt; to react if there are any
    errors. Other methods on the notification can drill into more
    details about the errors. &lt;a href="#footnote-boolean"&gt;[1]&lt;/a&gt;&lt;/p&gt;

&lt;img src="replaceThrowWithNotification/sketch.png" width="600"&gt;
&lt;p class="photoCaption"&gt;&lt;/p&gt;

&lt;h2&gt;When to use this refactoring&lt;/h2&gt;

&lt;p&gt;I need to stress here, that I'm not advocating getting rid of
      exceptions throughout your code base. Exceptions are a very
      useful technique for handling exceptional behavior and getting
      it away from the main flow of logic. This refactoring is a good
      one to use only when the outcome signaled by the exception
      isn't really exceptional, and thus should be handled through the
      main logic of the program. The example I'm looking at here,
      validation, is a common case of that.&lt;/p&gt;

&lt;p&gt;A useful rule of thumb when considering exceptions comes
      from the Pragmatic Programmers:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We believe that exceptions should rarely be used as part of
        a program's normal flow: exceptions should be reserved for
        unexpected events. Assume that an uncaught exception will
        terminate your program and ask yourself, "Will this code still
        run if I remove all the exception handlers?" If the answer is
        "no", then maybe exceptions are being used in nonexceptional
        circumstances.&lt;/p&gt;

&lt;p class="quote-attribution"&gt;&lt;a href="http://www.amazon.com/gp/product/020161622X?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=020161622X"&gt;-- Dave Thomas and Andy Hunt&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;An important consequence of this is that whether to use
      exceptions for a particular task is dependent on the context.
      So, as the prags go on to say, reading from a file that isn't
      there may or may not be an exception depending on the
      circumstances. If you are trying to read a well known file
      location, such as &lt;code&gt;/etc/hosts&lt;/code&gt; on a unix system, then
      it's likely you can assume the file should be there, so
      throwing an exception is reasonable. On the other hand if you
      are trying to read a file from a path that the user has typed in
      on the command-line, then you should expect that it's likely the
      file isn't there, and should use another mechanism - one that
      communicates the unexceptional nature of the error.&lt;/p&gt;

&lt;p&gt;There is a case when it may be sensible to use exceptions
      for validation failures. This would be situations where you have
      data that you expect to have already been validated earlier in
      processing, but you want to run the validation checks again to
      guard against a programming error letting some invalid data slip
      through.&lt;/p&gt;

&lt;p&gt;This article is about replacing exceptions for notification
      in the context of validating raw input. You may also find this
      technique useful in other situations where a notification is a
      better choice than throwing an exception, but I'm focusing on
      the validation case here, as it is a common one.&lt;/p&gt;

&lt;h2&gt;Starting Point&lt;/h2&gt;

&lt;p&gt;I've not mentioned the example domain so far, since I was just
      interested in the broad shape of the code. But as we explore the
      example further, I'll need to engage with the domain. In this
      case it's some code that receives JSON messages booking seats at a
      theater. The code is in a booking request class that's populated
      from the JSON using the gson library.&lt;/p&gt;

&lt;pre&gt;gson.fromJson(jsonString, BookingRequest.class)
&lt;/pre&gt;

&lt;p class="code-remark"&gt;Gson takes a class, looks for any fields that match a key in
      the JSON document, and then populates the matching fields.&lt;/p&gt;

&lt;p&gt;The booking request contains just two elements that we
      are validating here, the date of the performance and how many
      seats are being requested&lt;/p&gt;

&lt;p class="code-label"&gt;class BookingRequest&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  private Integer numberOfSeats; 
  private String date;
&lt;/pre&gt;

&lt;p&gt;The validation checks are the ones I showed above&lt;/p&gt;

&lt;p class="code-label"&gt;class BookingRequest&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  public void check() {
     if (date == null) throw new IllegalArgumentException("date is missing");
     LocalDate parsedDate;
     try {
       parsedDate = LocalDate.parse(date);
     }
     catch (DateTimeParseException e) {
       throw new IllegalArgumentException("Invalid format for date", e);
     }
     if (parsedDate.isBefore(LocalDate.now())) throw new IllegalArgumentException("date cannot be before today");
     if (numberOfSeats == null) throw new IllegalArgumentException("number of seats cannot be null");
     if (numberOfSeats &amp;lt; 1) throw new IllegalArgumentException("number of seats must be positive");
   }&lt;/pre&gt;

&lt;h2&gt;Building a Notification&lt;/h2&gt;

&lt;p&gt;In order to use a notification, you have to create the
      notification object. A notification can be really simple,
      sometimes just a list of strings will do the trick. &lt;/p&gt;

&lt;p&gt;A Notification collects together errors&lt;/p&gt;
&lt;pre&gt;List&amp;lt;String&amp;gt; notification = new ArrayList&amp;lt;&amp;gt;();
if (numberOfSeats &amp;lt; 5) notification.add("number of seats too small");
// do some more checks

// then later&amp;#8230;
if ( ! notification.isEmpty()) // handle the error condition
&lt;/pre&gt;

&lt;p&gt;Although a simple list idiom makes a lightweight implementation of
      the pattern, I usually like to do a bit more than this, creating
      a simple class instead. &lt;/p&gt;

&lt;pre&gt;public class Notification {
  private List&amp;lt;String&amp;gt; errors = new ArrayList&amp;lt;&amp;gt;();

  public void addError(String message) { errors.add(message); }
  public boolean hasErrors() {
    return ! errors.isEmpty();
  }
  &amp;#8230;
&lt;/pre&gt;

&lt;p&gt;By using a real class, I can make my intention clearer - the
      reader doesn't have to perform the mental map between the idiom
      and its full meaning.&lt;/p&gt;

&lt;h2&gt;Splitting the check method&lt;/h2&gt;

&lt;p&gt;My first step is to split the check method into two parts, an
      inner part that will eventually deal only with notifications and
      not throw any exceptions, and an outer part that will preserve
      the current behavior of the check method, which is to throw an
      exception is there are any validation failures.&lt;/p&gt;

&lt;p&gt;My first step to do this is to use &lt;a href="http://refactoring.com/catalog/extractMethod.html"&gt;Extract Method&lt;/a&gt; in an unusual way in that I'm extracting the
      entire body of the check method into a validation method.&lt;/p&gt;

&lt;p class="code-label"&gt;class BookingRequest&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  public void check() {
&lt;p class="highlight"&gt;    validation();&lt;/p&gt;
  }

  public void validation() {
    if (date == null) throw new IllegalArgumentException("date is missing");
    LocalDate parsedDate;
    try {
      parsedDate = LocalDate.parse(date);
    }
    catch (DateTimeParseException e) {
      throw new IllegalArgumentException("Invalid format for date", e);
    }
    if (parsedDate.isBefore(LocalDate.now())) throw new IllegalArgumentException("date cannot be before today");
    if (numberOfSeats == null) throw new IllegalArgumentException("number of seats cannot be null");
    if (numberOfSeats &amp;lt; 1) throw new IllegalArgumentException("number of seats must be positive");
  }&lt;/pre&gt;

&lt;p&gt;I then adjust the validation method to create and return a
      notification.&lt;/p&gt;

&lt;p class="code-label"&gt;class BookingRequest&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  public Notification validation() {
&lt;p class="highlight"&gt;    Notification note = new Notification();&lt;/p&gt;
    if (date == null) throw new IllegalArgumentException("date is missing");
    LocalDate parsedDate;
    try {
      parsedDate = LocalDate.parse(date);
    }
    catch (DateTimeParseException e) {
      throw new IllegalArgumentException("Invalid format for date", e);
    }
    if (parsedDate.isBefore(LocalDate.now())) throw new IllegalArgumentException("date cannot be before today");
    if (numberOfSeats == null) throw new IllegalArgumentException("number of seats cannot be null");
    if (numberOfSeats &amp;lt; 1) throw new IllegalArgumentException("number of seats must be positive");
&lt;p class="highlight"&gt;    return note;&lt;/p&gt;
  }&lt;/pre&gt;

&lt;p&gt;I can now test the notification and throw an exception if it
      contains errors.&lt;/p&gt;

&lt;p class="code-label"&gt;class BookingRequest&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  public void check() {
&lt;p class="highlight"&gt;    if (validation().hasErrors()) 
      throw new IllegalArgumentException(validation().errorMessage());&lt;/p&gt;
  }&lt;/pre&gt;

&lt;p&gt;I made the validation method public, because I'm expecting
      that most callers in the future will prefer to use this method,
      rather than the check method.&lt;/p&gt;

&lt;p&gt;Splitting the original method allows me to separate the
      validation check from the decision about how to respond to failure.&lt;/p&gt;
&lt;p&gt;At this point I haven't changed the behavior of the code at
      all, the notification won't contain any errors and any
      validation checks that fail will continue to throw an exception
      and ignore the new machinery I've put in. But I've now set
      things up ready to start replacing exception throws with
      manipulating the notification.&lt;/p&gt;

&lt;p&gt;Before I go on to that, however, I need to say something
      about error messages. When we're doing a refactoring, the rule
      is to avoid changes in observable behavior. In situations like
      this, such a rule leads immediately to the question of what
      behavior is observable. Obviously the throwing of the correct
      exception is something the outer program will observe - but to
      what extent do they care about the error message? The
      notification will eventually collect multiple errors and could
      summarize them together into a single message with something
      like&lt;/p&gt;

&lt;p class="code-label"&gt;class Notification&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  public String errorMessage() {
    return errors.stream()
      .collect(Collectors.joining(", "));
  }
&lt;/pre&gt;

&lt;p&gt;But that would be a problem if the higher levels of the
      program was relying on only getting the message from the first
      error that's detected, in which case you'd need something like&lt;/p&gt;

&lt;p class="code-label"&gt;class Notification&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  public String errorMessage() { return errors.get(0); }
&lt;/pre&gt;

&lt;p&gt;You have to look not just at the calling function, but also
      any exception handlers to figure out what the right response is
      in this situation.&lt;/p&gt;

&lt;p&gt;Although there's no way I should have introduced any problems
      at this point, I would certainly compile and test before making
      the next changes. Just because there's no chance any sensible
      person could have messed those changes up doesn't mean I can't
      mess it up.&lt;/p&gt;

&lt;h2&gt;Validating the number&lt;/h2&gt;

&lt;p&gt;The obvious thing to do now is to replace the first
      validation&lt;/p&gt;

&lt;p class="code-label"&gt;class BookingRequest&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  public Notification validation() {
    Notification note = new Notification();
    if (date == null) &lt;p class="highlight"&gt;note.addError&lt;/p&gt;("date is missing");
    LocalDate parsedDate;
    try {
      parsedDate = LocalDate.parse(date);
    }
    catch (DateTimeParseException e) {
      throw new IllegalArgumentException("Invalid format for date", e);
    }
    if (parsedDate.isBefore(LocalDate.now())) throw new IllegalArgumentException("date cannot be before today");
    if (numberOfSeats == null) throw new IllegalArgumentException("number of seats cannot be null");
    if (numberOfSeats &amp;lt; 1) throw new IllegalArgumentException("number of seats must be positive");
    return note;
  }
&lt;/pre&gt;

&lt;p&gt;An obvious move, but a bad one, as this will break the code.
      If we pass a null date into the function, it will add an error
      to the notification, but then merrily attempt to parse it and
      throw a null pointer exception - which isn't the exception we
      were looking for.&lt;/p&gt;

&lt;p&gt;So the non-obvious, but more effective thing to do in this
      case is to go backwards. &lt;/p&gt;

&lt;p class="code-label"&gt;class BookingRequest&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  public Notification validation() {
    Notification note = new Notification();
    if (date == null) throw new IllegalArgumentException("date is missing");
    LocalDate parsedDate;
    try {
      parsedDate = LocalDate.parse(date);
    }
    catch (DateTimeParseException e) {
      throw new IllegalArgumentException("Invalid format for date", e);
    }
    if (parsedDate.isBefore(LocalDate.now())) throw new IllegalArgumentException("date cannot be before today");
    if (numberOfSeats == null) throw new IllegalArgumentException("number of seats cannot be null");
    if (numberOfSeats &amp;lt; 1) &lt;p class="highlight"&gt;note.addError&lt;/p&gt;("number of seats must be positive");
    return note;
  }
&lt;/pre&gt;

&lt;p&gt;The previous check is a null check, so we need to use a
      conditional to avoid creating a null pointer exception.&lt;/p&gt;

&lt;p class="code-label"&gt;class BookingRequest&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  public Notification validation() {
    Notification note = new Notification();
    if (date == null) throw new IllegalArgumentException("date is missing");
    LocalDate parsedDate;
    try {
      parsedDate = LocalDate.parse(date);
    }
    catch (DateTimeParseException e) {
      throw new IllegalArgumentException("Invalid format for date", e);
    }
    if (parsedDate.isBefore(LocalDate.now())) throw new IllegalArgumentException("date cannot be before today");
&lt;p class="highlight"&gt;    if (numberOfSeats == null) note.addError("number of seats cannot be null");
    else if (numberOfSeats &amp;lt; 1) note.addError("number of seats must be positive");&lt;/p&gt;
    return note;
  }
&lt;/pre&gt;

&lt;p&gt;I see the next check involves a different field. Together
      with having to introduce a conditional with the previous
      refactoring, I'm now thinking this validation method is getting
      too complex and could do with being decomposed. So I extract the
      number validation parts.&lt;/p&gt;

&lt;p class="code-label"&gt;class BookingRequest&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  public Notification validation() {
    Notification note = new Notification();
    if (date == null) throw new IllegalArgumentException("date is missing");
    LocalDate parsedDate;
    try {
      parsedDate = LocalDate.parse(date);
    }
    catch (DateTimeParseException e) {
      throw new IllegalArgumentException("Invalid format for date", e);
    }
    if (parsedDate.isBefore(LocalDate.now())) throw new IllegalArgumentException("date cannot be before today");
&lt;p class="highlight"&gt;    validateNumberOfSeats(note);&lt;/p&gt;
    return note;
  }

  private void validateNumberOfSeats(Notification note) {
    if (numberOfSeats == null) note.addError("number of seats cannot be null");
    else if (numberOfSeats &amp;lt; 1) note.addError("number of seats must be positive");
  }
&lt;/pre&gt;

&lt;p&gt;Looking at the extracted validation for the number, I don't
      really like its structure. I don't like using if-then-else
      blocks in validation, since it can easily lead to overly nested
      code. I prefer linear code that aborts once it can't go on any
      further, which we can do with a guard clause. So I apply
      &lt;a href="http://refactoring.com/catalog/replaceNestedConditionalWithGuardClauses.html"&gt;Replace Nested Conditional with Guard Clauses&lt;/a&gt;.&lt;/p&gt;

&lt;p class="code-label"&gt;class BookingRequest&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  private void validateNumberOfSeats(Notification note) {
    if (numberOfSeats == null) {
      note.addError("number of seats cannot be null");
      return;
    }
    if (numberOfSeats &amp;lt; 1) note.addError("number of seats must be positive");
  }
&lt;/pre&gt;

&lt;p&gt;
        when we refactor we
      should always try to take the smallest steps we can that
      preserve behavior
      &lt;/p&gt;
&lt;p&gt;My decision to go backwards in order to keep the code green
      is an example of a crucial element of refactoring. Refactoring
      is a specific technique to restructure code through a series of
      behavior-preserving transformations. So when we refactor we
      should always try to take the smallest steps we can that
      preserve behavior. By doing this we reduce the chances of an
      error that will trap us in the debugger&lt;/p&gt;

&lt;h2&gt;Moving up the stack&lt;/h2&gt;

&lt;p&gt;Once we have the new method, the next task is to look at the
      callers of the original check method and consider adjusting them
      to make use of the new validate method instead. This will entail
      a broader look at how validation fits into the flow of the
      application, so it's outside the scope of this refactoring. But
      the medium-term target should be to eliminate the use of
      exceptions in any circumstance where we might expect validation failures. &lt;/p&gt;

&lt;p&gt;In many cases this should lead to being able to get rid of
      the check method entirely. In which case any tests on that
      method should be reworked to use the validate method. We might
      also want to adjust the tests to probe for proper collection of
      multiple errors using the notification.&lt;/p&gt;


&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/articles/replaceThrowWithNotification.html&amp;amp;text=Replacing%20Throwing%20Exceptions%20with%20Notification%20in%20Validations" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/articles/replaceThrowWithNotification.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/articles/replaceThrowWithNotification.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;For articles on similar topics&amp;#8230;&lt;/h2&gt;

&lt;p&gt;&amp;#8230;take a look at the tag: &lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sat, 20 Jun 2015 23:46:26 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-20:replacing-throwing-exceptions-with-notification-in-validations.html</guid></item><item><title>APIs should not be copyrightable</title><link>http://www.ciandcd.com/apis-should-not-be-copyrightable.html</link><description>&lt;div&gt;&lt;p&gt;Last month, the Electronic Frontier Foundation (EFF)
    &lt;a href="https://www.eff.org/press/releases/computer-scientists-ask-supreme-court-rule-apis-cant-be-copyrighted"&gt;filed
    an amicus brief with the Supreme Court of the United States&lt;/a&gt;, asking the
    justices to review an earlier lower court decision that allows
    APIs (Application Programming Interfaces) to be copyrightable. I'm
    one of the 77 software professionals who signed the brief,
    although rather intimidated by a group that includes Abelson &amp;amp;
    Sussman, Aho &amp;amp; Ullman, Josh Bloch, Fred Brooks, Vint Cerf,
    Peter Deutsch, Mitch Kapor, Alan Kay, Brian Kernighan, Barbara
    Liskov, Guido van Rossum, Bruce Schneier, and Ken Thompson.&lt;/p&gt;

&lt;img src="images/copyright-api/sketch.png" width="400"&gt;
&lt;p class="photoCaption"&gt;&lt;/p&gt;
&lt;p&gt;The original lawsuit was brought by Oracle against Google,
    claiming that Oracle held a copyright on the Java APIs, and that
    Google infringed these APIs when they built Android. My support in
    this brief has nothing to do with the details of the dispute
    between these two tech giants, but everything to do with the
    question of how intellectual property law should apply to
    software, particularly software interfaces.&lt;/p&gt;

&lt;p&gt;I'm not part of the thinking that asserts that nothing in
    software should be intellectual property. While I do think that
    &lt;a href="/bliki/SoftwarePatent.html"&gt;software patents are inherently
    broken&lt;/a&gt;, copyright is a good mechanism to allow software
    authors to have some degree of control over of what happens with their hard work.&lt;/p&gt;

&lt;p&gt;Software has always been a tricky source of income, because
    it's trivial to copy. Copyright provides a legal basis to control at least
    some copying. Without something like this, it
    becomes very hard for someone to work on creating things and still
    be able to pay the mortgage. While we all like free stuff, I think
    it's only fair to give people the chance to earn a living from the
    work they do.&lt;/p&gt;

&lt;p&gt;But any intellectual property mechanism has to balance this
    benefit with the danger that excessive intellectual property
    restrictions can impede further innovation, whether that be
    extending an invention, or reimagining a creative work. As a
    result, patent and copyright regimes have some form of limitation
    built in. One limitation is one of time: patents and copyrights
    expire (although the &lt;a href="http://artlawjournal.com/mickey-mouse-keeps-changing-copyright-law//"&gt;Mickey Mouse
    discontinuity&lt;/a&gt; is threatening that).&lt;/p&gt;

&lt;p&gt;Interfaces are how things plug together. An example from the
    physical world is cameras with interchangeable lenses. Many camera
    makers don't encourage other companies to make lenses for their
    cameras, but such third-party companies can reverse-engineer how
    the interface works and build a lens that will mount on a camera.
    We regularly see this happen with third-party parts providers -
    and these third parties do a great deal to provide lower costs and
    features that the main company doesn't support. I used a Sigma
    lens with my Canon camera because Canon didn't (at the time)
    make an 18-200mm lens. I've bought third party batteries for
    cameras because they're cheaper. Similarly I've repaired my car with third party
    parts again to lower costs or get an audio system that better
    matched my needs.&lt;/p&gt;

&lt;p&gt;Software interfaces are much the same, and the ability to
    extend open interfaces, or reverse-engineer interfaces, has played
    a big role in advancing software systems. Open interfaces were a
    vital part of allowing the growth of the internet, nobody has to
    pay a copyright licence to build a HTTP server, nor to connect to
    one. The growth of Unix-style operating systems relied greatly on
    the fact that although much of the source code for AT&amp;amp;T's Unix
    was copyrighted, the interfaces were not. This allowed offshoots
    such as BSD and Linux to follow Unix's interfaces, which helped
    these open-source systems to get traction by making it easier for
    programs built on top of Unix to interact with new
    implementations.&lt;/p&gt;

&lt;blockquote class="twitter-tweet" lang="en"&gt;A picture is worth a 1000 words, so here's a picture of some books written by signatories of the EFF amicus brief &lt;a href="https://twitter.com/joshbloch/status/531937881703452673"&gt;-- Josh Bloch&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;The story of SMB and Samba is a good example of how
    non-copyrightable APIs spurred competition. When Windows became a
    dominent desktop operating system, its SMB protocol dominated
    simple networks. If non-windows computers wanted to communicate
    effectively with the dominant windows platform, they needed to
    talk to SMB. Microsoft didn't provide any documentation to help
    competitors do this, since an inability to communicate with SMB
    was a barrier to their competitors. However, Andrew Tridgell was
    able to deduce the specification for SMB and build an
    implementation for Unix, called Samba. By using Samba non-windows
    computers could collaborate on a network, thus encouraging the
    competition from Mac and Linux based systems. A similar story
    happened years before with the IBM BIOS, which was
    reverse-engineered by competitors.&lt;/p&gt;

&lt;p&gt;The power of a free-market system comes from competition, the
    notion that if I can find a way to bake bread that's either
    cheaper or tastier than my local bakers, I can start a bakery and
    compete with them. Over time my small bakery can grow and compete
    with the largest bakers. For this to work, it's vital that we
    construct the market so that existing players that dominate the
    market cannot build barriers to prevent new people coming in with
    innovations to reduce cost or improve quality. &lt;/p&gt;

&lt;p&gt;Software interfaces are critical points for this with software.
    By keeping interfaces open, we encourage a competitive
    marketplace of software systems that encourage innovation to
    provide more features and reduce costs. Closing this off will
    lead to incompatible islands of computer systems, unable to
    communicate.&lt;/p&gt;

&lt;p&gt;Such islands of incompatibility present a considerable barrier
    to new competitors, and are bad for that reason alone. But it's
    they are bad for users too. Users value software
    that can work together, and even if the various vendors of
    software aren't interested in communication, we should encourage
    other providers to step in and fill the gaps. Tying systems
    together requires open interfaces, so that integrators can safely
    implement an interface in order to create communication links. We
    value standard connectors in the physical world, and while
    software connections are often too varied for everything to be
    standardized, we shouldn't use copyright law to add further hurdles.&lt;/p&gt;

&lt;p&gt;The need to implement interfaces also goes much deeper than
    this. As programmers we often have to implement interfaces defined
    outside our code base in order to do our jobs. It's common to have
    to modify software that was written with one library in mind to
    work with another - a useful way to do this is to write &lt;a href="http://www.amazon.com/gp/product/0201634988?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=0201634988"&gt;adapters&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt; that implement the interface of the
    first library by forwarding the second. Implementing interfaces is
    also vital in testing, as it allows you to create &lt;a href="http://martinfowler.com/bliki/TestDouble.html"&gt;Test Doubles&lt;/a&gt;.
    &lt;/p&gt;

&lt;p&gt;So for the sake of our ability to write programs properly, our
    users' desire to have software work together, and for society's
    desire for free markets that spur competition &amp;#8212; copyright should
    not be used for APIs.&lt;/p&gt;



&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/articles/copyright-api.html&amp;amp;text=APIs%20should%20not%20be%20copyrightable" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/articles/copyright-api.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/articles/copyright-api.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;For articles on similar topics&amp;#8230;&lt;/h2&gt;

&lt;p&gt;&amp;#8230;take a look at the following tags:&lt;/p&gt;

 
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sat, 20 Jun 2015 23:46:24 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-20:apis-should-not-be-copyrightable.html</guid></item><item><title>Agile Architecture</title><link>http://www.ciandcd.com/agile-architecture.html</link><description>&lt;div&gt;&lt;p&gt;











Rethink featured three talks, examining the last 13 years and what's evolved since the Agile Manifesto was published.
[MUSIC PLAYING] The word "agile" is actually an adjective. [LAUGHING] You can't sell adjectives. You can only sell nouns. So the first thing that we had to do was totally corrupt the English language and to put "agile" from being an adjective into a noun. When you think about Agile, there is usually some sort of plan. The benefit is that it can change. I value building cathedrals more than I value cutting stone, and I think a lot of this is driven is around the notion that we, as software professionals, have taken a lot of craft back, but we still haven't gotten to the point where we're building cathedrals. With our Agile approach, what you're doing instead is you're trying to break down by pieces of functionality. Let's build a little piece of the system over the course of these days on the most valuable product in the thinnest slice that we can. But we're basically breaking it down by complete slices. I'm calling it food that way. Now, an interesting tie-in down here is, well, how does this lie with the notion of this kind of architecture thing? You cannot do good architecture without programming, and you cannot do good programming without thinking about architecture. [MUSIC PLAYING]
&lt;/p&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sat, 20 Jun 2015 23:46:23 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-20:agile-architecture.html</guid></item><item><title>perl 读写文件 - iTech</title><link>http://www.ciandcd.com/perl-du-xie-wen-jian-itech.html</link><description>&lt;div&gt;&lt;p&gt;#http://perlmaven.com/open-and-read-from-files&lt;/p&gt;&amp;#13;
&lt;p&gt;#mode operand create truncate&lt;br&gt;#read	&amp;lt;		&lt;br&gt;#write	&amp;gt;	    yes     yes	&lt;br&gt;#append	&amp;gt;&amp;gt;      yes	&lt;/p&gt;&amp;#13;
&lt;p&gt;Case 1: Throw an exception if you cannot open the file:&lt;/p&gt;&amp;#13;
&lt;pre class="brush:Perl;gutter:false;"&gt;use strict;&amp;#13;
use warnings;&amp;#13;
&amp;#13;
my $filename = 'data.txt';&amp;#13;
open(my $fh, '&amp;lt;:encoding(UTF-8)', $filename)&amp;#13;
or die "Could not open file '$filename' with the error $!";&amp;#13;
&amp;#13;
while (my $row = &amp;lt;$fh&amp;gt;) {&amp;#13;
chomp $row;&amp;#13;
print "$row\n";&amp;#13;
}&amp;#13;
close($fh);&amp;#13;
&lt;/pre&gt;&amp;#13;
&lt;p&gt;&amp;#12288;&amp;#12288;&lt;/p&gt;&amp;#13;
&lt;p&gt;Case 2: Give a warning if you cannot open the file, but keep running:&lt;/p&gt;&amp;#13;
&lt;pre class="brush:Perl;gutter:false;"&gt;use strict;&amp;#13;
use warnings;&amp;#13;
&amp;#13;
my $filename = 'data.txt';&amp;#13;
if (open(my $fh, '&amp;lt;:encoding(UTF-8)', $filename)) {&amp;#13;
while (my $row = &amp;lt;$fh&amp;gt;) {&amp;#13;
chomp $row;&amp;#13;
print "$row\n";&amp;#13;
}&amp;#13;
close($fh);&amp;#13;
} else {&amp;#13;
warn "Could not open file '$filename' $!";&amp;#13;
}&amp;#13;
&lt;/pre&gt;&amp;#13;
&lt;p&gt;&amp;#12288;&amp;#12288;&lt;/p&gt;&amp;#13;
&lt;p&gt;Case 3: Read one file into array&lt;/p&gt;&amp;#13;
&lt;pre class="brush:Perl;gutter:false;"&gt;use strict;&amp;#13;
use warnings;&amp;#13;
&amp;#13;
my $filename = 'data.txt';&amp;#13;
open (FILEIN, "&amp;lt;", $filename) &amp;#13;
or die "Could not open file '$filename' with the error $!";&amp;#13;
my @FileContents = &amp;lt;FILEIN&amp;gt;;&amp;#13;
for my $l (@FileContents){&amp;#13;
print "$l\n";&amp;#13;
}&amp;#13;
close FILEIN;&amp;#13;
&lt;/pre&gt;&amp;#13;
&lt;p&gt;&amp;#12288;&amp;#12288;&lt;/p&gt;&amp;#13;
&lt;p&gt;end&lt;/p&gt;&amp;#13;
&lt;p&gt;&amp;#160;&lt;/p&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sat, 20 Jun 2015 23:45:21 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-20:perl-du-xie-wen-jian-itech.html</guid></item><item><title>简单的proxy之TinyHTTPProxy.py - iTech</title><link>http://www.ciandcd.com/jian-dan-de-proxyzhi-tinyhttpproxypy-itech.html</link><description>&lt;div&gt;&lt;p&gt;&amp;#22914;&amp;#26524;&amp;#26159;&amp;#22312;&amp;#22806;&amp;#20225;&amp;#24037;&amp;#20316;&amp;#30340;&amp;#35805;&amp;#65292;&amp;#21487;&amp;#20197;&amp;#35775;&amp;#38382;&amp;#32654;&amp;#22269;&amp;#30340;&amp;#26426;&amp;#22120;&amp;#65292;&amp;#36825;&amp;#26679;&amp;#23601;&amp;#21487;&amp;#20197;&amp;#22312;&amp;#32654;&amp;#22269;&amp;#30340;&amp;#26426;&amp;#22120;&amp;#19978;&amp;#20026;&amp;#33258;&amp;#24049;&amp;#35013;&amp;#20010;proxy&amp;#65292;&amp;#28982;&amp;#21518;&amp;#26412;&amp;#22320;&amp;#23601;&amp;#21487;&amp;#20197;&amp;#24456;&amp;#23481;&amp;#26131;&amp;#30340;&amp;#20351;&amp;#29992;proxy&amp;#26469;&amp;#19978;&amp;#32593;&amp;#20102;&amp;#12290;&lt;/p&gt;&lt;p&gt;&amp;#20027;&amp;#39029;&amp;#65306;&lt;a href="http://www.voidtrance.net/2010/01/simple-python-http-proxy/"&gt;http://www.voidtrance.net/2010/01/simple-python-http-proxy/&lt;/a&gt;&amp;#160;&lt;br&gt;&amp;#19979;&amp;#36733;&amp;#65306;&lt;a href="http://www.voidtrance.net/downloads/tiny-proxy-0.3.1.tar.gz"&gt;http://www.voidtrance.net/downloads/tiny-proxy-0.3.1.tar.gz&lt;/a&gt;&amp;#160;&lt;/p&gt;&lt;p&gt;1&amp;#65289;&amp;#24456;&amp;#22909;&amp;#29992;&amp;#65292;&amp;#19979;&amp;#36733;&amp;#28982;&amp;#21518;&amp;#22312;&amp;#21518;&amp;#21488;&amp;#36816;&amp;#34892;&amp;#12290;&amp;#21482;&amp;#20381;&amp;#36182;&amp;#20110;&amp;#22522;&amp;#26412;&amp;#30340;python modules&amp;#65292;&amp;#36816;&amp;#34892;&amp;#30340;&amp;#26102;&amp;#20505;&amp;#19981;&amp;#38656;&amp;#35201;root&amp;#26435;&amp;#38480;&amp;#12290;&lt;/p&gt;&lt;p&gt;2&amp;#65289; Chrome&amp;#20013;&amp;#30340;switchsharper&amp;#25554;&amp;#20214;&amp;#30340;&amp;#37197;&amp;#32622;&amp;#65306;&lt;/p&gt;&lt;pre class="brush:python;gutter:true;"&gt;#!/usr/bin/python&amp;#13;
 &amp;#13;
__doc__ = """Tiny HTTP Proxy.&amp;#13;
 &amp;#13;
This module implements GET, HEAD, POST, PUT and DELETE methods&amp;#13;
on BaseHTTPServer, and behaves as an HTTP proxy.  The CONNECT&amp;#13;
method is also implemented experimentally, but has not been&amp;#13;
tested yet.&amp;#13;
 &amp;#13;
Any help will be greatly appreciated.       SUZUKI Hisao&amp;#13;
 &amp;#13;
2009/11/23 - Modified by Mitko Haralanov&amp;#13;
             * Added very simple FTP file retrieval&amp;#13;
             * Added custom logging methods&amp;#13;
             * Added code to make this a standalone application&amp;#13;
"""&amp;#13;
 &amp;#13;
__version__ = "0.3.1"&amp;#13;
 &amp;#13;
import BaseHTTPServer, select, socket, SocketServer, urlparse&amp;#13;
import logging&amp;#13;
import logging.handlers&amp;#13;
import getopt&amp;#13;
import sys&amp;#13;
import os&amp;#13;
import signal&amp;#13;
import threading&amp;#13;
from types import FrameType, CodeType&amp;#13;
from time import sleep&amp;#13;
import ftplib&amp;#13;
 &amp;#13;
DEFAULT_LOG_FILENAME = "proxy.log"&amp;#13;
 &amp;#13;
class ProxyHandler (BaseHTTPServer.BaseHTTPRequestHandler):&amp;#13;
    __base = BaseHTTPServer.BaseHTTPRequestHandler&amp;#13;
    __base_handle = __base.handle&amp;#13;
 &amp;#13;
    server_version = "TinyHTTPProxy/" + __version__&amp;#13;
    rbufsize = 0                        # self.rfile Be unbuffered&amp;#13;
 &amp;#13;
    def handle(self):&amp;#13;
        (ip, port) =  self.client_address&amp;#13;
        self.server.logger.log (logging.INFO, "Request from '%s'", ip)&amp;#13;
        if hasattr(self, 'allowed_clients') and ip not in self.allowed_clients:&amp;#13;
            self.raw_requestline = self.rfile.readline()&amp;#13;
            if self.parse_request(): self.send_error(403)&amp;#13;
        else:&amp;#13;
            self.__base_handle()&amp;#13;
 &amp;#13;
    def _connect_to(self, netloc, soc):&amp;#13;
        i = netloc.find(':')&amp;#13;
        if i &amp;gt;= 0:&amp;#13;
            host_port = netloc[:i], int(netloc[i+1:])&amp;#13;
        else:&amp;#13;
            host_port = netloc, 80&amp;#13;
        self.server.logger.log (logging.INFO, "connect to %s:%d", host_port[0], host_port[1])&amp;#13;
        try: soc.connect(host_port)&amp;#13;
        except socket.error, arg:&amp;#13;
            try: msg = arg[1]&amp;#13;
            except: msg = arg&amp;#13;
            self.send_error(404, msg)&amp;#13;
            return 0&amp;#13;
        return 1&amp;#13;
 &amp;#13;
    def do_CONNECT(self):&amp;#13;
        soc = socket.socket(socket.AF_INET, socket.SOCK_STREAM)&amp;#13;
        try:&amp;#13;
            if self._connect_to(self.path, soc):&amp;#13;
                self.log_request(200)&amp;#13;
                self.wfile.write(self.protocol_version +&amp;#13;
                                 " 200 Connection established\r\n")&amp;#13;
                self.wfile.write("Proxy-agent: %s\r\n" % self.version_string())&amp;#13;
                self.wfile.write("\r\n")&amp;#13;
                self._read_write(soc, 300)&amp;#13;
        finally:&amp;#13;
            soc.close()&amp;#13;
            self.connection.close()&amp;#13;
 &amp;#13;
    def do_GET(self):&amp;#13;
        (scm, netloc, path, params, query, fragment) = urlparse.urlparse(&amp;#13;
            self.path, 'http')&amp;#13;
        if scm not in ('http', 'ftp') or fragment or not netloc:&amp;#13;
            self.send_error(400, "bad url %s" % self.path)&amp;#13;
            return&amp;#13;
        soc = socket.socket(socket.AF_INET, socket.SOCK_STREAM)&amp;#13;
        try:&amp;#13;
            if scm == 'http':&amp;#13;
                if self._connect_to(netloc, soc):&amp;#13;
                    self.log_request()&amp;#13;
                    soc.send("%s %s %s\r\n" % (self.command,&amp;#13;
                                               urlparse.urlunparse(('', '', path,&amp;#13;
                                                                    params, query,&amp;#13;
                                                                    '')),&amp;#13;
                                               self.request_version))&amp;#13;
                    self.headers['Connection'] = 'close'&amp;#13;
                    del self.headers['Proxy-Connection']&amp;#13;
                    for key_val in self.headers.items():&amp;#13;
                        soc.send("%s: %s\r\n" % key_val)&amp;#13;
                    soc.send("\r\n")&amp;#13;
                    self._read_write(soc)&amp;#13;
            elif scm == 'ftp':&amp;#13;
                # fish out user and password information&amp;#13;
                i = netloc.find ('@')&amp;#13;
                if i &amp;gt;= 0:&amp;#13;
                    login_info, netloc = netloc[:i], netloc[i+1:]&amp;#13;
                    try: user, passwd = login_info.split (':', 1)&amp;#13;
                    except ValueError: user, passwd = "anonymous", None&amp;#13;
                else: user, passwd ="anonymous", None&amp;#13;
                self.log_request ()&amp;#13;
                try:&amp;#13;
                    ftp = ftplib.FTP (netloc)&amp;#13;
                    ftp.login (user, passwd)&amp;#13;
                    if self.command == "GET":&amp;#13;
                        ftp.retrbinary ("RETR %s"%path, self.connection.send)&amp;#13;
                    ftp.quit ()&amp;#13;
                except Exception, e:&amp;#13;
                    self.server.logger.log (logging.WARNING, "FTP Exception: %s",&amp;#13;
                                            e)&amp;#13;
        finally:&amp;#13;
            soc.close()&amp;#13;
            self.connection.close()&amp;#13;
 &amp;#13;
    def _read_write(self, soc, max_idling=20, local=False):&amp;#13;
        iw = [self.connection, soc]&amp;#13;
        local_data = ""&amp;#13;
        ow = []&amp;#13;
        count = 0&amp;#13;
        while 1:&amp;#13;
            count += 1&amp;#13;
            (ins, _, exs) = select.select(iw, ow, iw, 1)&amp;#13;
            if exs: break&amp;#13;
            if ins:&amp;#13;
                for i in ins:&amp;#13;
                    if i is soc: out = self.connection&amp;#13;
                    else: out = soc&amp;#13;
                    data = i.recv(8192)&amp;#13;
                    if data:&amp;#13;
                        if local: local_data += data&amp;#13;
                        else: out.send(data)&amp;#13;
                        count = 0&amp;#13;
            if count == max_idling: break&amp;#13;
        if local: return local_data&amp;#13;
        return None&amp;#13;
 &amp;#13;
    do_HEAD = do_GET&amp;#13;
    do_POST = do_GET&amp;#13;
    do_PUT  = do_GET&amp;#13;
    do_DELETE=do_GET&amp;#13;
 &amp;#13;
    def log_message (self, format, *args):&amp;#13;
        self.server.logger.log (logging.INFO, "%s %s", self.address_string (),&amp;#13;
                                format % args)&amp;#13;
 &amp;#13;
    def log_error (self, format, *args):&amp;#13;
        self.server.logger.log (logging.ERROR, "%s %s", self.address_string (),&amp;#13;
                                format % args)&amp;#13;
 &amp;#13;
class ThreadingHTTPServer (SocketServer.ThreadingMixIn,&amp;#13;
                           BaseHTTPServer.HTTPServer):&amp;#13;
    def __init__ (self, server_address, RequestHandlerClass, logger=None):&amp;#13;
        BaseHTTPServer.HTTPServer.__init__ (self, server_address,&amp;#13;
                                            RequestHandlerClass)&amp;#13;
        self.logger = logger&amp;#13;
 &amp;#13;
def logSetup (filename, log_size, daemon):&amp;#13;
    logger = logging.getLogger ("TinyHTTPProxy")&amp;#13;
    logger.setLevel (logging.INFO)&amp;#13;
    if not filename:&amp;#13;
        if not daemon:&amp;#13;
            # display to the screen&amp;#13;
            handler = logging.StreamHandler ()&amp;#13;
        else:&amp;#13;
            handler = logging.handlers.RotatingFileHandler (DEFAULT_LOG_FILENAME,&amp;#13;
                                                            maxBytes=(log_size*(1&amp;lt;&amp;lt;20)),&amp;#13;
                                                            backupCount=5)&amp;#13;
    else:&amp;#13;
        handler = logging.handlers.RotatingFileHandler (filename,&amp;#13;
                                                        maxBytes=(log_size*(1&amp;lt;&amp;lt;20)),&amp;#13;
                                                        backupCount=5)&amp;#13;
    fmt = logging.Formatter ("[%(asctime)-12s.%(msecs)03d] "&amp;#13;
                             "%(levelname)-8s {%(name)s %(threadName)s}"&amp;#13;
                             " %(message)s",&amp;#13;
                             "%Y-%m-%d %H:%M:%S")&amp;#13;
    handler.setFormatter (fmt)&amp;#13;
 &amp;#13;
    logger.addHandler (handler)&amp;#13;
    return logger&amp;#13;
 &amp;#13;
def usage (msg=None):&amp;#13;
    if msg: print msg&amp;#13;
    print sys.argv[0], "[-p port] [-l logfile] [-dh] [allowed_client_name ...]]"&amp;#13;
    print&amp;#13;
    print "   -p       - Port to bind to"&amp;#13;
    print "   -l       - Path to logfile. If not specified, STDOUT is used"&amp;#13;
    print "   -d       - Run in the background"&amp;#13;
    print&amp;#13;
 &amp;#13;
def handler (signo, frame):&amp;#13;
    while frame and isinstance (frame, FrameType):&amp;#13;
        if frame.f_code and isinstance (frame.f_code, CodeType):&amp;#13;
            if "run_event" in frame.f_code.co_varnames:&amp;#13;
                frame.f_locals["run_event"].set ()&amp;#13;
                return&amp;#13;
        frame = frame.f_back&amp;#13;
 &amp;#13;
def daemonize (logger):&amp;#13;
    class DevNull (object):&amp;#13;
        def __init__ (self): self.fd = os.open ("/dev/null", os.O_WRONLY)&amp;#13;
        def write (self, *args, **kwargs): return 0&amp;#13;
        def read (self, *args, **kwargs): return 0&amp;#13;
        def fileno (self): return self.fd&amp;#13;
        def close (self): os.close (self.fd)&amp;#13;
    class ErrorLog:&amp;#13;
        def __init__ (self, obj): self.obj = obj&amp;#13;
        def write (self, string): self.obj.log (logging.ERROR, string)&amp;#13;
        def read (self, *args, **kwargs): return 0&amp;#13;
        def close (self): pass&amp;#13;
 &amp;#13;
    if os.fork () != 0:&amp;#13;
        ## allow the child pid to instanciate the server&amp;#13;
        ## class&amp;#13;
        sleep (1)&amp;#13;
        sys.exit (0)&amp;#13;
    os.setsid ()&amp;#13;
    fd = os.open ('/dev/null', os.O_RDONLY)&amp;#13;
    if fd != 0:&amp;#13;
        os.dup2 (fd, 0)&amp;#13;
        os.close (fd)&amp;#13;
    null = DevNull ()&amp;#13;
    log = ErrorLog (logger)&amp;#13;
    sys.stdout = null&amp;#13;
    sys.stderr = log&amp;#13;
    sys.stdin = null&amp;#13;
    fd = os.open ('/dev/null', os.O_WRONLY)&amp;#13;
    #if fd != 1: os.dup2 (fd, 1)&amp;#13;
    os.dup2 (sys.stdout.fileno (), 1)&amp;#13;
    if fd != 2: os.dup2 (fd, 2)&amp;#13;
    if fd not in (1, 2): os.close (fd)&amp;#13;
 &amp;#13;
def main ():&amp;#13;
    logfile = None&amp;#13;
    daemon  = False&amp;#13;
    max_log_size = 20&amp;#13;
    port = 8000&amp;#13;
    allowed = []&amp;#13;
    run_event = threading.Event ()&amp;#13;
    local_hostname = socket.gethostname ()&amp;#13;
 &amp;#13;
    try: opts, args = getopt.getopt (sys.argv[1:], "l:dhp:", [])&amp;#13;
    except getopt.GetoptError, e:&amp;#13;
        usage (str (e))&amp;#13;
        return 1&amp;#13;
 &amp;#13;
    for opt, value in opts:&amp;#13;
        if opt == "-p": port = int (value)&amp;#13;
        if opt == "-l": logfile = value&amp;#13;
        if opt == "-d": daemon = not daemon&amp;#13;
        if opt == "-h":&amp;#13;
            usage ()&amp;#13;
            return 0&amp;#13;
 &amp;#13;
    # setup the log file&amp;#13;
    logger = logSetup (logfile, max_log_size, daemon)&amp;#13;
 &amp;#13;
    if daemon:&amp;#13;
        daemonize (logger)&amp;#13;
    signal.signal (signal.SIGINT, handler)&amp;#13;
 &amp;#13;
    if args:&amp;#13;
        allowed = []&amp;#13;
        for name in args:&amp;#13;
            client = socket.gethostbyname(name)&amp;#13;
            allowed.append(client)&amp;#13;
            logger.log (logging.INFO, "Accept: %s (%s)" % (client, name))&amp;#13;
        ProxyHandler.allowed_clients = allowed&amp;#13;
    else:&amp;#13;
        logger.log (logging.INFO, "Any clients will be served...")&amp;#13;
 &amp;#13;
    server_address = (socket.gethostbyname (local_hostname), port)&amp;#13;
    ProxyHandler.protocol = "HTTP/1.0"&amp;#13;
    httpd = ThreadingHTTPServer (server_address, ProxyHandler, logger)&amp;#13;
    sa = httpd.socket.getsockname ()&amp;#13;
    print "Servering HTTP on", sa[0], "port", sa[1]&amp;#13;
    req_count = 0&amp;#13;
    while not run_event.isSet ():&amp;#13;
        try:&amp;#13;
            httpd.handle_request ()&amp;#13;
            req_count += 1&amp;#13;
            if req_count == 1000:&amp;#13;
                logger.log (logging.INFO, "Number of active threads: %s",&amp;#13;
                            threading.activeCount ())&amp;#13;
                req_count = 0&amp;#13;
        except select.error, e:&amp;#13;
            if e[0] == 4 and run_event.isSet (): pass&amp;#13;
            else:&amp;#13;
                logger.log (logging.CRITICAL, "Errno: %d - %s", e[0], e[1])&amp;#13;
    logger.log (logging.INFO, "Server shutdown")&amp;#13;
    return 0&amp;#13;
 &amp;#13;
if __name__ == '__main__':&amp;#13;
    sys.exit (main ())&amp;#13;
&lt;/pre&gt;&amp;#13;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sat, 20 Jun 2015 23:45:21 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-20:jian-dan-de-proxyzhi-tinyhttpproxypy-itech.html</guid></item><item><title>20个有用的linux命令行技巧 - iTech</title><link>http://www.ciandcd.com/20ge-you-yong-de-linuxming-ling-xing-ji-qiao-itech.html</link><description>&lt;div&gt;&lt;p&gt;&amp;#160;&lt;/p&gt;&amp;#13;
&lt;h2&gt;Deleting a HUGE file&lt;/h2&gt;&amp;#13;
&lt;p&gt;I had a huge log file 200GB I need to delete on a production web server. My rm and ls command was crashed and I was afraid that the system to a crawl with huge disk I/O load. To remove a HUGE file, enter:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;&amp;gt; /path/to/file.log&amp;#13;
# or use the following syntax&amp;#13;
: &amp;gt; /path/to/file.log&amp;#13;
&amp;#160;&amp;#13;
# finally delete it &amp;#13;
rm /path/to/file.log&amp;#13;
&amp;#160;&lt;/pre&gt;&amp;#13;
&lt;h2&gt;Want to cache console output?&lt;/h2&gt;&amp;#13;
&lt;p&gt;Try the&amp;#160;script&amp;#160;command line utility to create a typescript of everything printed on your terminal.&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;script my.terminal.sessio&lt;/pre&gt;&amp;#13;
&lt;p&gt;Type commands:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;ls&amp;#13;
date&amp;#13;
sudo service foo stop&lt;/pre&gt;&amp;#13;
&lt;p&gt;To exit (to end script session) type&amp;#160;exit&amp;#160;or&amp;#160;logout&amp;#160;or press&amp;#160;control-D&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;exit&lt;/pre&gt;&amp;#13;
&lt;p&gt;To view type:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;&amp;#160;&amp;#13;
more my.terminal.session&amp;#13;
less my.terminal.session&amp;#13;
cat my.terminal.session&amp;#13;
&amp;#160;&lt;/pre&gt;&amp;#13;
&lt;h2&gt;Restoring deleted /tmp folder&lt;/h2&gt;&amp;#13;
&lt;p&gt;As my journey continues with&amp;#160;&lt;a href="http://www.cyberciti.biz/tips/my-10-unix-command-line-mistakes.html"&gt;Linux and Unix shell, I made a few mistakes&lt;/a&gt;. I accidentally deleted /tmp folder. To restore it all you have to do is:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;mkdir /tmp&amp;#13;
chmod 1777 /tmp&amp;#13;
chown root:root /tmp&amp;#13;
ls -ld /tmp&lt;/pre&gt;&amp;#13;
&lt;h2&gt;Locking a directory&lt;/h2&gt;&amp;#13;
&lt;p&gt;For privacy of my data I wanted to lock down /downloads on my file server. So I ran:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;chmod 0000 /downloads&lt;/pre&gt;&amp;#13;
&lt;p&gt;The root user can still has access and ls and cd commands will not work. To go back:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;chmod 0755 /downloads&lt;/pre&gt;&amp;#13;
&lt;h2&gt;Password protecting file in vim text editor&lt;/h2&gt;&amp;#13;
&lt;p&gt;Afraid that root user or someone may snoop into your personal text files? Try password protection to a file in vim, type:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;&amp;#160;&amp;#13;
vim +X filename&amp;#13;
&amp;#160;&lt;/pre&gt;&amp;#13;
&lt;p&gt;Or, before quitting in vim use&amp;#160;:X&amp;#160;vim command to encrypt your file and vim will prompt for a password.&lt;/p&gt;&amp;#13;
&lt;h2&gt;Clear gibberish all over the screen&lt;/h2&gt;&amp;#13;
&lt;p&gt;Just type:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;&amp;#160;&amp;#13;
reset&lt;/pre&gt;&amp;#13;
&lt;h2&gt;Becoming human&lt;/h2&gt;&amp;#13;
&lt;p&gt;Pass the&amp;#160;-h&amp;#160;or&amp;#160;-H&amp;#160;(and other options) command line option to GNU or BSD utilities to get output of command commands like ls, df, du, in human-understandable formats:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;&amp;#160;&amp;#13;
ls -lh&amp;#13;
# print sizes in human readable format (e.g., 1K 234M 2G)&amp;#13;
df -h&amp;#13;
df -k&amp;#13;
# show output in bytes, KB, MB, or GB&amp;#13;
free -b&amp;#13;
free -k&amp;#13;
free -m&amp;#13;
free -g&amp;#13;
# print sizes in human readable format (e.g., 1K 234M 2G)&amp;#13;
du -h&amp;#13;
# get file system perms in human readable format&amp;#13;
stat -c %A /boot&amp;#13;
# compare human readable numbers&amp;#13;
sort -h -a file&amp;#13;
# display the CPU information in human readable format on a Linux&amp;#13;
lscpu&amp;#13;
lscpu -e&amp;#13;
lscpu -e=cpu,node&amp;#13;
# Show the  size of each file but in a more human readable way&amp;#13;
tree -h&amp;#13;
tree -h /boot&amp;#13;
&amp;#160;&lt;/pre&gt;&amp;#13;
&lt;h2&gt;Show information about known users in the Linux based system&lt;/h2&gt;&amp;#13;
&lt;p&gt;Just type:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;## linux version ##&amp;#13;
lslogins&amp;#13;
&amp;#160;&amp;#13;
## BSD version ##&amp;#13;
logins&amp;#13;
&amp;#160;&lt;/pre&gt;&amp;#13;
&lt;p&gt;Sample outputs:&lt;/p&gt;&amp;#13;
&lt;pre&gt;UID USER      PWD-LOCK PWD-DENY LAST-LOGIN GECOS&amp;#13;
  0 root             0        0   22:37:59 root&amp;#13;
  1 bin              0        1            bin&amp;#13;
  2 daemon           0        1            daemon&amp;#13;
  3 adm              0        1            adm&amp;#13;
  4 lp               0        1            lp&amp;#13;
  5 sync             0        1            sync&amp;#13;
  6 shutdown         0        1 2014-Dec17 shutdown&amp;#13;
  7 halt             0        1            halt&amp;#13;
  8 mail             0        1            mail&amp;#13;
 10 uucp             0        1            uucp&amp;#13;
 11 operator         0        1            operator&amp;#13;
 12 games            0        1            games&amp;#13;
 13 gopher           0        1            gopher&amp;#13;
 14 ftp              0        1            FTP User&amp;#13;
 27 mysql            0        1            MySQL Server&amp;#13;
 38 ntp              0        1&amp;#13;
 48 apache           0        1            Apache&amp;#13;
 68 haldaemon        0        1            HAL daemon&amp;#13;
 69 vcsa             0        1            virtual console memory owner&amp;#13;
 72 tcpdump          0        1&amp;#13;
 74 sshd             0        1            Privilege-separated SSH&amp;#13;
 81 dbus             0        1            System message bus&amp;#13;
 89 postfix          0        1&amp;#13;
 99 nobody           0        1            Nobody&amp;#13;
173 abrt             0        1&amp;#13;
497 vnstat           0        1            vnStat user&amp;#13;
498 nginx            0        1            nginx user&amp;#13;
499 saslauth         0        1            "Saslauthd user"&lt;/pre&gt;&amp;#13;
&lt;h2&gt;How do I fix mess created by accidentally untarred files in the current dir?&lt;/h2&gt;&amp;#13;
&lt;p&gt;So I accidentally untar a tarball in /var/www/html/ directory instead of /home/projects/www/current. It created mess in /var/www/html/. The easiest way to fix this mess:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;&amp;#160;&amp;#13;
cd /var/www/html/&amp;#13;
/bin/rm -f "$(tar ztf /path/to/file.tar.gz)"&amp;#13;
&amp;#160;&lt;/pre&gt;&amp;#13;
&lt;h2&gt;Confused on a top command output?&lt;/h2&gt;&amp;#13;
&lt;p&gt;Seriously, you need to try out htop instead of top:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;sudo htop&lt;/pre&gt;&amp;#13;
&lt;h2&gt;Want to run the same command again?&lt;/h2&gt;&amp;#13;
&lt;p&gt;Just type&amp;#160;!!. For example:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;&amp;#160;&amp;#13;
/myhome/dir/script/name arg1 arg2&amp;#13;
&amp;#160;&amp;#13;
# To run the same command again &amp;#13;
!!&amp;#13;
&amp;#160;&amp;#13;
## To run the last command again as root user&amp;#13;
sudo !!&amp;#13;
&amp;#160;&lt;/pre&gt;&amp;#13;
&lt;p&gt;The&amp;#160;!!&amp;#160;repeats the most recent command. To run the most recent command beginning with "foo":&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;!foo&amp;#13;
# Run the most recent command beginning with "service" as root&amp;#13;
sudo !service&lt;/pre&gt;&amp;#13;
&lt;p&gt;The&amp;#160;!$&amp;#160;use to run command with the last argument of the most recent command:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;# Edit nginx.conf&amp;#13;
sudo vi /etc/nginx/nginx.conf&amp;#13;
&amp;#160;&amp;#13;
# Test nginx.conf for errors&amp;#13;
/sbin/nginx -t -c /etc/nginx/nginx.conf&amp;#13;
&amp;#160;&amp;#13;
# After testing a file with "/sbin/nginx -t -c /etc/nginx/nginx.conf", you&amp;#13;
# can edit file again with vi&amp;#13;
sudo vi !$&lt;/pre&gt;&amp;#13;
&lt;h2&gt;Get a reminder you when you have to leave&lt;/h2&gt;&amp;#13;
&lt;p&gt;If you need a reminder to leave your terminal, type the following command:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;&amp;#160;&amp;#13;
leave +hhmm&amp;#13;
&amp;#160;&lt;/pre&gt;&amp;#13;
&lt;p&gt;Where,&lt;/p&gt;&amp;#13;
&lt;ul&gt;&amp;#13;
&lt;li&gt;hhmm&amp;#160;- The time of day is in the form hhmm where hh is a time in hours (on a 12 or 24 hour clock), and mm are minutes. All times are converted to a 12 hour clock, and assumed to be in the next 12 hours.&lt;/li&gt;&amp;#13;
&lt;/ul&gt;&amp;#13;
&lt;h2&gt;Home sweet home&lt;/h2&gt;&amp;#13;
&lt;p&gt;Want to go the directory you were just in? Run:&lt;br&gt;&lt;code&gt;cd -&lt;/code&gt;&lt;br&gt;Need to quickly return to your home directory? Enter:&lt;br&gt;&lt;code&gt;cd&lt;/code&gt;&lt;br&gt;The variable&amp;#160;CDPATH&amp;#160;defines the search path for the directory containing directories:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;export CDPATH=/var/www:/nas10&lt;/pre&gt;&amp;#13;
&lt;p&gt;Now, instead of typing&amp;#160;cd /var/www/html/&amp;#160;I can simply type the following to cd into /var/www/html path:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;cd html&lt;/pre&gt;&amp;#13;
&lt;h2&gt;Editing a file being viewed with less pager&lt;/h2&gt;&amp;#13;
&lt;p&gt;To edit a file being viewed with less pager, press&amp;#160;v. You will have the file for edit under $EDITOR:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;&amp;#160;&amp;#13;
less *.c&amp;#13;
less foo.html&amp;#13;
## Press v to edit file ##&amp;#13;
## Quit from editor and you would return to the less pager again ##&amp;#13;
&amp;#160;&lt;/pre&gt;&amp;#13;
&lt;h2&gt;List all files or directories on your system&lt;/h2&gt;&amp;#13;
&lt;p&gt;To see all of the directories on your system, run:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;find / -type d | less&amp;#13;
&amp;#160;&amp;#13;
# List all directories in your $HOME&amp;#13;
find $HOME -type d -ls | less&lt;/pre&gt;&amp;#13;
&lt;p&gt;To see all of the files, run:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;find / -type f | less&amp;#13;
&amp;#160;&amp;#13;
# List all files in your $HOME&amp;#13;
find $HOME -type f -ls | less&lt;/pre&gt;&amp;#13;
&lt;h2&gt;Build directory trees in a single command&lt;/h2&gt;&amp;#13;
&lt;p&gt;You can create directory trees one at a time using mkdir command by passing the&amp;#160;-p&amp;#160;option:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;mkdir -p /jail/{dev,bin,sbin,etc,usr,lib,lib64}&amp;#13;
ls -l /jail/&lt;/pre&gt;&amp;#13;
&lt;h2&gt;Copy file into multiple directories&lt;/h2&gt;&amp;#13;
&lt;p&gt;Instead of running:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;cp /path/to/file /usr/dir1&amp;#13;
cp /path/to/file /var/dir2&amp;#13;
cp /path/to/file /nas/dir3&lt;/pre&gt;&amp;#13;
&lt;p&gt;Run the following command to copy file into multiple dirs:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;echo /usr/dir1 /var/dir2 /nas/dir3 |  xargs -n 1 cp -v /path/to/file&lt;/pre&gt;&amp;#13;
&lt;p&gt;&lt;a href="http://bash.cyberciti.biz/guide/Writing_your_first_shell_function"&gt;Creating a shell function&lt;/a&gt;&amp;#160;is left as an exercise for the reader&lt;/p&gt;&amp;#13;
&lt;h2&gt;Quickly find differences between two directories&lt;/h2&gt;&amp;#13;
&lt;p&gt;The diff command compare files line by line. It can also compare two directories:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;ls -l /tmp/r&amp;#13;
ls -l /tmp/s&amp;#13;
# Compare two folders using diff ##&amp;#13;
diff /tmp/r/ /tmp/s/&lt;/pre&gt;&amp;#13;
&lt;h2&gt;Text formatting&lt;/h2&gt;&amp;#13;
&lt;p&gt;You can reformat each paragraph with fmt command. In this example, I'm going to reformat file by wrapping overlong lines and filling short lines:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;fmt file.txt&lt;/pre&gt;&amp;#13;
&lt;p&gt;You can also split long lines, but do not refill i.e. wrap overlong lines, but do not fill short lines:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;fmt -s file.txt&lt;/pre&gt;&amp;#13;
&lt;h2&gt;See the output and write it to a file&lt;/h2&gt;&amp;#13;
&lt;p&gt;Use the tee command as follows to see the output on screen and also write to a log file named my.log:&lt;/p&gt;&amp;#13;
&lt;pre class="bash"&gt;mycoolapp arg1 arg2 input.file | tee my.log&lt;/pre&gt;&amp;#13;
&lt;p&gt;The tee command ensures that you will see mycoolapp output on on the screen and to a file same time.&lt;/p&gt;&amp;#13;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sat, 20 Jun 2015 23:45:20 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-20:20ge-you-yong-de-linuxming-ling-xing-ji-qiao-itech.html</guid></item><item><title>4个可以发送完整电子邮件的命令行工具 - iTech</title><link>http://www.ciandcd.com/4ge-ke-yi-fa-song-wan-zheng-dian-zi-you-jian-de-ming-ling-xing-gong-ju-itech.html</link><description>&lt;div&gt;&lt;p&gt;&amp;#20170;&amp;#22825;&amp;#30340;&amp;#25991;&amp;#31456;&amp;#37324;&amp;#25105;&amp;#20204;&amp;#20250;&amp;#35762;&amp;#21040;&amp;#19968;&amp;#20123;&amp;#20351;&amp;#29992;&lt;a href="http://www.linuxeye.com/" target="_blank"&gt;Linux&lt;/a&gt;&amp;#21629;&amp;#20196;&amp;#34892;&amp;#24037;&amp;#20855;&amp;#26469;&amp;#21457;&amp;#36865;&amp;#24102;&amp;#38468;&amp;#20214;&amp;#30340;&amp;#30005;&amp;#23376;&amp;#37038;&amp;#20214;&amp;#30340;&amp;#26041;&amp;#27861;&amp;#12290;&amp;#23427;&amp;#26377;&amp;#24456;&amp;#22810;&amp;#29992;&amp;#22788;&amp;#65292;&amp;#27604;&amp;#22914;&amp;#22312;&amp;#24212;&amp;#29992;&amp;#31243;&amp;#24207;&amp;#25152;&amp;#22312;&amp;#26381;&amp;#21153;&amp;#22120;&amp;#19978;&amp;#65292;&amp;#20351;&amp;#29992;&amp;#30005;&amp;#23376;&amp;#37038;&amp;#20214;&amp;#21457;&amp;#36865; &amp;#19968;&amp;#20010;&amp;#25991;&amp;#20214;&amp;#36807;&amp;#26469;&amp;#65292;&amp;#25110;&amp;#32773;&amp;#20320;&amp;#21487;&amp;#20197;&amp;#22312;&amp;#33050;&amp;#26412;&amp;#20013;&amp;#20351;&amp;#29992;&amp;#36825;&amp;#20123;&amp;#21629;&amp;#20196;&amp;#26469;&amp;#20570;&amp;#19968;&amp;#20123;&amp;#33258;&amp;#21160;&amp;#21270;&amp;#25805;&amp;#20316;&amp;#12290;&amp;#22312;&amp;#26412;&amp;#25991;&amp;#30340;&amp;#20363;&amp;#23376;&amp;#20013;&amp;#65292;&amp;#25105;&amp;#20204;&amp;#20250;&amp;#20351;&amp;#29992;foo.tar.gz&amp;#25991;&amp;#20214;&amp;#20316;&amp;#20026;&amp;#38468;&amp;#20214;&amp;#12290;&lt;/p&gt;&amp;#13;
&lt;p&gt;&amp;#160;&lt;/p&gt;&amp;#13;
&lt;p&gt;&amp;#26377;&amp;#19981;&amp;#21516;&amp;#30340;&amp;#21629;&amp;#20196;&amp;#34892;&amp;#24037;&amp;#20855;&amp;#21487;&amp;#20197;&amp;#21457;&amp;#36865;&amp;#37038;&amp;#20214;&amp;#65292;&amp;#36825;&amp;#37324;&amp;#25105;&amp;#20998;&amp;#20139;&amp;#20960;&amp;#20010;&amp;#22810;&amp;#25968;&amp;#29992;&amp;#25143;&amp;#20250;&amp;#20351;&amp;#29992;&amp;#30340;&amp;#24037;&amp;#20855;&amp;#65292;&amp;#22914;&lt;code&gt;mailx&lt;/code&gt;&amp;#12289;&lt;code&gt;mutt&lt;/code&gt;&amp;#21644;&lt;code&gt;swaks&lt;/code&gt;&amp;#12290;&lt;/p&gt;&amp;#13;
&lt;p&gt;&amp;#25105;&amp;#20204;&amp;#21363;&amp;#23558;&amp;#21576;&amp;#29616;&amp;#30340;&amp;#36825;&amp;#20123;&amp;#24037;&amp;#20855;&amp;#37117;&amp;#26159;&amp;#38750;&amp;#24120;&amp;#26377;&amp;#21517;&amp;#30340;&amp;#65292;&amp;#24182;&amp;#19988;&amp;#23384;&amp;#22312;&amp;#20110;&amp;#22810;&amp;#25968;Linux&amp;#21457;&amp;#34892;&amp;#29256;&amp;#40664;&amp;#35748;&amp;#30340;&amp;#36719;&amp;#20214;&amp;#20179;&amp;#24211;&amp;#20013;&amp;#65292;&amp;#20320;&amp;#21487;&amp;#20197;&amp;#20351;&amp;#29992;&amp;#22914;&amp;#19979;&amp;#21629;&amp;#20196;&amp;#23433;&amp;#35013;&amp;#65306;&lt;/p&gt;&amp;#13;
&lt;p&gt;&amp;#22312;&amp;#160;&lt;strong&gt;Debian / Ubuntu&lt;/strong&gt;&amp;#160;&amp;#31995;&amp;#32479;&lt;/p&gt;&amp;#13;
&lt;pre class="prettyprint prettyprinted"&gt;&lt;p class="pln"&gt;apt&lt;p class="pun"&gt;-&lt;p class="kwd"&gt;get&lt;p class="pln"&gt; install mutt&amp;#13;
apt&lt;p class="pun"&gt;-&lt;p class="kwd"&gt;get&lt;p class="pln"&gt; install swaks&amp;#13;
apt&lt;p class="pun"&gt;-&lt;p class="kwd"&gt;get&lt;p class="pln"&gt; install mailx&amp;#13;
apt&lt;p class="pun"&gt;-&lt;p class="kwd"&gt;get&lt;p class="pln"&gt; install sharutils&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/pre&gt;&amp;#13;
&lt;p&gt;&amp;#22312;&amp;#22522;&amp;#20110;Red Hat&amp;#30340;&amp;#31995;&amp;#32479;&amp;#65292;&amp;#22914;&amp;#160;&lt;strong&gt;&lt;a href="http://www.linuxeye.com/tags.php?/CentOS/" target="_blank"&gt;CentOS&lt;/a&gt;&lt;/strong&gt;&amp;#160;&amp;#25110;&amp;#32773;&amp;#160;&lt;strong&gt;Fedora&lt;/strong&gt;&lt;/p&gt;&amp;#13;
&lt;pre class="prettyprint prettyprinted"&gt;&lt;p class="pln"&gt;yum install mutt&amp;#13;
yum install swaks&amp;#13;
yum install mailx&amp;#13;
yum install sharutils&lt;/p&gt;&lt;/pre&gt;&amp;#13;
&lt;h3 id="toc_1"&gt;1) &amp;#20351;&amp;#29992; mail / mailx&lt;/h3&gt;&amp;#13;
&lt;p&gt;&lt;code&gt;mailx&lt;/code&gt;&amp;#24037;&amp;#20855;&amp;#22312;&amp;#22810;&amp;#25968;Linux&amp;#21457;&amp;#34892;&amp;#29256;&amp;#20013;&amp;#26159;&amp;#40664;&amp;#35748;&amp;#30340;&amp;#37038;&amp;#20214;&amp;#31243;&amp;#24207;&amp;#65292;&amp;#29616;&amp;#22312;&amp;#24050;&amp;#32463;&amp;#25903;&amp;#25345;&amp;#21457;&amp;#36865;&amp;#38468;&amp;#20214;&amp;#20102;&amp;#12290;&amp;#22914;&amp;#26524;&amp;#23427;&amp;#19981;&amp;#22312;&amp;#20320;&amp;#30340;&amp;#31995;&amp;#32479;&amp;#20013;&amp;#65292;&amp;#20320;&amp;#21487;&amp;#20197;&amp;#20351;&amp;#29992;&amp;#19978;&amp;#36793;&amp;#30340;&amp;#21629;&amp;#20196;&amp;#23433;&amp;#35013;&amp;#12290;&amp;#26377;&amp;#19968;&amp;#28857;&amp;#38656;&amp;#35201;&amp;#27880;&amp;#24847;&amp;#65292;&amp;#32769;&amp;#29256;&amp;#26412;&amp;#30340;mailx&amp;#21487;&amp;#33021;&amp;#19981;&amp;#25903;&amp;#25345;&amp;#21457;&amp;#36865;&amp;#38468;&amp;#20214;&amp;#65292;&amp;#36816;&amp;#34892;&amp;#22914;&amp;#19979;&amp;#21629;&amp;#20196;&amp;#26597;&amp;#30475;&amp;#26159;&amp;#21542;&amp;#25903;&amp;#25345;&amp;#12290;&lt;/p&gt;&amp;#13;
&lt;pre class="prettyprint prettyprinted"&gt;&lt;p class="pln"&gt;$ man mail&lt;/p&gt;&lt;/pre&gt;&amp;#13;
&lt;p&gt;&amp;#31532;&amp;#19968;&amp;#34892;&amp;#30475;&amp;#36215;&amp;#26469;&amp;#26159;&amp;#36825;&amp;#26679;&amp;#30340;&amp;#65306;&lt;/p&gt;&amp;#13;
&lt;pre class="prettyprint prettyprinted"&gt;&lt;p class="pln"&gt;mailx &lt;p class="pun"&gt;[-&lt;p class="typ"&gt;BDdEFintv&lt;p class="pun"&gt;~]&lt;p class="pln"&gt; &lt;p class="pun"&gt;[-&lt;p class="pln"&gt;s subject&lt;p class="pun"&gt;]&lt;p class="pln"&gt; &lt;p class="pun"&gt;[-&lt;p class="pln"&gt;a attachment &lt;p class="pun"&gt;]&lt;p class="pln"&gt; &lt;p class="pun"&gt;[-&lt;p class="pln"&gt;c cc&lt;p class="pun"&gt;-&lt;p class="pln"&gt;addr&lt;p class="pun"&gt;]&lt;p class="pln"&gt; &lt;p class="pun"&gt;[-&lt;p class="pln"&gt;b bcc&lt;p class="pun"&gt;-&lt;p class="pln"&gt;addr&lt;p class="pun"&gt;]&lt;p class="pln"&gt; &lt;p class="pun"&gt;[-&lt;p class="pln"&gt;r &lt;p class="kwd"&gt;from&lt;p class="pun"&gt;-&lt;p class="pln"&gt;addr&lt;p class="pun"&gt;]&lt;p class="pln"&gt; &lt;p class="pun"&gt;[-&lt;p class="pln"&gt;h hops&lt;p class="pun"&gt;]&lt;p class="pln"&gt; &lt;p class="pun"&gt;[-&lt;p class="pln"&gt;A account&lt;p class="pun"&gt;]&lt;p class="pln"&gt; &lt;p class="pun"&gt;[-&lt;p class="pln"&gt;S variable&lt;p class="pun"&gt;[=&lt;p class="pln"&gt;value&lt;p class="pun"&gt;]]&lt;p class="pln"&gt; to&lt;p class="pun"&gt;-&lt;p class="pln"&gt;addr &lt;p class="pun"&gt;.&lt;p class="pln"&gt; &lt;p class="pun"&gt;.&lt;p class="pln"&gt; &lt;p class="pun"&gt;.&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/pre&gt;&amp;#13;
&lt;p&gt;&amp;#22914;&amp;#26524;&amp;#20320;&amp;#30475;&amp;#21040;&amp;#23427;&amp;#25903;&amp;#25345;&lt;code&gt;-a&lt;/code&gt;&amp;#30340;&amp;#36873;&amp;#39033;&amp;#65288;-a &amp;#25991;&amp;#20214;&amp;#21517;&amp;#65292;&amp;#23558;&amp;#25991;&amp;#20214;&amp;#20316;&amp;#20026;&amp;#38468;&amp;#20214;&amp;#28155;&amp;#21152;&amp;#21040;&amp;#37038;&amp;#20214;&amp;#65289;&amp;#21644;&lt;code&gt;-s&lt;/code&gt;&amp;#36873;&amp;#39033;&amp;#65288;-s &amp;#20027;&amp;#39064;&amp;#65292;&amp;#25351;&amp;#23450;&amp;#37038;&amp;#20214;&amp;#30340;&amp;#20027;&amp;#39064;&amp;#65289;&amp;#65292;&amp;#37027;&amp;#23601;&amp;#26159;&amp;#25903;&amp;#25345;&amp;#30340;&amp;#12290;&amp;#21487;&amp;#20197;&amp;#20351;&amp;#29992;&amp;#22914;&amp;#19979;&amp;#30340;&amp;#20960;&amp;#20010;&amp;#20363;&amp;#23376;&amp;#21457;&amp;#36865;&amp;#37038;&amp;#20214;&amp;#12290;&lt;/p&gt;&amp;#13;
&lt;p&gt;&lt;strong&gt;a) &amp;#31616;&amp;#21333;&amp;#30340;&amp;#37038;&amp;#20214;&lt;/strong&gt;&lt;/p&gt;&amp;#13;
&lt;p&gt;&amp;#36816;&amp;#34892;&lt;code&gt;mail&lt;/code&gt;&amp;#21629;&amp;#20196;&amp;#65292;&amp;#28982;&amp;#21518;&lt;code&gt;mailx&lt;/code&gt;&amp;#20250;&amp;#31561;&amp;#24453;&amp;#20320;&amp;#36755;&amp;#20837;&amp;#37038;&amp;#20214;&amp;#20869;&amp;#23481;&amp;#12290;&amp;#20320;&amp;#21487;&amp;#20197;&amp;#25353;&amp;#22238;&amp;#36710;&amp;#26469;&amp;#25442;&amp;#34892;&amp;#12290;&amp;#24403;&amp;#36755;&amp;#20837;&amp;#23436;&amp;#25104;&amp;#21518;&amp;#65292;&amp;#25353;Ctrl + D&amp;#65292;&lt;code&gt;mailx&lt;/code&gt;&amp;#20250;&amp;#26174;&amp;#31034;EOT&amp;#34920;&amp;#31034;&amp;#32467;&amp;#26463;&amp;#12290;&lt;/p&gt;&amp;#13;
&lt;p&gt;&amp;#28982;&amp;#21518;&lt;code&gt;mailx&lt;/code&gt;&amp;#20250;&amp;#33258;&amp;#21160;&amp;#23558;&amp;#37038;&amp;#20214;&amp;#21457;&amp;#36865;&amp;#32473;&amp;#25910;&amp;#20214;&amp;#20154;&amp;#12290;&lt;/p&gt;&amp;#13;
&lt;pre class="prettyprint prettyprinted"&gt;&lt;p class="pln"&gt;$ mail &lt;p class="pln"&gt;user@example&lt;p class="pun"&gt;.&lt;p class="pln"&gt;com&lt;p class="pln"&gt;&amp;#13;
&amp;#13;
HI&lt;p class="pun"&gt;,&lt;p class="pln"&gt;&amp;#13;
&lt;p class="typ"&gt;Good&lt;p class="pln"&gt; &lt;p class="typ"&gt;Morning&lt;p class="pln"&gt;&amp;#13;
&lt;p class="typ"&gt;How&lt;p class="pln"&gt; are you&amp;#13;
EOT&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/pre&gt;&amp;#13;
&lt;p&gt;&lt;strong&gt;b) &amp;#21457;&amp;#36865;&amp;#26377;&amp;#20027;&amp;#39064;&amp;#30340;&amp;#37038;&amp;#20214;&lt;/strong&gt;&lt;/p&gt;&amp;#13;
&lt;pre class="prettyprint prettyprinted"&gt;&lt;p class="pln"&gt;$ echo &lt;p class="str"&gt;"Email text"&lt;p class="pln"&gt; &lt;p class="pun"&gt;|&lt;p class="pln"&gt; mail &lt;p class="pun"&gt;-&lt;p class="pln"&gt;s &lt;p class="str"&gt;"Test Subject"&lt;p class="pln"&gt; &lt;p class="pln"&gt;user@example&lt;p class="pun"&gt;.&lt;p class="pln"&gt;com&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/pre&gt;&amp;#13;
&lt;p&gt;&lt;code&gt;-s&lt;/code&gt;&amp;#30340;&amp;#29992;&amp;#22788;&amp;#26159;&amp;#25351;&amp;#23450;&amp;#37038;&amp;#20214;&amp;#30340;&amp;#20027;&amp;#39064;&amp;#12290;&lt;/p&gt;&amp;#13;
&lt;p&gt;&lt;strong&gt;c) &amp;#20174;&amp;#25991;&amp;#20214;&amp;#20013;&amp;#35835;&amp;#21462;&amp;#37038;&amp;#20214;&amp;#20869;&amp;#23481;&amp;#24182;&amp;#21457;&amp;#36865;&lt;/strong&gt;&lt;/p&gt;&amp;#13;
&lt;pre class="prettyprint prettyprinted"&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;a&gt;&lt;/a&gt;&lt;p class="str"&gt;"&lt;p class="pln"&gt; &lt;p class="pln"&gt;user@example&lt;p class="pun"&gt;.&lt;p class="pln"&gt;com&lt;p class="pln"&gt; &lt;p class="pun"&gt;&amp;lt;&lt;p class="pln"&gt; &lt;p class="str"&gt;/path/&lt;p class="pln"&gt;to&lt;p class="pun"&gt;/&lt;p class="pln"&gt;file&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;p&gt;"message send from &lt;a href="http://www.linuxeye.com/command/file.html" target="_blank"&gt;&lt;span&gt;&lt;span class="str"&gt;file&lt;/span&gt;&lt;/span&gt;&lt;/a&gt; &lt;/p&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;p&gt;$ mail&lt;/p&gt;&lt;/span&gt;&lt;/pre&gt;&amp;#13;
&lt;p&gt;&lt;strong&gt;d) &amp;#23558;&amp;#20174;&amp;#31649;&amp;#36947;&amp;#33719;&amp;#21462;&amp;#21040;&amp;#30340;&lt;code&gt;echo&lt;/code&gt;&amp;#21629;&amp;#20196;&amp;#36755;&amp;#20986;&amp;#20316;&amp;#20026;&amp;#37038;&amp;#20214;&amp;#20869;&amp;#23481;&amp;#21457;&amp;#36865;&lt;/strong&gt;&lt;/p&gt;&amp;#13;
&lt;pre class="prettyprint prettyprinted"&gt;&lt;p class="pln"&gt;$ echo &lt;p class="str"&gt;"This is message body"&lt;p class="pln"&gt; &lt;p class="pun"&gt;|&lt;p class="pln"&gt; mail &lt;p class="pun"&gt;-&lt;p class="pln"&gt;s &lt;p class="str"&gt;"This is Subject"&lt;p class="pln"&gt; &lt;p class="pln"&gt;user@example&lt;p class="pun"&gt;.&lt;p class="pln"&gt;com&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/pre&gt;&amp;#13;
&lt;p&gt;&lt;strong&gt;e) &amp;#21457;&amp;#36865;&amp;#24102;&amp;#38468;&amp;#20214;&amp;#30340;&amp;#37038;&amp;#20214;&lt;/strong&gt;&lt;/p&gt;&amp;#13;
&lt;pre class="prettyprint prettyprinted"&gt;&lt;p class="pln"&gt;$ echo &lt;p class="pun"&gt;&amp;#8220;&lt;p class="typ"&gt;Body&lt;p class="pln"&gt; &lt;p class="kwd"&gt;with&lt;p class="pln"&gt; attachment &lt;p class="str"&gt;"| mail -a foo.tar.gz -s "&lt;p class="pln"&gt;attached file&lt;p class="str"&gt;" &lt;p class="str"&gt;user@example.com&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/pre&gt;&amp;#13;
&lt;p&gt;&lt;code&gt;-a&lt;/code&gt;&amp;#36873;&amp;#39033;&amp;#29992;&amp;#20110;&amp;#25351;&amp;#23450;&amp;#38468;&amp;#20214;&amp;#12290;&lt;/p&gt;&amp;#13;
&lt;p&gt;&amp;#160;&lt;/p&gt;&amp;#13;
&lt;h3 id="toc_2"&gt;2) mutt&lt;/h3&gt;&amp;#13;
&lt;p&gt;Mutt&amp;#26159;&amp;#31867;Unix&amp;#31995;&amp;#32479;&amp;#19978;&amp;#30340;&amp;#19968;&amp;#20010;&amp;#25991;&amp;#26412;&amp;#30028;&amp;#38754;&amp;#37038;&amp;#20214;&amp;#23458;&amp;#25143;&amp;#31471;&amp;#12290;&amp;#23427;&amp;#26377;20&amp;#22810;&amp;#24180;&amp;#30340;&amp;#21382;&amp;#21490;&amp;#65292;&amp;#22312;Linux&amp;#21382;&amp;#21490;&amp;#20013;&amp;#20063;&amp;#26159;&amp;#19968;&amp;#20010;&amp;#24456;&amp;#37325;&amp;#35201;&amp;#30340;&amp;#37096;&amp;#20998;&amp;#65292;&amp;#23427;&amp;#26159;&amp;#26368;&amp;#26089;&amp;#25903;&amp;#25345;&amp;#36827;&amp;#31243;&amp;#25171;&amp;#20998;&amp;#21644;&amp;#22810;&amp;#32447;&amp;#31243;&amp;#22788;&amp;#29702;&amp;#30340;&amp;#23458;&amp;#25143;&amp;#31471;&amp;#31243;&amp;#24207;&amp;#20043;&amp;#19968;&amp;#12290;&amp;#25353;&amp;#29031;&amp;#22914;&amp;#19979;&amp;#30340;&amp;#20363;&amp;#23376;&amp;#26469;&amp;#21457;&amp;#36865;&amp;#37038;&amp;#20214;&amp;#12290;&lt;/p&gt;&amp;#13;
&lt;p&gt;&lt;strong&gt;a) &amp;#24102;&amp;#26377;&amp;#20027;&amp;#39064;&amp;#65292;&amp;#20174;&amp;#25991;&amp;#20214;&amp;#20013;&amp;#35835;&amp;#21462;&amp;#37038;&amp;#20214;&amp;#30340;&amp;#27491;&amp;#25991;&amp;#65292;&amp;#24182;&amp;#21457;&amp;#36865;&lt;/strong&gt;&lt;/p&gt;&amp;#13;
&lt;pre class="prettyprint prettyprinted"&gt;&lt;p class="pln"&gt;$ mutt &lt;p class="pun"&gt;-&lt;p class="pln"&gt;s &lt;p class="str"&gt;"Testing from mutt"&lt;p class="pln"&gt; &lt;p class="pln"&gt;user@example&lt;p class="pun"&gt;.&lt;p class="pln"&gt;com&lt;p class="pln"&gt; &lt;p class="pun"&gt;&amp;lt;&lt;p class="pln"&gt; &lt;p class="str"&gt;/tmp/&lt;p class="pln"&gt;message&lt;p class="pun"&gt;.&lt;p class="pln"&gt;txt&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/pre&gt;&amp;#13;
&lt;p&gt;&lt;strong&gt;b) &amp;#36890;&amp;#36807;&amp;#31649;&amp;#36947;&amp;#33719;&amp;#21462;&lt;code&gt;echo&lt;/code&gt;&amp;#21629;&amp;#20196;&amp;#36755;&amp;#20986;&amp;#20316;&amp;#20026;&amp;#37038;&amp;#20214;&amp;#20869;&amp;#23481;&amp;#21457;&amp;#36865;&lt;/strong&gt;&lt;/p&gt;&amp;#13;
&lt;pre class="prettyprint prettyprinted"&gt;&lt;p class="pln"&gt;$ echo &lt;p class="str"&gt;"This is the body"&lt;p class="pln"&gt; &lt;p class="pun"&gt;|&lt;p class="pln"&gt; mutt &lt;p class="pun"&gt;-&lt;p class="pln"&gt;s &lt;p class="str"&gt;"Testing mutt"&lt;p class="pln"&gt; &lt;p class="pln"&gt;user@example&lt;p class="pun"&gt;.&lt;p class="pln"&gt;com&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/pre&gt;&amp;#13;
&lt;p&gt;&lt;strong&gt;c) &amp;#21457;&amp;#36865;&amp;#24102;&amp;#38468;&amp;#20214;&amp;#30340;&amp;#37038;&amp;#20214;&lt;/strong&gt;&lt;/p&gt;&amp;#13;
&lt;pre class="prettyprint prettyprinted"&gt;&lt;p class="pln"&gt;$ echo &lt;p class="str"&gt;"This is the body"&lt;p class="pln"&gt; &lt;p class="pun"&gt;|&lt;p class="pln"&gt; mutt &lt;p class="pun"&gt;-&lt;p class="pln"&gt;s &lt;p class="str"&gt;"Testing mutt"&lt;p class="pln"&gt; &lt;p class="pln"&gt;user@example&lt;p class="pun"&gt;.&lt;p class="pln"&gt;com&lt;p class="pln"&gt; &lt;p class="pun"&gt;-&lt;p class="pln"&gt;a &lt;p class="pun"&gt;/&lt;p class="pln"&gt;tmp&lt;p class="pun"&gt;/&lt;p class="pln"&gt;foo&lt;p class="pun"&gt;.&lt;p class="pln"&gt;tar&lt;p class="pun"&gt;.&lt;p class="pln"&gt;gz&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/pre&gt;&amp;#13;
&lt;p&gt;&lt;strong&gt;d) &amp;#21457;&amp;#36865;&amp;#24102;&amp;#26377;&amp;#22810;&amp;#20010;&amp;#38468;&amp;#20214;&amp;#30340;&amp;#37038;&amp;#20214;&lt;/strong&gt;&lt;/p&gt;&amp;#13;
&lt;pre class="prettyprint prettyprinted"&gt;&lt;p class="pln"&gt;$ echo &lt;p class="str"&gt;"This is the body"&lt;p class="pln"&gt; &lt;p class="pun"&gt;|&lt;p class="pln"&gt; mutt &lt;p class="pun"&gt;-&lt;p class="pln"&gt;s &lt;p class="str"&gt;"Testing"&lt;p class="pln"&gt; &lt;p class="pln"&gt;user@example&lt;p class="pun"&gt;.&lt;p class="pln"&gt;com&lt;p class="pln"&gt; &lt;p class="pun"&gt;-&lt;p class="pln"&gt;a foo&lt;p class="pun"&gt;.&lt;p class="pln"&gt;tar&lt;p class="pun"&gt;.&lt;p class="pln"&gt;gz &lt;p class="pun"&gt;&amp;#8211;&lt;p class="pln"&gt;a bar&lt;p class="pun"&gt;.&lt;p class="pln"&gt;tar&lt;p class="pun"&gt;.&lt;p class="pln"&gt;gz&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/pre&gt;&amp;#13;
&lt;h3 id="toc_3"&gt;3) swaks&lt;/h3&gt;&amp;#13;
&lt;p&gt;Swaks&amp;#65288;Swiss Army Knife&amp;#65292;&amp;#29790;&amp;#22763;&amp;#20891;&amp;#20992;&amp;#65289;&amp;#26159;SMTP&amp;#26381;&amp;#21153;&amp;#19978;&amp;#30340;&amp;#29790;&amp;#22763;&amp;#20891;&amp;#20992;&amp;#65292;&amp;#23427;&amp;#26159;&amp;#19968;&amp;#20010;&amp;#21151;&amp;#33021;&amp;#24378;&amp;#22823;&amp;#12289;&amp;#28789;&amp;#27963;&amp;#12289;&amp;#21487;&amp;#32534;&amp;#31243;&amp;#12289;&amp;#38754;&amp;#21521;&amp;#20107;&amp;#21153;&amp;#30340;SMTP&amp;#27979;&amp;#35797;&amp;#24037;&amp;#20855;&amp;#65292;&amp;#30001;John Jetmore&amp;#24320;&amp;#21457;&amp;#21644;&amp;#32500;&amp;#25252;&amp;#12290;&amp;#20320;&amp;#21487;&amp;#20197;&amp;#20351;&amp;#29992;&amp;#22914;&amp;#19979;&amp;#35821;&amp;#27861;&amp;#21457;&amp;#36865;&amp;#24102;&amp;#38468;&amp;#20214;&amp;#30340;&amp;#37038;&amp;#20214;&amp;#65306;&lt;/p&gt;&amp;#13;
&lt;pre class="prettyprint prettyprinted"&gt;&lt;p class="pln"&gt;$ swaks &lt;p class="pun"&gt;-&lt;p class="pln"&gt;t &lt;p class="str"&gt;"&lt;p class="str"&gt;foo@bar.com&lt;p class="str"&gt;"&lt;p class="pln"&gt; &lt;p class="pun"&gt;--&lt;p class="pln"&gt;header &lt;p class="str"&gt;"Subject: Subject"&lt;p class="pln"&gt; &lt;p class="pun"&gt;--&lt;p class="pln"&gt;body &lt;p class="str"&gt;"Email Text"&lt;p class="pln"&gt; &lt;p class="pun"&gt;--&lt;p class="pln"&gt;attach foo&lt;p class="pun"&gt;.&lt;p class="pln"&gt;tar&lt;p class="pun"&gt;.&lt;p class="pln"&gt;gz&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/pre&gt;&amp;#13;
&lt;p&gt;&amp;#20851;&amp;#20110;Swaks&amp;#19968;&amp;#20010;&amp;#37325;&amp;#35201;&amp;#30340;&amp;#22320;&amp;#26041;&amp;#26159;&amp;#65292;&amp;#23427;&amp;#20250;&amp;#20026;&amp;#20320;&amp;#26174;&amp;#31034;&amp;#25972;&amp;#20010;&amp;#37038;&amp;#20214;&amp;#21457;&amp;#36865;&amp;#36807;&amp;#31243;&amp;#65292;&amp;#25152;&amp;#20197;&amp;#22914;&amp;#26524;&amp;#20320;&amp;#24819;&amp;#35843;&amp;#35797;&amp;#37038;&amp;#20214;&amp;#21457;&amp;#36865;&amp;#36807;&amp;#31243;&amp;#65292;&amp;#23427;&amp;#26159;&amp;#19968;&amp;#20010;&amp;#38750;&amp;#24120;&amp;#26377;&amp;#29992;&amp;#30340;&amp;#24037;&amp;#20855;&amp;#12290;&lt;/p&gt;&amp;#13;
&lt;p&gt;&amp;#23427;&amp;#20250;&amp;#32473;&amp;#20320;&amp;#25552;&amp;#20379;&amp;#20102;&amp;#37038;&amp;#20214;&amp;#21457;&amp;#36865;&amp;#36807;&amp;#31243;&amp;#30340;&amp;#25152;&amp;#26377;&amp;#32454;&amp;#33410;&amp;#65292;&amp;#21253;&amp;#25324;&amp;#37038;&amp;#20214;&amp;#25509;&amp;#25910;&amp;#26381;&amp;#21153;&amp;#22120;&amp;#30340;&amp;#21151;&amp;#33021;&amp;#25903;&amp;#25345;&amp;#12289;&amp;#20004;&amp;#20010;&amp;#26381;&amp;#21153;&amp;#22120;&amp;#20043;&amp;#38388;&amp;#30340;&amp;#27599;&amp;#19968;&amp;#27493;&amp;#20132;&amp;#20114;&amp;#12290;&lt;/p&gt;&amp;#13;
&lt;p&gt;&amp;#65288;LCTT &amp;#35793;&amp;#27880;&amp;#65306;&amp;#21407;&amp;#25991;&amp;#27492;&amp;#22788;&amp;#23569;&amp;#20102;&amp;#160;sharutils &amp;#30340;&amp;#30456;&amp;#20851;&amp;#20171;&amp;#32461;&amp;#65292;&amp;#32780;&amp;#22810;&amp;#20102; uuencode &amp;#30340;&amp;#20171;&amp;#32461;&amp;#12290;&amp;#65289;&lt;/p&gt;&amp;#13;
&lt;p&gt;&amp;#160;&lt;/p&gt;&amp;#13;
&lt;h3 id="toc_4"&gt;4) uuencode&lt;/h3&gt;&amp;#13;
&lt;p&gt;&amp;#37038;&amp;#20214;&amp;#20256;&amp;#36755;&amp;#31995;&amp;#32479;&amp;#26368;&amp;#21021;&amp;#26159;&amp;#34987;&amp;#35774;&amp;#35745;&amp;#26469;&amp;#20256;&amp;#36865;7&amp;#20301;&amp;#32534;&amp;#30721;&amp;#65288;&amp;#31867;&amp;#20284;ASCII&amp;#65289;&amp;#30340;&amp;#20869;&amp;#23481;&amp;#30340;&amp;#12290;&amp;#36825;&amp;#23601;&amp;#24847;&amp;#21619;&amp;#36825;&amp;#23427;&amp;#26159;&amp;#29992;&amp;#26469;&amp;#21457;&amp;#36865;&amp;#25991;&amp;#26412;&amp;#20869;&amp;#23481;&amp;#65292;&amp;#32780;&amp;#19981;&amp;#33021;&amp;#21457;&amp;#20250;&amp;#20351;&amp;#29992;8&amp;#20301;&amp;#30340;&amp;#20108;&amp;#36827;&amp;#21046;&amp;#20869;&amp;#23481;&amp;#65288;&amp;#22914;&amp;#31243;&amp;#24207;&amp;#25991;&amp;#20214;&amp;#25110;&amp;#32773;&amp;#22270;&amp;#29255;&amp;#65289;&amp;#12290;&lt;code&gt;uuencode&lt;/code&gt;&amp;#65288;&amp;#8220;UNIX to UNIX encoding&amp;#8221;&amp;#65292;UNIX&amp;#20043;&amp;#38388;&amp;#20351;&amp;#29992;&amp;#30340;&amp;#32534;&amp;#30721;&amp;#26041;&amp;#24335;&amp;#65289;&amp;#31243;&amp;#24207;&amp;#29992;&amp;#26469;&amp;#35299;&amp;#20915;&amp;#36825;&amp;#20010;&amp;#38480;&amp;#21046;&amp;#12290;&amp;#20351;&amp;#29992;&lt;code&gt;uuencode&lt;/code&gt;&amp;#65292;&amp;#21457;&amp;#36865;&amp;#31471;&amp;#23558;&amp;#20108;&amp;#36827;&amp;#21046;&amp;#26684;&amp;#24335;&amp;#30340;&amp;#36716;&amp;#25442;&amp;#25104;&amp;#25991;&amp;#26412;&amp;#26684;&amp;#24335;&amp;#26469;&amp;#20256;&amp;#36755;&amp;#65292;&amp;#25509;&amp;#25910;&amp;#31471;&amp;#20877;&amp;#36716;&amp;#25442;&amp;#22238;&amp;#21435;&amp;#12290;&lt;/p&gt;&amp;#13;
&lt;p&gt;&amp;#25105;&amp;#20204;&amp;#21487;&amp;#20197;&amp;#31616;&amp;#21333;&amp;#22320;&amp;#20351;&amp;#29992;&lt;code&gt;uuencode&lt;/code&gt;&amp;#21644;&lt;code&gt;mailx&lt;/code&gt;&amp;#25110;&amp;#32773;&lt;code&gt;mutt&lt;/code&gt;&amp;#37197;&amp;#21512;&amp;#65292;&amp;#26469;&amp;#21457;&amp;#36865;&amp;#20108;&amp;#36827;&amp;#21046;&amp;#20869;&amp;#23481;&amp;#65292;&amp;#31867;&amp;#20284;&amp;#36825;&amp;#26679;&amp;#65306;&lt;/p&gt;&amp;#13;
&lt;pre class="prettyprint prettyprinted"&gt;&lt;p class="pln"&gt;$ uuencode example&lt;p class="pun"&gt;.&lt;p class="pln"&gt;jpeg example&lt;p class="pun"&gt;.&lt;p class="pln"&gt;jpeg &lt;p class="pun"&gt;|&lt;p class="pln"&gt; mail &lt;p class="pln"&gt;user@example&lt;p class="pun"&gt;.&lt;p class="pln"&gt;com&lt;br&gt;&lt;br&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/pre&gt;&amp;#13;
&lt;h3 id="toc_5"&gt;Shell&amp;#33050;&amp;#26412;&amp;#65306;&amp;#35299;&amp;#37322;&amp;#22914;&amp;#20309;&amp;#21457;&amp;#36865;&amp;#37038;&amp;#20214;&lt;/h3&gt;&amp;#13;
&lt;pre class="prettyprint prettyprinted"&gt;&lt;p class="com"&gt;#!/bin/bash&lt;p class="pln"&gt;&amp;#13;
&amp;#13;
FROM&lt;p class="pun"&gt;=&lt;p class="str"&gt;""&lt;p class="pln"&gt;&amp;#13;
SUBJECT&lt;p class="pun"&gt;=&lt;p class="str"&gt;""&lt;p class="pln"&gt;&amp;#13;
ATTACHMENTS&lt;p class="pun"&gt;=&lt;p class="str"&gt;""&lt;p class="pln"&gt;&amp;#13;
TO&lt;p class="pun"&gt;=&lt;p class="str"&gt;""&lt;p class="pln"&gt;&amp;#13;
BODY&lt;p class="pun"&gt;=&lt;p class="str"&gt;""&lt;p class="pln"&gt;&amp;#13;
&amp;#13;
&lt;p class="com"&gt;# &amp;#26816;&amp;#26597;&amp;#25991;&amp;#20214;&amp;#21517;&amp;#23545;&amp;#24212;&amp;#30340;&amp;#25991;&amp;#20214;&amp;#26159;&amp;#21542;&amp;#23384;&amp;#22312;&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;function&lt;p class="pln"&gt; check_files&lt;p class="pun"&gt;()&lt;p class="pln"&gt;&amp;#13;
&lt;p class="pun"&gt;{&lt;p class="pln"&gt;&amp;#13;
output_files&lt;p class="pun"&gt;=&lt;p class="str"&gt;""&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;for&lt;p class="pln"&gt; file &lt;p class="kwd"&gt;in&lt;p class="pln"&gt; $1&amp;#13;
&lt;p class="kwd"&gt;do&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;if&lt;p class="pln"&gt; &lt;p class="pun"&gt;[&lt;p class="pln"&gt; &lt;p class="pun"&gt;-&lt;p class="pln"&gt;s $file &lt;p class="pun"&gt;]&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;then&lt;p class="pln"&gt;&amp;#13;
output_files&lt;p class="pun"&gt;=&lt;p class="str"&gt;"${output_files}${file} "&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;fi&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;done&lt;p class="pln"&gt;&amp;#13;
echo $output_files&amp;#13;
&lt;p class="pun"&gt;}&lt;p class="pln"&gt;&amp;#13;
&amp;#13;
echo &lt;p class="str"&gt;"*********************"&lt;p class="pln"&gt;&amp;#13;
echo &lt;p class="str"&gt;"E-mail sending script."&lt;p class="pln"&gt;&amp;#13;
echo &lt;p class="str"&gt;"*********************"&lt;p class="pln"&gt;&amp;#13;
echo&amp;#13;
&amp;#13;
&lt;p class="com"&gt;# &amp;#35835;&amp;#21462;&amp;#29992;&amp;#25143;&amp;#36755;&amp;#20837;&amp;#30340;&amp;#37038;&amp;#20214;&amp;#22320;&amp;#22336;&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;while&lt;p class="pln"&gt; &lt;p class="pun"&gt;[&lt;p class="pln"&gt; &lt;p class="lit"&gt;1&lt;p class="pln"&gt; &lt;p class="pun"&gt;]&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;do&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;if&lt;p class="pln"&gt; &lt;p class="pun"&gt;[&lt;p class="pln"&gt; &lt;p class="pun"&gt;!&lt;p class="pln"&gt; $FROM &lt;p class="pun"&gt;]&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;then&lt;p class="pln"&gt;&amp;#13;
echo &lt;p class="pun"&gt;-&lt;p class="pln"&gt;n &lt;p class="pun"&gt;-&lt;p class="pln"&gt;e &lt;p class="str"&gt;"Enter the e-mail address you wish to send mail from:\n[Enter] "&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;else&lt;p class="pln"&gt;&amp;#13;
echo &lt;p class="pun"&gt;-&lt;p class="pln"&gt;n &lt;p class="pun"&gt;-&lt;p class="pln"&gt;e &lt;p class="str"&gt;"The address you provided is not valid:\n[Enter] "&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;fi&lt;p class="pln"&gt;&amp;#13;
&amp;#13;
read FROM&amp;#13;
echo $FROM &lt;p class="pun"&gt;|&lt;p class="pln"&gt; grep &lt;p class="pun"&gt;-&lt;p class="pln"&gt;E &lt;p class="str"&gt;'^.+@.+$'&lt;p class="pln"&gt; &lt;p class="pun"&gt;&amp;gt;&lt;p class="pln"&gt; &lt;p class="str"&gt;/dev/&lt;p class="kwd"&gt;null&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;if&lt;p class="pln"&gt; &lt;p class="pun"&gt;[&lt;p class="pln"&gt; $&lt;p class="pun"&gt;?&lt;p class="pln"&gt; &lt;p class="pun"&gt;-&lt;p class="pln"&gt;eq &lt;p class="lit"&gt;0&lt;p class="pln"&gt; &lt;p class="pun"&gt;]&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;then&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;break&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;fi&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;done&lt;p class="pln"&gt;&amp;#13;
&amp;#13;
echo&amp;#13;
&amp;#13;
&lt;p class="com"&gt;# &amp;#35835;&amp;#21462;&amp;#29992;&amp;#25143;&amp;#36755;&amp;#20837;&amp;#30340;&amp;#25910;&amp;#20214;&amp;#20154;&amp;#22320;&amp;#22336;&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;while&lt;p class="pln"&gt; &lt;p class="pun"&gt;[&lt;p class="pln"&gt; &lt;p class="lit"&gt;1&lt;p class="pln"&gt; &lt;p class="pun"&gt;]&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;do&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;if&lt;p class="pln"&gt; &lt;p class="pun"&gt;[&lt;p class="pln"&gt; &lt;p class="pun"&gt;!&lt;p class="pln"&gt; $TO &lt;p class="pun"&gt;]&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;then&lt;p class="pln"&gt;&amp;#13;
echo &lt;p class="pun"&gt;-&lt;p class="pln"&gt;n &lt;p class="pun"&gt;-&lt;p class="pln"&gt;e &lt;p class="str"&gt;"Enter the e-mail address you wish to send mail to:\n[Enter] "&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;else&lt;p class="pln"&gt;&amp;#13;
echo &lt;p class="pun"&gt;-&lt;p class="pln"&gt;n &lt;p class="pun"&gt;-&lt;p class="pln"&gt;e &lt;p class="str"&gt;"The address you provided is not valid:\n[Enter] "&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;fi&lt;p class="pln"&gt;&amp;#13;
&amp;#13;
read TO&amp;#13;
echo $TO &lt;p class="pun"&gt;|&lt;p class="pln"&gt; grep &lt;p class="pun"&gt;-&lt;p class="pln"&gt;E &lt;p class="str"&gt;'^.+@.+$'&lt;p class="pln"&gt; &lt;p class="pun"&gt;&amp;gt;&lt;p class="pln"&gt; &lt;p class="str"&gt;/dev/&lt;p class="kwd"&gt;null&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;if&lt;p class="pln"&gt; &lt;p class="pun"&gt;[&lt;p class="pln"&gt; $&lt;p class="pun"&gt;?&lt;p class="pln"&gt; &lt;p class="pun"&gt;-&lt;p class="pln"&gt;eq &lt;p class="lit"&gt;0&lt;p class="pln"&gt; &lt;p class="pun"&gt;]&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;then&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;break&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;fi&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;done&lt;p class="pln"&gt;&amp;#13;
&amp;#13;
echo&amp;#13;
&amp;#13;
&lt;p class="com"&gt;# &amp;#35835;&amp;#21462;&amp;#29992;&amp;#25143;&amp;#36755;&amp;#20837;&amp;#30340;&amp;#37038;&amp;#20214;&amp;#20027;&amp;#39064;&lt;p class="pln"&gt;&amp;#13;
echo &lt;p class="pun"&gt;-&lt;p class="pln"&gt;n &lt;p class="pun"&gt;-&lt;p class="pln"&gt;e &lt;p class="str"&gt;"Enter e-mail subject:\n[Enter] "&lt;p class="pln"&gt;&amp;#13;
read SUBJECT&amp;#13;
&amp;#13;
echo&amp;#13;
&amp;#13;
&lt;p class="kwd"&gt;if&lt;p class="pln"&gt; &lt;p class="pun"&gt;[&lt;p class="pln"&gt; &lt;p class="str"&gt;"$SUBJECT"&lt;p class="pln"&gt; &lt;p class="pun"&gt;==&lt;p class="pln"&gt; &lt;p class="str"&gt;""&lt;p class="pln"&gt; &lt;p class="pun"&gt;]&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;then&lt;p class="pln"&gt;&amp;#13;
echo &lt;p class="str"&gt;"Proceeding without the subject..."&lt;p class="pln"&gt;&amp;#13;
&lt;p class="kwd"&gt;fi&lt;p class="pln"&gt;&amp;#13;
&amp;#13;
&lt;p class="com"&gt;# &amp;#35835;&amp;#21462;&amp;#20316;&amp;#20026;&amp;#38468;&amp;#20214;&amp;#30340;&amp;#25991;&amp;#20214;&amp;#21517;&lt;p class="pln"&gt;&amp;#13;
echo &lt;p class="pun"&gt;-&lt;p class="pln"&gt;e &lt;p class="str"&gt;"Provide the list of attachments. Separate names by space.&amp;#13;
If there are spaces in file name, quote file name with \"."&lt;p class="pln"&gt;&amp;#13;
read att&amp;#13;
&amp;#13;
echo&amp;#13;
&amp;#13;
&lt;p class="com"&gt;# &amp;#30830;&amp;#20445;&amp;#25991;&amp;#20214;&amp;#21517;&amp;#25351;&amp;#21521;&amp;#30495;&amp;#23454;&amp;#25991;&amp;#20214;&lt;p class="pln"&gt;&amp;#13;
attachments&lt;p class="pun"&gt;=&lt;p class="pln"&gt;$&lt;p class="pun"&gt;(&lt;p class="pln"&gt;check_files &lt;p class="str"&gt;"$att"&lt;p class="pun"&gt;)&lt;p class="pln"&gt;&amp;#13;
echo &lt;p class="str"&gt;"Attachments: $attachments"&lt;p class="pln"&gt;&amp;#13;
&amp;#13;
&lt;p class="kwd"&gt;for&lt;p class="pln"&gt; attachment &lt;p class="kwd"&gt;in&lt;p class="pln"&gt; $attachments&amp;#13;
&lt;p class="kwd"&gt;do&lt;p class="pln" class="pun"&gt;&amp;#13;
ATTACHMENTS&lt;p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/p&gt;&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sat, 20 Jun 2015 23:45:19 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-20:4ge-ke-yi-fa-song-wan-zheng-dian-zi-you-jian-de-ming-ling-xing-gong-ju-itech.html</guid></item><item><title>可以替代putty的ssh客户端 - iTech</title><link>http://www.ciandcd.com/ke-yi-ti-dai-puttyde-sshke-hu-duan-itech.html</link><description>&lt;div&gt;&lt;p&gt;&amp;#160;&lt;/p&gt;&amp;#13;
&lt;p&gt;1.&amp;#160;Bitvise SSH Client&lt;/p&gt;&amp;#13;
&lt;p&gt;http://www.putty.org/&lt;/p&gt;&amp;#13;
&lt;p&gt;Bitvise SSH Client is an SSH and SFTP client for Windows. It is developed and supported professionally by Bitvise. The SSH Client is robust, easy to install, easy to use, and supports all features supported by PuTTY, as well as the following:&lt;/p&gt;&amp;#13;
&lt;ul&gt;&amp;#13;
&lt;li&gt;graphical SFTP file transfer;&lt;/li&gt;&amp;#13;
&lt;li&gt;single-click Remote Desktop tunneling;&lt;/li&gt;&amp;#13;
&lt;li&gt;auto-reconnecting capability;&lt;/li&gt;&amp;#13;
&lt;li&gt;dynamic port forwarding through an integrated proxy;&lt;/li&gt;&amp;#13;
&lt;li&gt;an FTP-to-SFTP protocol bridge.&lt;/li&gt;&amp;#13;
&lt;/ul&gt;&amp;#13;
&lt;p&gt;Bitvise SSH Client is&amp;#160;&lt;strong&gt;free for personal use&lt;/strong&gt;, as well as for individual commercial use inside organizations. You can&amp;#160;&lt;a href="http://www.bitvise.com/download-area"&gt;download Bitvise SSH Client here&lt;/a&gt;.&lt;/p&gt;&amp;#13;
&lt;p&gt;&amp;#160;&lt;/p&gt;&amp;#13;
&lt;p&gt;2.&amp;#160;MobaXterm X server and SSH client &amp;#160;&lt;/p&gt;&amp;#13;
&lt;p&gt;http://mobaxterm.mobatek.net/&amp;#160;&lt;/p&gt;&amp;#13;
&lt;p&gt;MobaXterm is your&amp;#160;&lt;strong&gt;ultimate toolbox for remote computing&lt;/strong&gt;. In a single Windows application, it provides loads of functions that are tailored for programmers, webmasters, IT administrators and pretty much all users who need to handle their remote jobs in a more simple fashion.&lt;/p&gt;&amp;#13;
&lt;p&gt;MobaXterm provides all the important&amp;#160;&lt;strong&gt;remote network tools&lt;/strong&gt;&amp;#160;(SSH, X11, RDP, VNC, FTP, MOSH, ...) and&amp;#160;&lt;strong&gt;Unix commands&lt;/strong&gt;&amp;#160;(bash, ls, cat, sed, grep, awk, rsync, ...) to Windows desktop, in a&amp;#160;&lt;strong&gt;single portable exe file&lt;/strong&gt;&amp;#160;which works out of the box.&amp;#160;&lt;a href="http://mobaxterm.mobatek.net/features.html"&gt;More info on supported network protocols&lt;/a&gt;&lt;/p&gt;&amp;#13;
&lt;p&gt;There are many advantages of having an&amp;#160;&lt;strong&gt;All-In-One network application&lt;/strong&gt;&amp;#160;for your remote tasks, e.g. when you use&amp;#160;&lt;strong&gt;SSH&lt;/strong&gt;&amp;#160;to connect to a remote server, a graphical&amp;#160;&lt;strong&gt;SFTP browser&lt;/strong&gt;&amp;#160;will automatically pop up in order to directly edit your remote files. Your remote applications will also display seamlessly on your Windows desktop using the embedded&amp;#160;&lt;strong&gt;X server&lt;/strong&gt;.&amp;#160;&lt;a href="http://mobaxterm.mobatek.net/demo.html"&gt;See demo&lt;/a&gt;&lt;/p&gt;&amp;#13;
&lt;p&gt;You can download and use MobaXterm Home Edition for free. If you want to use it inside your company, you should consider subscribing to MobaXterm Professional Edition: this will give you access to much more features, professional support and "Customizer" software.&amp;#160;&lt;a href="http://mobaxterm.mobatek.net/download.html"&gt;Features comparison&lt;/a&gt;&lt;/p&gt;&amp;#13;
&lt;p&gt;&amp;#160;&lt;/p&gt;&amp;#13;
&lt;p&gt;done&amp;#160;&lt;/p&gt;&amp;#13;
&lt;p&gt;&amp;#160;&lt;/p&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sat, 20 Jun 2015 23:45:18 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-20:ke-yi-ti-dai-puttyde-sshke-hu-duan-itech.html</guid></item><item><title>linux目录的权限 - iTech</title><link>http://www.ciandcd.com/linuxmu-lu-de-quan-xian-itech.html</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;Answer:&lt;/strong&gt;&lt;/p&gt;&amp;#13;
&lt;p&gt;When applying permissions to directories on Linux, the permission bits have different meanings than on regular files.&lt;/p&gt;&amp;#13;
&lt;ul&gt;&amp;#13;
&lt;li&gt;The write bit allows the affected user to create, rename, or delete files within the directory, and modify the directory&amp;#8217;s attributes&lt;/li&gt;&amp;#13;
&lt;li&gt;The read bit allows the affected user to list the files within the directory&lt;/li&gt;&amp;#13;
&lt;li&gt;The execute bit allows the affected user to enter the directory, and access files and directories inside&lt;/li&gt;&amp;#13;
&lt;li&gt;The sticky bit states that files and directories within that directory may only be deleted or renamed by their owner (or root)&lt;/li&gt;&amp;#13;
&lt;/ul&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sat, 20 Jun 2015 23:45:17 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-20:linuxmu-lu-de-quan-xian-itech.html</guid></item><item><title>nohup和disown - iTech</title><link>http://www.ciandcd.com/nohuphe-disown-itech.html</link><description>&lt;div&gt;&lt;p&gt;Many system administrators make a practice of using GNU Screen or tmux to manage jobs running in the terminal. If you have a long-running job that you want to "detach" from the terminal, you can simply use your terminal multiplexer to do it. But what if you don't use tmux or Screen, or you just forgot? For those times, there's nohup and disown.&lt;/p&gt;&amp;#13;
&lt;p&gt;Have a long-running job you want to "detach" from the terminal? Don't use tmux or Screen? If you haven't started the job yet, nohup is an easy to use option, and if you must stop a job in the middle, there's disown.&lt;/p&gt;&amp;#13;
&lt;p&gt;&lt;br&gt;If you haven't started the job yet, there's nohup. Short for "no hangup," nohup will detach the program from the current shell and send its output to nohup.out. If you quit the shell or whatever, the process will continue to run until it completes.&lt;/p&gt;&amp;#13;
&lt;p&gt;Ah, but what if you forget to use nohup, or if you didn't expect to be leaving the computer but get called away? Then there's disown.&lt;/p&gt;&amp;#13;
&lt;p&gt;The use of disown is a bit more complex. While the command is running, use Ctrl-z to stop it and then use bg to put it in the background. Then you'll use disown %n where n is the job number (jobspec). And, of course, you can find the job number using the jobs command. Run jobs again to verify that the job has been detached -- and you can use ps or top to verify that the job is actually still running.&lt;/p&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sat, 20 Jun 2015 23:45:17 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-20:nohuphe-disown-itech.html</guid></item></channel></rss>