<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ciandcd</title><link>http://www.ciandcd.com/</link><description>软件持续集成和持续发布 QQ群：172758282，437085002</description><atom:link href="http://www.ciandcd.com/feeds/devops.rss.xml" rel="self"></atom:link><lastBuildDate>Mon, 22 Jun 2015 00:00:00 +0800</lastBuildDate><item><title>3 Reasons Why Testing Software Security Should Start Early</title><link>http://www.ciandcd.com/3-reasons-why-testing-software-security-should-start-early.html</link><description>from:http://java.dzone.com/articles/3-reasons-why-testing-software&lt;br&gt;&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;p&gt;The software development life cycle&amp;#160;is an extremely intensive process for developers and quality assurance professionals alike. If even one element is neglected, it can delay project schedules and affect user performance. Security is one &lt;a href="https://www.getzephyr.com/products/enterprise-test-management/zephyr-enterprise-edition"&gt;aspect that must be built in&lt;/a&gt; from the inception of any app, and here are a few reasons&amp;#160;why:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Breaches can cost&amp;#160;your business&lt;/strong&gt;&lt;br&gt;
 Let's say that an organization uses its application&amp;#160;to&amp;#160;order and manage inventory, payroll and other operational&amp;#160;needs. If a malicious entity were&amp;#160;to&amp;#160;access this information, it could easily make fraudulent transactions, costing the company more than what was intended. Not&amp;#160;to&amp;#160;mention it will create a massive headache&amp;#160;to&amp;#160;set the record straight. TechTarget contributor Peter Gregory noted that this can happen when&amp;#160;&lt;a href="http://searchsecurity.techtarget.com/tip/Security-in-the-software-development-life-cycle" target="_blank"&gt;programs lack audit trails&lt;/a&gt;&amp;#160;and processes required for secure purchasing. By building in this functionality early on, this type of situation can be avoided, allowing organizations&amp;#160;to&amp;#160;retain customer trust and money.&lt;/p&gt;

&lt;p&gt;"Organizations that fail&amp;#160;to&amp;#160;involve information security in the life cycle will pay the price in the form of costly and disruptive events," Gregory wrote. "Many bad things can happen&amp;#160;to&amp;#160;information systems that lack the required security interfaces and characteristics."&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Access&amp;#160;to&amp;#160;confidential data can be damaging&lt;/strong&gt;&lt;br&gt;
 If a business aims&amp;#160;to&amp;#160;use an app for information sharing and availability, protection must be at the forefront of this project throughout its life cycle. While some data may not be as costly&amp;#160;to&amp;#160;leak, the&amp;#160;loss&amp;#160;of confidential reports and documents can severely affect the organization's ability&amp;#160;tofunction.&lt;/p&gt;

&lt;p&gt;QA teams must ensure that security practices are implemented and built upon constantly. TechTarget contributor Nick Lewis noted that&amp;#160;&lt;a href="http://searchsecurity.techtarget.com/tip/How-to-negate-business-logic-attack-risk-Improve-security-in-the-SDLC" target="_blank"&gt;firewalls and traditional methods will not be enough&lt;/a&gt;&amp;#160;to&amp;#160;keep targeted attacks at bay. Instead, testing the app for insufficient process validation, abuse of functionality, weak password recovery validation and information leakage will be critical&amp;#160;toguarding the program.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Analyze&amp;#160;initial risk before jumping in&lt;/strong&gt;&lt;br&gt;
 One SDLC security practice&amp;#160;to&amp;#160;observe is a primary risk assessment before the start of a new project. Not all applications are equal, which means each program will be labeled with a different risk level. Some software will be publicly accessible, whereas others will be more business-critical and involve processing sensitive data. These uses will largely determine how much risk would be involved with a breach on such activities. This information will give QA teams a clear picture of the security roadmap&amp;#160;needed, and can be implemented.&amp;#160;&lt;/p&gt;

&lt;p&gt;"Doing the preliminary risk assessment&amp;#160;to&amp;#160;establish the&amp;#160;need&amp;#160;for the system helps&amp;#160;&lt;a href="http://software-security.sans.org/resources/paper/cissp/defining-understanding-security-software-development-life-cycle" target="_blank"&gt;identify any security show stoppers&lt;/a&gt;&amp;#160;before&amp;#160;too much time and effort goes into the next SDLC phases," a SANS white paper stated. "It also gets the design team thinking about security issues early in the design process."&lt;/p&gt;

&lt;p&gt;Cyberattacks and malware in the headlines have made security more prominent than ever before. By building in protections early in the SDLC, QA teams can ensure that they will be better able&amp;#160;tohandle these threats without interruptions&amp;#160;to&amp;#160;regular business activities.&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Mon, 22 Jun 2015 00:00:00 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-22:3-reasons-why-testing-software-security-should-start-early.html</guid></item><item><title>Organisation Pattern: Trunk Based Development</title><link>http://www.ciandcd.com/organisation-pattern-trunk-based-development.html</link><description>from:http://java.dzone.com/articles/organisation-pattern-trunk-based-development&lt;br&gt;&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;p&gt;Trunk Based Development is a version control strategy in which developers commit their changes to the shared trunk of a source code repository with minimal branching. Trunk Based Development became well known in the mid 2000s as&amp;#160;&lt;a href="http://www.martinfowler.com/articles/continuousIntegration.html"&gt;Continuous Integration&lt;/a&gt;&amp;#160;became a mainstream development practice, and today it is equally applicable to centralised Version Control Systems (VCS) and Distributed Version Control Systems (DVCS).&lt;/p&gt;&lt;p&gt;In Trunk Based Development new features are developed concurrently on trunk as a series of small, incremental steps that preserve existing functionality and minimise merge complexity. Features are always released from trunk, and defect fixes are either released from trunk or a short-lived release branch.&lt;/p&gt;&lt;p&gt;When development of a feature spans multiple releases its entry point is concealed to ensure the ongoing changes do not impede release cadence. The addition of a new feature can be concealed with a&amp;#160;&lt;a href="http://www.martinfowler.com/bliki/FeatureToggle.html"&gt;Feature Toggle&lt;/a&gt;, which means a configuration parameter or business rule is used to turn a feature on or off at runtime. As shown below&amp;#160;a&amp;#160;Feature Toggle is turned off while its feature is in development (v1), turned on when its feature is in production (v2), and removed after a period of time (v3).&lt;/p&gt;&lt;p&gt;&lt;a href="http://www.alwaysagileconsulting.com/blog/wp-content/uploads/2015/04/Organisation-Pattern-Trunk-Based-Development-Feature-Toggle-Step-By-Step.png"&gt;&lt;img class="aligncenter wp-image-4456" src="http://www.alwaysagileconsulting.com/blog/wp-content/uploads/2015/04/Organisation-Pattern-Trunk-Based-Development-Feature-Toggle-Step-By-Step.png" alt="Organisation Pattern - Trunk Based Development - Feature Toggle Step By Step" width="650" height="208"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Updates to an existing feature can be concealed with a&amp;#160;&lt;a href="http://martinfowler.com/bliki/BranchByAbstraction.html"&gt;Branch By Abstraction&lt;/a&gt;, which means an&amp;#160;abstraction layer is temporarily introduced to encapsulate both the old behaviour in use and the new behaviour in development. As shown below a&amp;#160;Branch By Abstraction&amp;#160;routes requests to the old behaviour while the new behaviour is in development (v1-v2), reroutes requests to the new behaviour when it is in production (v3), and is removed after a period of time (v4).&lt;/p&gt;&lt;p&gt;&lt;a href="http://www.alwaysagileconsulting.com/blog/wp-content/uploads/2015/04/Organisation-Pattern-Trunk-Based-Development-Branch-By-Abstraction-Step-By-Step.png"&gt;&lt;img class="aligncenter wp-image-4458" src="http://www.alwaysagileconsulting.com/blog/wp-content/uploads/2015/04/Organisation-Pattern-Trunk-Based-Development-Branch-By-Abstraction-Step-By-Step.png" alt="Organisation Pattern - Trunk Based Development - Branch By Abstraction Step By Step" width="650" height="223"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Trunk Based Development is synonymous with Continuous Integration, which has been described by Jez Humble et al as &amp;#8220;&lt;a href="http://www.amazon.co.uk/dp/1449368425"&gt;the most important technical practice in the agile canon&lt;/a&gt;&amp;#8220;. Continuous Integration is a development practice where&amp;#160;all members of a team integrate and test their changes together on at least a daily basis, resulting in a shared mindset of collaboration and an always releasable codebase. This is verified by an automated build server continuously building the latest changes, and can include pre- and post-build actions such as code reviews and auto-revert on failure.&lt;/p&gt;&lt;p&gt;Consider an organisation that provides an online Company Accounts Service, with its codebase maintained by a team practicing Trunk Based Development and Continuous Integration. In iteration 1&amp;#160;two features are requested &amp;#8211; F1 Computations and F2 Write Offs &amp;#8211; so the team discuss their concurrent development&amp;#160;and decide on a&amp;#160;Feature Toggle for F1 as it is a larger change. The developers commit their changes for F1 and F2 to trunk multiple times a day, with F1 tested in its on and off states to verify its progress alongside F2.&lt;/p&gt;&lt;p&gt;&lt;a href="http://www.alwaysagileconsulting.com/blog/wp-content/uploads/2015/06/Organisation-Pattern-Trunk-Based-Development-Trunk-Based-Development-1.png"&gt;&lt;img class="aligncenter wp-image-4464" src="http://www.alwaysagileconsulting.com/blog/wp-content/uploads/2015/06/Organisation-Pattern-Trunk-Based-Development-Trunk-Based-Development-1.png" alt="Organisation Pattern - Trunk Based Development - Trunk Based Development 1" width="882" height="450"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In iteration 2 more features &amp;#8211; F3 Bank Details and F4 Accounting Periods &amp;#8211; begin development. F4 requires a different downstream submissions system, so the team design&amp;#160;a Branch By Abstraction for submissions to ensure F1 and F3 can continue with the legacy&amp;#160;submissions system until F4 is complete.&amp;#160;F2 is signed off and released into production with F1 still toggled off at runtime. Some changes for F3 break the build, which triggers an automatic revert and a team discussion on a better design for F3.&lt;/p&gt;&lt;p&gt;&lt;a href="http://www.alwaysagileconsulting.com/blog/wp-content/uploads/2015/06/Organisation-Pattern-Trunk-Based-Development-Trunk-Based-Development-2.png"&gt;&lt;img class="aligncenter wp-image-4465" src="http://www.alwaysagileconsulting.com/blog/wp-content/uploads/2015/06/Organisation-Pattern-Trunk-Based-Development-Trunk-Based-Development-2.png" alt="Organisation Pattern - Trunk Based Development - Trunk Based Development 2" width="882" height="450"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In iteration 3 a&amp;#160;production defect is found in F2, and after the defect is fixed on trunk a release branch is agreed for risk mitigation. An F2.1 release branch is created from the last commit of the&amp;#160;F2&amp;#160;release,&amp;#160;the&amp;#160;fix is merged to the branch, and F2.1 is released into production. F4 continues on trunk, with the&amp;#160;submissions Branch By Abstraction tested in both modes. F3 is signed off and released into production using the legacy submissions system.&lt;/p&gt;&lt;p&gt;&lt;a href="http://www.alwaysagileconsulting.com/blog/wp-content/uploads/2015/06/Organisation-Pattern-Trunk-Based-Development-Trunk-Based-Development-3.png"&gt;&lt;img class="aligncenter wp-image-4466" src="http://www.alwaysagileconsulting.com/blog/wp-content/uploads/2015/06/Organisation-Pattern-Trunk-Based-Development-Trunk-Based-Development-3.png" alt="Organisation Pattern - Trunk Based Development - Trunk Based Development 3" width="882" height="450"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In iteration 4&amp;#160;F1 is signed off and its Feature Toggle is turned on in production following a release. F4 is signed off and released into production, but when the Branch By Abstraction is switched to the&amp;#160;new submissions system a defect is found. As a result the Branch By Abstraction is reverted at runtime to the legacy submissions system, and a F4.1 fix is&amp;#160;released from trunk.&lt;/p&gt;&lt;p&gt;&lt;a href="http://www.alwaysagileconsulting.com/blog/wp-content/uploads/2015/06/Organisation-Pattern-Trunk-Based-Development-Trunk-Based-Development-4.png"&gt;&lt;img class="aligncenter wp-image-4467" src="http://www.alwaysagileconsulting.com/blog/wp-content/uploads/2015/06/Organisation-Pattern-Trunk-Based-Development-Trunk-Based-Development-4.png" alt="Organisation Pattern - Trunk Based Development - Trunk Based Development 4" width="882" height="450"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this example F1, F2, F3, and F4 clearly benefit from being developed by a team collaborating on a single shared code stream. For F1 the team agrees on the why and how of the Feature Toggle, with F1 tested in both its on and off states. For F2 the defect fix is made available from trunk and everyone is aware of the decision to use a release branch for risk mitigation. For F3 the prominence of a reverted build failure encourages people to contribute to a better design. For F4 there is a team&amp;#160;decision&amp;#160;to create a submissions Branch By Abstraction, with the new abstraction layer offering fresh insights into the legacy system and incremental commits enabling regular feedback on the new approach. Furthermore, when the new submissions system is switched on and a defect is found in F4 the ability to revert at runtime to the legacy submissions means the Company Accounts Service can remain online with zero downtime.&lt;/p&gt;&lt;p&gt;This&amp;#160;highlights the advantages of Trunk Based Development:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Continuous Integration &amp;#8211; incremental commits to trunk ensure an&amp;#160;always&amp;#160;integrated, always tested codebase with minimal integration costs and a predictable flow of features&lt;/li&gt;&lt;li&gt;Adaptive scheduling &amp;#8211; an always releasable codebase separates the release schedule from development efforts, meaning features can be released on demand&amp;#160;according to customer needs&lt;/li&gt;&lt;li&gt;Collaborative design &amp;#8211; everyone working&amp;#160;on&amp;#160;the same code encourages constant communication,&amp;#160;with team members sharing responsibility for design changes and a cohesive&amp;#160;&lt;a href="http://www.ibm.com/developerworks/java/library/j-eaed1/index.html"&gt;Evolutionary Architecture&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Operational and business empowerment&amp;#160;&amp;#8211;&amp;#160;techniques such as&amp;#160;Feature Toggle and Branch By Abstraction decouple release from launch, providing&amp;#160;the operational benefit of graceful degradation&amp;#160;on failure and the business benefit of&amp;#160;&lt;a href="http://www.informit.com/articles/article.aspx?p=1833567&amp;amp;seqNum=2"&gt;Dark Launching&lt;/a&gt;&amp;#160;features&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Breaking down features and re-architecting an existing system in incremental&amp;#160;steps requires discipline, planning, and ingenuity from an entire team on a daily basis, and Trunk Based Development can incur a development overhead for some time if multiple technologies are in play and/or the codebase is poorly structured. However, those&amp;#160;additional efforts&amp;#160;will substantially reduce integration costs and gradually push&amp;#160;the codebase in the right&amp;#160;direction &amp;#8211; as shown&amp;#160;by Dave Farley and Jez Humble praising Trunk Based Development for &amp;#8220;&lt;a href="http://www.amazon.co.uk/dp/0321601912"&gt;the gentle, subtle pressure it applies to make the design of your software better&lt;/a&gt;&amp;#8220;.&lt;/p&gt;&lt;p&gt;A common misconception of Trunk Based Development is that it is slow, as features take longer to complete and team velocity is&amp;#160;often lower than expected. However, an organisation should optimise globally for cycle time not locally for velocity, and by mandating a single code stream Trunk Based Development ensures developers work at the maximum rate of the team not the individual, with reduced integration costs resulting in lower lead times.&lt;/p&gt;&lt;p&gt;Trunk Based Development is&amp;#160;simple, but not easy. It has a steep learning curve but the continuous integration&amp;#160;of&amp;#160;small changesets into trunk&amp;#160;will minimise integration costs,&amp;#160;encourage collaborative design, empower runtime&amp;#160;operational and business decisions, and&amp;#160;ultimately drive the engine of Continuous&amp;#160;Delivery. It is for this reason Dave Farley and Jez Humble&amp;#160;declared&amp;#160;&amp;#8220;&lt;a href="http://www.amazon.co.uk/dp/0321601912"&gt;we can&amp;#8217;t emphasise enough how important this practice is in enabling continuous delivery of valuable, working software&lt;/a&gt;&amp;#8220;.&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Mon, 22 Jun 2015 00:00:00 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-22:organisation-pattern-trunk-based-development.html</guid></item><item><title>Get Back Up and Try Again: Retrying in Python</title><link>http://www.ciandcd.com/get-back-up-and-try-again-retrying-in-python.html</link><description>from:http://python.dzone.com/articles/get-back-and-try-again&lt;br&gt;&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;p&gt;I don't often write about tools I use when for my daily software development
tasks. I recently realized that I really should start to share more often my
workflows and weapons of choice.&lt;/p&gt;
&lt;p&gt;One thing that I have a hard time enduring while doing Python code reviews, is
people writing utility code that is not directly tied to the core of their
business. This looks to me as wasted time maintaining code that should be
reused from elsewhere.&lt;/p&gt;
&lt;p&gt;So today I'd like to start with
&lt;a href="https://pypi.python.org/pypi/retrying"&gt;retrying&lt;/a&gt;, a Python package that you
can use to&amp;#8230; retry anything.&lt;/p&gt;
&lt;h2&gt;It's OK to fail&lt;/h2&gt;


&lt;p&gt;Often in computing, you have to deal with external resources. That means
accessing resources you don't control. Resources that can fail, become
flapping, unreachable or unavailable.&lt;/p&gt;
&lt;p&gt;Most applications don't deal with that at all, and explode in flight, leaving a
skeptical user in front of the computer. A lot of software engineers refuse to
deal with failure, and don't bother handling this kind of scenario in their
code.&lt;/p&gt;
&lt;p&gt;In the best case, applications usually handle simply the case where the
external reached system is out of order. They log something, and inform the
user that it should try again later.&lt;/p&gt;
&lt;p&gt;In this cloud computing area, we tend to design software components with
&lt;a href="https://en.wikipedia.org/wiki/Service-oriented_architecture"&gt;service-oriented architecture&lt;/a&gt;
in mind. That means having a lot of different services talking to each others
over the network. And we all know that networks tend to fail, and distributed
systems too. Writing software with failing being part of normal operation is a
terrific idea.&lt;/p&gt;
&lt;h2&gt;Retrying&lt;/h2&gt;


&lt;p&gt;In order to help applications with the handling of these potential failures,
you need a plan. Leaving to the user the burden to "try again later" is rarely
a good choice. Therefore, most of the time you want your application to
retry.&lt;/p&gt;
&lt;p&gt;Retrying an action is a full strategy on its own, with a lot of options. You
can retry only on certain condition, and with the number of tries based on time
(e.g. every second), based on a number of tentative (e.g. retry 3 times and
abort), based on the problem encountered, or even on all of those.&lt;/p&gt;
&lt;p&gt;For all of that, I use the &lt;a href="https://github.com/rholder/retrying"&gt;retrying&lt;/a&gt;
library that you can retrieve easily on
&lt;a href="https://pypi.python.org/pypi/retrying"&gt;PyPI&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;retrying provides a decorator called &lt;code&gt;retry&lt;/code&gt; that you can use on top of any
function or method in Python to make it retry in case of failure. By default,
&lt;code&gt;retry&lt;/code&gt; calls your function endlessly until it returns rather than raising an
error.&lt;/p&gt;
&lt;pre class="brush:python"&gt;import random&lt;br&gt;from retrying import retry&lt;br&gt;&amp;#160;&lt;br&gt;@retry&lt;br&gt;def pick_one():&lt;br&gt;    if random.randint(0, 10) != 1:&lt;br&gt;        raise Exception("1 was not picked")&lt;/pre&gt;

&lt;p&gt;

This will execute the function &lt;code&gt;pick_one&lt;/code&gt; until &lt;code&gt;1&lt;/code&gt; is returned by
&lt;code&gt;random.randint&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;retry&lt;/code&gt; accepts a few arguments, such as the minimum and maximum delays to use,
which also can be randomized. Randomizing delay is a good strategy to avoid
detectable pattern or congestion. But more over, it supports exponential delay,
which can be used to implement
&lt;a href="https://en.wikipedia.org/wiki/Exponential_backoff"&gt;exponential backoff&lt;/a&gt;, a
good solution for retrying tasks while really avoiding congestion. It's
especially handy for background tasks.&lt;/p&gt;
&lt;pre class="brush:python"&gt;@retry(wait_exponential_multiplier=1000, wait_exponential_max=10000)&lt;br&gt;def wait_exponential_1000():&lt;br&gt;    print "Wait 2^x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards"&lt;br&gt;    raise Exception("Retry!")&lt;/pre&gt;

&lt;p&gt;

You can mix that with a maximum delay, which can give you a good strategy to
retry for a while, and then fail anyway:&lt;/p&gt;
&lt;pre class="brush:python"&gt;# Stop retrying after 30 seconds anyway&lt;br&gt;&amp;gt;&amp;gt;&amp;gt; @retry(wait_exponential_multiplier=1000, wait_exponential_max=10000, stop_max_delay=30000)&lt;br&gt;... def wait_exponential_1000():&lt;br&gt;...     print "Wait 2^x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards"&lt;br&gt;...     raise Exception("Retry!")&lt;br&gt;...&lt;br&gt;&amp;gt;&amp;gt;&amp;gt; wait_exponential_1000()&lt;br&gt;Wait 2^x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards&lt;br&gt;Wait 2^x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards&lt;br&gt;Wait 2^x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards&lt;br&gt;Wait 2^x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards&lt;br&gt;Wait 2^x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards&lt;br&gt;Wait 2^x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards&lt;br&gt;Traceback (most recent call last):&lt;br&gt;  File "&amp;lt;stdin&amp;gt;", line 1, in &amp;lt;module&amp;gt;&lt;br&gt;  File "/usr/local/lib/python2.7/site-packages/retrying.py", line 49, in wrapped_f&lt;br&gt;    return Retrying(*dargs, **dkw).call(f, *args, **kw)&lt;br&gt;  File "/usr/local/lib/python2.7/site-packages/retrying.py", line 212, in call&lt;br&gt;    raise attempt.get()&lt;br&gt;  File "/usr/local/lib/python2.7/site-packages/retrying.py", line 247, in get&lt;br&gt;    six.reraise(self.value[0], self.value[1], self.value[2])&lt;br&gt;  File "/usr/local/lib/python2.7/site-packages/retrying.py", line 200, in call&lt;br&gt;    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)&lt;br&gt;  File "&amp;lt;stdin&amp;gt;", line 4, in wait_exponential_1000&lt;br&gt;  Exception: Retry!
&lt;/pre&gt;

&lt;p&gt;

A pattern I use very often, is the ability to retry only based on some
exception type. You can specify a function to filter out exception you want to
ignore or the one you want to use to retry.&lt;/p&gt;
&lt;pre class="brush:python"&gt;def retry_on_ioerror(exc):&lt;br&gt;    return isinstance(exc, IOError)&lt;br&gt;&amp;#160;&lt;br&gt;@retry(retry_on_exception=retry_on_ioerror)&lt;br&gt;def read_file():&lt;br&gt;    with open("myfile", "r") as f:&lt;br&gt;        return f.read()&lt;/pre&gt;

&lt;p&gt;

&lt;code&gt;retry&lt;/code&gt; will call the function passed as &lt;code&gt;retry_on_exception&lt;/code&gt; with the
exception raised as first argument. It's up to the function to then return a
boolean indicating if a retry should be performed or not. In the example above,
this will only retry to read the file if an &lt;code&gt;IOError&lt;/code&gt; occurs; if any other
exception type is raised, no retry will be performed.&lt;/p&gt;
&lt;p&gt;The same pattern can be implemented using the keyword argument
&lt;code&gt;retry_on_result&lt;/code&gt;, where you can provide a function that analyses the result
and retry based on it.&lt;/p&gt;
&lt;pre class="brush:python"&gt;def retry_if_file_empty(result):&lt;br&gt;    return len(result) &amp;lt;= 0&lt;br&gt;&amp;#160;&lt;br&gt;@retry(retry_on_result=retry_if_file_empty)&lt;br&gt;def read_file():&lt;br&gt;    with open("myfile", "r") as f:&lt;br&gt;        return f.read()&lt;/pre&gt;

&lt;p&gt;

This example will read the file until it stops being empty. If the file does
not exist, an &lt;code&gt;IOError&lt;/code&gt; is raised, and the default behavior which triggers
retry on all exceptions kicks-in &amp;#8211; the retry is therefore performed.&lt;/p&gt;
&lt;p&gt;That's it! &lt;code&gt;retry&lt;/code&gt; is really a good and small library that you should leverage
rather than implementing your own half-baked solution!&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 04:00:00 -0400</pubDate><guid>tag:www.ciandcd.com,2015-06-21:get-back-up-and-try-again-retrying-in-python.html</guid></item><item><title>Continuous Integration and Delivery with Docker</title><link>http://www.ciandcd.com/continuous-integration-and-delivery-with-docker.html</link><description>from:http://java.dzone.com/articles/continuous-integration-and-0&lt;br&gt;&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;
              &lt;p align="center"&gt;&lt;i&gt;Written by&amp;#160;&lt;a href="http://blog.codeship.com/author/jaroslavholub/" title="Posts by Jaroslav Holub" rel="author"&gt;Jaroslav Holub&lt;/a&gt; for &lt;a href="http://blog.codeship.com/" target=""&gt;The Codeship Blog&lt;/a&gt;.&lt;/i&gt;&lt;br&gt;&lt;/p&gt;&lt;p&gt;Continuous delivery is all about reducing risk and delivering value faster by producing reliable software in short iterations. &lt;a href="http://martinfowler.com/bliki/ContinuousDelivery.html"&gt;As Martin Fowler says&lt;/a&gt;, you actually do continuous delivery if:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Your software is deployable throughout its lifecycle.&lt;/li&gt;
&lt;li&gt;Your team prioritizes keeping the software deployable over working on new features.&lt;/li&gt;
&lt;li&gt;Anybody can get fast, automated feedback on the production readiness of their systems any time somebody makes a change to them.&lt;/li&gt;
&lt;li&gt;You can perform push-button deployments of any version of the software to any environment on demand.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Containerization of software allows us to further improve on this process. The biggest improvements are in speed and in the level of abstraction used as a cornerstone for further innovations in this field.&lt;/p&gt;
&lt;p&gt;In this post, I&amp;#8217;ll show you how to set up a continuous delivery pipeline using Docker. We&amp;#8217;ll see how using this tool for Linux containers as part of the continuous delivery pipeline lets us nicely encapsulate the build process of a service. It also lets us deploy any revision with a few simple steps.&lt;/p&gt;
&lt;p&gt;
&lt;/p&gt;&lt;p class="tm-tweet-clear"&gt;&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ll mainly use the term continuous delivery in this article, because it stands for the full circle of steps leading to our ultimate goal. However, continuous integration is the most substantial part of continuous delivery.&lt;/p&gt;
&lt;h2&gt;Continuous Integration with Docker&lt;/h2&gt;
&lt;p&gt;Let&amp;#8217;s take a Hello World web server written in &lt;a href="https://golang.org/"&gt;Go&lt;/a&gt; as an example service. You can find all the code used in this example here: &lt;a href="https://github.com/ContainerSolutions/cd-with-docker-tutorial"&gt;https://github.com/ContainerSolutions/cd-with-docker-tutorial&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The continuous integration setup consists of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;running unit tests&lt;/li&gt;
&lt;li&gt;building the Docker image that we use to build our service&lt;/li&gt;
&lt;li&gt;running the build container and compiling our service&lt;/li&gt;
&lt;li&gt;building the Docker image that we run and deploy&lt;/li&gt;
&lt;li&gt;pushing the final image to a Docker registry&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Automated testing&lt;/h3&gt;
&lt;p&gt;Running tests in this example is as trivial as it should be:&lt;/p&gt;
&lt;pre class="brush:shell"&gt;go test&lt;/pre&gt;
&lt;h3&gt;Building Docker image&lt;/h3&gt;
&lt;p&gt;The core of a single service integration is making the end artifact &amp;#8212; Docker image in our case.&lt;/p&gt;
&lt;p&gt;Because I&amp;#8217;ve deliberately chosen the compiled language Go in this example, we need to build an executable file as part of our integration process. We&amp;#8217;ll eventually place the executable file inside this Docker image.&lt;/p&gt;
&lt;p&gt;Now one might think that we would build our web server executable file using build tools installed on the host dedicated to continuous integration and then somehow copy the binary to the Docker image. But this is a no-no in the containerized world. Let&amp;#8217;s do it all in containers. That way, we won&amp;#8217;t rely on any build tools installed on hosts, and it&amp;#8217;ll make the whole setup easily reproducible and encapsulated.&lt;/p&gt;
&lt;p&gt;Building an executable file can be part of a single Docker image build process together with runtime environment setup. Or we can separate the two. Having everything in a single build process, we would end up with extra content (build process leftovers) in our Docker image filesystem, &lt;a href="http://docs.docker.com/introduction/understanding-docker/#how-does-a-docker-image-work"&gt;even if we clean it afterwards&lt;/a&gt; in separate RUN commands within the Dockerfile.&lt;/p&gt;
&lt;p&gt;Some people use tricks to create, manipulate, and remove unwanted stuff in a single &lt;strong&gt;RUN&lt;/strong&gt; command. Although it&amp;#8217;s sometimes handy, I can&amp;#8217;t generally recommend it; in my opinion this adds to Dockerfile complexity. Of course, there are situations where you might want to retain your sources and all in the end artifact.&lt;/p&gt;
&lt;p&gt;The approach I recommend, however, is to create separate &amp;#8220;build&amp;#8221; and &amp;#8220;distribution&amp;#8221; Dockerfiles. Use &lt;strong&gt;Dockerfile.build&lt;/strong&gt; to do the heavy lifting during building the software, and use &lt;strong&gt;Dockerfile.dist&lt;/strong&gt; to create the distributable Docker image, as light and clean as possible.&lt;/p&gt;
&lt;p&gt;The following is &lt;strong&gt;Dockerfile.build&lt;/strong&gt;. As you can see, once we run the build file, we create the container from a golang image, compile our example service, and output the binary.&lt;/p&gt;
&lt;pre class="brush:shell"&gt;FROM golang:1.4

RUN mkdir -p /tmp/build 
ADD hello-world.go /tmp/build/ 
WORKDIR /tmp/build 
RUN go build hello-world.go 
CMD tar -czf - hello-world&lt;/pre&gt;
&lt;p&gt;In &lt;strong&gt;Dockerfile.dist&lt;/strong&gt;, we only use this binary and run it on runtime:&lt;/p&gt;
&lt;pre class="brush:shell"&gt;FROM debian:jessie

RUN mkdir /app 
ADD build.tar.gz /app/ 
ENTRYPOINT /app/hello-world&lt;/pre&gt;
&lt;p&gt;Our &lt;strong&gt;build.sh&lt;/strong&gt; script &amp;#8212; the essential part of our continuous integration pipeline &amp;#8212; then looks like this:&lt;/p&gt;
&lt;pre class="brush:shell"&gt;# !/bin/sh
docker build -t hello-world-build -f Dockerfile.build . 
docker run hello-world-build &amp;gt; build.tar.gz 
docker build -t hello-world -f Dockerfile.dist .&lt;/pre&gt;
&lt;p&gt;As you can see, these three simple Docker commands get us a clean, small &lt;strong&gt;Hello-World&lt;/strong&gt; Docker image that&amp;#8217;s ready to be deployed and run on demand. Once both images used in the &lt;strong&gt;FROM&lt;/strong&gt; clauses are pulled and cached locally, our build process will be a matter of milliseconds or at most a few seconds, with a very small resources footprint.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://codeship.com/?utm_source=CodeshipBlog&amp;amp;utm_medium=blogbanner&amp;amp;utm_campaign=Docker" target="_blank"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Storing Docker images&lt;/h3&gt;
&lt;p&gt;Once our build process artifact is created, we want to push it to Docker Registry, where it will be available for deployments.&lt;/p&gt;
&lt;p&gt;Please note that tagging images properly is very important. &lt;a href="http://container-solutions.com/2015/01/docker-latest-confusion/"&gt;Docker ecosystems suffer&lt;/a&gt; from the usage of &amp;#8220;latest&amp;#8221; tag. If you use a unique tag for every new image, then all your image versions will be easily accessible for deployment in the future.&lt;/p&gt;
&lt;p&gt;We can choose whether we want to use our own &lt;a href="http://blog.codeship.com/running-secured-docker-registry-2-0/"&gt;Docker Registry&lt;/a&gt; or rely on &lt;a href="https://hub.docker.com/"&gt;Docker Hub&lt;/a&gt;. On Docker Hub, you can store public or private repositories of images. It&amp;#8217;s also the first place people would look for your images (if you want anyone to look for them).&lt;/p&gt;
&lt;p&gt;Your own Docker Registry on the other hand gives you full control over your images storage, performance, and security. More advanced setups might combine both approaches.&lt;/p&gt;
&lt;p&gt;This way you can tag the new image with an appropriate tag and push it to a public hub (replace &lt;strong&gt;your_username&lt;/strong&gt; and &lt;strong&gt;your_tag&lt;/strong&gt; with actual values):&lt;/p&gt;
&lt;pre class="brush:shell"&gt;# !/bin/sh
docker tag hello-world:latest your_username/hello-world:your_tag 
docker push your_username/hello-world:your_tag&lt;/pre&gt;
&lt;h2&gt;Continuously Delivered Containers&lt;/h2&gt;
&lt;p&gt;Once we have our Docker images building pipeline working and images nicely stashed in a repository, we definitely want to get our service deployed.&lt;/p&gt;
&lt;p&gt;How you deploy your applications depends on your infrastructure or cloud provider. A few cloud providers support Docker images in their APIs these days (e.g., &lt;a href="http://aws.amazon.com/documentation/ecs/"&gt;Amazon EC2 Container Service&lt;/a&gt;, &lt;a href="https://www.digitalocean.com/features/one-click-apps/docker/"&gt;Digital Ocean&lt;/a&gt;, or &lt;a href="http://docs.giantswarm.io/"&gt;Giant Swarm&lt;/a&gt;). You can further leverage the power of containerized applications with resource abstraction tools like &lt;a href="http://mesos.apache.org/"&gt;Apache Mesos&lt;/a&gt; (read more about &lt;a href="http://container-solutions.com/2015/04/how-to-set-up-mesos-on-google-cloud-with-terraform/"&gt;running containers on Mesos&lt;/a&gt;) or &lt;a href="http://kubernetes.io/"&gt;Google Kubernetes&lt;/a&gt; that let you deploy and manage containers in their own ways.&lt;/p&gt;
&lt;p&gt;In case of our &lt;strong&gt;Hello World&lt;/strong&gt; example, deploying remotely means running the following command remotely on a target machine with Docker installed on it:&lt;/p&gt;
&lt;pre class="brush:shell"&gt;# !/bin/sh
docker stop hello-production 
docker run --rm -p 8000:80 --name hello-production hello-world&lt;/pre&gt;
&lt;h2&gt;Beyond Continuous Delivery with Docker&lt;/h2&gt;
&lt;p&gt;Using containerized software does not inherently mean one is implementing microservices. However, containers enable this architectural pattern because they encourage developers to split their monoliths based on separation of concerns.&lt;/p&gt;
&lt;p&gt;Microservices also promote communication between containerized components over a plain network using standardized and easily replaceable tubes. To learn more about microservices and why they might be a good architectural pattern for your software project, I recommend &lt;a href="http://shop.oreilly.com/product/0636920033158.do"&gt;Building Microservices&lt;/a&gt; by Sam Newman.&lt;/p&gt;
&lt;p&gt;A continuous delivery pipeline with containerized software also allows you to set up a new kind of testing environment; subsets of (micro)services are deployed in small clusters that represent the system under test running with some parts intentionally disabled or disconnected.&lt;/p&gt;
&lt;p&gt;Creation of such a matrix of deployments and programming against it has little to no additional costs in terms of a continuous integration time. It does have a dramatic impact on the stability and resilience of software in production. Such a testing system allows teams to get ready to deal with any kind of &lt;a href="https://github.com/Netflix/SimianArmy/wiki/Chaos-Monkey"&gt;Chaos Monkey&lt;/a&gt;.&lt;/p&gt;
	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 01:00:00 -0400</pubDate><guid>tag:www.ciandcd.com,2015-06-21:continuous-integration-and-delivery-with-docker.html</guid></item><item><title>Enabling DataOps with Easy Log Analytics</title><link>http://www.ciandcd.com/enabling-dataops-with-easy-log-analytics.html</link><description>from:http://java.dzone.com/articles/enabling-dataops-easy-log&lt;br&gt;&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;p&gt;DataOps is becoming an important consideration for organizations. Why? Well, DataOps is about making sure data is collected, analyzed,&amp;#160;and available across the company &amp;#8211; i.e. Ops insight for your decision-making systems like Hubspot, Tableau, Salesforce and more. Such systems are key to day-to-day operations and in many cases are as important as keeping your customer facing systems up and running.&amp;#160; &lt;/p&gt;
&lt;h2&gt;If you think about it, today every online business is a data driven business! Everyone is accountable to have up to the minute answers on what is happening across their systems. You can&amp;#8217;t do this reliably without having DataOps in place.&lt;/h2&gt;
 

&lt;p&gt;We have seen this trend across our own customer base at &lt;a href="https://logentries.com" target="_blank"&gt;Logentries&lt;/a&gt; where more and more customers using log data to implement &lt;a href="https://blog.logentries.com/2015/04/what-is-dataops-why-you-need-it/"&gt;DataOps across their organization&lt;/a&gt;. Using log data for DataOps allows you to perform the following:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;&lt;strong&gt;Troubleshoot your systems&lt;/strong&gt; managing your data by identifying errors and correlating data sources&lt;/li&gt;
	&lt;li&gt;Get notified when one of these systems is experiencing issues via &lt;strong&gt;real time alerts or anomaly detection&lt;/strong&gt;&lt;/li&gt;
	&lt;li&gt;&lt;strong&gt;Analyze how these systems are used&lt;/strong&gt; by the organization&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Logentries has always been great at 1 and 2 above, and this week we have enhanced Logentries to now allow you to perform easier and more powerful analytics with our n&lt;a href="https://logentries.com/logentries-new-analytics-language-makes-the-power-of-log-data-accessible-to-the-masses/" target="_blank"&gt;ew easy-to-use SQL like query language &amp;#8211; Logentries QL (LEQL)&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;LEQL&amp;#160;is designed to make analyzing your log data dead simple.&lt;/h2&gt;

&lt;p&gt;There are too many log management tools that are built&amp;#160;around complex query languages and require data scientists to operate.&lt;/p&gt;

&lt;p&gt;Logentries is all about making log data accessible to anyone. With LEQL you are going to be able to use analytical functions like CountUnique, Min, Max, GroupBy, Sort&amp;#8230;A number of our users have already been testing these out via our beta program. One great example is &lt;a href="https://blog.logentries.com/2015/04/dataops-how-pluralsight-uses-tableau-logentries-for-better-analytics/" target="_blank"&gt;how Pluralsight has been using Logentries to manage and understand the usage of their Tableau environment&lt;/a&gt;. For example:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Calculating the rate of errors over the the past 24 hours e.g. using LEQL Count function&lt;/li&gt;
	&lt;li&gt;Understanding user usage patterns e.g. using GroupBy to understand queries performed grouped by different users&lt;/li&gt;
	&lt;li&gt;Sorting the data to find the most popular queries and how long they are taking&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Being able to answer these types of questions enables&amp;#160;DataOps teams to understand where they need to invest time going forward. For example, do I need to add capacity to improve query performance? Are internal teams having a good user experience or are they getting a lot of errors when they try to access data?&lt;/p&gt;

&lt;p&gt;At Logentries we are all about making the power of log data accessible to everyone and as we do this we are constantly seeing cool new use cases when using logs. If you have some cool use cases do let us know!&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 01:00:00 -0400</pubDate><guid>tag:www.ciandcd.com,2015-06-21:enabling-dataops-with-easy-log-analytics.html</guid></item><item><title>How to Make Sure Your Mobile App is Secure</title><link>http://www.ciandcd.com/how-to-make-sure-your-mobile-app-is-secure.html</link><description>from:http://java.dzone.com/articles/how-make-sure-your-mobile-app&lt;br&gt;&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;p&gt;Mobile app development has become vital for enterprises as they look to support new devices (phones, tablets, wearables, etc.) for internal use while also reaching out to their increasingly mobile customers. This approach makes sense: According to&amp;#160;&lt;a href="http://www.smartinsights.com/mobile-marketing/mobile-marketing-analytics/mobile-marketing-statistics/" target="_blank"&gt;a comScore report&lt;/a&gt;, the number of mobile Internet users outnumbered desktop ones for the first time at some point in late 2013, and has since achieved significant separation. Many companies have responded to this change by implementing bring-your-own-device policies and &lt;a href="http://www.getzephyr.com/products/enterprise-test-management"&gt;building mobile apps&lt;/a&gt; that complement their full websites, mobile Web presence and/or desktop applications.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Watch out for pitfalls in mobile apps: General risks and the recent Starbucks example&lt;/strong&gt;&lt;br&gt;
 However, both BYOD policies and mobile app development require due diligence around cybersecurity if they are to be worthwhile. Safety starts with well-designed applications that are strongly authenticated, do not leak sensitive data and are safe from popular attack vectors like brute-force password guessing. Unfortunately, many apps still have a long way to go on these fronts.&lt;/p&gt;

&lt;p&gt;An early 2014 study from MetaIntell discovered that 92 percent of the top 500 most popular Android apps at the time&amp;#160;&lt;a href="http://webcache.googleusercontent.com/search?q=cache:IlFkOTzivnkJ:https://metaintelli.com/blog/2014/01/22/metaintell-identifies-enterprise-security-risks-privacy-risks-and-data-leakage-in-92-of-top-500-android-mobile-applications/+&amp;amp;cd=2&amp;amp;hl=en&amp;amp;ct=clnk&amp;amp;gl=us" target="_blank"&gt;created privacy risks due to data leakage&lt;/a&gt;. Wary of leaky apps as well as what kinds of information users put into them, enterprises have understandably been concerned about the impact of mobile apps on their operations and BYOD initiatives. Security is often the biggest barrier to effective BYOD, and justifiably so considering that&amp;#160;&lt;a href="http://www.usatoday.com/story/tech/2014/08/26/byod-bring-your-own-device/14393635/" target="_blank"&gt;barely more than 40 percent of employees&lt;/a&gt;&amp;#160;are required to have a security tool installed, according to Webroot.&lt;/p&gt;

&lt;p&gt;To get a sense of what could go wrong with today's mobile apps, consider what recently happened to Starbucks. The company's app is a mainstay on many phones, and at one time it accounted for the bulk of all mobile payments made in North America. The issue that arose over the last few months involved unauthorized card reloads and apparent account hijackings.&lt;/p&gt;

&lt;p&gt;The causes may have been mixed, with&amp;#160;&lt;a href="http://www.pcmag.com/article2/0,2817,2484245,00.asp" target="_blank"&gt;poor password management&lt;/a&gt;&amp;#160;on the part of users possibly exacerbated by exploitation of the app's auto-reload feature and an April 2015 outage of the coffee chain's point-of-sale systems. At the end of the day, Starbucks implemented additional security questions and has been urged to add two-factor authentication into the app to prevent erroneous transactions.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Catching mobile app security issues with a test management solution&lt;/strong&gt;&lt;br&gt;
 As we can see, mobile app security is multifactorial, requiring best efforts on the parts of end users, developers and infrastructure/network providers. For enterprises, the best approach to ensuring long-term security is to catch potential vulnerabilities early and often with a test management system.&lt;/p&gt;

&lt;p&gt;A test management solution supports both &lt;a href="https://www.getzephyr.com/insights/when-use-manual-testing-vs-automated-testing"&gt;automated and manual testing&lt;/a&gt;, and receiving updates&amp;#160;in real-time offers you the ability to make important decisions once issues arise. Regardless of how many tests, sprints and projects your company is running, all of them should be conveniently viewed from a lone interface, enabling a single source of truth that keeps your mobile app development initiatives on track.&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 01:00:00 -0400</pubDate><guid>tag:www.ciandcd.com,2015-06-21:how-to-make-sure-your-mobile-app-is-secure.html</guid></item><item><title>Ode to a Workstation</title><link>http://www.ciandcd.com/ode-to-a-workstation.html</link><description>from:http://java.dzone.com/articles/ode-workstation&lt;br&gt;&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;p&gt;Every now and then I get work done in the home office.&amp;#160; I&amp;#8217;ve written previously about my setup, but after churning out some solution design today, I sat back and really took some time to appreciate the workspace.&amp;#160; I&amp;#8217;m really pleased with the configuration, it&amp;#8217;s probably the best setup I&amp;#8217;ve had in years.&lt;/p&gt;

&lt;p align="center"&gt;&lt;a title="Desktop" href="http://sanderstechnology.com/wp-content/uploads/2015/06/Desktop.jpg" class="grouped_elements" rel="tc-fancybox-group13773"&gt;&lt;img title="Desktop" alt="Desktop" src="http://sanderstechnology.com/wp-content/uploads/2015/06/Desktop_thumb.jpg" border="0" height="443" width="589"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The desk is a former QLD police desk from the 1940s, so it wasn&amp;#8217;t built for modern computers &amp;#8211; not a problem, the cables run down the back which is just a minor annoyance.&amp;#160; The keyboard and mouse are gaming varieties so that they perform well &amp;#8211; the old Sennheiser (RF) wireless headset has been with me since 2006 and still works very well.&lt;/p&gt;

&lt;p&gt;The wooden clock (&lt;a href="http://sanderstechnology.com/2015/review-olixar-qi-tone-alarm-clock-bluetooth-qi-charging-speaker/13633/" target="_blank"&gt;recently reviewed&lt;/a&gt;) acts as external speakers, a Bluetooth receiver and has a built in microphone so it can be used as a hands-free option for conference calls.&amp;#160; It also features Qi wireless charging capability and also features a thermostat.&lt;/p&gt;

&lt;p&gt;Under the second monitor is a HDD caddy which supports USB3, and features 4 bays which can be used in parallel.&amp;#160; I try to keep the desk reasonably neat, and there&amp;#8217;s plenty of space so it doesn&amp;#8217;t get too cluttered.&amp;#160; I have a nice view out the window to a small courtyard which gets early morning sun.&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 01:00:00 -0400</pubDate><guid>tag:www.ciandcd.com,2015-06-21:ode-to-a-workstation.html</guid></item><item><title>The Coming Donkey Apocalypse: Devops Days Austin</title><link>http://www.ciandcd.com/the-coming-donkey-apocalypse-devops-days-austin.html</link><description>from:http://java.dzone.com/articles/coming-donkey-apocalypse&lt;br&gt;&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;p&gt;After a bit of a gap I&amp;#8217;m continuing the my series from&amp;#160;&lt;a href="http://www.devopsdays.org/events/2015-austin/program/" target="_blank"&gt;DevOps Days Austin&lt;/a&gt;. &amp;#160;After Damon Edwards &lt;a href="http://bartongeorge.net/2015/05/21/opening-keynote-devops-days-austin-2/" target="_blank"&gt;kicked off the event&lt;/a&gt;, Michael Cote of Pivotal took the stage. &amp;#160;Cote presented &amp;#8220;The coming donkey apocalypse&amp;#160;&amp;#8212;&amp;#160;what happens when Devops goes mainstream.&amp;#8221;&lt;/p&gt;

&lt;p&gt;Take a listen (you can find his slides below):&lt;/p&gt;&lt;p align="center"&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Some of the ground that Cote covers:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;What DevOps as a community needs to focus on next to expand&lt;/li&gt;
	&lt;li&gt;Unicorns (eg Uber and Netflix), Horses (eg top banks) and Donkeys (mainstream organizations)&lt;/li&gt;
	&lt;li&gt;3 key areas of DevOps to focus on today 
&lt;ol&gt;
	&lt;li&gt;Culture and process&lt;/li&gt;
	&lt;li&gt;Supporting legacy code&lt;/li&gt;
	&lt;li&gt;Tools and technology&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p align="center"&gt;&lt;strong&gt;  &lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Interviews on tap:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Cameron Haight &amp;#8211; Gartner&lt;/li&gt;
	&lt;li&gt;John Willis &amp;#8211; Docker&lt;/li&gt;
	&lt;li&gt;Paul Read &amp;#8211;&amp;#160;Release Engineering Approaches&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Extra Credit reading&lt;/strong&gt;&lt;/p&gt;

 
&lt;p&gt;Pau for now&amp;#8230;&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 01:00:00 -0400</pubDate><guid>tag:www.ciandcd.com,2015-06-21:the-coming-donkey-apocalypse-devops-days-austin.html</guid></item><item><title>Ensure Software Security by Understanding the Attack Surface</title><link>http://www.ciandcd.com/ensure-software-security-by-understanding-the-attack-surface.html</link><description>from:http://java.dzone.com/articles/ensure-software-security&lt;br&gt;&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;p&gt;For many organizations, it seems like cyberattacks can come from anywhere, at any time. This sense is heightened by the number&amp;#160;of endpoints in play that could be vulnerable to threats. Quality assurance teams must ensure that they &lt;a href="https://www.getzephyr.com/insights/top-5-security-threats-software-testers-need-look"&gt;have the data on hand&lt;/a&gt; to keep these risks at bay. By gathering information on current dangers, companies can better understand the attack surface and establish safeguards.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Breaking down elements in play&lt;/strong&gt;&lt;br&gt;
 The attack surface contains all possible vulnerabilities - known and unknown - that may exist across your infrastructure, and sums up your risk of exposure. While the attack surface may seem like one big scary entity, it's actually made up of several parts. Tripwire broke considerations down into&amp;#160;&lt;a href="http://www.tripwire.com/state-of-security/featured/understanding-constitutes-attack-surface-2/" target="_blank"&gt;software, network and human attack surfaces&lt;/a&gt;&amp;#160;to make this large picture easier to manage. QA professionals should approach the attack surface this way in order to ensure that all aspects are accommodated for rather than being overwhelmed by the big picture. Everything from coding to devices and human error must be considered when gathering information and preparing for potential threats.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Analyze data and act on it&lt;/strong&gt;&lt;br&gt;
 Testing results can be a critical indicator of what types of vulnerabilities may be present within a program. The Open Web Application Security Project&amp;#160;noted that an attack surface analysis will help QA and developers&amp;#160;&lt;a href="https://www.owasp.org/index.php/Attack_Surface_Analysis_Cheat_Sheet" target="_blank"&gt;better understand what they're up against&lt;/a&gt;&amp;#160;and build in security accordingly. During this evaluation, they must determine high risk areas of code, what functions should be reviewed for defects and when the attack surface has changed. This last consideration will be especially critical as further tests and adjustments will be needed to secure the software.&lt;/p&gt;

&lt;p&gt;Anything that an organization does could affect the attack surface, which means that it will have to be constantly monitored. QA teams need to ask what's changed, how it's different from before and what potential holes were opened in the process. This will help keep the attack surface visibly mapped out, making it easy to strategize how to protect the business, its employees and customers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reduce the noise&lt;/strong&gt;&lt;br&gt;
 While a breach is certainly possible, that doesn't mean it should be easy for attackers to gain entry into business systems. Organizations can reduce their attack surface by decreasing the amount of noise within their infrastructure. Accuvant pointed out that doing this will&amp;#160;&lt;a href="http://www.accuvant.com/blog/how-to-reduce-attack-surface" target="_blank"&gt;reduce an attack's operating surface&lt;/a&gt;, minimizing the likelihood of malicious access. QA teams can use tactics like configuration management, exploit analysis, patching, sandboxing and secure application development to effectively reduce or eliminate the impact of a vulnerability.&lt;/p&gt;

&lt;p&gt;"Integrating these strategies into your security program make it much harder for exploits to attack your organization's systems," Accuvant stated. "By reducing your adversaries' operating surface, you are effectively limiting their attack surface."&lt;/p&gt;

&lt;p&gt;The threat of a vulnerability is a very real concern for businesses. By gathering information on what types of attacks are becoming prevalent and understanding how they can affect company software, QA teams can prepare for these risks and protect their users from the growing attack surface.&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sat, 20 Jun 2015 01:30:00 -0400</pubDate><guid>tag:www.ciandcd.com,2015-06-20:ensure-software-security-by-understanding-the-attack-surface.html</guid></item><item><title>Reducing Risk Through Security Qa Automation</title><link>http://www.ciandcd.com/reducing-risk-through-security-qa-automation.html</link><description>from:http://java.dzone.com/articles/reducing-risk-through-security&lt;br&gt;&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;p&gt;Organizations are under constant pressure to protect their critical assets from cyberattacks that have plagued a wide variety of industries. However, there is currently no set method of how to ensure that company applications will be safe from these threats. &lt;a href="https://www.getzephyr.com/insights/how-empower-your-quality-assurance-teams"&gt;Quality assurance&lt;/a&gt; teams have implemented a wide range of approaches to ensure security, but manually executing all of these cases can be time-consuming and lead to potential vulnerabilities. For this reason, QA should look into security automation to reduce risks and &lt;a href="http://www.getzephyr.com/products/enterprise-test-management"&gt;improve overall program capabilities&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Have realistic expectations&lt;/strong&gt;&lt;br&gt;
 When building security into the software development life cycle, there are numerous benefits businesses can see, including seamless protection integration and awareness of team members. An AT&amp;amp;T white paper noted that automated vulnerability scanning can be a great first step for QA teams to implement as it can easily and&amp;#160;&lt;a href="http://www.business.att.com/content/whitepaper/Integrated_Security_QC_wp.pdf" target="_blank"&gt;quickly identify commonly occurring issues&lt;/a&gt;. At the same time, however, it's not foolproof, since it cannot detect more sophisticated defects like authentication issues or business logic vulnerabilities.&lt;/p&gt;

&lt;p&gt;That being said, security QA automation can be a major asset to development efforts and can reduce overall risk, but will still require other tools like manual testing to fully evaluate the threat landscape. After the app has been released, automation can often be essential for finding threats, while enabling QA teams to focus on current projects that are still underway. This helps lower the potential risk across the board while still ensuring that each program gets the attention it needs, no matter where it is in its life cycle.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tools for the job&lt;/strong&gt;&lt;br&gt;
 There are a number of resources that QA teams can utilize to test the security of their projects. TechTarget contributor Michael Cobb noted that automated QA verification is often&amp;#160;&lt;a href="http://searchsecurity.techtarget.com/answer/Which-automated-quality-assurance-tools-can-be-used-to-test-software" target="_blank"&gt;executed through code analysis and vulnerability testing&lt;/a&gt;. Both of these assets can quickly find errors that may be easily missed during manual evaluations. This alone helps significantly reduce risks to app functionality and security capabilities while ensuring that QA teams are eliminating common vulnerabilities. These tools paired with human testers can effectively find issues and better protect their projects for the future.&lt;/p&gt;

&lt;p&gt;"Despite advances in computer automation, humans are still superior at ensuring applications are developed securely, probably because the best challenge is posed by humans, notably those who can think as an attacker would," Cobb wrote. "However, human work is often more effective if a framework guides it."&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relying on QA for better security&lt;/strong&gt;&lt;br&gt;
 Even if QA teams leverage automated tools for security needs, they must still have an understanding of how these tests work and be able to execute them. Chiron Professional Journal noted that while QA professionals may not often be security experts,&amp;#160;&lt;a href="http://www.chiron-solutions.com/chiron-professional-journal/2011/04/06/how-qa-teams-can-find-and-remediate-security-vulnerabilities-earlier-in-the-development-process-reducing-downstream-cost-and-corporate-risk/" target="_blank"&gt;having the tools on hand&lt;/a&gt;&amp;#160;can help them perform the necessary processes and mitigate critical risks.&lt;/p&gt;

&lt;p&gt;"Let's be clear here &amp;#8211; we're not expecting a QA analyst to be able to cobble together a complicated script to evade an anti-cross-site scripting library &amp;#8230;&amp;#160;but we should reasonably expect that the analyst can either effectively use a tool, or follow a well-documented process that has varying tests and permutations allowing the analyst to think for themselves and flag questionable results for review by the security experts," the Chiron Professional Journal stated.&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sat, 20 Jun 2015 01:00:00 -0400</pubDate><guid>tag:www.ciandcd.com,2015-06-20:reducing-risk-through-security-qa-automation.html</guid></item><item><title>Why We Need Continuous Integration</title><link>http://www.ciandcd.com/why-we-need-continuous-integration.html</link><description>from:http://java.dzone.com/articles/why-we-need-continuous&lt;br&gt;&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;h2&gt;Introduction&lt;/h2&gt;&lt;p&gt;Continuous integration is a practice that helps developers deliver better software in a more reliable and predictable manner.&lt;/p&gt;&lt;p&gt;This article deals with the problems developers face while writing, testing and delivering software to end users. Through exploring continuous integration, we will cover how we can overcome these issues.&lt;/p&gt;&lt;h2&gt;The Problem&lt;/h2&gt;&lt;p&gt;First, we will take a look at the source of the problem, which lies in the software development cycle. Next, we will cover some of the change conflicts that can take place during that process, and finally we will explore the main factors that can make these problems escalate, followed by an explanation of how continuous integration solves these issues.&lt;/p&gt;&lt;h3&gt;The Source of the Problem&lt;/h3&gt;&lt;p&gt;Let's take a look at what a traditional software development cycle looks like. Each developer gets a copy of the code from the central repository. The starting point is usually the latest stable version of the application. All developers begin at the same starting point, and work on adding a new feature or fixing a bug.&lt;/p&gt;&lt;p&gt;Each developer makes progress by working on their own or in a team. They add or change classes, methods and functions, shaping the code to meet their needs, and eventually they complete the task they were assigned to do.&lt;/p&gt;&lt;p&gt;Meanwhile, the other developers and teams continue working on their own tasks, changing the code or adding new code, solving the problems they have been assigned.&lt;/p&gt;&lt;p&gt;If we take a step back and look at the big picture, i.e. the entire project, we can see that all developers working on a project are changing the context for the other developers as they are working on the source code.&lt;/p&gt;&lt;p&gt;As teams finish their tasks, they copy their code to the central repository. There are two scenarios that can take place at this point.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;The code in the central repository is unchanged&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The code is the same as the initial copy. If this is the case, things are simple, because the system is unchanged. All the ideas we had about the system still stand.&lt;/p&gt;&lt;p&gt;This is always the case if you are the only developer working on the application and if you have finished your work before the other members of your team. Either way, things are looking good for you. The system you have created and tested can be delivered to users without additional changes.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;The code in the central repository has changed&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The second scenario is that the application you have been working on has changed, and you discover this at the point when you try to copy your code over to the central repository. Changes in the code may or may not be in conflict with the ones you've made.&lt;/p&gt;&lt;p&gt;If there are conflicts, you need to resolve them in order to be able to successfully deliver your code to the users. In this case, things could get complicated.&lt;/p&gt;&lt;p&gt;Next, we'll explore the types of conflicts that can happen and what you may need to do to resolve them.&lt;/p&gt;&lt;h3&gt;Change Conflicts&lt;/h3&gt;&lt;p&gt;There are several types of change conflicts that can occur when integrating code. Here are some of the most common ones. We'll start with the simplest scenarios, and gradually explore the more complex ones.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;The implementation details have changed&lt;/strong&gt;&amp;#160;- You refactored a method, but so did the developer that has already integrated their code into the central repository. The behavior of the method is the same in all three implementations. You will need to pick the version that will stay, and remove the other implementations. You can even come up with a fourth implementation. This is a simple type of conflict, which you can usually resolve within a few minutes.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;The APIs you have been relying on have changed&lt;/strong&gt;&amp;#160;- For instance, the behavior of a certain method has changed. This could affect your code in a number of ways &amp;#8212; from minor changes that you might need to make, to major structural changes. There is no silver bullet in such cases. You will need to carefully study the changes and make all the fixes.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;An entire subsystem of the application behaves in a different way&lt;/strong&gt;&amp;#160;- in such cases you will almost certainly be facing a partial, if not a full rewrite of your solution. If this is the case, you will probably need to speak with all the developers working on the application, because such a significant change should not happen without letting the rest of the team know about it.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;These and a number of other issues could come up, caused by various factors. Different versions of frameworks, libraries, databases are another potential source of conflicts.&lt;/p&gt;&lt;p&gt;Once you have updated your code so it can be compiled or interpreted, you also need to remember to repeat all the tests that you have previously ran.&lt;/p&gt;&lt;p&gt;These examples show that the amount of work needed to solve a problem that was initially assigned to a developer can easily double.&lt;/p&gt;&lt;p&gt;&lt;img src="https://d2l3jyjp24noqc.cloudfront.net/uploads/image/img/21/integration-traditional.png" alt="Traditional Integration" height="417" width="563"&gt;&lt;/p&gt;&lt;h3&gt;Escalating Factors&lt;/h3&gt;&lt;p&gt;Here are some of the main factors that can make these problems escalate.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;The size of the team working on the project.&lt;/strong&gt;&amp;#160;The number of changes that are being pushed back into the main repository is proportional to the number of people on the project. This makes the process of integrating code into the main repository significantly harder.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;The amount of time passed since the developer got the latest version of the code from the central repository.&lt;/strong&gt;&amp;#160;As time passes, other people working on the same project are integrating more and more of their work, and changing the context in which your code needs to run. Sometimes the changes in the main repository are so big that it's easier to do a complete rewrite of your solution.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;A large number of changes in the system make integration events more complex and can have a huge effect on the productivity of the team. Such situations are even referred to as "integration hell".&lt;/p&gt;&lt;p&gt;This process has a number of other negative consequences for your business. Testing and fixing bugs can take forever. Your releases are running late. Teams are stressed out because of long and unpredictable release cycles, and morale deteriorates.&lt;/p&gt;&lt;h2&gt;Solution: Integrate Continuously&lt;/h2&gt;&lt;p&gt;The solution to the problem of managing a large number of changes in big integration events is conceptually simple. We need to split these big integration events into much smaller integration events. This way, developers need to deal with a much smaller number of changes, which are easier to understand and manage. To keep integration events small and easily manageable, we need them to happen often. A couple of times a day is ideal. The practice of doing small integrations often is called&amp;#160;&lt;strong&gt;Continuous Integration&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;The idea is simple, but at the same time it often appears to be impossible to implement in practice. This is because changing the process requires us to change some of our own habits, and changing habits is difficult.&lt;/p&gt;&lt;p&gt;&lt;img src="https://d2l3jyjp24noqc.cloudfront.net/uploads/image/img/22/integration-continuous.png" alt="Continuous Integration" height="417" width="563"&gt;&lt;/p&gt;&lt;h3&gt;The Practice of Continuous Integration&lt;/h3&gt;&lt;p&gt;In order to avoid the previously described issues, developers need to integrate their partially complete work back into the main repository on a daily basis, or even a couple of times a day. To accomplish this, they first need to pull in all the changes added to the main repository while they were working on the code. They also must make sure that their code will work once it is integrated into the main repository. The only way to ensure this is to test every feature of the application.&lt;/p&gt;&lt;p&gt;What first comes into mind when we start considering continuous integration is that the developers would need to spend half of their time every day testing the code in order not to break the code in the main repository for everyone else.&lt;/p&gt;&lt;p&gt;This is why the prerequisite for continuous integration is having an automated test suite. Automated tests take away the burden of the manual, repetitive, and error-prone testing process from the developers. They also make the entire testing process much quicker. A computer can replace hours of manual testing with just minutes of automated testing.&amp;#160;&lt;a href="https://semaphoreci.com/community/tutorials/behavior-driven-development"&gt;Behavior-driven&lt;/a&gt;&amp;#160;and test-driven development are techniques that help developers write clean, maintainable code while writing tests at the same time. Testing techniques are out of the scope of this article, and you can read more about them in&amp;#160;&lt;a href="https://semaphoreci.com/community/tutorials"&gt;other articles on Semaphore Community&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Tests make sense only if they are executed every time the source code changes, without exception. A continuous integration service&amp;#160;such as Semaphore CI is a tool which can automate this process by monitoring the central code repository and running tests on every change in the source code. Apart from running tests, they also collect test results and communicate those results to the entire team working on the project.&lt;/p&gt;&lt;p&gt;The result of continuous integration is so important that many teams have a rule to stop working on their current task if the version in the central repository is broken. They join the team which is working on fixing the code until tests are passing again. The role of a continuous integration service is to improve the communication between developers by communicating the status of a project's source code.&lt;/p&gt;&lt;h2&gt;How to Adopt Continuous Integration&lt;/h2&gt;&lt;p&gt;Continuous integration as a practice makes a big contribution to improving the development process, but also calls for essential changes in the everyday development routine. Adopting it comes with challenges that are easy to overcome if the process is introduced gradually.&lt;/p&gt;&lt;p&gt;One of the biggest challenges teams face is the lack of an automated testing suite. A good recipe for overcoming this situation is to start adding automated tests for all new features as they are being developed. At the same time, the developer working on a bug fix should also work to cover the related code with tests. Whenever a bug is reported, the team should first write a failing test to demonstrate the existence of bug. Once the fix is created, the tests should pass.&lt;/p&gt;&lt;p&gt;Over time, the automated tests suite gradually becomes more comprehensive, and the developers begin relying on it more and more. Adopting a continuous integration service to communicate the status of the tests to the entire team in the early stages of a project is also important, because it raises awareness of the project status among team members.&lt;/p&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;Introducing continuous integration and automated testing into the development process changes the way software is developed from the ground up. It requires effort from all team members, and a cultural shift in the organization.&lt;/p&gt;&lt;p&gt;Big changes in the workflow are not easy to pull off quickly. Changes have to be introduced gradually, and all team members and stakeholders need to be on board with the idea. Educating team members about the practice of continuous integration practice and building the automated tests suite needs to be done systematically. Once the first steps have been taken, the process usually continues on its own, as both developers and stakeholders begin seeing the benefits of automated testing suites and the peace of mind that this practice brings to the entire team.&lt;/p&gt;&lt;p&gt;Article originally posted on&amp;#160;&lt;a href="https://semaphoreci.com/community/tutorials"&gt;the Semaphore Community&lt;/a&gt;.&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sat, 20 Jun 2015 01:00:00 -0400</pubDate><guid>tag:www.ciandcd.com,2015-06-20:why-we-need-continuous-integration.html</guid></item><item><title>How to Monitor a Java EE DataSource</title><link>http://www.ciandcd.com/how-to-monitor-a-java-ee-datasource.html</link><description>from:http://java.dzone.com/articles/how-monitor-java-ee-datasource&lt;br&gt;&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;h3&gt;Introduction&lt;/h3&gt;&lt;p&gt;&lt;a href="https://github.com/vladmihalcea/flexy-pool"&gt;FlexyPool&lt;/a&gt;&amp;#160;is an open-source framework that can monitor a&amp;#160;&lt;a href="http://docs.oracle.com/javase/7/docs/api/javax/sql/DataSource.html"&gt;DataSource&lt;/a&gt;connection usage. This tool come out of necessity, since we previously lacked support for provisioning connection pools.&lt;/p&gt;&lt;p&gt;FlexyPool was initially designed for stand-alone environments and the&lt;a href="http://docs.oracle.com/javase/7/docs/api/javax/sql/DataSource.html"&gt;DataSource&lt;/a&gt;&amp;#160;proxy configuration was done programmatically. Using&amp;#160;&lt;a href="http://vladmihalcea.com/2013/12/15/why-i-like-spring-bean-aliasing/"&gt;Spring bean aliases&lt;/a&gt;, we could even substitute an already configured DataSource with the FlexyPool Metrics-aware proxy alternative.&lt;/p&gt;&lt;h3&gt;Java EE support&lt;/h3&gt;&lt;p&gt;Recently, I&amp;#8217;ve been asked about supporting Java EE environments and in the true open-source spirit, I accepted the challenge. Supporting a managed environment is tricky because the DataSource is totally decoupled from the application-logic and made available through a&amp;#160;&lt;a href="https://en.wikipedia.org/wiki/Java_Naming_and_Directory_Interface"&gt;JNDI&lt;/a&gt;&amp;#160;lookup.&lt;/p&gt;&lt;p&gt;One drawback is that we can&amp;#8217;t use automatic pool sizing strategies, since most Application Servers return a custom DataSource implementation (which is closely integrated with their in-house JTA transaction manager solution), that doesn&amp;#8217;t offer access to reading/writing the connection pool size.&lt;/p&gt;&lt;p&gt;While the DataSource might not be adjustable, we can at least monitor the connection usage and that&amp;#8217;s enough reason to support Java EE environments too.&lt;/p&gt;&lt;h3&gt;Adding declarative configuration&lt;/h3&gt;&lt;p&gt;Because we operate in a managed environment, we can no longer configure the DataSource programmatically, so we need to use the declarative configuration support.&lt;/p&gt;&lt;p&gt;By default, FlexyPool looks for the&amp;#160;flexy-pool.properties&amp;#160;file in the current Class-path. The location can be customized using the&amp;#160;flexy.pool.properties.pathSystem property , which can be a:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;URL (e.g.&amp;#160;file:/D:/wrk/vladmihalcea/flexy-pool/flexy-pool-core/target/test-classes/flexy-pool.properties)&lt;/li&gt;&lt;li&gt;File system path (e.g.&amp;#160;D:\wrk\vladmihalcea\flexy-pool\flexy-pool-core\target\test-classes\flexy-pool.properties)&lt;/li&gt;&lt;li&gt;Class-path nested path (e.g.&amp;#160;nested/fp.properties)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The properties file may contain the following configuration options:&lt;/p&gt;Parameter nameDescription&lt;p&gt;flexy.pool.data.source.unique.name&lt;/p&gt;&lt;p&gt;Each FlexyPool instance requires a unique name so that JMX domains won&amp;#8217;t clash&lt;/p&gt;&lt;p&gt;flexy.pool.data.source.jndi.name&lt;/p&gt;&lt;p&gt;The JNDI DataSource location&lt;/p&gt;&lt;p&gt;flexy.pool.data.source.jndi.lazy.lookup&lt;/p&gt;&lt;p&gt;Whether to lookup the DataSource lazily (useful when the target DataSource is not available when the FlexyPoolDataSource is instantiated)&lt;/p&gt;&lt;p&gt;flexy.pool.data.source.class.name&lt;/p&gt;&lt;p&gt;The DataSource can be instantiated at Runtime using this Class name&lt;/p&gt;&lt;p&gt;flexy.pool.data.source.property.*&lt;/p&gt;&lt;p&gt;If the DataSource is instantiated at Runtime, each flexy.pool.data.source.property.${java-bean-property} will set the java-bean-property of the newly instantiated DataSource (e.g. flexy.pool.data.source.property.user=sa)&lt;/p&gt;&lt;p&gt;flexy.pool.adapter.factory&lt;/p&gt;&lt;p&gt;Specifies the PoolAdaptorFactory, in case the DataSource supports dynamic sizing. By default it uses the generic DataSourcePoolAdapter which doesn&amp;#8217;t support auto-scaling&lt;/p&gt;&lt;p&gt;flexy.pool.metrics.factory&lt;/p&gt;&lt;p&gt;Specifies the MetricsFactory used for creating Metrics&lt;/p&gt;&lt;p&gt;flexy.pool.metrics.reporter.log.millis&lt;/p&gt;&lt;p&gt;Specifies the metrics log reported interval&lt;/p&gt;&lt;p&gt;flexy.pool.metrics.reporter.jmx.enable&lt;/p&gt;&lt;p&gt;Specifies if the jmx reporting should be enabled&lt;/p&gt;&lt;p&gt;flexy.pool.metrics.reporter.jmx.auto.start&lt;/p&gt;&lt;p&gt;Specifies if the jmx service should be auto-started (set this to true in Java EE environments)&lt;/p&gt;&lt;p&gt;flexy.pool.strategies.factory.resolver&lt;/p&gt;&lt;p&gt;Specifies a ConnectionAcquiringStrategyFactoryResolver class to be used for obtaining a list of ConnectionAcquiringStrategyFactory objects. This should be set only if the PoolAdaptor supports accessing the DataSource pool size.&lt;/p&gt;&lt;h3&gt;Hibernate ConnectionProvider&lt;/h3&gt;&lt;p&gt;Most Java EE applications already use&amp;#160;&lt;a href="https://en.wikipedia.org/wiki/Java_Persistence_API"&gt;JPA&lt;/a&gt;&amp;#160;and for those who happen to be using Hibernate, we can make use of the&amp;#160;&lt;a href="https://docs.jboss.org/hibernate/orm/4.3/manual/en-US/html/ch03.html#configuration-jdbc-properties"&gt;hibernate.connection.provider_class&lt;/a&gt;configuration property for injecting our proxy DataSource.&lt;/p&gt;&lt;p&gt;Hibernate provides many built-in extension points and the connection management is totally configurable. By providing a custom&lt;a href="https://docs.jboss.org/hibernate/orm/4.3/javadocs/org/hibernate/engine/jdbc/connections/spi/ConnectionProvider.html"&gt;ConnectionProvider&lt;/a&gt;&amp;#160;we can substitute the original DataSource with the FlexyPool proxy.&lt;/p&gt;&lt;p&gt;All we have to do is adding the following property to our&amp;#160;persistence.xml&amp;#160;file:&lt;/p&gt;&lt;pre class="xml"&gt;&amp;lt;property name="hibernate.connection.provider_class"
          value="com.vladmihalcea.flexypool.adaptor.FlexyPoolHibernateConnectionProvider"/&amp;gt;&lt;/pre&gt;&lt;p&gt;Behind the scenes, this provider will configure a FlexyPoolDataSource and use it whenever a new connection is requested:&lt;/p&gt;&lt;pre class="java"&gt;private FlexyPoolDataSource&amp;lt;DataSource&amp;gt; flexyPoolDataSource;
 
@Override
public void configure(Map props) {
    super.configure(props);
    LOGGER.debug(
        "Hibernate switched to using FlexyPoolDataSource
    ");
    flexyPoolDataSource = new FlexyPoolDataSource&amp;lt;DataSource&amp;gt;(
        getDataSource()
    );
}
 
@Override
public Connection getConnection() throws SQLException {
    return flexyPoolDataSource.getConnection();
}&lt;/pre&gt;&lt;h3&gt;Instantiating the actual DataSource at runtime&lt;/h3&gt;&lt;p&gt;If you&amp;#8217;re not using Hibernate, you need to have the FlexyPoolDataSource ready before the&amp;#160;&lt;a href="https://docs.oracle.com/javaee/7/api/javax/persistence/EntityManagerFactory.html"&gt;EntityManagerFactory&lt;/a&gt;&amp;#160;finishes bootstrapping:&lt;/p&gt;&lt;pre class="xml"&gt;&amp;lt;?xml version="1.0" encoding="UTF-8"?&amp;gt;
&amp;lt;persistence version="2.0" xmlns="http://java.sun.com/xml/ns/persistence"
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xsi:schemaLocation="
        http://java.sun.com/xml/ns/persistence
        http://java.sun.com/xml/ns/persistence/persistence_2_0.xsd"&amp;gt;
 
    &amp;lt;persistence-unit name="persistenceUnit" transaction-type="JTA"&amp;gt;
 
        &amp;lt;provider&amp;gt;org.hibernate.jpa.HibernatePersistenceProvider&amp;lt;/provider&amp;gt;
 
        &amp;lt;jta-data-source&amp;gt;java:global/jdbc/flexypool&amp;lt;/jta-data-source&amp;gt;
 
        &amp;lt;properties&amp;gt;
            &amp;lt;property
                name="hibernate.hbm2ddl.auto"
                value="update"/&amp;gt;
 
            &amp;lt;property
                name="hibernate.show_sql"
                value="true"/&amp;gt;
 
            &amp;lt;property
                name="hibernate.dialect"
                value="org.hibernate.dialect.HSQLDialect"/&amp;gt;
 
            &amp;lt;property
                name="hibernate.transaction.jta.platform"
                value="org.hibernate.service.jta.platform.internal.SunOneJtaPlatform"/&amp;gt;
        &amp;lt;/properties&amp;gt;
    &amp;lt;/persistence-unit&amp;gt;
&amp;lt;/persistence&amp;gt;&lt;/pre&gt;&lt;p&gt;While in a production Java EE environment we use an Application server specific DataSource configuration, for simplicity sake, I&amp;#8217;m going to configure the FlexyPooldataSource using the&amp;#160;&lt;a href="http://docs.oracle.com/javaee/7/api/javax/annotation/sql/DataSourceDefinition.html"&gt;DataSourceDefinition&lt;/a&gt;&amp;#160;annotation:&lt;/p&gt;&lt;pre class="java"&gt;@DataSourceDefinition(
    name = "java:global/jdbc/flexypool",
    className = "com.vladmihalcea.flexypool.FlexyPoolDataSource")
@Stateless
public class FlexyPoolDataSourceConfiguration {}&lt;/pre&gt;&lt;p&gt;We now need to pass the actual DataSource properties to FlexyPool and this is done through the flexy-pool.properties configuration file:&lt;/p&gt;&lt;pre class="shell"&gt;flexy.pool.data.source.unique.name=unique-name
flexy.pool.data.source.class.name=org.hsqldb.jdbc.JDBCDataSource
flexy.pool.data.source.property.user=sa
flexy.pool.data.source.property.password=
flexy.pool.data.source.property.url=jdbc:hsqldb:mem:test
flexy.pool.metrics.reporter.jmx.auto.start=true&lt;/pre&gt;&lt;p&gt;The actual DataSource is going to be created by the FlexyPoolDataSource on start-up.&lt;/p&gt;&lt;h3&gt;Locating the actual DataSource from JNDI&lt;/h3&gt;&lt;p&gt;If the actual DataSource is already configured by the Application Server, we can instruct FlexyPool to fetch it from JNDI. Let&amp;#8217;s say we have the following DataSource configuration:&lt;/p&gt;&lt;pre class="java"&gt;@DataSourceDefinition(
    name = "java:global/jdbc/default",
    className = "org.hsqldb.jdbc.JDBCDataSource",
    url = "jdbc:hsqldb:mem:test",
    initialPoolSize = 3,
    maxPoolSize = 5
)
@Stateless
public class DefaultDataSourceConfiguration {}&lt;/pre&gt;&lt;p&gt;To proxy the JNDI DataSource, we need to configure FlexyPool like this:&lt;/p&gt;&lt;pre class="shell"&gt;flexy.pool.data.source.unique.name=unique-name
flexy.pool.data.source.jndi.name=java:global/jdbc/default
flexy.pool.metrics.reporter.jmx.auto.start=true&lt;/pre&gt;&lt;p&gt;The FlexyPoolDataSource is defined alongside the actual DataSource:&lt;/p&gt;&lt;pre class="java"&gt;@DataSourceDefinition(
    name = "java:global/jdbc/flexypool",
    className = "com.vladmihalcea.flexypool.FlexyPoolDataSource")
@Stateless
public class FlexyPoolDataSourceConfiguration {}&lt;/pre&gt;&lt;p&gt;The JPA will have to fetch the FlexyPoolDataSource instead of the actual one:&lt;/p&gt;&lt;pre class="xml"&gt;&amp;lt;jta-data-source&amp;gt;java:global/jdbc/flexypool&amp;lt;/jta-data-source&amp;gt;
&lt;/pre&gt;&lt;p&gt;In&amp;#160;&lt;a href="https://github.com/vladmihalcea/flexy-pool/tree/master/flexy-tomee"&gt;TomEE&lt;/a&gt;, because the DataSourceDefinitions are not lazily instantiated, the actual DataSource might not be available in the JNDI registry when the FlexyPoolDataSource definition is processed.&lt;/p&gt;&lt;p&gt;For this, we need to instruct FlexyPool to dely the JNDI lookup until the DataSource is actually requested:&lt;/p&gt;&lt;pre class="shell"&gt;flexy.pool.data.source.jndi.lazy.lookup=true&lt;/pre&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;The last time I used Java EE was in 2008, on a project that was using Java EE 1.4 with EJB 2.1. After 7 years of using Spring exclusively, I&amp;#8217;m pleasantly surprised by the Java EE experience.&amp;#160;&lt;a href="http://arquillian.org/"&gt;Arquillian&lt;/a&gt;&amp;#160;is definitely my favourite add-on, since integration testing is of paramount importance in enterprise applications.&amp;#160;&lt;a href="http://antoniogoncalves.org/2011/09/25/injection-with-cdi-part-iii/"&gt;CDI&lt;/a&gt;&amp;#160;is both easy and powerful and I&amp;#8217;m glad the dependency injection got standardised.&lt;/p&gt;&lt;p&gt;But the best asset of the Java EE platform is the community itself. Java EE has very strong community, willing to give you a hand when in need. I&amp;#8217;d like to thank&amp;#160;&lt;a href="https://twitter.com/l33tj4v4"&gt;Steve Millidge (Founder of Payara and C2B2)&lt;/a&gt;&amp;#160;for giving me some great tips on designing the FlexyPool Java EE integration,&amp;#160;&lt;a href="https://twitter.com/alexsotob"&gt;Alex Soto&lt;/a&gt;,&amp;#160;&lt;a href="https://twitter.com/agoncal"&gt;Antonio Goncalves&lt;/a&gt;&amp;#160;and all the other Java EE members whom I had some very interesting conversations on Twitter.&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Fri, 19 Jun 2015 02:00:35 -0400</pubDate><guid>tag:www.ciandcd.com,2015-06-19:how-to-monitor-a-java-ee-datasource.html</guid></item><item><title>Better and Fewer Suppliers (2015 Software Supply Chain Report)</title><link>http://www.ciandcd.com/better-and-fewer-suppliers-2015-software-supply-chain-report.html</link><description>from:http://devops.com/2015/06/19/better-fewer-suppliers-2015-software-supply-chain-report/&lt;br&gt;&lt;div&gt;&lt;p&gt;&lt;strong&gt;That&amp;#160;Supplier is Better For You&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Since releasing the 2015 State of the Software Supply Chain Report, there has been a lot of great discussion across the industry on best practices for managing the complexity introduced by the volume and velocity of the components used across your software supply chain.&lt;/p&gt;
&lt;p&gt;Today I want to focus on the huge ecosystem of open source projects (&amp;#8220;suppliers&amp;#8221;) that feed a steady stream of innovative components into our software supply chains. &amp;#160;In the Java ecosystem alone, there are now over &lt;a href="https://search.maven.org/#stats"&gt;108,000 suppliers&lt;/a&gt; of open source components. &amp;#160;Across all component types available to developers (e.g., RubyGems, NuGet, npm, Bower, PyPI, etc.), estimates now reach over &lt;a href="https://www.openhub.net"&gt;650,000 suppliers&lt;/a&gt; of open source projects.&lt;/p&gt;
&lt;p&gt;However, like in traditional manufacturing, not all suppliers deliver parts of comparable quality and integrity. My latest research, the &lt;a href="http://www.sonatype.com/get-it-now/new-research"&gt;2015 State of the Software Supply Chain Report&lt;/a&gt;, shows that some open source projects use restrictive licenses and vulnerable sub-components, while other projects are far more diligent at updating the overall quality of their components. Choosing the best and fewest suppliers can improve the quality and integrity of the applications we deliver &amp;#160;to our customers.&lt;/p&gt;
&lt;p&gt;While I am hosting a &lt;a href="http://go.sonatype.com/ssc-webinar"&gt;webinar&lt;/a&gt; next week to share many of the detailed report findings, I wanted to share a few of the more meaningful stats here.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Your 7,600 Suppliers&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;My research for the report revealed many new perspectives on &amp;#8220;suppliers&amp;#8221; across the software supply chains. &amp;#160;First of all, I saw that the average large development organization consumed over 240,000 open source components last year &amp;#8212; sourced from over 7,600 open source projects.&lt;/p&gt;
&lt;p&gt;On the surface, the huge reliance on open source projects is a great thing. &amp;#160;Development teams have chosen to not write those pieces themselves, but have sourced the needed components from outside suppliers. &amp;#160;This practice speeds development, enables more innovation, and ensures time-to-release goals are &amp;#160;achieved. &amp;#160;The use of open source is so prolific today, few of us could ever imagine reducing the use of those &amp;#160;components and their suppliers in the future.&lt;/p&gt;
&lt;p&gt;At the same time that we benefit from open source, our high paced, high volume consumption practices don&amp;#8217;t allow us the time needed to do the due diligence on the suppliers or open source projects where we source our component parts from.&lt;/p&gt;
&lt;p&gt;For example, of the 240,000 average component downloads in 2014, the same businesses sourced an average of 15,000 components that included known security vulnerabilities. &amp;#160;In many cases, developers were downloading vulnerable component versions, when safer versions of those same components were available from the open source projects. While no one intends to download components with known vulnerabilities, the problem is exacerbated due to the lack the visibility into a better recommended version.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Fewer Suppliers, Less Context Switching&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Choosing an open source project supplier should be considered an important strategic decision in organizations because changing a supplier (&amp;#8220;open source project&amp;#8221;) used &amp;#160;is far more effort than swapping out a specific component. Like traditional suppliers, open source projects have good and bad practices impacting the overall quality of their component parts.&lt;/p&gt;
&lt;p&gt;Traditional manufacturing supply chains intentionally select specific parts from approved suppliers. &amp;#160;They also rely on formalized sourcing and procurement practices. &amp;#160;This practice also focuses the organization on using the best and fewest suppliers &amp;#8212; an effort that improves quality, reduces context switching, and also accelerates mean time to repair when defects are discovered. &amp;#160;One industry example from the report describes how Toyota manages 125 suppliers for their Prius to help sustain competitive advantages over GM who manages over 800 suppliers for the Chevy Volt.&lt;/p&gt;
&lt;p&gt;By contrast, development teams working with software supply chains often rely on an unchecked variety of supply, where each developer or development team can make their own sourcing and procurement decisions. &amp;#160;The effort of managing over 7,600 suppliers introduces a drag on development and is contrary to their need to develop faster as part of agile, continuous delivery and devops practices.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Coming to Terms&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;When you come to terms with the volume of consumption and the massive ecosystem of suppliers you can source your components from, you quickly realize it is impossible to address this issue with a manual review process. &amp;#160;Any organizations clutching to these outdated manual practices are will continue to be outgunned by the velocity by their software supply chains.&lt;/p&gt;
&lt;p&gt;Just as traditional manufacturing supply chains have turned to automation, software development teams need to take the same approach by further automating their software supply chains. &amp;#160;Information about suppliers and the quality of their projects needs to be made available to developers at the time they are selecting components. &amp;#160;Information about the latest versions, features, licenses, known vulnerabilities, popularity of versions being used, and the cadence of new releases should be &amp;#160;made available to developers in an automated way. &amp;#160;Automating the availability of this information about suppliers can lead to better and fewer suppliers being used.&lt;/p&gt;
&lt;p&gt;Be sure to read the full &lt;a href="http://www.sonatype.com/get-it-now/new-research"&gt;2015 State of the Software Supply Chain Report &lt;/a&gt;for more information about open source suppliers and organizations sourcing practices. &amp;#160;The report also highlights current and best practices being used in organizations that are managing their use of suppliers that feed their software supply chains.&lt;/p&gt;
&lt;p&gt;To hear more about the overall report findings and industry best practices, please join me on Wednesday, June 24th (1pm ET) for our &lt;a href="http://go.sonatype.com/ssc-webinar"&gt;webinar&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&amp;#160;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Fri, 19 Jun 2015 00:00:00 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-19:better-and-fewer-suppliers-2015-software-supply-chain-report.html</guid></item><item><title>Internap’s DevOps Culture: PrivateStack + CD = ? [Read On &amp; Draw Your Own Conclusions]</title><link>http://www.ciandcd.com/internaps-devops-culture-privatestack-cd-read-on-draw-your-own-conclusions.html</link><description>from:http://devops.com/2015/06/19/internaps-devops-culture-privatestack-cd-read-draw-conclusions/&lt;br&gt;&lt;div&gt;&lt;p&gt;Engineers at &lt;a href="http://www.internap.com/" target="_blank"&gt;Internap&lt;/a&gt;, a hosting company and public cloud vendor designed a new OpenStack-based cloud platform and development environment. DevOps.com tells the story of Internap&amp;#8217;s DevOps cultural evolution, which grew virally, interwoven with the company&amp;#8217;s development of PrivateStack.&lt;/p&gt;
&lt;p&gt;Challenges in Internap&amp;#8217;s Common Development Environment&lt;/p&gt;
&lt;p&gt;Challenges that slowed Internap&amp;#8217;s cloud development process triggered a hunger for change and for a new / altered development scheme. &amp;#8220;The Internap public cloud product is essentially composed of micro services that make cloud resources available to our customers,&amp;#8221; says Mathieu Mitchell, Senior Software Developer, Internap. Internap engineering teams tested their cloud services simultaneously with each affecting the other and transferring adverse effects to production or pre-production. Internap needed to let the billing team test their services, integrated with a duplicate of the production software and environment, without affecting the virtualization team.&lt;/p&gt;
&lt;p&gt;These challenges were rooted in a commonly shared development environment for all the engineering teams and team members, which introduced tedium for engineers as they worked to build and deploy stable code.&lt;/p&gt;
&lt;p&gt;When sharing a common development environment, each engineer&amp;#8217;s tests depended on environment consistency and reliability. When engineers / developers tested two changes at the same time, there was no way to tell which change introduced a regression. The most reasonable way to address this prior to PrivateStack was to push all changes into the environment and dedicate specific engineers to identify and fix the issues that occurred.&lt;/p&gt;
&lt;p&gt;The challenge with dedicating specific engineers to troubleshoot these regressions was that they had a diminished context to work with when compared to the original developer&amp;#8217;s understanding. The team that introduced a change needed feedback from the environment and to shoulder responsibility for fixing the issue. This would result in a stable codebase and the ability to write more thorough, automated tests.&lt;/p&gt;
&lt;p&gt;&amp;#8220;This is why we created PrivateStack&amp;#8211;to allow us to independently test a single change, integrated with other services, in an environment that is the equivalent of a private production setup,&amp;#8221; says Mitchell. PrivateStack enables development teams to innovate while ensuring that changes behave correctly in production. This in turn enables CD and speeds development.&lt;/p&gt;
&lt;p&gt;DevOps Culture Leads Teams to Success&lt;/p&gt;
&lt;p&gt;As Internap developed PrivateStack, engineering observed a DevOps belief system spreading across the project and the teams. &amp;#8220;At first, it was only a minority among us, mostly people with an Agile background, who advocated the Continuous Delivery approach while we worked on this project. We really believe in delivering value to our customers as quickly as possible. To achieve Continuous Delivery, we needed people to understand that automating everything was the way to go,&amp;#8221; says Mitchell.&lt;/p&gt;
&lt;p&gt;As more engineers realized that they were the solution, they became increasingly motivated to enhance processes, create the new development environment, and use CD to develop the Internap public cloud product.&lt;/p&gt;
&lt;p&gt;Results with PrivateStack&lt;/p&gt;
&lt;p&gt;PrivateStack enables Internap teams to share the same code and system configurations while individual developers have each their own private production environment to test their software without affecting others. &amp;#8220;We heavily leverage virtualization to be able to recreate our environments, since buying racks and racks of hardware to make this available to all of our developers would be cost prohibitive,&amp;#8221; says Mitchell.&lt;/p&gt;
&lt;p&gt;With PrivateStack, Internap isolates production issues during development, keeping R&amp;amp;D efficient and slashing pre-production troubleshooting time. &amp;#8220;PrivateStack also improves our time-to-market, is much less costly, and most importantly, reduces the chance of any issues reaching the customer,&amp;#8221; says Mitchell.&lt;/p&gt;
&lt;p&gt;DevOps Culture Wrap Up&lt;/p&gt;
&lt;p&gt;To finish the PrivateStack project and foster a DevOps culture at the same time, Internap&amp;#8217;s internal core of CD true believers lead by example. Mitchell and his colleagues adhered to consistent principles while hearing out other team members on their concerns. This helped them to encourage the larger engineering department including billing and virtualization teams to adopt a DevOps and CD frame of mind.&lt;/p&gt;
&lt;p&gt;&amp;#8220;We dedicated two people to drive this initiative, along with a few others who believed in the Continuous Delivery approach but were not involved directly,&amp;#8221; says Mitchell. That was enough to get the job done. Now the vast majority of the engineering department is targeting CD.&lt;/p&gt;
&lt;p&gt;&amp;#8220;We are eager to share our PrivateStack platform with the open source community to enable other developers to run production-like environments for development in their day-to-day operations,&amp;#8221; says Mitchell.&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Fri, 19 Jun 2015 00:00:00 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-19:internaps-devops-culture-privatestack-cd-read-on-draw-your-own-conclusions.html</guid></item><item><title>Two paths to metal devops: cloud-like API driven &amp; cluster building</title><link>http://www.ciandcd.com/two-paths-to-metal-devops-cloud-like-api-driven-cluster-building.html</link><description>from:http://devops.com/2015/06/19/two-paths-metal-devops-cloud-like-api-driven-cluster-building/&lt;br&gt;&lt;div&gt;&lt;p&gt;I&amp;#8217;ve been seeing a rising interest in &lt;a href="http://devops.com/2015/03/06/vms-vs-containers-vms-becoming-el-caminos/"&gt;metal DevOps&lt;/a&gt;&amp;#160;fueled by containers and scale-out data center platforms (like Hadoop, Ceph &amp;amp; OpenStack) that run at the metal level. While I see this is a growing general trend (&lt;a href="http://packet.net"&gt;Packet&lt;/a&gt;, &lt;a href="http://www.internap.com/bare-metal/bare-metal-cloud/"&gt;Internap&lt;/a&gt;&lt;a href="https://wiki.openstack.org/wiki/Ironic"&gt;,&lt;/a&gt;&amp;#160;&lt;a href="http://www.rackspace.com/en-us/cloud/servers/onmetal"&gt;RackSpace&lt;/a&gt;,&amp;#160;&lt;a href="https://wiki.openstack.org/wiki/Ironic"&gt;OpenStack Ironic&lt;/a&gt;, &lt;a href="https://maas.ubuntu.com/"&gt;MaaS&lt;/a&gt;), I&amp;#8217;m going to stay firmly within &lt;a href="http://rackn.com"&gt;my wheelhouse&lt;/a&gt; and use &lt;a href="http://github.com/opencrowbar/core"&gt;OpenCrowbar&lt;/a&gt; as my reference here.&lt;/p&gt;
&lt;p&gt;Building on the API-driven metal features of OpenCrowbar, this has translated into two paths for workloads to run on metal:&lt;/p&gt;
&lt;p&gt;1) &lt;b&gt;&amp;#8220;Cloudify&amp;#8221; the metal using APIs&lt;/b&gt; from tools like &lt;a href="https://github.com/chef/chef-provisioning"&gt;Chef Provision&lt;/a&gt;, &lt;a href="http://docs.saltstack.com/en/latest/topics/cloud/install/index.html"&gt;SaltStack Libcloud&lt;/a&gt;, &lt;a href="https://github.com/docker/machine"&gt;Docker Machine&lt;/a&gt;, &lt;a href="https://github.com/cloudfoundry/bosh"&gt;Cloud Foundry BOSH&lt;/a&gt;. These tools have clients that target cloud APIs like OpenStack and Amazon. These same clients work against cloud are easily ported to Crowbar&amp;#8217;s APIs. &amp;#160;Five years ago, conventional wisdom was that we&amp;#8217;d need a universal cloud API; however, practice has shown it&amp;#8217;s not very difficult to wrap APIs in a way that does not reduce every cloud to a least common denominator.&lt;/p&gt;
&lt;p&gt;2) &lt;b&gt;DevOps deploy the workload&lt;/b&gt; using hand-offs to tools like Chef, Saltstack, Puppet or Ansible. This approach leverages the community scripts (Cookbooks, Modules, Playbooks) for the workload with the critical ability to create a tuned environment and inject the needed parameters directly into the scripts. &amp;#160;A critical lesson we learned going from Crowbar v1 to v2 was for our scripts to have crisp attribute input/output boundary to avoid embedding environmental knowledge into the code.&lt;/p&gt;
&lt;p&gt;While I&amp;#8217;m casting this in Crowbar terms, I see this approach to metal as coming into the market by force fuels by a desire for containers-on-metal and devops-on-metal.&lt;/p&gt;
&lt;h2&gt;Let&amp;#8217;s look at some of the unique and shared use-cases for each approach:&lt;/h2&gt;




&lt;p&gt;&lt;strong&gt;Metal API&lt;/strong&gt;&lt;/p&gt;

&lt;strong&gt;Both&lt;/strong&gt;

&lt;p&gt;&lt;strong&gt;Metal Cluster&lt;/strong&gt;&lt;/p&gt;




&lt;ul&gt;
&lt;li&gt;Easy Cloud to Metal Migration&lt;/li&gt;
&lt;li&gt;Minimal Tool Customization&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
&lt;li&gt;Portability of DevOps Scripts&lt;/li&gt;
&lt;li&gt;Take advantage of power cycling&lt;/li&gt;
&lt;li&gt;Enables constant refresh cycles&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
&lt;li&gt;Leverage Hardware features&lt;/li&gt;
&lt;li&gt;Advanced Network topologies&lt;/li&gt;
&lt;/ul&gt;




&lt;p&gt;In either case, you have to handle bespoke (hipster word for custom) steps in the provisioning flow that are unique to the your operational needs. Our experience is that each site (even each server!) is unique in some incremental way. &amp;#160;For example, one site may require teamed networks with VLANs while another requires flat networks with an SDN layer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;These differences are not mistakes or errors&lt;/strong&gt;: the reality of physical ops and individual operational choices mean that there are a lot of valid configurations. Rather than attempt the Sisyphean task of enforced conformity, we work to abstract differences so that they can be ignored when they are not material.&lt;/p&gt;
&lt;p&gt;In the end, the choices are &lt;b&gt;not &lt;/b&gt;mutually exclusive. Metal APIs are often faster but harder to optimize. You can use them to get started quickly and then invest time to optimize a cluster for long term operations. &lt;b&gt;The underlying physical orchestration can support both.&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Are you looking at getting closer to metal? &amp;#160;Which of the options above makes the most sense to you? &amp;#160;I&amp;#8217;d love to hear about your use-cases, architecture and configuration requirements.&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Fri, 19 Jun 2015 00:00:00 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-19:two-paths-to-metal-devops-cloud-like-api-driven-cluster-building.html</guid></item><item><title>Does DevOps Reduce Technical Debt – or Make it Worse?</title><link>http://www.ciandcd.com/does-devops-reduce-technical-debt-or-make-it-worse.html</link><description>from:http://java.dzone.com/articles/does-devops-reduce-technical&lt;br&gt;&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;p&gt;DevOps can help&amp;#160;&lt;a href="http://www.informationweek.com/strategic-cio/how-devops-can-cut-innovation-crushing-technical-debt/a/d-id/1318677"&gt;reduce technical debt&lt;/a&gt;&amp;#160;in some fundamental ways.&lt;/p&gt;&lt;h2&gt;Continuous Delivery/Deployment&lt;/h2&gt;&lt;p&gt;First, building a&amp;#160;&lt;a href="http://devops.com/2014/07/29/continuous-delivery-pipeline/"&gt;Continuous Delivery/Deployment pipeline&lt;/a&gt;, automating the work of migration and deployment, will force you to clean up inconsistencies and holes in configuration and code deployment, and inconsistencies between development, test and production environments.&lt;/p&gt;&lt;p&gt;And automated Continuous Delivery and&amp;#160;&lt;a href="http://shop.oreilly.com/product/0636920039297.do"&gt;Infrastructure as Code&lt;/a&gt;&amp;#160;gets rid of dangerous one-of-a-kind&lt;a href="http://martinfowler.com/bliki/SnowflakeServer.html"&gt;snowflakes&lt;/a&gt;&amp;#160;and&amp;#160;&lt;a href="http://java.dzone.com/articles/configuration-drift"&gt;configuration drift&lt;/a&gt;&amp;#160;caused by making configuration changes and applying patches manually over time. Which makes systems easier to setup and manage, and reduces the risk of an un-patched system becoming the target of a security attack or the&amp;#160;&lt;a href="https://news.ycombinator.com/item?id=7652036"&gt;cause of an operational problem&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;A CD pipeline also makes it easier, cheaper and faster to pay down other kinds of technical debt. With Continuous Delivery/Deployment, you can test and push out patches and refactoring changes and platform upgrades faster and with more confidence.&lt;/p&gt;&lt;h2&gt;Positive Feedback&lt;/h2&gt;&lt;p&gt;The Lean feedback cycle and&amp;#160;&lt;a href="http://leankit.com/blog/2015/03/3-reasons-it-ops-uses-lean-flow-kanban-for-devops-part-2-of-3/"&gt;Just-in-Time prioritization&lt;/a&gt;&amp;#160;in DevOps ensures that you&amp;#8217;re working on whatever is most important to the business. This means that bugs and usability issues and security vulnerabilities don&amp;#8217;t have to wait until after the next feature release to get fixed. Instead, problems that impact operations or the users will get fixed immediately.&lt;/p&gt;&lt;p&gt;Teams that do&amp;#160;&lt;a href="https://codeascraft.com/2012/05/22/blameless-postmortems/"&gt;Blameless Post-Mortems&lt;/a&gt;&amp;#160;and&amp;#160;&lt;a href="http://www.kitchensoap.com/2014/11/14/the-infinite-hows-or-the-dangers-of-the-five-whys/"&gt;Root Cause(s) Analysis&lt;/a&gt;&amp;#160;when problems come up will go even further, and fix problems at the source and improve in fundamental and important ways.&lt;/p&gt;&lt;p&gt;But there&amp;#8217;s a negative side to DevOps that can add to technical debt costs.&lt;/p&gt;&lt;h2&gt;Erosive Change&lt;/h2&gt;&lt;p&gt;Michael Feathers&amp;#8217; research has shown that constant,&amp;#160;&lt;a href="http://swreflections.blogspot.ca/2012/10/bad-things-happen-to-good-code.html"&gt;iterative change is erosive&lt;/a&gt;: the same code gets changed over and over, the same classes and methods become bloated (because it is naturally easier to add code to an existing method or a method to an existing class), structure breaks down and the design is eventually lost.&lt;/p&gt;&lt;p&gt;DevOps can make this even worse.&lt;/p&gt;&lt;p&gt;DevOps and Continuous Delivery/Deployment involves pushing out lots of small changes, running experiments and iteratively tuning features and the user experience based on continuous feedback from production use.&lt;/p&gt;&lt;p&gt;Many DevOps teams work directly on the code mainline, &amp;#8220;&lt;a href="http://theagileadmin.com/2010/06/24/velocity-2010-always-ship-trunk/"&gt;branching in code&lt;/a&gt;&amp;#8221; to &amp;#8220;&lt;a href="http://agiletesting.blogspot.ca/2009/07/dark-launching-and-other-lessons-from.html"&gt;dark launch&lt;/a&gt;&amp;#8221; code changes, while code is still being developed, using conditional logic and flags to skip over sections of code at run-time. This can make the code hard to understand, and potentially dangerous: if a&amp;#160;&lt;a href="http://martinfowler.com/bliki/FeatureToggle.html"&gt;feature toggle&lt;/a&gt;&amp;#160;is turned on before the code is ready,&lt;a href="http://web.archive.org/web/20110721063430/http://jamesmckay.net/2011/07/why-does-martin-fowler-not-understand-feature-branches/"&gt;&amp;#160;bad things can happen&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Feature flags are also used to run A/B experiments and control risk on release, by rolling out a change incrementally to a few users to start. But the longer that feature flags are left in the code, the&lt;a href="http://swreflections.blogspot.ca/2014/08/feature-toggles-are-one-of-worst-kinds.html"&gt;harder it is to understand and change&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;There is a lot of housekeeping that needs to be done in DevOps: upgrading the CD pipeline and making sure that all of the tests are working; maintaining Puppet or Chef (or whatever configuration management tool you are using) recipes; disciplined,&amp;#160;&lt;a href="http://swreflections.blogspot.com/2012/04/what-refactoring-is-and-what-it-isnt.html"&gt;day-to-day refactoring&lt;/a&gt;; keeping track of features and options and cleaning them up when they are no longer needed, getting rid of dead code and trying to keep the code as simple as possible.&lt;/p&gt;&lt;h2&gt;Microservices and Technology Choices&lt;/h2&gt;&lt;p&gt;&lt;a href="http://martinfowler.com/articles/microservices.html"&gt;Microservices&amp;#160;&lt;/a&gt;are a&amp;#160;&lt;a href="http://www.infoq.com/news/2014/08/microservices-monoliths"&gt;popular architectural approach for DevOps&lt;/a&gt;&amp;#160;teams.&lt;/p&gt;&lt;p&gt;This is because loosely-coupled Microservices are easier for individual teams to independently deploy, change,&amp;#160;&lt;a href="http://jimplush.com/talk/2015/02/28/microservices-allow-for-localized-tech-debt/"&gt;refactor or even replace&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;And a Microservices-based approach provides developers with more freedom when deciding on language or technology stack: teams don&amp;#8217;t necessarily have to work the same way, they can choose the right tool for the job, as long as they support an API contract for the rest of the system.&lt;/p&gt;&lt;p&gt;In the short term there are obvious advantages to giving teams more freedom in making technology choices. They can deliver code faster, quickly try out prototypes, and teams get a chance to experiment and learn about different technologies and languages.&lt;/p&gt;&lt;p&gt;But Microservices &amp;#8220;&lt;a href="http://highscalability.com/blog/2014/4/8/microservices-not-a-free-lunch.html"&gt;are not a free lunch&lt;/a&gt;&amp;#8221;. As you add more services, system testing costs and complexity increase. Debugging and problem solving gets harder. And as more teams choose different languages and frameworks, it&amp;#8217;s harder to track vulnerabilities, harder to operate, and harder for people to switch between teams. Code gets duplicated because teams want to minimize coupling and it is difficult or impossible to share libraries in a polyglot environment. Data is often duplicated between services for the same reason, and data inconsistencies creep in over time.&lt;/p&gt;&lt;h2&gt;Negative Feedback&lt;/h2&gt;&lt;p&gt;There is a potentially negative side to the Lean delivery feedback cycle too.&lt;/p&gt;&lt;p&gt;Constantly responding to production feedback, always working on what&amp;#8217;s most immediately important to the organization, doesn&amp;#8217;t leave much space or time to consider bigger, longer-term technical issues, and to work on paying off deeper architectural and technical design debt that result from poor early decisions or incorrect assumptions.&lt;/p&gt;&lt;p&gt;Smaller, more immediate problems get fixed fast in DevOps. Bugs that matter to operations and the users can get fixed right away instead of waiting until all the features are done, and patches and upgrades to the run-time can be pushed out more often. Which means that you can pay off a lot of debt before costs start to compound.&lt;/p&gt;&lt;p&gt;But behind-the-scenes, strategic debt will continue to add up. Nothing&amp;#8217;s broke, so you don&amp;#8217;t have to fix anything right away. And you can&amp;#8217;t refactor your way out of it either, at least not easily. So you end up living with a poor design or an aging technology platform, slowly slowing down your ability to respond to changes, to come up with new solutions. Or forcing you to continue filling in security holes as they come up, or scrambling to scale as load increases.&lt;/p&gt;&lt;p&gt;DevOps can reduce technical debt. But only if you work in a highly disciplined way. And only if you raise your head up from tactical optimization to deal with bigger, more strategic issues before they become real problems.&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Thu, 18 Jun 2015 10:01:21 -0400</pubDate><guid>tag:www.ciandcd.com,2015-06-18:does-devops-reduce-technical-debt-or-make-it-worse.html</guid></item><item><title>ClusterHQ and DevOps.com survey show Containers poised for mass adoption</title><link>http://www.ciandcd.com/clusterhq-and-devopscom-survey-show-containers-poised-for-mass-adoption.html</link><description>from:http://devops.com/2015/06/18/clusterhq-and-devops-com-survey-show-containers-poised-for-mass-adoption/&lt;br&gt;&lt;div&gt;&lt;p&gt;DevOps.com and ClusterHQ, conducted a survey on Container usage that shows an overwhelming majority of users have either already using, testing or investigating Container usage. With 285 respondents representing a wide range of organizations, it shows that Containers will be part of many production environments in the very near future.&lt;/p&gt;
&lt;p&gt;Currently, only 38 percent of respondents reported using containers in production environments, but that number is projected to increase 69 percent over the next 12 months as organizations find new ways to address important barriers to adoption. It verified that Docker is overwhelmingly the container of choice, with 92% of respondents having used or investigated it, followed by LXC (32%) a distant second, but still far ahead of Rocket (21%). To access the complete survey and report visit&amp;#160;&lt;a href="https://clusterhq.com/assets/pdfs/state-of-container-usage-june-2015.pdf"&gt;https://clusterhq.com/assets/pdfs/state-of-container-usage-june-2015.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Companies ranging in size from small organizations with 1 to 500 employees (69%), to mid-size companies with 501-2,500 personnel (12%), all the way up to large enterprises with over 2,500 employees (19%) are represented in the survey. This demonstrates that containers are being embraced by all businesses from the startup stage to Fortune 500 companies.&lt;/p&gt;
&lt;p&gt;Respondents came predominantly from Development, Operations and DevOps teams. QA and security teams were a smaller share. The survey revealed how container technologies are being used today, as well as research-based insights while providing clues as to where the industry is trending.&lt;/p&gt;
&lt;p&gt;From the ClusterHQ release on the survey:&lt;/p&gt;
&lt;p&gt;The survey also revealed insights into what is perceived to be the primary barriers to container adoption. Security seems to be emerging as a consistent concern throughout the DevOps community in these times of never ending breaches throughout the world:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Security &amp;#8212; 61%&lt;/li&gt;
&lt;li&gt;Data Management &amp;#8212; 53%&lt;/li&gt;
&lt;li&gt;Networking &amp;#8212; 51%&lt;/li&gt;
&lt;li&gt;Skills and Knowledge &amp;#8212; 48%&lt;/li&gt;
&lt;li&gt;Persistent Storage &amp;#8212; 48%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Data Management capabilities also emerged as essential to the success of container strategies and that the vast majority of organizations want to run databases as well as additional services in containers.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When asked to rate how important data management&amp;#160;is to container strategies, 66 percent reported it as a critical or&amp;#160;important gating factor, 29 percent ranked it as moderately important and&amp;#160;only 5 percent reported that it carries no importance.&lt;/li&gt;
&lt;li&gt;Over 70% of respondents said they would like to&amp;#160;run a database or other stateful service in their container&amp;#160;environments.&lt;/li&gt;
&lt;li&gt;Respondents were also asked which specific features of&amp;#160;container data management they considered to be most important, selecting&amp;#160;the &amp;#8220;integration of data management capabilities into existing container&amp;#160;workflows and tools&amp;#8221; as their first choice, with &amp;#8220;seamless&amp;#160;movement of data between dev, test and production environments&amp;#8221; a&amp;#160;close second.&lt;/li&gt;
&lt;li&gt;MySQL (53%), Redis (52%), PostgreSQL (50%), and&amp;#160;Elasticsearch (43%) were reported as the top four most frequently used&amp;#160;stateful services.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Containers have become known for portability and flexibility, the survey reveals that organizations are using them in different infrastructures but most frequently in on-premises data centers (57%), followed by Amazon Web Services (52%).&lt;/p&gt;
&lt;p&gt;So I think it is safe to say Containers are rapidly becoming one of the staples of development and are here to stay to stay in the foreseeable future.&lt;/p&gt;
&lt;p&gt;DevOps.com along with ElasticBox are currently &lt;a href="https://www.surveymonkey.com/r/devopsdot"&gt;conducting another survey &lt;/a&gt;on &amp;#8220;What is DevOps to you?&amp;#8221; One in 50 respondents wins a $50 dollar Amazon gift card and one grand prize winner will win a new 3DR Drone. Take a few minutes to help us with this &lt;a href="https://www.surveymonkey.com/r/devopsdot"&gt;survey&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Thu, 18 Jun 2015 00:00:00 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-18:clusterhq-and-devopscom-survey-show-containers-poised-for-mass-adoption.html</guid></item><item><title>DOGs at Digital 2015</title><link>http://www.ciandcd.com/dogs-at-digital-2015.html</link><description>from:http://blog.devopsguys.com/2015/06/18/dogs-at-digital-2015/&lt;br&gt;&lt;div&gt;&lt;p&gt;&lt;a href="https://devopsguys.files.wordpress.com/2015/06/digital-2015.jpg"&gt;&lt;img class="aligncenter size-medium wp-image-879" src="https://devopsguys.files.wordpress.com/2015/06/digital-2015.jpg?w=300&amp;amp;h=102" alt="Digital 2015" width="300" height="102"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Last week the DevOpsGuys headed up to Newport&amp;#8217;s Celtic Manor to take part in Digital 2015 &amp;#8211; the Welsh Government&amp;#8217;s initiative to bring digital innovators and business professionals together. The 2-day event saw more than 2,000 delegates and 140 speakers.&lt;/p&gt;
&lt;p&gt;DevOpsGuy co-founder Steve Thair says:&lt;/p&gt;
&lt;p&gt;&amp;#8220;These initiatives are invaluable to the digital sector because they expose the wide variety of digital and technological services that are available in South Wales to business professionals who can use them to take online business services to the next level. It&amp;#8217;s a relaxed environment where people can chat and form connections that will have a direct impact on the future of business and technology in Wales.&amp;#8221;&lt;/p&gt;
&lt;p&gt;The diverse range of speakers at the event included Microsoft, the WRU, Amazon and the DVLA. The opportunity to discuss the needs of businesses directly with those running them is invaluable. This dialogue can lead to collaborative projects and further development of the burgeoning tech industry in the area.&lt;/p&gt;
&lt;p&gt;The team are excited to see more events like this one springing up in the near future. Look out for us at the up-coming Agile Cymru in the Wales Millennium Centre on the 7th and 8th of July.&lt;/p&gt;
&lt;p&gt;&amp;#160;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Thu, 18 Jun 2015 00:00:00 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-18:dogs-at-digital-2015.html</guid></item><item><title>DOGWalking in Brecon</title><link>http://www.ciandcd.com/dogwalking-in-brecon.html</link><description>from:http://blog.devopsguys.com/2015/06/17/dogwalking-in-brecon/&lt;br&gt;&lt;div&gt;&lt;p&gt;&amp;#160;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://devopsguys.files.wordpress.com/2015/06/010_herecomethedog_s.png"&gt;&lt;img class="aligncenter size-medium wp-image-876" src="https://devopsguys.files.wordpress.com/2015/06/010_herecomethedog_s.png?w=300&amp;amp;h=193" alt="010_HereComeTheDog_s" width="300" height="193"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So, aching, tired and happy the DOGs returned from &lt;a href="http://www.trekfest.org.uk/"&gt;TrekFest 2015&lt;/a&gt;&amp;#160;in the Welsh mountains having raised &amp;#163;1,143.00 for the Countess Mountbatten Hospice and SSNAP &amp;#8211; two charities close to the heart of the team. That&amp;#8217;s 114% of our initial target, so a huge thank you to everyone who donated so generously.&lt;/p&gt;
&lt;p&gt;The weather could not have been better for a long, scenic ramble in one of the UKs most beautiful spots; beautifully sunny with a refreshing breeze kept the team going. We completed the trek in approximately 5 and a half hours, just in time for a piece of cake and a glass of celebratory champagne at the finish line.&lt;/p&gt;
&lt;p&gt;Everyone had a thoroughly enjoyable time and we&amp;#8217;re all looking forward to the next DOG adventure!&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Wed, 17 Jun 2015 00:00:00 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-17:dogwalking-in-brecon.html</guid></item><item><title>Git Simple Feature Branch Workflow</title><link>http://www.ciandcd.com/git-simple-feature-branch-workflow.html</link><description>from:http://java.dzone.com/articles/git-simple-feature-branch&lt;br&gt;&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;p&gt;In my&amp;#160;&lt;a href="http://madhukaudantha.blogspot.com/2015/06/workflows-for-git.html" target="_blank"&gt;previous post&lt;/a&gt;, I wrote about git work flows. Now I will going to try out simple '&lt;a href="http://madhukaudantha.blogspot.com/2015/06/workflows-for-git.html" target="_blank"&gt;Feature Branch Workflow&lt;/a&gt;'.&lt;br&gt;&lt;/p&gt;1. I pull down the latest changes from mastergit checkout mastergit pull origin master2. I make branch to make changes&amp;#160;git checkout -b new-feature3. Now I am working on the feature4. I keep my feature branch fresh and up to date with the latest changes in master, using 'rebase'Every once in a while during the development update the feature branch with the latest changes in master.git fetch origingit rebase origin/masterIn the case where other devs are also working on the same shared remote feature branch, also rebase changes coming from it:git rebase origin/new-featureResolving conflicts during the rebase allows me to have always clean merges at the end of the feature development.5. When I am ready I commit my changesgit add -pgit commit -m "my changes"6. rebasing keeps my code working, merging easy, and history clean.git fetch origingit rebase origin/new-featuregit rebase origin/masterBelow two points are optional6.1 push my branch for discussion (pull-request)git push origin new-feature6.2 feel free to rebase within my feature branch, my team can handle it!git rebase -i origin/master&lt;br&gt;&lt;i&gt;&lt;b&gt;Few point that can be happen in developing phase.&lt;/b&gt;&lt;/i&gt;&lt;br&gt;Another new feature is needed and it need some commits from my new branch 'new-feature' that new feature need new branch and few commits need to push to it and clean from my branch.&lt;br&gt;&lt;br&gt;7.1 Creating x-new-feature branch on top of 'new-feature'&lt;br&gt;git checkout -b&amp;#160;x-new-feature&amp;#160;new-feature&lt;br&gt;&lt;br&gt;7.2 Cleaning commits&lt;br&gt;//revert a commit&lt;br&gt;git revert --no-commit&amp;#160;&lt;br&gt;//reverting few steps a back from current HEAD&lt;br&gt;git reset --hard HEAD~2&lt;br&gt;&lt;br&gt;7.3 Updating the git&lt;br&gt;//Clean new-feature branch&lt;br&gt;git push origin HEAD --force	
	&lt;p&gt;&lt;/p&gt;

    &lt;p&gt;1. I pull down the latest changes from mastergit checkout mastergit pull origin master2. I make branch to make changes git checkout -b new-feature3. Now I am working on the feature4. I keep my feature branch fresh and up to date with the latest changes in master, using 'rebase'Every once in a while during the development update the feature branch with the latest changes in master.git fetch origingit rebase origin/masterIn the case where other devs are also working on the same shared remote feature branch, also rebase changes coming from it:git rebase origin/new-featureResolving conflicts during the rebase allows me to have always clean merges at the end of the feature development.5. When I am ready I commit my changesgit add -pgit commit -m "my changes"6. rebasing keeps my code working, merging easy, and history clean.git fetch origingit rebase origin/new-featuregit rebase origin/masterBelow two points are optional6.1 push my branch for discussion (pull-request)git push origin new-feature6.2 feel free to rebase within my feature branch, my team can handle it!git rebase -i origin/masterAnother new feature is needed and it need some commits from my new branch 'new-feature' that new feature need new branch and few commits need to push to it and clean from my branch.7.1 Creating x-new-feature branch on top of 'new-feature'git checkout -b x-new-feature new-feature7.2 Cleaning commits//revert a commitgit revert --no-commit//reverting few steps a back from current HEADgit reset --hard HEAD~27.3 Updating the git//Clean new-feature branchgit push origin HEAD --force&lt;/p&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Tue, 16 Jun 2015 01:30:29 -0400</pubDate><guid>tag:www.ciandcd.com,2015-06-16:git-simple-feature-branch-workflow.html</guid></item><item><title>Know Thy MVN Plugins: Keeping One's Sanity Amidst Open Source Version Hell</title><link>http://www.ciandcd.com/know-thy-mvn-plugins-keeping-ones-sanity-amidst-open-source-version-hell.html</link><description>from:http://java.dzone.com/articles/know-thy-mvn-plugins-or&lt;br&gt;&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;p&gt;Problem #1: Everyone knows that keeping up with versions is tough.&amp;#160; This is the reason tools such as &lt;a href="http://www.openlogic.com/"&gt;OpenLogic&lt;/a&gt; come to exist. Some companies pay, some companies develop home grown solutions, some live with the version which are getting older every day.&lt;/p&gt;

&lt;p&gt;Problem #2:&amp;#160; When multiple open source projects once consumed by your product can bring in different versions of the same library. One never knows which will be picked up at run-time and behavior on the developer&amp;#8217;s box based on the Murphy&amp;#8217;s Law will be different from the server run-time.&lt;/p&gt;

&lt;p&gt;This does not have to be such an ordeal.&amp;#160; With the power of maven plugins this can be solved relatively easy. &lt;a href="http://www.mojohaus.org/versions-maven-plugin/"&gt;Versions Maven Plugin&lt;/a&gt; and &lt;a href="http://maven.apache.org/enforcer/maven-enforcer-plugin/"&gt;Maven Enforcer Plugin&lt;/a&gt; to the rescue! &lt;/p&gt;

&lt;p&gt;Versions Maven Plugin will keep versions up-to-date and as you probably guessed from the name Maven Enforcer plugin will be guarding against multiple versions of the maven artifact in the build package produced.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s see how one introduced enforcer into the mix first. The code below can go in the parent pom.xml of the project, or global parent of the projects if one exists.&lt;/p&gt;&lt;pre class="as3"&gt;&amp;lt;project &amp;#8230;&amp;gt;
&amp;#8230;
&amp;lt;plugins&amp;gt;
&amp;#8230;
&amp;lt;plugin&amp;gt;
        &amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt;
        &amp;lt;artifactId&amp;gt;maven-enforcer-plugin&amp;lt;/artifactId&amp;gt;
        &amp;lt;version&amp;gt;1.4&amp;lt;/version&amp;gt;
        &amp;lt;executions&amp;gt;
          &amp;lt;execution&amp;gt;
            &amp;lt;id&amp;gt;enforce-versions&amp;lt;/id&amp;gt;
            &amp;lt;goals&amp;gt;
              &amp;lt;goal&amp;gt;enforce&amp;lt;/goal&amp;gt;
            &amp;lt;/goals&amp;gt;
            &amp;lt;configuration&amp;gt;
              &amp;lt;rules&amp;gt;
                &amp;lt;requireMavenVersion&amp;gt;
                  &amp;lt;version&amp;gt;[2.2.*,)&amp;lt;/version&amp;gt;
                &amp;lt;/requireMavenVersion&amp;gt;
                &amp;lt;requireJavaVersion&amp;gt;
                  &amp;lt;version&amp;gt;[1.7.*,)&amp;lt;/version&amp;gt;
                &amp;lt;/requireJavaVersion&amp;gt;
                &amp;lt;DependencyConvergence/&amp;gt;
              &amp;lt;/rules&amp;gt;
            &amp;lt;/configuration&amp;gt;
          &amp;lt;/execution&amp;gt;
        &amp;lt;/executions&amp;gt;
      &amp;lt;/plugin&amp;gt;
&amp;#8230;
&amp;lt;/plugins&amp;gt;
&amp;#8230;
&amp;lt;/project&amp;gt;&lt;/pre&gt;&lt;p&gt;For each dependency version collision one has to pick the version to keep and exclude the ones that are mismatched. See maven &lt;a href="https://maven.apache.org/guides/introduction/introduction-to-optional-and-excludes-dependencies.html"&gt;help page&lt;/a&gt; for details. If one wants absolute guarantee of the version used, this is the only way to go. An example of excluded dependencies will be:&lt;/p&gt;&lt;pre class="as3"&gt;&amp;lt;project &amp;#8230;&amp;gt;
&amp;#8230;
	&amp;lt;dependencies&amp;gt;
&amp;#8230;
		&amp;lt;dependency&amp;gt;
			&amp;lt;groupId&amp;gt;com.lordofthejars&amp;lt;/groupId&amp;gt;
			&amp;lt;artifactId&amp;gt;nosqlunit-mongodb&amp;lt;/artifactId&amp;gt;
			&amp;lt;version&amp;gt;${nosqlunit.veresion}&amp;lt;/version&amp;gt;
		  &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;
		  &amp;lt;exclusions&amp;gt;
	      	&amp;lt;exclusion&amp;gt;
	      		&amp;lt;groupId&amp;gt;org.slf4j&amp;lt;/groupId&amp;gt;
	      		&amp;lt;artifactId&amp;gt;slf4j-api&amp;lt;/artifactId&amp;gt;
	      	&amp;lt;/exclusion&amp;gt;
	      	&amp;lt;exclusion&amp;gt;
	      		&amp;lt;groupId&amp;gt;com.github.fakemongo&amp;lt;/groupId&amp;gt;
	      		&amp;lt;artifactId&amp;gt;fongo&amp;lt;/artifactId&amp;gt;
	      	&amp;lt;/exclusion&amp;gt;
	      	&amp;lt;exclusion&amp;gt;
	      		&amp;lt;groupId&amp;gt;org.mongodb&amp;lt;/groupId&amp;gt;
	      		&amp;lt;artifactId&amp;gt;mongo-java-driver&amp;lt;/artifactId&amp;gt;
	      	&amp;lt;/exclusion&amp;gt;
		  &amp;lt;/exclusions&amp;gt;
		&amp;lt;/dependency&amp;gt;
&amp;#8230;
	&amp;lt;/dependencies&amp;gt;
&amp;#8230;
&amp;lt;/project&amp;gt;&lt;/pre&gt;&lt;p&gt;From the &lt;b&gt;open source developer side&lt;/b&gt;, producing two flavors of the package for consumption with and without dependencies can make world a better place. This is the difference of maven scope &amp;#8216;provided&amp;#8217; vs. default scope &amp;#8216;compile&amp;#8217;. &lt;a href="https://maven.apache.org/plugins/maven-shade-plugin/"&gt;Apache Maven Shade Plugin&lt;/a&gt; can be used to produce the version with the dependencies to be used there no conflicts can arise along with the version that is artifact version independent and can be used without version conflicts.&lt;/p&gt;

&lt;p&gt;Once all conflicts are resolved and one version of each artifact is in the final build product guaranteed, one should check if ones project is up to date. &amp;#160;It is a good practice to do the check at the beginning of the new version.&lt;/p&gt;

&lt;p&gt;To check for outdates dependencies, and plugins, and bring those up to date in the controlled manner (manually) one can execute:&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;mvn &lt;a href="http://www.mojohaus.org/versions-maven-plugin/display-dependency-updates-mojo.html"&gt;versions:display-dependency-updates&lt;/a&gt;&lt;br&gt;&lt;/li&gt;&lt;li&gt;mvn &lt;a href="http://www.mojohaus.org/versions-maven-plugin/display-plugin-updates-mojo.html"&gt;versions:display-plugin-updates&lt;/a&gt;&lt;br&gt;&lt;/li&gt;&lt;li&gt;mvn &lt;a href="http://www.mojohaus.org/versions-maven-plugin/display-property-updates-mojo.html"&gt;versions:display-property-updates&lt;/a&gt;&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;





&lt;p&gt;As an alternative one can let the plugin update the versions by executing following&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;mvn &lt;a href="http://www.mojohaus.org/versions-maven-plugin/use-latest-releases-mojo.html"&gt;versions:use-latest-releases&lt;/a&gt;&lt;br&gt;&lt;/li&gt;&lt;li&gt;mvn &lt;a href="http://www.mojohaus.org/versions-maven-plugin/update-properties-mojo.html"&gt;versions:update-properties&lt;/a&gt;&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;



&lt;p&gt;This last mvn command I am going to share is a bonus for the dedicated reader. It allows changing versions across the project and its modules painlessly:&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;mvn &lt;a href="http://www.mojohaus.org/versions-maven-plugin/set-mojo.html"&gt;versions:set&lt;/a&gt; -DgenerateBackupPoms=false&amp;#8203; -DnewVersion=&amp;lt;version&amp;gt;&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Tue, 16 Jun 2015 00:51:44 -0400</pubDate><guid>tag:www.ciandcd.com,2015-06-16:know-thy-mvn-plugins-keeping-ones-sanity-amidst-open-source-version-hell.html</guid></item><item><title>The Ops Mgr at QCon 2015</title><link>http://www.ciandcd.com/the-ops-mgr-at-qcon-2015.html</link><description>from:http://blog.devopsguys.com/2015/06/16/the-ops-mgr-at-qcon-2015/&lt;br&gt;&lt;div&gt;&lt;p&gt;&lt;a href="https://devopsguys.files.wordpress.com/2013/04/steve-profile.jpg"&gt;&lt;img class="aligncenter size-full wp-image-378" src="https://devopsguys.files.wordpress.com/2013/04/steve-profile.jpg?w=474" alt="steve-profile"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&amp;#8220;When you&amp;#8217;re in a startup, the divide between Dev and Ops is normally the width of the desk&amp;#8230;it&amp;#8217;s far easier to collaborate in that small environment. In larger enterprises not only are they&amp;#160;in different buildings but they&amp;#8217;re in different countries with different cultures and different languages&amp;#8230;&amp;#8221;&lt;/p&gt;
&lt;p&gt;The DOG Ops Manager Steve Thair chats to Manuel Pais at QCon 2015.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.infoq.com/interviews/steve-thair-qcon-2015"&gt;Steve Thair at QCon&lt;/a&gt; to hear Steve talk about Enterprise DevOps; taking the first steps on the road to DevOps and &amp;#160;cultural change.&lt;/p&gt;
&lt;p&gt;&amp;#160;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Tue, 16 Jun 2015 00:00:00 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-16:the-ops-mgr-at-qcon-2015.html</guid></item><item><title>Steve Thair’s QCon Talk – now available online</title><link>http://www.ciandcd.com/steve-thairs-qcon-talk-now-available-online.html</link><description>from:http://blog.devopsguys.com/2015/05/20/steve-thairs-qcon-talk-now-available-online/&lt;br&gt;&lt;div&gt;&lt;p&gt;&amp;#160;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://devopsguys.files.wordpress.com/2013/04/steve-profile.jpg"&gt;&lt;img class="aligncenter size-full wp-image-378" src="https://devopsguys.files.wordpress.com/2013/04/steve-profile.jpg?w=474" alt="steve-profile"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;DevOps and the Need for Speed, the talk from our very own Steve Thair is now available online. You can check it out &lt;a href="http://www.infoq.com/presentations/devops-speed"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Steve&amp;#8217;s just spoken at Krakow&amp;#8217;s &lt;a href="http://atmosphere-conference.com/"&gt;Atmosphere Conference&lt;/a&gt;. Stay tuned for more, coming soon.&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Wed, 20 May 2015 00:00:00 +0800</pubDate><guid>tag:www.ciandcd.com,2015-05-20:steve-thairs-qcon-talk-now-available-online.html</guid></item><item><title>Increase your ELK herd with Consul.io</title><link>http://www.ciandcd.com/increase-your-elk-herd-with-consulio.html</link><description>from:http://blog.devopsguys.com/2015/05/18/increase-your-elk-herd-with-consul-io/&lt;br&gt;&lt;div&gt;&lt;p class="reblog-from"&gt;&lt;img alt="" src="http://0.gravatar.com/avatar/91f3566a38a35d6b2ed4c7b4b0d5e3d2?s=48&amp;amp;d=identicon&amp;amp;r=G" class="avatar avatar-48" height="48" width="48"&gt;Originally posted on &lt;a href="http://doics.co/2015/05/18/increase-your-elk-herd-with-consul-io"&gt;DevOps Is Common Sense...&lt;/a&gt;:&lt;/p&gt;&lt;p&gt;At work, I recently had a need to put in place a scalable logging solution based around the ELK stack.&lt;/p&gt;

&lt;p&gt;&lt;a href="http://elasticsearch-users.115913.n3.nabble.com/Java-client-unable-to-connect-ClusterBlockException-td4019825.html" title="Elasticsearch Users Mailing List"&gt;Issues&lt;/a&gt;&lt;a href="https://community.rackspace.com/products/f/18/t/4055" title="Rackspace Community Forums"&gt;with&lt;/a&gt;&lt;a href="http://jontai.me/blog/2013/03/elasticsearch-ec2-discovery/" title="How to get Elasticsearch configured in AWS"&gt;Multicast&lt;/a&gt;&lt;a href="http://elasticsearch-users.115913.n3.nabble.com/discovery-multicast-vs-unicast-td3689223.html" title="Elasticsearch Users &amp;amp;quot;Should I use Multicast or Unicast?&amp;amp;quot;"&gt;networking&lt;/a&gt; aside, &amp;#160;Elasticsearch scales pretty well on its own without the need for any additional overheads, however&amp;#160;discovering whether a node is online or not and connecting only to available nodes can be tricky.&lt;/p&gt;

&lt;p&gt;Scaling Logstash can be tricky, but it basically involves &lt;a href="http://serverfault.com/questions/459303/scaling-logstash-with-redis-elasticsearch" title="Advice on scaling Logstash from Stack Overflow"&gt;adding more Logstash servers&lt;/a&gt; to the mix and pointing them at your Elasticsearch cluster by &lt;a href="http://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-host" title="Logstash Documentation on adding multiple hosts"&gt;defining multiple hosts&lt;/a&gt; in your Logstash configuration.&lt;/p&gt;

&lt;p&gt;Kibana (like most web applications) can &lt;a href="http://www.elastic.co/guide/en/kibana/current/_setting_kibana_server_properties.html" title="Kibana Documentation"&gt;only have one Elasticsearch host defined&lt;/a&gt; in the config, so scaling out Kibana is more difficult.&lt;/p&gt;

&lt;p&gt;The above raises the question &amp;#8211; how do I know&amp;#160;which Elasticsearch node to point my configuration at if I don&amp;#8217;t know whether they are there or not.&lt;/p&gt;

&lt;p&gt;The answer came in the form of &lt;a href="http://consul.io/" title="Consul.io&amp;#x27;s Home Page"&gt;consul.io&lt;/a&gt;. &amp;#160;If you&amp;#8217;ve not looked at&amp;#8230;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Mon, 18 May 2015 00:00:00 +0800</pubDate><guid>tag:www.ciandcd.com,2015-05-18:increase-your-elk-herd-with-consulio.html</guid></item><item><title>DevOpsGuys announce RedGate partnership</title><link>http://www.ciandcd.com/devopsguys-announce-redgate-partnership.html</link><description>from:http://blog.devopsguys.com/2015/05/11/devopsguys-partner-redgate/&lt;br&gt;&lt;div&gt;&lt;p&gt;&lt;strong&gt;More and more companies are now considering source control, continuous integration, and automated deployment for their database. To help them adopt each of these stages of Database Lifecycle Management (DLM), Redgate Software has launched a new partner program. Redgate Certified Consultants are now being trained in the USA, Europe and Australia &amp;#8211; and many of them will be familiar to SQL Server professionals.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The advantages of implementing any stage of DLM are many. Just as with Application Lifecycle Management (ALM), it speeds up the introduction of new features, and makes deployments reliable and error-free.&lt;/p&gt;
&lt;p&gt;But even though Redgate tools are designed to plug into the tools companies already use for their application d&lt;/p&gt;
&lt;p&gt;&amp;#160;&lt;/p&gt;
&lt;p&gt;evelopment, questions can arise during the implementation process.&lt;/p&gt;
&lt;p&gt;As Dan Wood of Northwest Cadence says: &amp;#8220;How can we take a system designed to develop, build and deliver applications and make it work with databases as well? It is a question that has plagued a vast majority of the clients I have worked with over the past few years.&amp;#8221;&lt;/p&gt;
&lt;p&gt;To address this issue, Redgate&amp;#8217;s new partner program is training expert consultants like Dan Wood in DLM &amp;#8211; and giving them the tools and support they need to help clients on-site, or in training sessions.&lt;/p&gt;
&lt;p&gt;The list of Certified Consultants is growing and already includes familiar faces like Ike Ellis and Northwest Cadence in the USA, The DevOpsGuys and Skelton Thatcher in the UK, and WARDY IT Solutions in Australia.&lt;/p&gt;
&lt;p&gt;As John Theron of Redgate points out, the advantages are clear. &amp;#8220;Redgate has spent a lot of time and effort joining the dots in DLM, and making it possible with a suite of dedicated tools, alongside learning materials and resources. The partner program complements this with a group of experts on the ground able to help companies on-site, and provide training in a series of public workshops.&amp;#8221;&lt;/p&gt;
&lt;p&gt;In places as far apart as Washington, London, San Diego, Philadelphia, Northern Ireland, and Baton Rouge, database professionals are how being trained in source control, continuous integration, and automated deployment for the database.&lt;/p&gt;
&lt;p&gt;A measure of the success the training is already achieving can be found in the reaction from database professionals like Jim Dorame. A Database Systems Manager for a large scale educational assessment corporation in the Greater Minneapolis area, he &lt;a href="http://www.jamesdorame.com/index.php/2014/08/22/continuous-integration-for-the-database-is-really-not-that-scary/"&gt;reviewed a continuous integration training day on his blog&lt;/a&gt;.&amp;#160; &amp;#8220;This tool makes the job of the DBA easier as there will be little doubt that the database is in a consistent and correct state. This alone makes me smile, I cannot tell you how many times I&amp;#8217;ve been executing a release and there was a piece missing that caused a failure.&amp;#8221;&lt;/p&gt;
&lt;p&gt;Further information about the training opportunities available can be found on the &lt;a href="http://www.red-gate.com/training/"&gt;Redgate training pages&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Mon, 11 May 2015 00:00:00 +0800</pubDate><guid>tag:www.ciandcd.com,2015-05-11:devopsguys-announce-redgate-partnership.html</guid></item><item><title>Sponsored DOG Walk</title><link>http://www.ciandcd.com/sponsored-dog-walk.html</link><description>from:http://blog.devopsguys.com/2015/05/08/sponsored-dog-walk/&lt;br&gt;&lt;div&gt;&lt;p&gt;&lt;a href="https://devopsguys.files.wordpress.com/2015/05/trekfest_jpg.jpg"&gt;&lt;img class="aligncenter wp-image-850 size-full" src="https://devopsguys.files.wordpress.com/2015/05/trekfest_jpg.jpg?w=474" alt="Trekfest_jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The DevOpsGuys team are pulling up our hiking socks to raise cash for SSNAP and Countess Mountbatten Hospice this summer with a 13 mile walk across Wales. TrekFest it&amp;#8217;s no mean feat. We cross the highest peaks in the Beacons and South Wales including Pen y Fan (886m), Corn Du (873m), Cribyn (795m) and Fan y Big (719m).&amp;#160;&lt;a href="http://www.trekfest.org.uk/" target="_blank"&gt;http://www.trekfest.org.uk/&amp;#160;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a title="Just Giving" href="https://www.justgiving.com/Dev15"&gt;Click here&amp;#160;to donate now through our&amp;#160;Just Giving Page&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Our first charity is special since last year, one of our brave team members was diagnosed with terminal cancer. Unfortunately, and with great sadness we know now they are losing their fight &amp;#8211; even after enduring endless rounds of chemotherapy and surgery.&lt;/p&gt;
&lt;p&gt;The amazing staff at Countess Mountbatten Hospice are proving specialist&amp;#160;palliative (end of life) care to many fighting a losing battles against advanced stage cancer. They also support their families and loved ones. We&amp;#8217;d love to show our thanks and support by raising money on their behalf.&lt;/p&gt;
&lt;p&gt;It&amp;#8217;s with a tear in our eye and sadness in our hearts, that we chose our second charity to support. Last year, little Ollie lost his fight for his life after being born with a heart defect. He was 4 days old. His parents have displayed so much courage during an immensely difficult time during which the amazing doctors and nurses, support by the SSNAP team provided them with much needed support.&lt;/p&gt;
&lt;p&gt;It is their wish to continue to champion SSNAP, who provide support for the sick newborns and their parents at new born intensive care unit at The John Radcliffe Hospital, Oxford as so, we&amp;#8217;ll match whatever we raise through our JustGiving page as a donation to SSNAP.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;These amazing charities are truly special to us here DevOpsGuys.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Please support us in raising funds for these brilliant charities who have done so much to support our close friends and employees.&lt;/p&gt;
&lt;p&gt;The team have signed up to complete the distance in six hours. The trek takes place in the Brecon Beacons and covers the highest peaks in South Wales: Pen y Fan, Corn Du and Fan y Big. The Beacons are a training ground for the SAS so, while the DOGs will have their work cut out for them, they&amp;#8217;re more than up for the challenge:&lt;/p&gt;
&lt;p&gt;&amp;#8220;I can&amp;#8217;t wait to get out there&amp;#8221; says office manager Rhian Owen. &amp;#8220;It&amp;#8217;s such a great opportunity to work together as a team to achieve personal goals and to raise money for good causes &amp;#8211; it&amp;#8217;s going to be brilliant!&amp;#8221;&lt;/p&gt;
&lt;p&gt;We&amp;#8217;ve set up a Just Giving page, so you can show your support here. It&amp;#8217;s a chance to donate to some good causes and get the DevOpsGuys and gals out from behind their screens and into the beautiful Welsh wilderness &amp;#8211; come on y&amp;#8217;all, &lt;a href="https://www.justgiving.com/Dev15/?utm_id=8&amp;amp;fb_action_ids=753124678140173&amp;amp;fb_action_types=jgdonation%3Asupport&amp;amp;fb_ref=pfp-share-facebook-test-B-giving&amp;amp;fb_source=other_multiline&amp;amp;action_object_map=%5B971301802894069%5D&amp;amp;action_type_map=%5B%22jgdonation%3Asupport%22%5D&amp;amp;action_ref_map=%5B%22pfp-share-facebook-test-B-giving%22%5D"&gt;dig deep!&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a title="Just Giving" href="https://www.justgiving.com/Dev15"&gt;Click here&amp;#160;to donate now through our&amp;#160;Just Giving Page&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Fri, 08 May 2015 00:00:00 +0800</pubDate><guid>tag:www.ciandcd.com,2015-05-08:sponsored-dog-walk.html</guid></item><item><title>DevOps and the Digital Supply Chain</title><link>http://www.ciandcd.com/devops-and-the-digital-supply-chain.html</link><description>from:http://blog.devopsguys.com/2015/05/06/devops-and-the-digital-supply-chain/&lt;br&gt;&lt;div&gt;&lt;p&gt;What is the &amp;#8220;Digital Supply Chain&amp;#8221; and why is it important to your organisation and to DevOps as a practice?&lt;/p&gt;
&lt;p&gt;The concept of the &amp;#8220;Digital Supply Chain&amp;#8221; is a different way of looking at the SDLC and the Continuous Delivery &amp;#8220;pipeline&amp;#8221; that we feel makes it easier for traditional organisations to understand the criticality of software delivery (and by extension DevOps) in the modern world.&lt;/p&gt;
&lt;p&gt;Any organisation that deals with physical goods understands the concept of the &lt;a href="http://en.wikipedia.org/wiki/Supply_chain" target="_blank"&gt;supply chain&lt;/a&gt;. They are intimately familiar with ideas like &lt;a href="http://en.wikipedia.org/wiki/Supply_chain_management" target="_blank"&gt;supply chain management&lt;/a&gt;, &lt;a href="http://en.wikipedia.org/wiki/Supply_chain_optimization" target="_blank"&gt;supply chain optimisation&lt;/a&gt; and, most importantly, they understand the economics of inventory in the supply chain e.g &lt;a href="http://www.accountingcoach.com/blog/calculate-inventory-carrying-cost" target="_blank"&gt;the carrying cost of inventory&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So what is the &amp;#8220;Digital Supply Chain&amp;#8221;?&lt;/p&gt;
&lt;p&gt;The current definitions of the digital supply chain are anchored in the &amp;#8220;New Media&amp;#8221; sector and focus on digital assets like music, video etc&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&amp;#8220;The &amp;#8220;&lt;strong&gt;digital supply chain&lt;/strong&gt;&amp;#8221; is a &amp;#8220;&lt;a href="http://en.wikipedia.org/wiki/New_media"&gt;new media&lt;/a&gt;&amp;#8221; term which encompasses the process of the delivery of digital media, be it music or video, by electronic means, from the point of origin (content provider) to destination (consumer).&amp;#8221; &amp;#8211; &lt;a href="http://en.wikipedia.org/wiki/Digital_supply_chain" target="_blank"&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The Wikipedia article references above breaks it down into a number of discrete steps as shown in Figure 1 below.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://devopsguys.files.wordpress.com/2015/05/new-media-digital-supply-chain.png"&gt;&lt;img class="aligncenter size-large wp-image-836" src="https://devopsguys.files.wordpress.com/2015/05/new-media-digital-supply-chain.png?w=474&amp;amp;h=355" alt="image of New Media Digital Supply Chain" width="474" height="355"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If we contrast this with our SDLC Continuous Delivery Pipeline (Figure 2) we can see that many of the steps are directly analogous &amp;#8211; we are creating digital assets (code) which we then &amp;#8220;compress&amp;#8221; (i.e. the Build/Integrate process), which we then subject to Quality Control (Test), we store in a Digital Asset Management system (e.g. like Nexus or Artifactory), we tag it with metadata (e.g. what release/version we&amp;#8217;re deploying) and when then deploy it out to servers, CDN&amp;#8217;s, the AppStore or wherever.&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter size-large wp-image-837" src="https://devopsguys.files.wordpress.com/2015/05/sdlc-digital-supply-chain.png?w=474&amp;amp;h=355" alt="image of SDLC Digital Supply Chain" width="474" height="355"&gt;&lt;/p&gt;
&lt;p&gt;Once your customers grasp the idea that software is a digital asset and that carrying excess inventory and delays in moving these digital assets along the supply chain is costing them money it can be a lightbulb moment for many organisations.&lt;/p&gt;
&lt;p&gt;Software assets can depreciate over time. Indeed &amp;#8220;&lt;a href="http://blog.devopsguys.com/2015/01/08/top-5-technical-debt-tips-for-businesses-in-2015/"&gt;technical debt&lt;/a&gt;&amp;#8221; can be looked at as the &amp;#8220;cost of deprecation&amp;#8221; of your software assets over time.&lt;/p&gt;
&lt;p&gt;Code that is &amp;#8220;stuck&amp;#8221; in your Digital Supply Chain waiting for your next release (as source code in Git, or as artefacts in an artefact repository) represents a capital investment in &amp;#8220;digital assets&amp;#8221; held as &amp;#8220;digital inventory&amp;#8221; and having it sat on the digital shelf in your digital warehouse is costing you money is analogous to &lt;a href="http://www.accountingcoach.com/blog/calculate-inventory-carrying-cost"&gt;the carrying cost of inventory&lt;/a&gt; for physical inventory.&lt;/p&gt;
&lt;p&gt;Sure, the warehousing costs of a digital asset &amp;#8211; your latest idea transformed into software code &amp;#8211; is fairly trivial compared to the costs of physical warehousing BUT the &amp;#8220;&lt;a title="Opportunity Cost Defined" href="http://en.wikipedia.org/wiki/Opportunity_cost" target="_blank"&gt;opportunity cost&lt;/a&gt;&amp;#8221; is very real.&lt;/p&gt;
&lt;p&gt;Each digital software asset represents a significant investment in time &amp;amp; money by your designers, developers, testers, project managers etc and it doesn&amp;#8217;t start generating a return on that investment until it gets to the end of your digital supply chain and into the hands of your customers.&lt;/p&gt;
&lt;p&gt;DevOps then becomes a way to optimise your digital supply chain to ensure that we:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;only build the right things (reducing waste and optimising our digital inventory),&lt;/li&gt;
&lt;li&gt;Supplier management (by improving the relationships between Dev, Test, Ops etc we ensure that we are getting the best from all of the &amp;#8220;suppliers&amp;#8221; in our digital supply chain)&lt;/li&gt;
&lt;li&gt;improving our logistics to get our digital assets in the hand of our customers (by automating testing, release and deployment to accelerate the movement of the digital assets from left to right)&lt;/li&gt;
&lt;li&gt;Constantly seeking &amp;#8220;flow&amp;#8221; across the supply chain (the 1st way of DevOps!)&lt;/li&gt;
&lt;li&gt;Gathering metrics along the supply chain to give us insight into the bottlenecks (the M in the C.A.L.M.S model)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So next time you&amp;#8217;re talking with people in the business try out the &amp;#8220;Digital Supply Chain&amp;#8221; analogy and see if it works for you &amp;#8211; we&amp;#8217;d love to hear your feedback!&lt;/p&gt;
&lt;p&gt;-TheOpsMgr&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Wed, 06 May 2015 00:00:00 +0800</pubDate><guid>tag:www.ciandcd.com,2015-05-06:devops-and-the-digital-supply-chain.html</guid></item><item><title>Cardiff: Silicon Valley comes to Wales</title><link>http://www.ciandcd.com/cardiff-silicon-valley-comes-to-wales.html</link><description>from:http://blog.devopsguys.com/2015/05/05/cardiff-silicon-valley-comes-to-wales/&lt;br&gt;&lt;div&gt;&lt;p&gt;&lt;a href="https://devopsguys.files.wordpress.com/2015/05/7383585190_59b1721f7d_n.jpg"&gt;&lt;img class="aligncenter size-medium wp-image-832" src="https://devopsguys.files.wordpress.com/2015/05/7383585190_59b1721f7d_n.jpg?w=300&amp;amp;h=184" alt="7383585190_59b1721f7d_n" width="300" height="184"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We&amp;#8217;ve been set up in Cardiff, South Wales for nearly six months now. Every week it becomes more and more apparent that this city is fast becoming an exciting IT and technical hub; an attractive area for emerging and experienced tech talent alike. The term &amp;#8216;Silicwm Valley&amp;#8217; is being bandied about as more and more tech start-ups spring up in, or near, the city centre.&lt;/p&gt;
&lt;p&gt;Companies like DevOpsGuys, Cardiff Start, Indycube, Method 4, BBC Cymru&amp;#8217;s Roath Lock studios and a huge collection of digital and design agencies are choosing Cardiff as their base. It seems to be a logical step; the community is small enough to be interconnected, influential and supportive, but large enough to allow for the freedom to develop, expand and learn from the huge range of related industries in the immediate area.&lt;/p&gt;
&lt;p&gt;With several major universities in and around the city the wealth of talent is growing and Cardiff is taking the reins and nurturing Welsh talent and ability; a very different picture from several years ago where work in Wales was hard to come by and the majority of experienced IT professionals were obliged to seek work further afield, in London or Cambridge.&lt;/p&gt;
&lt;p&gt;Founder James Smith says:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&amp;#8220;Cardiff has historically been built on industry, from the days of exporting coal. It&amp;#8217;s also frequently voted one of the top places to live and work in the UK, so it&amp;#8217;s no wonder that this tradition is developing and changing shape with the emergence of the tech industry &amp;#8211; Cardiff is moving with the times.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&amp;#8220;We&amp;#8217;ve set up DevOpsGuys in Cardiff in order to be a part of this development. We wanted to provide opportunities for people in Wales &amp;#8211; there&amp;#8217;s so much skill here. Plus we are working with international companies and forming partnerships with industry giants across the world; this is a great opportunity to share some of the home-grown Welsh talent, create unique, fulfilling career opportunities and forge connections all over the world. It&amp;#8217;s a really exciting time.&amp;#8221;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The movement has been supported by the Welsh Government, with DevOpsGuys receiving &amp;#160;funding to grow as a business and provide career opportunities in the Welsh capital. Meet-ups, tech events, talks and conferences all taking place in the city, give related, but wildly diverse businesses a chance to meet, mix, talk, share thoughts; ideas flow freely, business connections are forged easily and some new and interesting work is emerging. We&amp;#8217;re excited about &lt;a href="http://blog.devopsguys.com/2015/04/16/the-first-2-day-agile-conference-to-hit-cardiff/"&gt;Agile Cymru&lt;/a&gt; &amp;#8211; the first event of its kind in Cardiff&amp;#160;&amp;#8211; this summer.&lt;/p&gt;
&lt;p&gt;There seems to be something new to see, do, read, visit, look at or enjoy every week! We&amp;#8217;re excited to see where Cardiff will take the DevOpsGuys and the future of the UK tech industry.&lt;/p&gt;
&lt;p&gt;&amp;#160;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Tue, 05 May 2015 00:00:00 +0800</pubDate><guid>tag:www.ciandcd.com,2015-05-05:cardiff-silicon-valley-comes-to-wales.html</guid></item><item><title>DevOpsGuys at RedGate</title><link>http://www.ciandcd.com/devopsguys-at-redgate.html</link><description>from:http://blog.devopsguys.com/2015/05/01/devopsguys-at-redgate/&lt;br&gt;&lt;div&gt;&lt;p&gt;&lt;a href="https://devopsguys.files.wordpress.com/2015/02/red-gate-e1424089521549.png"&gt;&lt;img class="aligncenter size-full wp-image-777" src="https://devopsguys.files.wordpress.com/2015/02/red-gate-e1424089521549.png?w=474" alt="red-gate"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The DevOpsGuys headed off on a road trip this week to meet with the RedGate team at their amazing offices in Cambridge.&lt;/p&gt;
&lt;p&gt;As well as working on some workshop training opportunities and guest blog articles (stay tuned to the DevOpsGuys Blog for some RedGate articles coming soon) the teams got together to brainstorm ideas and share skills.&lt;/p&gt;
&lt;p&gt;We were able to look at some of their newest tools and we&amp;#8217;re excited to announce that we will be delivering workshops on RedGate DLM tools at various sessions across the country this summer.&lt;/p&gt;
&lt;p&gt;We&amp;#8217;ve already implemented these tools for a many of our&amp;#160;customers and we&amp;#8217;re delighted to be able to introduce their qualities, in detail, to a wide range of industry professionals as part of an effective, independent DevOps adoption process.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The workshops will be running on:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;May 20 &amp;#8211; Automated Database Deployment, London&lt;/p&gt;
&lt;p&gt;June 26 &amp;#8211;&amp;#160;Automated Database Deployment, Belfast&lt;/p&gt;
&lt;p&gt;July 8 &amp;#8211; Database Source Control, London&lt;/p&gt;
&lt;p&gt;July 24 &amp;#8211;&amp;#160;Database Source Control, Manchester&lt;/p&gt;
&lt;p&gt;August 20 &amp;#8211; Database Continuous Integration, Cardiff&lt;/p&gt;
&lt;p&gt;Spaces are limited, so &lt;a href="http://www.red-gate.com/training/"&gt;register now to take part in a workshop or request a workshop near you.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Fri, 01 May 2015 00:00:00 +0800</pubDate><guid>tag:www.ciandcd.com,2015-05-01:devopsguys-at-redgate.html</guid></item></channel></rss>