<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>ciandcd</title><link>http://www.ciandcd.com/</link><description>continuous integration and continuous delivery</description><atom:link href="http://www.ciandcd.com/feeds/devops.rss.xml" rel="self"></atom:link><lastBuildDate>Sun, 21 Jun 2015 00:44:33 +0800</lastBuildDate><item><title>DevOpsGuys at RedGate</title><link>http://www.ciandcd.com/devopsguys-at-redgate.html</link><description>&lt;div&gt;&lt;p&gt;&lt;a href="https://devopsguys.files.wordpress.com/2015/02/red-gate-e1424089521549.png"&gt;&lt;img class="aligncenter size-full wp-image-777" src="https://devopsguys.files.wordpress.com/2015/02/red-gate-e1424089521549.png?w=474" alt="red-gate"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The DevOpsGuys headed off on a road trip this week to meet with the RedGate team at their amazing offices in Cambridge.&lt;/p&gt;
&lt;p&gt;As well as working on some workshop training opportunities and guest blog articles (stay tuned to the DevOpsGuys Blog for some RedGate articles coming soon) the teams got together to brainstorm ideas and share skills.&lt;/p&gt;
&lt;p&gt;We were able to look at some of their newest tools and we&amp;#8217;re excited to announce that we will be delivering workshops on RedGate DLM tools at various sessions across the country this summer.&lt;/p&gt;
&lt;p&gt;We&amp;#8217;ve already implemented these tools for a many of our&amp;#160;customers and we&amp;#8217;re delighted to be able to introduce their qualities, in detail, to a wide range of industry professionals as part of an effective, independent DevOps adoption process.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The workshops will be running on:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;May 20 &amp;#8211; Automated Database Deployment, London&lt;/p&gt;
&lt;p&gt;June 26 &amp;#8211;&amp;#160;Automated Database Deployment, Belfast&lt;/p&gt;
&lt;p&gt;July 8 &amp;#8211; Database Source Control, London&lt;/p&gt;
&lt;p&gt;July 24 &amp;#8211;&amp;#160;Database Source Control, Manchester&lt;/p&gt;
&lt;p&gt;August 20 &amp;#8211; Database Continuous Integration, Cardiff&lt;/p&gt;
&lt;p&gt;Spaces are limited, so &lt;a href="http://www.red-gate.com/training/"&gt;register now to take part in a workshop or request a workshop near you.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:44:33 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:devopsguys-at-redgate.html</guid></item><item><title>Cardiff: Silicon Valley comes to Wales</title><link>http://www.ciandcd.com/cardiff-silicon-valley-comes-to-wales.html</link><description>&lt;div&gt;&lt;p&gt;&lt;a href="https://devopsguys.files.wordpress.com/2015/05/7383585190_59b1721f7d_n.jpg"&gt;&lt;img class="aligncenter size-medium wp-image-832" src="https://devopsguys.files.wordpress.com/2015/05/7383585190_59b1721f7d_n.jpg?w=300&amp;amp;h=184" alt="7383585190_59b1721f7d_n" width="300" height="184"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We&amp;#8217;ve been set up in Cardiff, South Wales for nearly six months now. Every week it becomes more and more apparent that this city is fast becoming an exciting IT and technical hub; an attractive area for emerging and experienced tech talent alike. The term &amp;#8216;Silicwm Valley&amp;#8217; is being bandied about as more and more tech start-ups spring up in, or near, the city centre.&lt;/p&gt;
&lt;p&gt;Companies like DevOpsGuys, Cardiff Start, Indycube, Method 4, BBC Cymru&amp;#8217;s Roath Lock studios and a huge collection of digital and design agencies are choosing Cardiff as their base. It seems to be a logical step; the community is small enough to be interconnected, influential and supportive, but large enough to allow for the freedom to develop, expand and learn from the huge range of related industries in the immediate area.&lt;/p&gt;
&lt;p&gt;With several major universities in and around the city the wealth of talent is growing and Cardiff is taking the reins and nurturing Welsh talent and ability; a very different picture from several years ago where work in Wales was hard to come by and the majority of experienced IT professionals were obliged to seek work further afield, in London or Cambridge.&lt;/p&gt;
&lt;p&gt;Founder James Smith says:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&amp;#8220;Cardiff has historically been built on industry, from the days of exporting coal. It&amp;#8217;s also frequently voted one of the top places to live and work in the UK, so it&amp;#8217;s no wonder that this tradition is developing and changing shape with the emergence of the tech industry &amp;#8211; Cardiff is moving with the times.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&amp;#8220;We&amp;#8217;ve set up DevOpsGuys in Cardiff in order to be a part of this development. We wanted to provide opportunities for people in Wales &amp;#8211; there&amp;#8217;s so much skill here. Plus we are working with international companies and forming partnerships with industry giants across the world; this is a great opportunity to share some of the home-grown Welsh talent, create unique, fulfilling career opportunities and forge connections all over the world. It&amp;#8217;s a really exciting time.&amp;#8221;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The movement has been supported by the Welsh Government, with DevOpsGuys receiving &amp;#160;funding to grow as a business and provide career opportunities in the Welsh capital. Meet-ups, tech events, talks and conferences all taking place in the city, give related, but wildly diverse businesses a chance to meet, mix, talk, share thoughts; ideas flow freely, business connections are forged easily and some new and interesting work is emerging. We&amp;#8217;re excited about &lt;a href="http://blog.devopsguys.com/2015/04/16/the-first-2-day-agile-conference-to-hit-cardiff/"&gt;Agile Cymru&lt;/a&gt; &amp;#8211; the first event of its kind in Cardiff&amp;#160;&amp;#8211; this summer.&lt;/p&gt;
&lt;p&gt;There seems to be something new to see, do, read, visit, look at or enjoy every week! We&amp;#8217;re excited to see where Cardiff will take the DevOpsGuys and the future of the UK tech industry.&lt;/p&gt;
&lt;p&gt;&amp;#160;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:44:31 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:cardiff-silicon-valley-comes-to-wales.html</guid></item><item><title>DevOps and the Digital Supply Chain</title><link>http://www.ciandcd.com/devops-and-the-digital-supply-chain.html</link><description>&lt;div&gt;&lt;p&gt;What is the &amp;#8220;Digital Supply Chain&amp;#8221; and why is it important to your organisation and to DevOps as a practice?&lt;/p&gt;
&lt;p&gt;The concept of the &amp;#8220;Digital Supply Chain&amp;#8221; is a different way of looking at the SDLC and the Continuous Delivery &amp;#8220;pipeline&amp;#8221; that we feel makes it easier for traditional organisations to understand the criticality of software delivery (and by extension DevOps) in the modern world.&lt;/p&gt;
&lt;p&gt;Any organisation that deals with physical goods understands the concept of the &lt;a href="http://en.wikipedia.org/wiki/Supply_chain" target="_blank"&gt;supply chain&lt;/a&gt;. They are intimately familiar with ideas like &lt;a href="http://en.wikipedia.org/wiki/Supply_chain_management" target="_blank"&gt;supply chain management&lt;/a&gt;, &lt;a href="http://en.wikipedia.org/wiki/Supply_chain_optimization" target="_blank"&gt;supply chain optimisation&lt;/a&gt; and, most importantly, they understand the economics of inventory in the supply chain e.g &lt;a href="http://www.accountingcoach.com/blog/calculate-inventory-carrying-cost" target="_blank"&gt;the carrying cost of inventory&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So what is the &amp;#8220;Digital Supply Chain&amp;#8221;?&lt;/p&gt;
&lt;p&gt;The current definitions of the digital supply chain are anchored in the &amp;#8220;New Media&amp;#8221; sector and focus on digital assets like music, video etc&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&amp;#8220;The &amp;#8220;&lt;strong&gt;digital supply chain&lt;/strong&gt;&amp;#8221; is a &amp;#8220;&lt;a href="http://en.wikipedia.org/wiki/New_media"&gt;new media&lt;/a&gt;&amp;#8221; term which encompasses the process of the delivery of digital media, be it music or video, by electronic means, from the point of origin (content provider) to destination (consumer).&amp;#8221; &amp;#8211; &lt;a href="http://en.wikipedia.org/wiki/Digital_supply_chain" target="_blank"&gt;Wikipedia&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The Wikipedia article references above breaks it down into a number of discrete steps as shown in Figure 1 below.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://devopsguys.files.wordpress.com/2015/05/new-media-digital-supply-chain.png"&gt;&lt;img class="aligncenter size-large wp-image-836" src="https://devopsguys.files.wordpress.com/2015/05/new-media-digital-supply-chain.png?w=474&amp;amp;h=355" alt="image of New Media Digital Supply Chain" width="474" height="355"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If we contrast this with our SDLC Continuous Delivery Pipeline (Figure 2) we can see that many of the steps are directly analogous &amp;#8211; we are creating digital assets (code) which we then &amp;#8220;compress&amp;#8221; (i.e. the Build/Integrate process), which we then subject to Quality Control (Test), we store in a Digital Asset Management system (e.g. like Nexus or Artifactory), we tag it with metadata (e.g. what release/version we&amp;#8217;re deploying) and when then deploy it out to servers, CDN&amp;#8217;s, the AppStore or wherever.&lt;/p&gt;
&lt;p&gt;&lt;img class="aligncenter size-large wp-image-837" src="https://devopsguys.files.wordpress.com/2015/05/sdlc-digital-supply-chain.png?w=474&amp;amp;h=355" alt="image of SDLC Digital Supply Chain" width="474" height="355"&gt;&lt;/p&gt;
&lt;p&gt;Once your customers grasp the idea that software is a digital asset and that carrying excess inventory and delays in moving these digital assets along the supply chain is costing them money it can be a lightbulb moment for many organisations.&lt;/p&gt;
&lt;p&gt;Software assets can depreciate over time. Indeed &amp;#8220;&lt;a href="http://blog.devopsguys.com/2015/01/08/top-5-technical-debt-tips-for-businesses-in-2015/"&gt;technical debt&lt;/a&gt;&amp;#8221; can be looked at as the &amp;#8220;cost of deprecation&amp;#8221; of your software assets over time.&lt;/p&gt;
&lt;p&gt;Code that is &amp;#8220;stuck&amp;#8221; in your Digital Supply Chain waiting for your next release (as source code in Git, or as artefacts in an artefact repository) represents a capital investment in &amp;#8220;digital assets&amp;#8221; held as &amp;#8220;digital inventory&amp;#8221; and having it sat on the digital shelf in your digital warehouse is costing you money is analogous to &lt;a href="http://www.accountingcoach.com/blog/calculate-inventory-carrying-cost"&gt;the carrying cost of inventory&lt;/a&gt; for physical inventory.&lt;/p&gt;
&lt;p&gt;Sure, the warehousing costs of a digital asset &amp;#8211; your latest idea transformed into software code &amp;#8211; is fairly trivial compared to the costs of physical warehousing BUT the &amp;#8220;&lt;a title="Opportunity Cost Defined" href="http://en.wikipedia.org/wiki/Opportunity_cost" target="_blank"&gt;opportunity cost&lt;/a&gt;&amp;#8221; is very real.&lt;/p&gt;
&lt;p&gt;Each digital software asset represents a significant investment in time &amp;amp; money by your designers, developers, testers, project managers etc and it doesn&amp;#8217;t start generating a return on that investment until it gets to the end of your digital supply chain and into the hands of your customers.&lt;/p&gt;
&lt;p&gt;DevOps then becomes a way to optimise your digital supply chain to ensure that we:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;only build the right things (reducing waste and optimising our digital inventory),&lt;/li&gt;
&lt;li&gt;Supplier management (by improving the relationships between Dev, Test, Ops etc we ensure that we are getting the best from all of the &amp;#8220;suppliers&amp;#8221; in our digital supply chain)&lt;/li&gt;
&lt;li&gt;improving our logistics to get our digital assets in the hand of our customers (by automating testing, release and deployment to accelerate the movement of the digital assets from left to right)&lt;/li&gt;
&lt;li&gt;Constantly seeking &amp;#8220;flow&amp;#8221; across the supply chain (the 1st way of DevOps!)&lt;/li&gt;
&lt;li&gt;Gathering metrics along the supply chain to give us insight into the bottlenecks (the M in the C.A.L.M.S model)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So next time you&amp;#8217;re talking with people in the business try out the &amp;#8220;Digital Supply Chain&amp;#8221; analogy and see if it works for you &amp;#8211; we&amp;#8217;d love to hear your feedback!&lt;/p&gt;
&lt;p&gt;-TheOpsMgr&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:44:29 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:devops-and-the-digital-supply-chain.html</guid></item><item><title>Sponsored DOG Walk</title><link>http://www.ciandcd.com/sponsored-dog-walk.html</link><description>&lt;div&gt;&lt;p&gt;&lt;a href="https://devopsguys.files.wordpress.com/2015/05/trekfest_jpg.jpg"&gt;&lt;img class="aligncenter wp-image-850 size-full" src="https://devopsguys.files.wordpress.com/2015/05/trekfest_jpg.jpg?w=474" alt="Trekfest_jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The DevOpsGuys team are pulling up our hiking socks to raise cash for SSNAP and Countess Mountbatten Hospice this summer with a 13 mile walk across Wales. TrekFest it&amp;#8217;s no mean feat. We cross the highest peaks in the Beacons and South Wales including Pen y Fan (886m), Corn Du (873m), Cribyn (795m) and Fan y Big (719m).&amp;#160;&lt;a href="http://www.trekfest.org.uk/" target="_blank"&gt;http://www.trekfest.org.uk/&amp;#160;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a title="Just Giving" href="https://www.justgiving.com/Dev15"&gt;Click here&amp;#160;to donate now through our&amp;#160;Just Giving Page&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Our first charity is special since last year, one of our brave team members was diagnosed with terminal cancer. Unfortunately, and with great sadness we know now they are losing their fight &amp;#8211; even after enduring endless rounds of chemotherapy and surgery.&lt;/p&gt;
&lt;p&gt;The amazing staff at Countess Mountbatten Hospice are proving specialist&amp;#160;palliative (end of life) care to many fighting a losing battles against advanced stage cancer. They also support their families and loved ones. We&amp;#8217;d love to show our thanks and support by raising money on their behalf.&lt;/p&gt;
&lt;p&gt;It&amp;#8217;s with a tear in our eye and sadness in our hearts, that we chose our second charity to support. Last year, little Ollie lost his fight for his life after being born with a heart defect. He was 4 days old. His parents have displayed so much courage during an immensely difficult time during which the amazing doctors and nurses, support by the SSNAP team provided them with much needed support.&lt;/p&gt;
&lt;p&gt;It is their wish to continue to champion SSNAP, who provide support for the sick newborns and their parents at new born intensive care unit at The John Radcliffe Hospital, Oxford as so, we&amp;#8217;ll match whatever we raise through our JustGiving page as a donation to SSNAP.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;These amazing charities are truly special to us here DevOpsGuys.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Please support us in raising funds for these brilliant charities who have done so much to support our close friends and employees.&lt;/p&gt;
&lt;p&gt;The team have signed up to complete the distance in six hours. The trek takes place in the Brecon Beacons and covers the highest peaks in South Wales: Pen y Fan, Corn Du and Fan y Big. The Beacons are a training ground for the SAS so, while the DOGs will have their work cut out for them, they&amp;#8217;re more than up for the challenge:&lt;/p&gt;
&lt;p&gt;&amp;#8220;I can&amp;#8217;t wait to get out there&amp;#8221; says office manager Rhian Owen. &amp;#8220;It&amp;#8217;s such a great opportunity to work together as a team to achieve personal goals and to raise money for good causes &amp;#8211; it&amp;#8217;s going to be brilliant!&amp;#8221;&lt;/p&gt;
&lt;p&gt;We&amp;#8217;ve set up a Just Giving page, so you can show your support here. It&amp;#8217;s a chance to donate to some good causes and get the DevOpsGuys and gals out from behind their screens and into the beautiful Welsh wilderness &amp;#8211; come on y&amp;#8217;all, &lt;a href="https://www.justgiving.com/Dev15/?utm_id=8&amp;amp;fb_action_ids=753124678140173&amp;amp;fb_action_types=jgdonation%3Asupport&amp;amp;fb_ref=pfp-share-facebook-test-B-giving&amp;amp;fb_source=other_multiline&amp;amp;action_object_map=%5B971301802894069%5D&amp;amp;action_type_map=%5B%22jgdonation%3Asupport%22%5D&amp;amp;action_ref_map=%5B%22pfp-share-facebook-test-B-giving%22%5D"&gt;dig deep!&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;a title="Just Giving" href="https://www.justgiving.com/Dev15"&gt;Click here&amp;#160;to donate now through our&amp;#160;Just Giving Page&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:44:26 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:sponsored-dog-walk.html</guid></item><item><title>DevOpsGuys announce RedGate partnership</title><link>http://www.ciandcd.com/devopsguys-announce-redgate-partnership.html</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;More and more companies are now considering source control, continuous integration, and automated deployment for their database. To help them adopt each of these stages of Database Lifecycle Management (DLM), Redgate Software has launched a new partner program. Redgate Certified Consultants are now being trained in the USA, Europe and Australia &amp;#8211; and many of them will be familiar to SQL Server professionals.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The advantages of implementing any stage of DLM are many. Just as with Application Lifecycle Management (ALM), it speeds up the introduction of new features, and makes deployments reliable and error-free.&lt;/p&gt;
&lt;p&gt;But even though Redgate tools are designed to plug into the tools companies already use for their application d&lt;/p&gt;
&lt;p&gt;&amp;#160;&lt;/p&gt;
&lt;p&gt;evelopment, questions can arise during the implementation process.&lt;/p&gt;
&lt;p&gt;As Dan Wood of Northwest Cadence says: &amp;#8220;How can we take a system designed to develop, build and deliver applications and make it work with databases as well? It is a question that has plagued a vast majority of the clients I have worked with over the past few years.&amp;#8221;&lt;/p&gt;
&lt;p&gt;To address this issue, Redgate&amp;#8217;s new partner program is training expert consultants like Dan Wood in DLM &amp;#8211; and giving them the tools and support they need to help clients on-site, or in training sessions.&lt;/p&gt;
&lt;p&gt;The list of Certified Consultants is growing and already includes familiar faces like Ike Ellis and Northwest Cadence in the USA, The DevOpsGuys and Skelton Thatcher in the UK, and WARDY IT Solutions in Australia.&lt;/p&gt;
&lt;p&gt;As John Theron of Redgate points out, the advantages are clear. &amp;#8220;Redgate has spent a lot of time and effort joining the dots in DLM, and making it possible with a suite of dedicated tools, alongside learning materials and resources. The partner program complements this with a group of experts on the ground able to help companies on-site, and provide training in a series of public workshops.&amp;#8221;&lt;/p&gt;
&lt;p&gt;In places as far apart as Washington, London, San Diego, Philadelphia, Northern Ireland, and Baton Rouge, database professionals are how being trained in source control, continuous integration, and automated deployment for the database.&lt;/p&gt;
&lt;p&gt;A measure of the success the training is already achieving can be found in the reaction from database professionals like Jim Dorame. A Database Systems Manager for a large scale educational assessment corporation in the Greater Minneapolis area, he &lt;a href="http://www.jamesdorame.com/index.php/2014/08/22/continuous-integration-for-the-database-is-really-not-that-scary/"&gt;reviewed a continuous integration training day on his blog&lt;/a&gt;.&amp;#160; &amp;#8220;This tool makes the job of the DBA easier as there will be little doubt that the database is in a consistent and correct state. This alone makes me smile, I cannot tell you how many times I&amp;#8217;ve been executing a release and there was a piece missing that caused a failure.&amp;#8221;&lt;/p&gt;
&lt;p&gt;Further information about the training opportunities available can be found on the &lt;a href="http://www.red-gate.com/training/"&gt;Redgate training pages&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:44:25 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:devopsguys-announce-redgate-partnership.html</guid></item><item><title>Increase your ELK herd with Consul.io</title><link>http://www.ciandcd.com/increase-your-elk-herd-with-consulio.html</link><description>&lt;div&gt;&lt;p class="reblog-from"&gt;&lt;img alt="" src="http://0.gravatar.com/avatar/91f3566a38a35d6b2ed4c7b4b0d5e3d2?s=48&amp;amp;d=identicon&amp;amp;r=G" class="avatar avatar-48" height="48" width="48"&gt;Originally posted on &lt;a href="http://doics.co/2015/05/18/increase-your-elk-herd-with-consul-io"&gt;DevOps Is Common Sense...&lt;/a&gt;:&lt;/p&gt;&lt;p&gt;At work, I recently had a need to put in place a scalable logging solution based around the ELK stack.&lt;/p&gt;

&lt;p&gt;&lt;a href="http://elasticsearch-users.115913.n3.nabble.com/Java-client-unable-to-connect-ClusterBlockException-td4019825.html" title="Elasticsearch Users Mailing List"&gt;Issues&lt;/a&gt;&lt;a href="https://community.rackspace.com/products/f/18/t/4055" title="Rackspace Community Forums"&gt;with&lt;/a&gt;&lt;a href="http://jontai.me/blog/2013/03/elasticsearch-ec2-discovery/" title="How to get Elasticsearch configured in AWS"&gt;Multicast&lt;/a&gt;&lt;a href="http://elasticsearch-users.115913.n3.nabble.com/discovery-multicast-vs-unicast-td3689223.html" title="Elasticsearch Users &amp;amp;quot;Should I use Multicast or Unicast?&amp;amp;quot;"&gt;networking&lt;/a&gt; aside, &amp;#160;Elasticsearch scales pretty well on its own without the need for any additional overheads, however&amp;#160;discovering whether a node is online or not and connecting only to available nodes can be tricky.&lt;/p&gt;

&lt;p&gt;Scaling Logstash can be tricky, but it basically involves &lt;a href="http://serverfault.com/questions/459303/scaling-logstash-with-redis-elasticsearch" title="Advice on scaling Logstash from Stack Overflow"&gt;adding more Logstash servers&lt;/a&gt; to the mix and pointing them at your Elasticsearch cluster by &lt;a href="http://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-host" title="Logstash Documentation on adding multiple hosts"&gt;defining multiple hosts&lt;/a&gt; in your Logstash configuration.&lt;/p&gt;

&lt;p&gt;Kibana (like most web applications) can &lt;a href="http://www.elastic.co/guide/en/kibana/current/_setting_kibana_server_properties.html" title="Kibana Documentation"&gt;only have one Elasticsearch host defined&lt;/a&gt; in the config, so scaling out Kibana is more difficult.&lt;/p&gt;

&lt;p&gt;The above raises the question &amp;#8211; how do I know&amp;#160;which Elasticsearch node to point my configuration at if I don&amp;#8217;t know whether they are there or not.&lt;/p&gt;

&lt;p&gt;The answer came in the form of &lt;a href="http://consul.io/" title="Consul.io&amp;#x27;s Home Page"&gt;consul.io&lt;/a&gt;. &amp;#160;If you&amp;#8217;ve not looked at&amp;#8230;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:44:23 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:increase-your-elk-herd-with-consulio.html</guid></item><item><title>Steve Thair’s QCon Talk – now available online</title><link>http://www.ciandcd.com/steve-thairs-qcon-talk-now-available-online.html</link><description>&lt;div&gt;&lt;p&gt;&amp;#160;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://devopsguys.files.wordpress.com/2013/04/steve-profile.jpg"&gt;&lt;img class="aligncenter size-full wp-image-378" src="https://devopsguys.files.wordpress.com/2013/04/steve-profile.jpg?w=474" alt="steve-profile"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;DevOps and the Need for Speed, the talk from our very own Steve Thair is now available online. You can check it out &lt;a href="http://www.infoq.com/presentations/devops-speed"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Steve&amp;#8217;s just spoken at Krakow&amp;#8217;s &lt;a href="http://atmosphere-conference.com/"&gt;Atmosphere Conference&lt;/a&gt;. Stay tuned for more, coming soon.&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:44:18 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:steve-thairs-qcon-talk-now-available-online.html</guid></item><item><title>The Ops Mgr at QCon 2015</title><link>http://www.ciandcd.com/the-ops-mgr-at-qcon-2015.html</link><description>&lt;div&gt;&lt;p&gt;&lt;a href="https://devopsguys.files.wordpress.com/2013/04/steve-profile.jpg"&gt;&lt;img class="aligncenter size-full wp-image-378" src="https://devopsguys.files.wordpress.com/2013/04/steve-profile.jpg?w=474" alt="steve-profile"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&amp;#8220;When you&amp;#8217;re in a startup, the divide between Dev and Ops is normally the width of the desk&amp;#8230;it&amp;#8217;s far easier to collaborate in that small environment. In larger enterprises not only are they&amp;#160;in different buildings but they&amp;#8217;re in different countries with different cultures and different languages&amp;#8230;&amp;#8221;&lt;/p&gt;
&lt;p&gt;The DOG Ops Manager Steve Thair chats to Manuel Pais at QCon 2015.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.infoq.com/interviews/steve-thair-qcon-2015"&gt;Steve Thair at QCon&lt;/a&gt; to hear Steve talk about Enterprise DevOps; taking the first steps on the road to DevOps and &amp;#160;cultural change.&lt;/p&gt;
&lt;p&gt;&amp;#160;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:44:15 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:the-ops-mgr-at-qcon-2015.html</guid></item><item><title>DOGWalking in Brecon</title><link>http://www.ciandcd.com/dogwalking-in-brecon.html</link><description>&lt;div&gt;&lt;p&gt;&amp;#160;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://devopsguys.files.wordpress.com/2015/06/010_herecomethedog_s.png"&gt;&lt;img class="aligncenter size-medium wp-image-876" src="https://devopsguys.files.wordpress.com/2015/06/010_herecomethedog_s.png?w=300&amp;amp;h=193" alt="010_HereComeTheDog_s" width="300" height="193"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;So, aching, tired and happy the DOGs returned from &lt;a href="http://www.trekfest.org.uk/"&gt;TrekFest 2015&lt;/a&gt;&amp;#160;in the Welsh mountains having raised &amp;#163;1,143.00 for the Countess Mountbatten Hospice and SSNAP &amp;#8211; two charities close to the heart of the team. That&amp;#8217;s 114% of our initial target, so a huge thank you to everyone who donated so generously.&lt;/p&gt;
&lt;p&gt;The weather could not have been better for a long, scenic ramble in one of the UKs most beautiful spots; beautifully sunny with a refreshing breeze kept the team going. We completed the trek in approximately 5 and a half hours, just in time for a piece of cake and a glass of celebratory champagne at the finish line.&lt;/p&gt;
&lt;p&gt;Everyone had a thoroughly enjoyable time and we&amp;#8217;re all looking forward to the next DOG adventure!&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:44:13 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:dogwalking-in-brecon.html</guid></item><item><title>DOGs at Digital 2015</title><link>http://www.ciandcd.com/dogs-at-digital-2015.html</link><description>&lt;div&gt;&lt;p&gt;&lt;a href="https://devopsguys.files.wordpress.com/2015/06/digital-2015.jpg"&gt;&lt;img class="aligncenter size-medium wp-image-879" src="https://devopsguys.files.wordpress.com/2015/06/digital-2015.jpg?w=300&amp;amp;h=102" alt="Digital 2015" width="300" height="102"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Last week the DevOpsGuys headed up to Newport&amp;#8217;s Celtic Manor to take part in Digital 2015 &amp;#8211; the Welsh Government&amp;#8217;s initiative to bring digital innovators and business professionals together. The 2-day event saw more than 2,000 delegates and 140 speakers.&lt;/p&gt;
&lt;p&gt;DevOpsGuy co-founder Steve Thair says:&lt;/p&gt;
&lt;p&gt;&amp;#8220;These initiatives are invaluable to the digital sector because they expose the wide variety of digital and technological services that are available in South Wales to business professionals who can use them to take online business services to the next level. It&amp;#8217;s a relaxed environment where people can chat and form connections that will have a direct impact on the future of business and technology in Wales.&amp;#8221;&lt;/p&gt;
&lt;p&gt;The diverse range of speakers at the event included Microsoft, the WRU, Amazon and the DVLA. The opportunity to discuss the needs of businesses directly with those running them is invaluable. This dialogue can lead to collaborative projects and further development of the burgeoning tech industry in the area.&lt;/p&gt;
&lt;p&gt;The team are excited to see more events like this one springing up in the near future. Look out for us at the up-coming Agile Cymru in the Wales Millennium Centre on the 7th and 8th of July.&lt;/p&gt;
&lt;p&gt;&amp;#160;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:44:10 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:dogs-at-digital-2015.html</guid></item><item><title>An example of preparatory refactoring</title><link>http://www.ciandcd.com/an-example-of-preparatory-refactoring.html</link><description>&lt;div&gt;&lt;p&gt;There are various ways in which &lt;a href="/articles/workflowsOfRefactoring/"&gt;refactoring can fit into our programming
    workflow&lt;/a&gt;. One useful notion is that of Preparatory
    Refactoring. This is where I'm adding a new feature, and I see
    that the existing code is not structured in such a way that makes
    adding the feature easy. So first I refactor the code into the
    structure that makes it easy to add the feature, or as Kent Beck
    pithily put it &lt;a href="https://twitter.com/kentbeck/status/250733358307500032"&gt;"make the change easy, then
    make the easy change"&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In a recent Ruby Rogues podcast, &lt;a href="http://jessitron.com"&gt;Jessica
    Kerr&lt;/a&gt; gave a lovely metaphor for preparatory refactoring.
    &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;It&amp;#8217;s like I want to go 100 miles east but instead of just
      traipsing through the woods, I&amp;#8217;m going to drive 20 miles north
      to the highway and then I&amp;#8217;m going to go 100 miles east at three
      times the speed I could have if I just went straight there. When
      people are pushing you to just go straight there, sometimes you
      need to say, &amp;#8220;Wait, I need to check the map and find the
      quickest route.&amp;#8221; The preparatory refactoring does that for
      me.&lt;/p&gt;

&lt;p class="quote-attribution"&gt;&lt;a href="http://devchat.tv/ruby-rogues/178-rr-book-club-refactoring-ruby-with-martin-fowler"&gt;-- Jessica Kerr&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;img src="preparatory-refactoring-example/jessitron.png" width="600"&gt;
&lt;p class="photoCaption"&gt;&lt;/p&gt;
&lt;p&gt;Another good metaphor I've come across is putting tape over
    electrical sockets, door frames, skirting boards and the like
    when painting a wall. The taping isn't doing the painting, but by
    spending the time to cover things up first, the painting can be
    much quicker and easier.&lt;/p&gt;

&lt;p&gt;General statements and metaphors are all very nice, but it's
    good to show an example. And recently I ran into an example myself
    that I thought may be worth sharing. &lt;/p&gt;


&lt;h2&gt;The Starting Point&lt;/h2&gt;

&lt;p&gt;My publication toolchain includes the ability to insert code
      from a live file into an article. By a "live file", I mean code
      that compiles and runs, usually a pedagogical (ie toy) example.
      Being able to slurp code from live files is really helpful as it
      avoids copy-paste problems and gives me confidence that the code in
      the article is code that actually compiles and passes tests. I
      mark the code sections by markers contained in comments.
      &lt;a href="#footnote-irony"&gt;[1]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;One thing I'm adding just now is the ability to highlight
      specific parts of these code fragments. This way I can take a
      line, or part of a line of code, and surround it with span
      elements in the html, allowing me to then use css to highlight
      it any way I want. You'll see examples of me doing this later on
      in this article, as it's particularly handy when discussing
      refactorings.&lt;/p&gt;

&lt;p&gt;At the start of the programming episode I'm talking about
      here, I already had the ability to highlight a given line, or a
      span of code within a given line. I wanted to add a third
      capability, to highlight a range of lines.&lt;/p&gt;

&lt;p&gt;In the source document for my article, I indicate I want to
      insert a code fragment with an &lt;code&gt;insertCode&lt;/code&gt; XML
      element. My
      current highlighting then allows me to define a bunch of
      highlights. Here's an example&lt;/p&gt;

&lt;pre&gt;
&amp;lt;insertCode file = "Notification.java" fragment = "notification-with-error"&amp;gt;
  &amp;lt;highlight line="add\(" span="new.*e\)"/&amp;gt;
  &amp;lt;highlight line="map"/&amp;gt;
&amp;lt;/insertCode&amp;gt;
&lt;/pre&gt;

&lt;p&gt;This highlights some code like this&lt;/p&gt;

&lt;pre&gt;public void addError(String message, Exception e) {
  errors.add(&lt;p class="highlight"&gt;new Error(message, e)&lt;/p&gt;);
}

public String errorMessage() {
  return errors.stream()
&lt;p class="highlight"&gt;          .map(e -&amp;gt; e.message)&lt;/p&gt;
          .collect(Collectors.joining(", "));
}&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;insertCode&lt;/code&gt; element has attributes for the
      file path and the name of the fragment that I want to extract. I
      can then specify highlights with child elements. Each highlight
      specifies a line by providing a regexp which I use to match the
      line. I may provide a span attribute, another regexp, in which
      case the highlighting is only for the part of the line that
      matches that regexp. If I don't provide a span the highlight is
      applied to the entire line.&lt;/p&gt;

&lt;p&gt;I had put the code that does the highlighting into its own
      class. Some separate code (which we don't need to worry about)
      extracts the code fragment from the source file, it then looks to see
      if we need any highlighting, if so it creates a CodeHighlighter
      object and tells it to do the highlighting. The invocation of
      the code higlighter looks something like this:&lt;/p&gt;

&lt;pre&gt;output &amp;lt;&amp;lt; CodeHighlighter.new(insertCodeElement, codeFragment).call
&lt;/pre&gt;

&lt;p&gt;This is using the method object pattern, where I use an
      object to represent a first-class function. I create the object with
      the arguments to that function, invoke another method to run the
      function which returns the result, and then let the method
      object be garbage collected away.&lt;/p&gt;

&lt;p&gt;Here's the implementation of that highlighter:&lt;/p&gt;

&lt;pre&gt;class CodeHighlighter
  def initialize insertCodeElement, fragment
    @data = insertCodeElement
    @fragment = fragment
  end
  def call
    @fragment.lines.map{|line| highlight_line line}.join
  end
  def highlight_line line
    highlights
      .select{|h| Regexp.new(h['line']).match(line)}
      .reduce(line){|acc, each| apply_markup acc, each}
  end
  def highlights
    @data.css('highlight')
  end
  def apply_markup line, element
    open = "&amp;lt;span class = 'highlight'&amp;gt;"
    close = "&amp;lt;/span&amp;gt;"                 
    if element.key? 'span'
      r = Regexp.new(element['span'])
      m = r.match line
      m.pre_match + open + m[0] + close + m.post_match
    else
      open + line.chomp + close + "\n"
    end
  end
end
&lt;/pre&gt;

&lt;p class="code-remark"&gt;The toolchain code is ruby, it uses the
      &lt;a href="http://www.nokogiri.org"&gt;Nokogiri&lt;/a&gt; library to manipulate XML&lt;/p&gt;

&lt;p&gt;I haven't worried much about edge cases here, such as if I
      specify multiple highlights that overlap in ways that will mess
      up the display. After all, if I run into any of these problems I
      know where I live. Such is the luxury of writing code where I'm
      the only user.&lt;/p&gt;

&lt;h2&gt;Testing the highlighter&lt;/h2&gt;

&lt;p&gt;Testing the highlighter is pretty simple, it's a pure function
     that takes some input and emits some output. But there's a little
     machinery I put in my test class to make it easier to write
     tests. The first thing is that I like to keep the hunks of text
     for input and output in a separate file, which looks like
     this&lt;/p&gt;

&lt;p class="code-label"&gt;codeHighlighterHunks.txt&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  input
  
    private void validateDate(Notification note) {
      if (date == null) {
        note.addError("date is missing");
        return;
      }
  
      LocalDate parsedDate;
      try {
        parsedDate = LocalDate.parse(getDate());
      }
    } //end
  
  &lt;p class="highlight"&gt;%%&lt;/p&gt; one-line
  
    private void validateDate(Notification note) {
      if (date == null) {
  &amp;lt;span class = 'highlight'&amp;gt;      note.addError("date is missing");&amp;lt;/span&amp;gt;
        return;
      }
  
      LocalDate parsedDate;
      try {
        parsedDate = LocalDate.parse(getDate());
      }
    } //end
  
  &lt;p class="highlight"&gt;%%&lt;/p&gt; one-span
  
    private void validateDate(Notification note) {
      if (date == null) {
        note.&amp;lt;span class = 'highlight'&amp;gt;addError&amp;lt;/span&amp;gt;("date is missing");
        return;
      }
  
      LocalDate parsedDate;
      try {
        parsedDate = LocalDate.parse(getDate());
      }
    } //end
  &amp;#8230;&lt;/pre&gt;

&lt;p&gt;Here you see three hunks of text, separated by
     &lt;code&gt;%%&lt;/code&gt;. The first hunk is my (first) input string, the
     next two are outputs for what happens with one line, and one span
     within a line. Each hunk has a key, which is the text following
     the &lt;code&gt;%%&lt;/code&gt; on the line. I can then easily get at the
     hunks in my tester class&lt;/p&gt;

&lt;p class="code-label"&gt;class CodeHighlighterTester&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def hunks
    raw = File.read('test/codeHighlighterHunks.txt').split("\n%%")
    raw.map {|r| process_raw_hunk r}.to_h
  end
  def process_raw_hunk hunk
    lines = hunk.lines
    key = lines.first.strip
    value = lines
      .drop(1)
      .drop_while {|line| (/[^[:space:]]/ !~ line)}
      .join
    return [key, value]
  end
&lt;/pre&gt;

&lt;p&gt;With the ability to extract hunks easily, I can then reference
     them in my tests.&lt;/p&gt;

&lt;p class="code-label"&gt;class CodeHighlighterTester&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def test_no_highlights
    assert_equal hunks['input'], with_highlights(form_element(""))
  end
  def test_one_line_highlight
    element = form_element "&amp;lt;highlight line = 'missing'/&amp;gt;"
    assert_equal hunks['one-line'], with_highlights(element)
  end
  def test_highlight_span
    element = form_element "&amp;lt;highlight line = 'missing' span = 'addError'/&amp;gt;"
    assert_equal hunks['one-span'], with_highlights(element)
  end
  def form_element s
    Nokogiri::XML("&amp;lt;insertCode&amp;gt;" + s + "&amp;lt;/insertCode").root
  end  
  def with_highlights element, input = nil
    input ||= hunks['input']
    CodeHighlighter.new(element,input).call
  end
&lt;/pre&gt;

&lt;p&gt;I could have used multi-line strings or here docs for
     this, but I think hunks of text are easier to work with.&lt;/p&gt;

&lt;h2&gt;Adding a highlight range&lt;/h2&gt;

&lt;p&gt;The new feature I wanted to add was to highlight a range of
     lines like this.&lt;/p&gt;

&lt;pre&gt;&amp;lt;insertCode file = "BookingRequest.java" fragment = "done"&amp;gt;
  &amp;lt;highlight-range start-line = "missing" end-line = "return"/&amp;gt;
&amp;lt;/insertCode&amp;gt;
&lt;/pre&gt;

&lt;p&gt;The start-line and end-line attributes are again regexps, to
     match the first and last lines in the range.&lt;/p&gt;

&lt;p&gt;I started by adding a test for the new markup behavior,
     checked that it failed, then marked it to be skipped. I like to
     start by writing the test for the final behavior I want, since
     that clarifies to me both exactly what the outcome I want is, and
     also how I want my API to work. But if I'm going to do any
     preparatory refactoring, I don't want that test's failure to
     clutter my test output, so after watching it fail once, I skip it while I'm working on it.&lt;/p&gt;

&lt;p class="code-label"&gt;class CodeHighlighterTester&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def test_highlight_range
    skip
    e = '&amp;lt;highlight-range start-line = "(date == null)" end-line = "}"/&amp;gt;'
    assert_equal hunks['range'], with_highlights(form_element(e))    
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;codeHighlighterHunks.txt&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  %% range
  
    private void validateDate(Notification note) {
  &amp;lt;span class = 'highlight'&amp;gt;    if (date == null) {
        note.addError("date is missing");
        return;
      }&amp;lt;/span&amp;gt;
  
      LocalDate parsedDate;
      try {
        parsedDate = LocalDate.parse(getDate());
      }
    } //end&lt;/pre&gt;

&lt;p&gt;As I was thinking about how to approach it, I began by
     deciding that I could treat the code highlighting as a sequence
     of transformations on the supplied text. I could first apply any
     highlight-range transformation, and then follow them with the
     existing highlights. I can now transfer that thought from my mind
     to the code.&lt;/p&gt;

&lt;p&gt;My first step is to simply use &lt;a href="http://refactoring.com/catalog/extractMethod.html"&gt;Extract Method&lt;/a&gt; on the entire body of &lt;code&gt;call&lt;/code&gt;&lt;/p&gt;

&lt;p class="code-label"&gt;class CodeHighlighter&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def call 
&lt;p class="highlight"&gt;    apply_highlights @fragment.lines&lt;/p&gt;
  end
  def apply_highlights lines
    lines.map{|line| highlight_line line}.join
  end&lt;/pre&gt;

&lt;p&gt;Now I introduce a nested, no-op function - that is one that
     just returns what you give it, without any change.&lt;/p&gt;

&lt;p class="code-label"&gt;class CodeHighlighter&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def call
    apply_highlights(&lt;p class="highlight"&gt;apply_ranges&lt;/p&gt;(@fragment.lines))
  end
  def apply_ranges lines
    lines
  end&lt;/pre&gt;

&lt;p&gt;This single refactoring is really the essence of this whole
     article, boiled down to a simple step. With this refactoring I'm
     doing a couple of things. First, by placing the
     &lt;code&gt;apply_ranges&lt;/code&gt; method into the call, I'm making a
     place for my new functionality to go. But secondly, and perhaps
     more importantly, I'm immediately implementing this new function
     in such a way that it preserves the current behavior. To some
     extent, this ability to easily insert placeholder functions is one
     of the great advantages of structuring the highlighting
     behavior as a series of smaller transformations - which is one of
     the reasons why the &lt;a href="http://www.amazon.com/gp/product/0471958697?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=0471958697"&gt;Pipes and Filters
     pattern&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt; is such powerful way of structuring computation.
     &lt;/p&gt;

&lt;h2&gt;The Two Hats&lt;/h2&gt;

&lt;img src="preparatory-refactoring-example/two-hats.png" width="200"&gt;
&lt;p class="photoCaption"&gt;&lt;/p&gt;
&lt;p&gt;In &lt;a href="/books/refactoring.html"&gt;the Refactoring book&lt;/a&gt;, I passed on Kent's metaphor of the
       two hats. His notion is that when you're programming, you can
       operate in one of two modes: refactoring and adding function.
       When you're wearing the refactoring hat, every change you make
       preservers observable behavior, keeps the tests green, and
       allows you to make many small changes without going near a
       debugger. &lt;/p&gt;

&lt;p&gt;When you add function, however, things are more open
       ended as you will add tests and break existing tests. The
       adding function hat is more stressful and riskier, so it's nice
       to wear the refactoring hat as much as possible.&lt;/p&gt;
&lt;p&gt;By defining &lt;code&gt;apply_ranges&lt;/code&gt; with this simple
     behavior-preserving implementation, as opposed to just leaving it
     blank, I can continue to run my tests and keep my refactoring hat
     on.&lt;/p&gt;

&lt;p&gt;I may have any number of highlight-range elements to apply, so
     I'll let each one compose on top the others.&lt;/p&gt;

&lt;p class="code-label"&gt;class CodeHighlighter&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def apply_ranges lines
    highlight_ranges.reduce(lines){|acc, each| apply_one_range(acc, each)}
  end
  def highlight_ranges
    @data.css('highlight-range')
  end
  def apply_one_range lines, element
    lines
  end&lt;/pre&gt;

&lt;p&gt;You'll see I'm doing the same trick again, I implement
     &lt;code&gt;apply_ranges&lt;/code&gt; by reducing the result of running
     &lt;code&gt;apply_one_range&lt;/code&gt; for each highlight-range element. I
     provide an initial implementation of &lt;code&gt;apply_one_range&lt;/code&gt;
     that preserves existing behavior and get to keep my trilby
     on. What I'm doing is steadily narrowing down the scope of the change in
     behavior I'm about to add.&lt;/p&gt;

&lt;p&gt;At this point, I add a no-op test for the highlight range
     condition.&lt;/p&gt;

&lt;p class="code-label"&gt;class CodeHighlighterTester&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def test_highlight_range_noop
    e = '&amp;lt;highlight-range start-line = "(date == null)" end-line = "}"/&amp;gt;'
    assert_equal &lt;p class="highlight"&gt;hunks['input']&lt;/p&gt;, with_highlights(form_element(e))    
  end
&lt;/pre&gt;

&lt;p&gt;This might seem like an odd move, essentially all this test
     says is that when I add a highlight-range element, I don't want
     any changes to the output. This is a temporary test, just while
     I'm working on the preparatory refactoring. While I'm doing this
     refactoring, I'm operating on the assumption that the
     refactorings I'm doing will result in no changes, even when the
     element is present. So I want to confirm that assumption with a
     test, since it's so easy to write. (This follows a general rule
     of mine: if I ever feel the urge to run the code and look at some
     output to see if things are correct, I should instead write a
     test. With a test the computer can check if the output is
     correct, so I don't have to.)&lt;/p&gt;

&lt;p&gt;My next move is back to the highlighter itself. I've now
     isolated a method to highlight a single range. I think that a good
     thing to do next is to identify the line where I want to add the opening tag,
     and split the lines into three lists: before the matched line,
     the line alone, and after the matched line. I'll worry about the
     closing tag later.&lt;/p&gt;

&lt;p class="code-label"&gt;class CodeHighlighter&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def apply_one_range lines, element
    start_ix = lines.find_index {|line| line =~ Regexp.new(element['start-line'])}
    pre = 0 == start_ix ? [] : lines[0..(start_ix - 1)]
    start = [lines[start_ix]]
    rest = lines.size == (start_ix + 1) ? [] : lines[(start_ix + 1)..-1]
    return pre + start + rest
  end&lt;/pre&gt;

&lt;p&gt;
       The essence of this refactoring is breaking down the text and
       putting it back together again, until I've broken it down to
       the right point to slip in the new behavior.
     &lt;/p&gt;
&lt;p&gt;By doing this I can test that I can correctly break the list
     of lines into pieces and put them back together. Since there
     aren't always three pieces, this is a bit more awkward than you
     might first think. Since I had to put conditional logic in to
     check if the range started on the first or last-but-one line, I
     added some tests to check for these cases.&lt;/p&gt;

&lt;p&gt;I'm only checking for the opening so far, and am almost ready
     to actually change the observable behavior, but first I need to
     move the html span strings into something at object
     scope.&lt;/p&gt;

&lt;p class="code-label"&gt;class CodeHighlighter&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def apply_markup line, element
&lt;p class="deleted"&gt;    open = "&amp;lt;span class = 'highlight'&amp;gt;"&lt;/p&gt;
&lt;p class="deleted"&gt;    close = "&amp;lt;/span&amp;gt;"&lt;/p&gt;
    if element.key? 'span'
      r = Regexp.new(element['span'])
      m = r.match line
      raise "unable to match span %s" % element['span'] unless m
      m.pre_match + opening + m[0] + closing + m.post_match
    else
      opening + line.chomp + closing + "\n"
    end
  end

  def opening
    "&amp;lt;span class = 'highlight'&amp;gt;"
  end
  def closing
    "&amp;lt;/span&amp;gt;"
  end
&lt;/pre&gt;

&lt;p&gt;I could make them constants, but it's my habit to just use
     methods in this situation. &lt;a href="#footnote-css"&gt;[2]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Now I'm finally ready to put my hard-hat on, and the change I
     need to make is too trivial to be easy.&lt;/p&gt;

&lt;p class="code-label"&gt;class CodeHighlighter&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def apply_one_range lines, element
    start_ix = lines.find_index {|line| line =~ Regexp.new(element['start-line'])}
    raise "unable to match %s in code insert" % element['start-line'] unless start_ix
    pre = 0 == start_ix ? [] : lines[0..(start_ix - 1)]
    start = [&lt;p class="highlight"&gt;opening +&lt;/p&gt; lines[start_ix]]
    rest = lines.size == (start_ix + 1) ? [] : lines[(start_ix + 1)..-1]
    return pre + start + rest
  end&lt;/pre&gt;

&lt;p&gt;I now remove the no-op test I added a couple of minutes ago,
     and modify the skipped test so it only includes the opening. &lt;/p&gt;

&lt;p class="code-label"&gt;class CodeHighlighterTester&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def test_highlight_range
    &lt;p class="deleted"&gt;skip&lt;/p&gt;
    e = '&amp;lt;highlight-range start-line = "(date == null)" end-line = "}"/&amp;gt;'
    assert_equal hunks['range'], with_highlights(form_element(e))    
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;codeHighlighterHunks.txt&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  %% range
  
    private void validateDate(Notification note) {
  &amp;lt;span class = 'highlight'&amp;gt;    if (date == null) {
        note.addError("date is missing");
        return;
      }&lt;p class="deleted"&gt;&amp;lt;/span&amp;gt;&lt;/p&gt;
  
      LocalDate parsedDate;
      try {
        parsedDate = LocalDate.parse(getDate());
      }
    } //end&lt;/pre&gt;

&lt;p&gt;This testing allows me to do a little preparatory refactoring
     before I add the closing tag.&lt;/p&gt;

&lt;p class="code-label"&gt;class CodeHighlighter&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def apply_one_range lines, element
    start_ix = lines.find_index {|line| line =~ Regexp.new(element['start-line'])}
    raise "unable to match %s in code insert" % element['start-line'] unless start_ix
&lt;p class="highlight"&gt;    finish_offset = lines[start_ix..-1].find_index do |line| 
      line =~ Regexp.new(element['end-line'])
    end
    raise "unable to match %s in code insert" % element['end-line'] unless finish_offset
    finish_ix = start_ix + finish_offset&lt;/p&gt;
    pre = 0 == start_ix ? [] : lines[0..(start_ix - 1)]
    start = [opening + lines[start_ix]]
&lt;p class="highlight"&gt;    mid = (lines[(start_ix + 1)..(finish_ix -1)])
    finish = [lines[finish_ix]]
    rest = lines.size == (finish_ix + 1) ? [] : lines[(finish_ix + 1)..-1]&lt;/p&gt;
    return pre + start + &lt;p class="highlight"&gt;mid + finish&lt;/p&gt; + rest
  end&lt;/pre&gt;

&lt;p&gt;The method is rather long for my taste, but I can't think of
     how to sensibly shorten it. It does keep everything green and set
     things up for my final easy change.&lt;/p&gt;

&lt;p class="code-label"&gt;class CodeHighlighter&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def apply_one_range lines, element
    start_ix = lines.find_index {|line| line =~ Regexp.new(element['start-line'])}
    raise "unable to match %s in code insert" % element['start-line'] unless start_ix
    finish_offset = lines[start_ix..-1].find_index do |line| 
      line =~ Regexp.new(element['end-line'])
    end
    raise "unable to match %s in code insert" % element['end-line'] unless finish_offset
    raise "start and end match same line" unless finish_offset &amp;gt; 0
    finish_ix = start_ix + finish_offset
    pre = 0 == start_ix ? [] : lines[0..(start_ix - 1)]
    start = [opening + lines[start_ix]]
    mid = (lines[(start_ix + 1)..(finish_ix -1)])
&lt;p class="highlight"&gt;    finish = [lines[finish_ix].chomp + closing + "\n"]&lt;/p&gt;
    rest = lines.size == (finish_ix + 1) ? [] : lines[(finish_ix + 1)..-1]
    return pre + start + mid + finish + rest
  end
&lt;/pre&gt;

&lt;h2&gt;Final Thoughts&lt;/h2&gt;

&lt;p&gt;I hope this little episode has given you some sense of what
     preparatory refactoring can be like: &lt;/p&gt;

&lt;blockquote class="twitter-tweet" lang="en"&gt;
       for each desired change, make the change easy (warning: this may be hard), then make the easy change
     &lt;a href="https://twitter.com/kentbeck/status/250733358307500032"&gt;-- Kent Beck&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;I made the change easy by creating a no-op function that
     simply returned what it was given, and then decomposing that
     function, gradually breaking it down while still retaining its
     no-opiness. Then once it was simple to add the new feature, it just
     slipped in.&lt;/p&gt;

&lt;p&gt;Every episode of preparatory refactoring is different. Some take
     a few minutes, some can take days. But I find that when I can
     spot how to do a preparatory refactoring it results in a faster
     and less-stressful programming experience, because the trilby is 
     faster and less stressful than the hard hat.&lt;/p&gt;


&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/articles/preparatory-refactoring-example.html&amp;amp;text=An%20example%20of%20preparatory%20refactoring" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/articles/preparatory-refactoring-example.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/articles/preparatory-refactoring-example.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;For articles on similar topics&amp;#8230;&lt;/h2&gt;

&lt;p&gt;&amp;#8230;take a look at the tag: &lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:44:06 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:an-example-of-preparatory-refactoring.html</guid></item><item><title>Bliki: DiversityMediocrityIllusion</title><link>http://www.ciandcd.com/bliki-diversitymediocrityillusion.html</link><description>&lt;div&gt; 

 

 

&lt;p class="tagLabel"&gt;tags:&lt;/p&gt;

&lt;p class="clear"&gt;&lt;/p&gt;

&lt;p&gt;I've often been involved in discussions about deliberately
  increasing the diversity of a group of people. The most common case
  in software is increasing the proportion of women. Two examples are
  in hiring and conference speaker rosters where we discuss trying to
  get the proportion of women to some level that's higher than usual.
  A common argument against pushing for greater diversity is that it
  will lower standards, raising the spectre of a diverse but mediocre
  group.&lt;/p&gt;

&lt;p&gt;To understand why this is an illusionary concern, I like to
  consider a little thought experiment. Imagine a giant bucket that
  contains a hundred thousand marbles. You know that 10% of these
  marbles have a special sparkle that you can see when you carefully
  examine them. You also know that 80% of these marbles are blue and
  20% pink, and that sparkles exist evenly across both colors &lt;a href="#footnote-sparkle"&gt;[1]&lt;/a&gt;. If you were
  asked to pick out ten sparkly marbles, you know you could
  confidently go through some and pick them out. So now imagine you're
  told to pick out ten marbles such that five were blue and five were
  pink.&lt;/p&gt;
&lt;img src="images/diversityMediocrityIllusion/sketch.png"&gt;
&lt;p&gt;I don't think you would react by saying &amp;#8220;that's impossible&amp;#8221;.
  After all there are two thousand pink sparkly marbles in there,
  getting five of them is not beyond the wit of even a man. Similarly
  in software, there may be less women in the software business, but
  there are still enough good women to fit the roles a company or a
  conference needs.&lt;/p&gt;

&lt;p&gt;The point of the marbles analogy, however, is to focus on the
  real consequence of the demand for 50:50 split. Yes it's possible to
  find the appropriate marbles, but the downside is that it takes
  longer. &lt;a href="#footnote-non-binary"&gt;[2]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;That notion applies to finding the right people too. Getting
  a better than base proportion of women isn't impossible, but it does
  require more work, often much more work. This extra effort
  reinforces the rarity, if people have difficulty finding good
  people as it is, it needs determined effort to spend the extra time
  to get a higher proportion of the minority group &amp;#8212; even if you are
  only trying to raise the proportion of women up to 30%, rather than
  a full 50%.&lt;/p&gt;

&lt;p&gt;In recent years we've made increasing our diversity a high
  priority at ThoughtWorks. This has led to a lot of effort trying to
  go to where we are more likely to run into the talented women we are
  seeking: women's colleges, women-in-IT groups and conferences. We
  encourage our women to speak at conferences, which helps let other
  women know we value a diverse workforce. &lt;/p&gt;

&lt;p&gt;When interviewing, we make a point of ensuring there are women
  involved. This gives women candidates someone to relate to, and
  someone to ask questions which are often difficult to ask men. It's
  also vital to have women interview men, since we've found that women
  often spot problematic behaviors that men miss as we just don't have
  the experiences of subtle discriminations. Getting a diverse group
  of people inside the company isn't just a matter of recruiting, it
  also means paying a lot of attention to the environment we have, to try to
  ensure we don't have the same &lt;a href="AlienatingAtmosphere.html"&gt;AlienatingAtmosphere&lt;/a&gt; that
  much of the industry exhibits. &lt;a href="#footnote-client-atmosphere"&gt;[3]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;One argument I've heard against this approach is that if everyone
  did this, then we would run out of pink, sparkly marbles. We'll know
  this is something to be worried about when women are paid
  significantly more than men for the same work.&lt;/p&gt;

&lt;p&gt;One anecdote that stuck in my memory was from a large,
  traditional company who wanted to improve the number of women in
  senior management positions. They didn't impose a quota on
  appointing women to those positions, but they did impose a quota for
  women on the list of candidates. (Something like: "there must be at
  least three credible women candidates for each post".) This
  candidate quota forced the company to actively seek out women
  candidates. The interesting point was that just doing this, with no
  mandate to actually appoint these women, correlated with an increased
  proportion of women in those positions.&lt;/p&gt;

&lt;p&gt;For conference planning it's a similar strategy:  just putting out a call for
  papers and saying you'd like a diverse speaker lineup isn't enough.
  Neither are such things as blind review of proposals (and I'm not
  sure that's a good idea anyway). The important thing is to seek out
  women and encourage them to submit ideas. Organizing conferences is
  hard enough work as it is, so I can sympathize with those that don't
  want to add to the workload, but those that do can get there. &lt;a href="http://continuousdelivery.com/2013/09/how-we-got-40-female-speakers-at-flowcon/"&gt;FlowCon
  is a good example&lt;/a&gt; of a conference that made this an explicit
  goal and did far better than the industry average (and in case you
  were wondering, there was &lt;a href="http://continuousdelivery.com/2013/12/flowcon-2013-wrap-up/"&gt;no
  difference between men's and women's evaluation scores&lt;/a&gt;). &lt;/p&gt;

&lt;p&gt;So now that we recognize that getting greater diversity is a
  matter of application and effort, we can ask ourselves whether the
  benefit is worth the cost. In a broad professional sense, I've
  argued that it is, because our &lt;a href="DiversityImbalance.html"&gt;DiversityImbalance&lt;/a&gt; is
  reducing our ability to bring the talent we need into our profession,
  and reducing the influence our profession needs to have on society.
  In addition I believe there is a moral argument to push back against
  long-standing wrongs faced by
  &lt;a href="HistoricallyDiscriminatedAgainst.html"&gt;HistoricallyDiscriminatedAgainst&lt;/a&gt; groups. &lt;/p&gt;

&lt;p&gt;Conferences have an important role to play in correcting this
  imbalance. The roster of speakers is, at least subconsciously, a
  statement of what the profession should look like. If it's all white
  guys like me, then that adds to the &lt;a href="AlienatingAtmosphere.html"&gt;AlienatingAtmosphere&lt;/a&gt;
  that pushes women out of the profession. Therefore I believe that
  conferences need to strive to get an increased proportion of
  historically-discriminated-against speakers. We, as a profession,
  need to push them to do this. It also means that women have an
  extra burden to become visible and act as part of that better
  direction for us. &lt;a href="#footnote-burden"&gt;[4]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For companies, the choice is more personal. For me,
  ThoughtWorks's efforts to improve its diversity are a major factor
  in why I've been an employee here for over a decade. I don't think
  it's a coincidence that ThoughtWorks is also a company that has a
  greater open-mindedness, and a lack of political maneuvering, than most of the
  companies I've consulted with over the years. I consider those
  attributes to be a considerable competitive advantage in attracting
  talented people, and providing an environment where we can
  collaborate effectively to do our work. &lt;/p&gt;

&lt;p&gt;But I'm not holding ThoughtWorks up as an example of perfection.
  We've made a lot of progress over the decade I've been here, but we
  still have a long way to go. In particular we are very short of
  senior technical women. We've introduced a number of programs around
  networks, and leadership development, to help grow women
  to fill those gaps. But these things take time - all you have to do
  is look at our &lt;a href="http://www.thoughtworks.com/radar/faq"&gt;Technical Advisory Board&lt;/a&gt;
  to see that we are a long way from the ratio we seek. &lt;/p&gt;

&lt;p&gt;Despite my knowledge of how far we still have to climb, I can
  glimpse the summit ahead. At a recent AwayDay in Atlanta I was
  delighted to see how many younger technical women we've managed to
  bring into the company. While struggling to keep my head above water
  as the sole male during a late night game of &lt;a href="/articles/eurogames/"&gt;Dominion&lt;/a&gt;, I enjoyed a
  great feeling of hope for our future.&lt;/p&gt;

 

&lt;p class="acknowledgements"&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;

  
 Camila Tartari, Carol Cintra, Dani Schufeldt, Derek Hammer, Isabella
  Degen, Korny Sietsma, Lindy Stephens, Mridula Jayaraman, Nikki
  Appleby, Rebecca Parsons, Sarah Taraporewalla, Stefanie Tinder, and Suzi
  Edwards-Alexander
  
  commented on
  drafts of this article.
&lt;/p&gt;

&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/bliki/DiversityMediocrityIllusion.html&amp;amp;text=Bliki:%20DiversityMediocrityIllusion" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/bliki/DiversityMediocrityIllusion.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/bliki/DiversityMediocrityIllusion.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:43:39 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:bliki-diversitymediocrityillusion.html</guid></item><item><title>Using oauth for a simple command line script to access Google's data</title><link>http://www.ciandcd.com/using-oauth-for-a-simple-command-line-script-to-access-googles-data.html</link><description>&lt;div&gt;&lt;p&gt;I recently needed to write a simple script to pull some data
    from a Google website. Since I was grabbing some private data, I
    needed authorize myself to do that. I found it much more work than
    I expected, not because it's hard, but because there wasn't much
    documentation there to guide me - I had to puzzle out what path
    to go based on lots of not particularly relevant documentation. So
    once I'd figured it out I decided to write a short account of what
    I'd done, partly in case I need to do this again, and partly to
    help anyone else who wants to do this.&lt;/p&gt;

&lt;p&gt;First a disclaimer. This is what I figured out, it works for
    me, at the moment. I haven't done extensive research of whether
    this is the best way to do what I want (although it sure felt like
    extensive research while I was doing it). So bear that in mind.
    (And if you have better ways do let me know.)&lt;/p&gt;

&lt;p&gt;I did all of this in Ruby, since that's my familiar scripting
    language. I also used Google's api library for Ruby. But much of
    the overall flow would be the same for other languages, so if
    you're operating outside of Ruby I think much of what I did
    would still be relevant. I'll try to describe what I'm doing
    in a language independent view as much as possible, in addition to
    the ruby examples.&lt;/p&gt;

&lt;p&gt;I need to access a private playlist of
    videos on YouTube and print the titles of the videos on that
    playlist. &lt;a href="#footnote-actual-need"&gt;[1]&lt;/a&gt; Since this is a private
    playlist, I need to authenticate to Google and set up the
    necessary authorization for the script so it can get at that
    private data. I want to run this script without any manual
    intervention, so I want whatever auth mechanism I use to be
    something that the script can access itself, at least once I've
    logged into my laptop.&lt;/p&gt;

&lt;p&gt;Before I describe the successful path I followed, I should
    mention a path I took to a dead end. One of the things that made
    this simple exercise so tricky is that most of the documentation I
    read assumed I wanted to write a web-app that was guiding a
    browser. But I wanted a simple command line app (I guess because
    I'm old-fashioned that way) that didn't involve a browser. Reading
    through the &lt;a href="https://developers.google.com/accounts/docs/GettingStarted"&gt;Google guide to
    authentication and authorization&lt;/a&gt; I decided to use OAuth
    2.0, as that seems to be where Google wants to go. Google then
    gives several scenarios for OAuth authorization, of which the
    natural (if complex) one to go for seemed to be &lt;a href="https://developers.google.com/accounts/docs/OAuth2ServiceAccount"&gt;&lt;b&gt;Service Accounts&lt;/b&gt;&lt;/a&gt;. These support
    server-to-server access with authentication done via
    public/private key pair. I spent a good bit of time fiddling to
    get this to work and eventually was able to access google with it
    successfully, at which point I ran into a wall. With a service
    account, you effectively create a new user on Google. You then
    need some mechanism to allow that user to access your personal
    data. If you are running a domain on Google, there is a way
    to authorize service accounts to access your domain's data.
    However I could find no such mechanism for accessing data from a
    direct google account such as mine. Documentation implied you
    could do for some properties (such as analytics) but there was no
    general mechanism, such as one that would work for youtube data.
    It's always frustrating to spend many hours working out a solution
    and running into a hard wall like that, if this article does
    nothing more than save a few people from that effort, then it's
    worth writing.&lt;/p&gt;


&lt;h2&gt;Outline flow for authorization&lt;/h2&gt;

&lt;p&gt;The path that did work for me is based on what Google calls the
    &lt;a href="https://developers.google.com/accounts/docs/OAuth2InstalledApp"&gt;Installed Application&lt;/a&gt; flow, but
    one that I needed to adapt to ensure I could (mostly) do it
    without having to manually intervene or use a browser.&lt;/p&gt;

&lt;p&gt;To best explain how this works, I'll begin with a simple
    request to get that youtube listing. Whenever a script makes a request to
    get google data, you need to include an &lt;b&gt;access token&lt;/b&gt; in your
    request. Google's docs show such an HTTP request like this.&lt;/p&gt;

&lt;pre&gt;GET /plus/v1/people/me HTTP/1.1
Authorization: Bearer 1/fFBGRNJru1FQd44AzqT3Zg
Host: googleapis.com
&lt;/pre&gt;

&lt;p&gt;The access token is just a random looking bunch of characters.
    It lasts for a short amount of time, the current documentation
    says it lasts for just an hour. The access token is what the
    script needs to do its work, but that just leads to the question - how
    do you get an access token in the first place?&lt;/p&gt;

&lt;p&gt;One way to get an access token is to have a different kind of
    token - a &lt;b&gt;refresh token&lt;/b&gt;. Unlike access tokens, refresh tokens last
    for a long time. They only expire when they are revoked, they are
    superseded by later refresh tokens, or when Google has a hissy fit. For
    our script's purpose a refresh token is just the job. Once I have
    a refresh token, I can store it in a moderately safe place that
    the script can get to without manual intervention. I can then
    access the refresh token when I run the script, and as a first
    step use the refresh token to get a brand new access token. I can
    then use the access token for the rest of the script run
    (providing my script doesn't run longer than the lifetime of an
    access token - and even Ruby isn't that slow). I don't mind if
    getting the refresh token involves a manual step, because I don't
    need to do it very often.&lt;/p&gt;

&lt;p&gt;Before I explain how to get the refresh token, there's one
    other thing about them. Each refresh token (and the access token
    they obtain) has a limited authorization scope - meaning you say
    what data they are allowed to access. I can create a refresh token
    that's only valid for reading my youtube data. If a bad guy were
    to get this token he could not read my calendar data, nor modify
    my youtube data. Having different tokens with different scopes
    helps me limit what I do with each token, which makes me a touch
    more secure (and less worried with how safely I store the tokens).&lt;/p&gt;

&lt;p&gt;To get the refresh token, I do have to get a browser to log
    into google and authenticate itself as me. Like most people I have
    browser instances permanently logged into Google on my laptop, so that's no
    big deal. What I do is go to a google URL that's constructed in
    such a way to specify the authorization scope that I want. If I
    do that, while logged into my Google account, google will give me
    a &lt;b&gt;one-time authorization code&lt;/b&gt;. I then take that code and visit
    another URL and google hands me the refresh token that I want.&lt;/p&gt;

&lt;p&gt;Before all of this, there's a further thing I need to do -
    setup google to use APIs and allow access to the apps I want API
    access to reach. This is a manual task, but I only need to do it
    once (unless Google has a really big hissy fit). &lt;/p&gt;

&lt;p&gt;So here's the steps I need to go through:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Set up Google for API access - a one-time manual action with
      logged in browser&lt;/li&gt;

&lt;li&gt;Get a one-time authorization code - needs logged in browser,
      done rarely&lt;/li&gt;

&lt;li&gt;Exchange the authorization code for a refresh token - 
       API, done rarely&lt;/li&gt;

&lt;li&gt;Use the refresh token to get a new access token - api only, done once
      each time I run the script&lt;/li&gt;

&lt;li&gt;Use the access token when calling google - api only, done every time I
      call a google api&lt;/li&gt;
&lt;/ul&gt;

&lt;h2&gt;Setting up Google&lt;/h2&gt;

&lt;p&gt;To use APIs with a google account I need to go into Google
      and set things up. The place I need to be is the &lt;a href="https://console.developers.google.com"&gt;Google
      Developers Console&lt;/a&gt;. I already had a project defined in the
      console, but you'll need to do that if you don't have one
      already. I then clicked on the project and then the APIs tab (on
      the left). This shows me of APIs and allows me to enable
      whichever APIs I wish. I need to ensure the API I want to use is
      enabled (in this case the Youtube Data API).&lt;/p&gt;

&lt;p class="figureImage"&gt;&lt;a name="command-line-google_enable-api.png"&gt;&lt;/a&gt;&lt;img alt="Figure 1" src="command-line-google/enable-api.png"&gt;&lt;/p&gt;
&lt;p&gt;I also need to have a OAuth client ID using the
      "Credentials" tab. I already had one set up.&lt;/p&gt;

&lt;p class="figureImage"&gt;&lt;a name="command-line-google_client-id.png"&gt;&lt;/a&gt;&lt;img alt="Figure 2" src="command-line-google/client-id.png"&gt;&lt;/p&gt;

&lt;h2&gt;Getting the one-time authorization code&lt;/h2&gt;

&lt;p&gt;To get the one-time authorization code you need to hit a
      specially crafted google URL while logged into Google. Google
      will then return the authorization code. Google's documentation,
      and various samples I ran into, explain doing this via a web
      app. In the course of your normal flow, the web app realizes it
      needs auth, and sends you over to google. &lt;/p&gt;

&lt;p&gt;Google can return the authorization code directly to your web
      app. All you need to do is run a server on your local machine
      and tell google its URL - eg &lt;code&gt;localhost:1234&lt;/code&gt;. Google
      will then issue a GET to that URL and include the authorization
      code as an parameter in the URL. Your code can then easily pick
      off the parameter. You don't need much of a webserver on this
      port to pick this up, all it ever needs to do is respond to this
      one request. This level of simple server doesn't even need
      Sinatra (Ruby's light weight web server framework), I remember
      many years ago being in an introductory Ruby class with Prag
      Dave where we wrote a simple web server in a few minutes. But I
      was too lazy to do even that.&lt;/p&gt;

&lt;p&gt;What I did instead was let my program craft the necessary
      google URL and print this URL out on the console. I then
      copy and paste it into my browser. Google (after a little dance
      to check I know what I'm doing) responds with the
      authorization code on a web page. I then copy and paste this code back
      into my script. It's not as smooth as an automated mechanism,
      but I don't care since I only have to do it once every blue moon.&lt;/p&gt;

&lt;p&gt;Let's look at my code for this. I divide any non-trivial
      command line script into multiple classes, separating the class
      that handles the command line interaction from an "engine" class
      that does all the work behind the scenes - essentially a use of
      &lt;a href="/eaaDev/SeparatedPresentation.html"&gt;Separated Presentation&lt;/a&gt;. I do this
      because I find it easier to separate the command line from the
      core code when I'm working on them.&lt;/p&gt;

&lt;p class="figureImage"&gt;&lt;a name="command-line-google_get-refresh.png"&gt;&lt;/a&gt;&lt;img alt="Figure 3" src="command-line-google/get-refresh.png" width="900"&gt;&lt;/p&gt;

&lt;p class="figureCaption"&gt;Figure 3: 
        Sequence diagram for how my code examples get a refresh token.
      &lt;/p&gt;
&lt;p&gt;For the command-line, I'm using &lt;a href="https://github.com/erikhuda/thor"&gt;Thor&lt;/a&gt;, which is a simple framework for building
      command-line applications in ruby &lt;a href="#footnote-cli-ruby"&gt;[2]&lt;/a&gt;.
      &lt;/p&gt;

&lt;pre&gt;require 'thor'
require_relative 'engine'

class CLI &amp;lt; Thor
  include Thor::Actions

  def initialize *args
    super(*args)
    @engine = Engine.new
  end

  desc "auth", "re-authorize the script"
  def auth
    puts "Point browser to following URL:\n\n"
    puts @engine.authorization_url
    puts "\n\n"
    auth_code = ask "paste in the authorization code"
    @engine.renew_refresh_token auth_code
  end
&lt;/pre&gt;

&lt;pre&gt;end

CLI.start(ARGV)
&lt;/pre&gt;

&lt;p&gt;Thor maps methods in the CLI class to sub-commands, so if
      the filename of the script is &lt;code&gt;get-vid&lt;/code&gt; I can invoke
      the authorization logic with &lt;code&gt;get-vid auth&lt;/code&gt;. &lt;a href="#footnote-cli-details"&gt;[3]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This authorization logic makes two calls to the underlying
      engine, one to get the url to display, the second to take the
      resulting authorization code that I paste in and use it renew
      the refresh token.&lt;/p&gt;

&lt;p&gt;I wrote the engine to handle the logic for futzing with
      youtube, but much of this code is really about dealing with
      google authorization. So I separated the authorization code out
      into a separate object, the GoogleAuthorizer. The engine creates
      an authorizer on initialization and delegates both the URL and
      renewal requests to it.&lt;/p&gt;

&lt;p class="code-label"&gt;class Engine...
&lt;/p&gt;

&lt;pre&gt;  def initialize
    @auth = GoogleAuthorizer.new(
      token_key: 'api-youtube',
      application_name: 'Gateway Youtube Example',
      application_version: '0.1'
      )
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;
&lt;/p&gt;

&lt;pre&gt;  def authorization_url
    @auth.authorization_url 'https://www.googleapis.com/auth/youtube.readonly'
  end
  
  def renew_refresh_token auth_code
    @auth.renew_refresh_token auth_code
  end
&lt;/pre&gt;

&lt;p&gt;I initialize the authorizer with three bits of data, the
      application name and version are used in some later API calls,
      the token key I'll expand on shortly.&lt;/p&gt;

&lt;p class="code-label"&gt;class GoogleAuthorizer
&lt;/p&gt;

&lt;pre&gt;  def initialize application_name: nil, application_version: "unknown", token_key: nil
    @application_name = application_name
    @token_store = TokenStore.new(token_key)
    @application_version = application_version
  end
&lt;/pre&gt;

&lt;p&gt;To construct the URL I use ruby's URL manipulation library&lt;/p&gt;

&lt;p class="code-label"&gt;class GoogleAuthorizer
&lt;/p&gt;

&lt;pre&gt;  def authorization_url scope
    params = {
      scope: scope,
      redirect_uri: 'urn:ietf:wg:oauth:2.0:oob',
      response_type: 'code',
      client_id: @token_store.client_id
    }
    url = {
      host: 'accounts.google.com',
      path: '/o/oauth2/auth',
      query: URI.encode_www_form(params)
    }

    return URI::HTTPS.build(url)
  end
&lt;/pre&gt;

&lt;p&gt;This code constructs a URL that looks something like: &lt;/p&gt;

&lt;pre&gt;https://accounts.google.com/o/oauth2/auth?
  scope=https://www.googleapis.com/auth/youtube.readonly&amp;amp;
  redirect_uri=urn:ietf:wg:oauth:2.0:oob&amp;amp;
  response_type=code&amp;amp;
  client_id=123456789012.apps.googleusercontent.com
&lt;/pre&gt;

&lt;p class="code-remark"&gt;To make it easier to read, I've added
       newlines and whitespace and decoded the URL escapes. I've also
       made up the client_id.&lt;/p&gt;

&lt;p&gt;The parameters to the URL are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;i&gt;scope:&lt;/i&gt; how much api we want to access, in this
         case we want readonly access to the youtube data api&lt;/li&gt;

&lt;li&gt;&lt;i&gt;redirect_uri:&lt;/i&gt; in the usual flow of using this with
         a web app,
         google redirects the browser to another URL (typically a
         localhost post) and deposits its response there. Using this
         value tells google I want it displayed in the browser for me
         to copy and paste&lt;/li&gt;

&lt;li&gt;&lt;i&gt;response_type:&lt;/i&gt; I want a one-time
         authorization code back&lt;/li&gt;

&lt;li&gt;&lt;i&gt;client_id&lt;/i&gt; I get this from the earlier interaction
         with the Google Developers Console&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Pasting that URL into my browser will (eventually) lead me
       to a web page from Google that shows the glistening
       authorization code.&lt;/p&gt;

&lt;h2&gt;Exchanging the authorization code for a refresh token&lt;/h2&gt;

&lt;p&gt;Now I have the authorization code I can initiate the second
       operation, obtaining the refresh token. I do this by contacting
       the Google authorization resource again, this time supplying
       the authorization code I just got from them and blending it
       with my client-secret, a code that identifies me to the
       google API. I don't need to be logged into Google for this
       step, nor do I need to use a browser.&lt;/p&gt;

&lt;p&gt;At this point I have to
       face up to another question: where do I store the refresh token
       once I have it? Since this is a script that I'm the only one
       using, I could just store it in the source code with something
       like&lt;/p&gt;

&lt;pre&gt;def refresh_token
  '1234567890WOxNS_gTztCGW3OBTKcSoKfLXDPc5TA7xz4MEudVrK5jSpoR30zcRFq6'
end
&lt;/pre&gt;

&lt;p&gt;I don't like this as I like to keep my code in repositories
       which are widely copied and often shared with others. Another
       option is to just dump the token in a file. My hard drive is
       encrypted, so that's reasonably safe - particularly since all
       I'm protecting is the dark secrets of my Youtube viewing
       habits. If I were being a bit more paranoid I could encrypt
       that file, but then that only raises the question of where to
       store the encryption key for the file, as I don't want to type
       in a password every time I use the script.&lt;/p&gt;

&lt;p&gt;Since I'm running this on a mac, I decided to use the Mac's
       built in keychain. This automatically opens when I log in and I
       can access it with the &lt;code&gt;security&lt;/code&gt; command-line
       application. I'll have to think of something else should I want
       to run this on my Ubuntu box, but I'll deal with that if I need
       to do that one day.&lt;/p&gt;

&lt;p&gt;Whatever my decision on where to store the refresh token, it
       is a decision, and one of the signs you need encapsulation is
       to hide decisions. That's why I created a
       &lt;code&gt;TokenStore&lt;/code&gt; class - to hide the decision of how I
       store my refresh token. I can also use the same class to store
       a couple of other little things, such as the client_id that I
       used earlier. The client_id is something I left in the source
       code, I'm sure if someone else wants to use this code they can
       figure out how to take it out.&lt;/p&gt;

&lt;p class="code-label"&gt;class TokenStore&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def client_id
    '123456789012.apps.googleusercontent.com'
  end
&lt;/pre&gt;

&lt;p&gt;To renew the refresh token, I need to use the one-time
       authorization code I got earlier to request new tokens, dig out
       the refresh token, and put it into my token store. (I say
       &amp;#8220;tokens&amp;#8221;, because Google responds with both an access
       token and a refresh token.)&lt;/p&gt;

&lt;p class="code-label"&gt;class GoogleAuthorizer&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def renew_refresh_token auth_code
    client = get_client_with_new_tokens(auth_code)
    token = client.authorization.refresh_token
    puts "new token: #{token}"
    @token_store.save_refresh_token token
  end
&lt;/pre&gt;

&lt;p&gt;To request these tokens, I talk again to Google, but this time
       I find it best to use the &lt;a href="https://github.com/google/google-api-ruby-client"&gt;ruby client
       library for the Google api&lt;/a&gt;. It took me a bit of time to
       figure out how to use it, as the documentation is sorely
       lacking. But it works pretty well once I'd got
       the hang of it. Here's the code to get the tokens:&lt;/p&gt;

&lt;p class="code-label"&gt;class GoogleAuthorizer...
&lt;/p&gt;

&lt;pre&gt;  def get_client_with_new_tokens auth_code
    client = Google::APIClient.new(
      application_name:  @application_name,
      application_version: @application_version
      )

    client.authorization = Signet::OAuth2::Client.new(
      token_credential_uri: 'https://www.googleapis.com/oauth2/v3/token',
      code: auth_code,
      client_id: @token_store.client_id,
      client_secret: @token_store.client_secret,
      redirect_uri: 'urn:ietf:wg:oauth:2.0:oob', 
      grant_type: 'authorization_code'
      )
    client.authorization.fetch_access_token!
    return client
  end
&lt;/pre&gt;

&lt;p&gt;This code first instantiates a google api client
       object and then gives it an authorization object, which uses the
       &lt;a href="https://github.com/google/signet"&gt;Signet library&lt;/a&gt;, which I also found
       wasn't very well documented. This particular combination of
       attributes worked for authorizing tokens&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;i&gt;application_name:&lt;/i&gt; not sure how this is used by google,
         but you need a value here to avoid an error in the client library&lt;/li&gt;

&lt;li&gt;&lt;i&gt;application_version:&lt;/i&gt; similar to the application name.
         Missing this out didn't get an error for me, but I left it in
         anyway as the error message for the name told me I needed it.&lt;/li&gt;

&lt;li&gt;&lt;i&gt;token_credential_uri:&lt;/i&gt; the URL to talk to for 
         authorization.  &lt;/li&gt;

&lt;li&gt;&lt;i&gt;code:&lt;/i&gt; the one-time authorization code&lt;/li&gt;

&lt;li&gt;&lt;i&gt;client_id:&lt;/i&gt; the client id from the earlier interaction
         with the Google Developers Console&lt;/li&gt;

&lt;li&gt;&lt;i&gt;client_secret:&lt;/i&gt; You also get this from the earlier interaction
         with the Google Developers Console. The google documentation
         says this isn't really a secret for applications like this,
         but they still call it a secret. I think of it as an enhanced
         identifier.&lt;/li&gt;

&lt;li&gt;&lt;i&gt;redirect_uri:&lt;/i&gt; I don't know what role this is
         playing, but when I left it off the library spanked me for
         a missing grant_type &lt;a href="#footnote-missing-grant-type"&gt;[4]&lt;/a&gt;. The google documentation implies I
         should set it to the same value that I did when requesting
         the authorization code in the first place.&lt;/li&gt;

&lt;li&gt;&lt;i&gt;grant_type:&lt;/i&gt; tells google I have an authorization
         code that I'd like to redeem for tokens. &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I didn't tap the HTTPS link to Google, but based on the
       google documentation, the resulting HTTP call should look
       something like this&lt;/p&gt;

&lt;pre&gt;POST /oauth2/v3/token HTTP/1.1
Host: www.googleapis.com
Content-Type: application/x-www-form-urlencoded

code=4/v6xr77ewYqhvHSyW6UJ1w7jKwAzu&amp;amp;
client_id=123456789012.apps.googleusercontent.com&amp;amp;
client_secret=ABC1234567890&amp;amp;
redirect_uri=urn:ietf:wg:oauth:2.0:oob&amp;amp;
grant_type=authorization_code
&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;fetch_access_token!&lt;/code&gt; method talks to google
       to get the tokens. Google sends back some JSON
       which the client library stores in the Signet authorization
       object. I can then get at the refresh token and save it in my 
       token store.&lt;/p&gt;

&lt;p class="code-label"&gt;class GoogleAuthorizer&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def renew_refresh_token auth_code
    client = get_client_with_new_tokens(auth_code)
    token = &lt;p class="highlight"&gt;client.authorization.refresh_token&lt;/p&gt;
    puts "new token: #{token}"
    @token_store.save_refresh_token token
  end
&lt;/pre&gt;

&lt;p&gt;I can then use the token store to save the token into my
       Mac's keychain.&lt;/p&gt;

&lt;p class="code-label"&gt;class TokenStore&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def save_refresh_token arg
    cmd = "security add-generic-password -a '#{@keychain_account}' -s '#{@keychain_account}' -w '#{arg}'"
    system cmd
  end
&lt;/pre&gt;

&lt;p&gt;I set the account for the token store to use
       when I created the engine.&lt;/p&gt;

&lt;p class="code-label"&gt;class Engine...
&lt;/p&gt;

&lt;pre&gt;  def initialize
    @auth = GoogleAuthorizer.new(
&lt;p class="highlight"&gt;      token_key: 'api-youtube',&lt;/p&gt;
      application_name: 'Gateway Youtube Example',
      application_version: '0.1'
      )
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class GoogleAuthorizer...
&lt;/p&gt;

&lt;pre&gt;  def initialize application_name: nil, application_version: "unknown", token_key: nil
    @application_name = application_name
    @token_store = &lt;p class="highlight"&gt;TokenStore.new(token_key)&lt;/p&gt;
    @application_version = application_version
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class TokenStore&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def initialize keychain_account
    @keychain_account = keychain_account
  end
&lt;/pre&gt;

&lt;p&gt;So the operating system command that's issued by
     &lt;code&gt;save_refresh_token&lt;/code&gt; is &lt;/p&gt;

&lt;pre&gt;security add-generic-password -a 'api-youtube' -s 'api-youtube' -w 'ABC123456-other-chars'
&lt;/pre&gt;

&lt;p&gt;The system command needs both a service (&lt;code&gt;-s&lt;/code&gt;)
       and an account (&lt;code&gt;-a&lt;/code&gt;) when storing a value. I use
       the same value for each of them, as I really just want a
       key-value store.&lt;/p&gt;

&lt;h2&gt;Using the refresh token to get an access token&lt;/h2&gt;

&lt;p&gt;The authorization logic is unusual, I expect only to invoke
       it once every blue moon. Also I now hope that once I've written
       it, I won't have to futz with it again, and I pulled it out
       into the authorizer so I can use that class with any other
       command line script that grabs Google data. &lt;/p&gt;

&lt;p&gt;What I do want to do is use all of this setup each time I
       want to do something useful, or in this case print out the
       videos on a private playlist. I begin with the Thor CLI&lt;/p&gt;

&lt;p class="code-label"&gt;class CLI&amp;#8230; (in file list-vid)
&lt;/p&gt;

&lt;pre&gt;  default_task :list
  desc "list", "list items in my playlist"
  def list
    puts @engine.list_playlist
  end
&lt;/pre&gt;

&lt;p class="code-remark"&gt;The &lt;code&gt;default_task&lt;/code&gt; annotation
       allows me to run the command &lt;code&gt;list-vid&lt;/code&gt; without a
       sub-command and invoke the list method.&lt;/p&gt;

&lt;p&gt;As usual, the CLI just delegates all the work to the
       engine. The outline of the engine method is pretty simple. First I
       define the parameters to a request with a simple hash. Next I
       ask a client object to execute the request, returning some JSON
       that I parse back into a ruby data structure. Finally I pull
       what I want out of that data structure to send back to the CLI.
       At this point, however, I want to keep focus on exchanging the
       refresh token for an access token. All this work is hidden
       behind the simple &lt;code&gt;client&lt;/code&gt; method call.&lt;/p&gt;

&lt;p class="code-label"&gt;class Engine...
&lt;/p&gt;

&lt;pre&gt;  def list_playlist
    request = playlist_request
    response = exec_request(request)
    return response['items'].map{|i| i['snippet']['title']}
  end
  def exec_request requestHash
    JSON.parse(&lt;p class="highlight"&gt;client&lt;/p&gt;.execute!(requestHash).body)
  end
&lt;/pre&gt;

&lt;p&gt;The client object is an instance of the Google API client
       that we saw earlier, although it's configured 
       differently. When executing the particular request, the client
      object will use an access token, but first it has to use the
      refresh token to get that access token. I set things up so that
      the first time the script wants to use a client object, I create
      one going through the dance with google to get the new access
      token. Once I've done that I keep this client object in the
      engine to use again for further requests.&lt;/p&gt;

&lt;p class="code-label"&gt;class Engine...
&lt;/p&gt;

&lt;pre&gt;  def client
    @client ||= @auth.api_client
    return @client
  end
&lt;/pre&gt;

&lt;p&gt;All of the business of exchanging refresh tokens for access
      tokens is handled by the Google client library. I just have to
      create a Google API object initialized with the right data
      and get it to fetch an access token. &lt;/p&gt;

&lt;p class="code-label"&gt;class GoogleAuthorizer...
&lt;/p&gt;

&lt;pre&gt;  def api_client
   client = Google::APIClient.new(
      application_name: @application_name,
      application_version: @application_version
      )
    
    client.authorization = Signet::OAuth2::Client.new(
      token_credential_uri: 'https://www.googleapis.com/oauth2/v3/token',
      client_id: @token_store.client_id,
      client_secret: @token_store.client_secret,
      refresh_token: @token_store.refresh_token,
      grant_type: 'refresh_token'
      )
    client.authorization.fetch_access_token!
    return client
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class TokenStore&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def client_secret
    @client_secret ||= `security find-generic-password -wa google-client-secret`.chomp
    @client_secret
  end
  def refresh_token
    @refresh_token ||= `security find-generic-password -wa #{@keychain_account}`.chomp
    @refresh_token
  end
&lt;/pre&gt;

&lt;p&gt;I can now use the returned Google API client object for the
      last step.&lt;/p&gt;

&lt;h2&gt;Using the access token when calling a Google API&lt;/h2&gt;

&lt;p&gt;If I use the Google API object to make calls on Google, it
       ensures that access token is added to the request, as well as
       organizing the request data in the right way. All I have to do
       is use &lt;code&gt;execute!&lt;/code&gt; method on the client object,
       passing in a hash with the data for my request.&lt;/p&gt;

&lt;p class="code-label"&gt;class Engine...
&lt;/p&gt;

&lt;pre&gt;  def list_playlist
    request = playlist_request
    response = exec_request(request)
    return response['items'].map{|i| i['snippet']['title']}
  end
  def exec_request requestHash
    JSON.parse(&lt;p class="highlight"&gt;client.execute!(requestHash)&lt;/p&gt;.body)
  end
&lt;/pre&gt;

&lt;p&gt;Since this article is really about using Oauth with Google,
        I can end it right here. However since I'm also futzing with
        the Google client library and trying to figure out how to use
        it to grab some data, I might as well continue with just
        enough to show how I pull out the titles of the videos in that
        private playlist.&lt;/p&gt;

&lt;h2&gt;Getting a list of videos from the Google API&lt;/h2&gt;

&lt;p&gt;To get data out of youtube (or any Google API) I have to
        decide which resource I want to talk to, what method I wish to
        invoke on that resource (they don't talk in terms of HTTP
        verbs) and use parameters to parameterize the call. &lt;/p&gt;

&lt;p&gt;The Google
        client library provides some affordances for determining the
        right api_method.  I can ask the client object to return me
        an API object which knows about a particular
        API.&lt;/p&gt;

&lt;p class="code-label"&gt;class Engine...
&lt;/p&gt;

&lt;pre&gt;  def youtube
    client.discovered_api('youtube', 'v3')
  end
&lt;/pre&gt;

&lt;p&gt;This call to &lt;code&gt;discovered_api&lt;/code&gt; does two things.
        Firstly it tells the client object to contact Google and
        download a description of Google's online API which it then
        caches. Secondly it returns a new Google API object configured
        with this information. Since the client caches the API
        description, I don't have to worry about caching the youtube
        API object myself. (If I need to talk to more than one Google
        API, I can use the &lt;code&gt;discovered_apis&lt;/code&gt; method, which
        will get multiple API's data from Google in a single
        call.)&lt;/p&gt;

&lt;p&gt;So what can I do with this Google API object? Sadly it's not
        immediately obvious. There's only one Google API class, and
        this one class operates with all the various Google APIs. So
        if I peruse its documentation (which I had to do from my local
        gem browser since the online version 404d on me) it doesn't
        tell me anything about methods to talk to youtube. What it does
        list are various methods that I can invoke to find out more,
        so I interrogated it with some ruby calls. First I invoked pry
        to allow me to interrogate the runtime object.&lt;/p&gt;

&lt;p class="code-label"&gt;...
&lt;/p&gt;

&lt;pre&gt;  def list_playlist
&lt;p class="highlight"&gt;    binding.pry&lt;/p&gt;
    request = playlist_request
    resp = exec_request(request)
    return resp['items'].map{|i| PlaylistTitle.new(i).title}    
  end
&lt;/pre&gt;

&lt;p&gt;That drops me into the pry REPL. I don't use this much, so
        I'm not that skilled with it, but I know it allows me to
        invoke methods on objects.&lt;/p&gt;

&lt;pre&gt;[1] pry(#&amp;lt;Engine&amp;gt;)&amp;gt; youtube.description
=&amp;gt; "Programmatic access to YouTube features."
[2] pry(#&amp;lt;Engine&amp;gt;)&amp;gt; youtube.discovered_methods
=&amp;gt; []
&lt;/pre&gt;

&lt;p&gt;OK - that's not helpful. But I suspect some clever
        metaprogramming is going on.&lt;/p&gt;

&lt;pre&gt;[3] pry(#&amp;lt;Engine&amp;gt;)&amp;gt; youtube.methods
=&amp;gt; [:activities,
 :channel_banners,
 :channel_sections,
 :channels,
 :guide_categories,
 :i18n_languages,
 :i18n_regions,
 :live_broadcasts,
 :live_streams,
 :playlist_items,
 :playlists,
&amp;#8230; lots more          
&lt;/pre&gt;

&lt;p&gt;So that's it &amp;#8212; the Google API does runtime code generation
        to add suitable methods to the api object. This way a single
        api class can have support different APIs, and each API can be
        generated at run time to be up to date with exactly what Google
        supports. Should Google change its API at run time, I don't
        need to update my client library, I'll just get a different
        set of runtime generated methods &amp;#8212; very clever.&lt;/p&gt;

&lt;p&gt;If you know me well, you'll know that when I use "clever"
        to describe some programming, it isn't usually a compliment.
        The downside of doing runtime code generation is that it's a
        pain in the neck to find out what methods are available to me.
        Usually when I want to know what an object does, I can look at
        some documentation. Even with minimal documentation I can at
        least see what methods are available. But in this case I can't
        even do that - I have to futz around with pry just to get a
        list of available method calls. And these method calls don't
        come with documentation to explain how to use them.&lt;/p&gt;

&lt;p&gt;My hope here is that these runtime-generated methods
        correspond to the Google API that's described in more generic
        terms. I see a method here called "playlists" - does this
        corresponds to the &lt;a href="https://developers.google.com/youtube/v3/docs/playlists"&gt;resource in
        the online API?&lt;/a&gt; Thankfully this correspondence holds
        pretty well. It seems that if I see a Google resource named
        something like &lt;code&gt;channels&lt;/code&gt; I can expect a
        runtime-generated method on the google api object called
        &lt;code&gt;channels&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;This is all enlightening, but how do I list the contents of
        a playlist? Each playlist on Google has an ID, so how do I see
        the what items are on playlist "1234"? Here it's important to
        understand how the Google youtube data API is organized (other
        Google APIs may be similar, I haven't looked enough to see).
        When we talk about an HTTP API using resources, I tend to
        imagine organizing the API in a URL style similar to that used
        on the web. So if I want the details about playlist 1234, I
        would expect a document at a URL that looks something like
        &lt;code&gt;youtube/api/playlists/1234&lt;/code&gt;. I can then acquire
        that document with an HTTP GET.&lt;/p&gt;

&lt;p&gt;But the Youtube Data API isn't structured like this. It has
        resources, but these resources have their own methods on them,
        and these methods are some subset of "list", "insert",
        "update", and "delete". What we have here isn't the usual HTTP
        verbs of "get", "post", "put" and "delete" but a CRUD style
        set of verbs &lt;a href="#footnote-crud"&gt;[5]&lt;/a&gt;. The key thing for me to
        realize was to think of this API like a set of relational
        tables. &lt;/p&gt;

&lt;p&gt;So to find the contents of a playlist, I have to know that
        a playlist consists of playlist items, and then formulate the
         equivalent of &lt;code&gt;select * from playlist_items where
        playlistId = '1234'&lt;/code&gt;&lt;/p&gt;

&lt;p class="code-label"&gt;class Engine...
&lt;/p&gt;

&lt;pre&gt;  def playlist_request
    {
&lt;p class="highlight"&gt;      api_method: youtube.playlist_items.list,&lt;/p&gt;
      parameters: {
&lt;p class="highlight"&gt;        playlistId: "PLJb2p0qX8R_ojWB5Bx4Q6TzaKcLMvsAim",&lt;/p&gt;
        part: 'snippet',
      }
    }    
  end
&lt;/pre&gt;

&lt;p&gt;A second thing to understand in the Google APIs is the
        notion of &lt;i&gt;parts&lt;/i&gt; of a resource. The full data referenced by a
        google resource like this can be large, so to reduce bandwidth
        Google breaks up resources into parts and requires me to
        specify which parts I want. In this case I'm just asking for
        the snippet part, since that will give the me the video titles
        that I'm looking for.&lt;/p&gt;

&lt;p class="code-label"&gt;class Engine...
&lt;/p&gt;

&lt;pre&gt;  def playlist_request
    {
      api_method: youtube.playlist_items.list,
      parameters: {
        playlistId: "PLJb2p0qX8R_ojWB5Bx4Q6TzaKcLMvsAim",
&lt;p class="highlight"&gt;        part: 'snippet',&lt;/p&gt;
      }
    }    
  end
&lt;/pre&gt;

&lt;p&gt;The &lt;a href="https://developers.google.com/youtube/v3/docs/playlistItems"&gt;documentation for
        playlist items&lt;/a&gt; shows me the structure of the playlist
        items.  &lt;/p&gt;

&lt;p&gt;When the API client executes the request, it returns a
        result object which contains lots of details about the
        interaction. To get to the meat of the result I can invoke the
        &lt;code&gt;body&lt;/code&gt; method on the result object and I'll get back the
        JSON which I can then parse into a ruby data structure:&lt;/p&gt;

&lt;p class="code-label"&gt;class Engine...
&lt;/p&gt;

&lt;pre&gt;  def exec_request requestHash
    JSON.parse(client.execute!(requestHash).body)
  end
&lt;/pre&gt;

&lt;pre&gt;pp @engine.exec_request(playlist_request)
&lt;/pre&gt;

&lt;pre&gt;# =&amp;gt;
{"kind"=&amp;gt;"youtube#playlistItemListResponse",
 "etag"=&amp;gt;"\"F9iA7pnxqNgrkOutjQAa9F2k8HY/bLvU5-1d6Q4rcW60TlK-JTVNovM\"",
 "pageInfo"=&amp;gt;{"totalResults"=&amp;gt;3, "resultsPerPage"=&amp;gt;5},
 "items"=&amp;gt;
  [{"kind"=&amp;gt;"youtube#playlistItem",
    "etag"=&amp;gt;"\"F9iA7pnxqNgrkOutjQAa9F2k8HY/9bdaIAc39A9cxUknU5cr7i2jzLg\"",
    "id"=&amp;gt;"PL_LBOgO7Mf7ZUfWrfLUl5YJuDy41Yo1j50eP8nIG35Wo",
    "snippet"=&amp;gt;
     {"publishedAt"=&amp;gt;"2014-01-14T23:09:37.000Z",
      "channelId"=&amp;gt;"UC0EbszLD1ceZeAkZ3JEI2GA",
&amp;#8230; lots more
&lt;/pre&gt;

&lt;p&gt;The snippet part contains a title element, so I can print
        out the titles in the returned list by digging a little way
        into the structure.&lt;/p&gt;

&lt;p class="code-label"&gt;class Engine...
&lt;/p&gt;

&lt;pre&gt;  def list_playlist
    request = playlist_request
    response = exec_request(request)
&lt;p class="highlight"&gt;    return response['items'].map{|i| i['snippet']['title']}&lt;/p&gt;
  end
  def exec_request requestHash
    JSON.parse(client.execute!(requestHash).body)
  end
&lt;/pre&gt;


&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:43:37 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:using-oauth-for-a-simple-command-line-script-to-access-googles-data.html</guid></item><item><title>Bliki: DataLake</title><link>http://www.ciandcd.com/bliki-datalake.html</link><description>&lt;div&gt; 

 

 

&lt;p class="tagLabel"&gt;tags:&lt;/p&gt;

&lt;p class="clear"&gt;&lt;/p&gt;

&lt;p&gt;Data Lake is a term that's appeared in this decade to describe
  an important component of the data analytics pipeline in the world of
  &lt;a href="/articles/bigData/"&gt;Big Data&lt;/a&gt;. The idea is to
  have a single store for all of the raw data that anyone in an
  organization might need to analyze. Commonly people use
  Hadoop to work on the data in the lake, but the concept is broader
  than just Hadoop.&lt;/p&gt;

&lt;p&gt;When I hear about a single point to pull together all the data
  an organization wants to analyze, I immediately think of the notion
  of the data warehouse (and data
  mart &lt;a href="#footnote-mart-v-warehouse"&gt;[1]&lt;/a&gt;). But there is a vital distinction between the
  data lake and the data warehouse. The data lake stores &lt;i&gt;raw&lt;/i&gt;
  data, in whatever form the data source provides. There is no
  assumptions about the schema of the data, each data source can use
  whatever schema it likes. It's up to
  the consumers of that data to make sense of that data for their own
  purposes.&lt;/p&gt;

&lt;img src="images/dataLake/contrast.png" width="600px"&gt;
&lt;p&gt;&lt;i&gt;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;This is an important step, many data warehouse initiatives didn't
  get very far because of schema problems. Data warehouses tend to
  go with the notion of a single schema for all analytics needs, but
  I've taken the view that a single unified data model is impractical
  for anything but the smallest organizations. To model even a
  slightly complex domain you need multiple
  &lt;a href="BoundedContext.html"&gt;BoundedContexts&lt;/a&gt;, each with its own data model. In
  analytics terms, you need each analytics user to use a model that
  makes sense for the analysis they are doing. By shifting to storing
  raw data only, this firmly puts the responsibility on the data
  analyst.&lt;/p&gt;

&lt;p&gt;Another source of problems for data warehouse initiatives is
  ensuring data quality. Trying to get an authoritative single source
  for data requires lots of analysis of how the data is acquired and
  used by different systems. System A may be good for some data, and
  system B for another. You run into rules where system A is better
  for more recent orders but system B is better for orders of a month
  or more ago, unless returns are involved. On top of this, data
  quality is often a subjective issue, different analysis has
  different tolerances for data quality issues, or even a different
  notion of what is good quality.&lt;/p&gt;

&lt;p&gt;This leads to a common criticism of the data lake - that it's just a
  dumping ground for data of widely varying quality, better named a
  data swamp. The criticism is both valid and irrelevant. The hot
  title of the New Analytics is "Data Scientist". Although it's a
  much-abused title, many of these folks do have a solid background in
  science. And any serious scientist knows all about data quality
  problems. Consider what you might think is the simple matter of analyzing temperature readings
  over time. You have to take into account that some weather stations
  are relocated in ways that may subtly affect the readings, anomalies due to problems
  in equipment, missing periods when the sensors aren't working. Many
  of the sophisticated statistical techniques out there are created
  to sort out data quality problems. Scientists are always skeptical about
  data quality and are used to dealing with questionable data. So for
  them the lake is important because they get to work with raw data
  and can be deliberate about applying techniques to make sense of it,
  rather than some opaque data cleansing mechanism that probably does
  more harm that good.&lt;/p&gt;

&lt;p&gt;Data warehouses usually would not just cleanse but also aggregate
  the data into a form that made it easier to analyze. But scientists
  tend to object to this too, because aggregation implies throwing
  away data. The data lake should contain all the data because you
  don't know what people will find valuable, either today or in a
  couple of years time. &lt;/p&gt;

&lt;p&gt;One of my colleagues illustrated this thinking with a recent
  example:

  "We were trying to compare our automated predictive models versus
  manual forecasts made by the company's contract managers. To do this
  we decided to train our models on year old data and compare our
  predictions to the ones made by managers at that time. Since we now
  know the correct results, this should be a fair test of accuracy.
  When we started to do this, it appeared that the manager's
  predictions were horrible and that even our simple models, made in
  just two weeks, were crushing them. We suspected that this
  out-performance was too good to be true. After a lot of testing and
  digging we discovered that the time stamps associated with those
  manager predictions were incorrect. They were being modified by some
  end-of-month processing report. So in short, these values in the
  data warehouse were useless; we feared that we would have no way of
  performing this comparison. After more digging we found that these
  reports had been stored and so we could extract the real forecasts
  made at that time. (We're crushing them again but it's taken many
  months to get there)."
  
&lt;/p&gt;

&lt;p&gt;The complexity of this raw data means that there is room for
  something that curates the data into a more manageable structure (as
  well as reducing the considerable volume of data.) The data lake shouldn't be
  accessed directly very much. Because the data is raw, you need a lot
  of skill to make any sense of it. You have relatively few people who
  work in the data lake, as they uncover generally useful views of
  data in the lake, they can create a number of data marts each of which
  has a specific model for a single bounded context. A larger number
  of downstream users can then treat these lakeshore marts as an
  authoritative source for that context. &lt;/p&gt;

&lt;img src="images/dataLake/context.png" width="600px"&gt;
&lt;p&gt;&lt;i&gt;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;So far I've described the data lake as singular point for
  integrating data across an enterprise, but I should mention that
  isn't how it was originally intended. The term &lt;a href="https://jamesdixon.wordpress.com/2010/10/14/pentaho-hadoop-and-data-lakes/"&gt;was
  coined by James Dixon in 2010&lt;/a&gt;, when he did that he intended a
  data lake to be used for a single data source, multiple data sources
  would instead form a "water garden". Despite its original formulation
  the prevalent usage now is to treat a data lake as combining many
  sources. &lt;a href="#footnote-usage"&gt;[2]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You should use a data lake for analytic purposes, not for
  collaboration between operational systems. When operational systems
  collaborate they should do this through services designed for the
  purpose, such as RESTful HTTP calls, or asynchronous messaging. The
  lake is too complex to trawl for operational communication. It may
  be that analysis of the lake can lead to new operational
  communication routes, but these should be built directly rather than
  through the lake.&lt;/p&gt;

&lt;p&gt;It is important that all data put in the lake should have a clear
  provenance in place and time. Every data item should have a clear
  trace to what system it came from and when the data was produced.
  The data lake thus contains a historical record. This might come
  from feeding &lt;a href="/eaaDev/DomainEvent.html"&gt;Domain Events&lt;/a&gt;
  into the lake, a natural fit with &lt;a href="/eaaDev/EventSourcing.html"&gt;Event Sourced&lt;/a&gt; systems. But it could
  also come from systems doing a regular dump of current state into the
  lake - an approach that's valuable when the source system doesn't
  have any temporal capabilities but you want a temporal
  analysis of its data. A consequence of this is that data put into
  the lake is immutable, an observation once stated cannot be removed
  (although it may be refuted later), you should also expect
  &lt;a href="ContradictoryObservations.html"&gt;ContradictoryObservations&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The data lake is schemaless, it's up to the source systems to
  decide what schema to use and for consumers to work out how to deal
  with the resulting chaos. Furthermore the source
  systems are free to change their inflow data schemas at will, and
  again the consumers have to cope. Obviously we prefer such changes
  to be as minimally disruptive as possible, but scientists prefer messy
  data to losing data.&lt;/p&gt;

&lt;p&gt;Data lakes are going to be very large, and much of the storage is
  oriented around the notion of a large schemaless structure - which
  is why Hadoop and HDFS are usually the technologies people use for
  data lakes. One of the vital tasks of the lakeshore marts is to
  reduce the amount of data you need to deal with, so that big data analytics
  doesn't have to deal with large amounts of data.&lt;/p&gt;

&lt;p&gt;The Data Lake's appetite for a deluge of raw data raises awkward
  questions about privacy and security. The principle of
  &lt;a href="Datensparsamkeit.html"&gt;Datensparsamkeit&lt;/a&gt; is very much in tension with the data
  scientists' desire to capture all data now. A data lake makes a
  tempting target for crackers, who might love to siphon choice bits
  into the public oceans. Restricting direct lake access to a small
  data science group may reduce this threat, but doesn't avoid the
  question of how that group is kept accountable for the privacy of
  the data they sail on. &lt;/p&gt;

 

&lt;p class="acknowledgements"&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;


    My thanks to 

    Anand Krishnaswamy, Danilo Sato, David Johnston, Derek Hammer, Duncan Cragg, Jonny Leroy, Ken
    Collier, Shripad Agashe, and Steven Lowe

    for discussing drafts of this post on our internal mailing lists
    
  &lt;/p&gt;

&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/bliki/DataLake.html&amp;amp;text=Bliki:%20DataLake" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/bliki/DataLake.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/bliki/DataLake.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:43:35 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:bliki-datalake.html</guid></item><item><title>Refactoring code that accesses external services</title><link>http://www.ciandcd.com/refactoring-code-that-accesses-external-services.html</link><description>&lt;div&gt;&lt;p class="abstract"&gt;&lt;i&gt;
    When I write code that deals with external services, I find it
    valuable to separate that access code into separate objects. Here
    I show how I would refactor some congealed code into a common
    pattern of this separation.
  &lt;/i&gt;&lt;/p&gt;&lt;p&gt;One of the characteristics of software systems is that they
    don't live on their own. In order to do something useful, they
    usually need to talk to other bits of software, written by
    different people, people that we don't know and who neither know
    or care about the software that we're writing.&lt;/p&gt;

&lt;p&gt;When we're writing software that does this kind of external
    collaboration, I think it's particularly useful to apply good
    modularity and encapsulation. There are common patterns which I
    see and have found valuable in doing this. &lt;/p&gt;

&lt;p&gt;In this article I'll take a simple example, and walk through
    the refactorings that introduce the kind of modularity I'm looking
    for.&lt;/p&gt;


&lt;h2&gt;The starting code&lt;/h2&gt;

&lt;p&gt;The example code's job is to read some data about videos from
      a JSON file, enrich it with data from YouTube, calculate some simple
      further data, and then return the data in JSON.&lt;/p&gt;

&lt;p&gt;Here is the starting code.&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json'))
    ids = @video_list.map{|v| v['youtubeID']}
    client = GoogleAuthorizer.new(
      token_key: 'api-youtube',
      application_name: 'Gateway Youtube Example',
      application_version: '0.1'
      ).api_client
    youtube = client.discovered_api('youtube', 'v3')
    request = {
      api_method: youtube.videos.list,
      parameters: {
        id: ids.join(","),
        part: 'snippet, contentDetails, statistics',
      }
    }
    response = JSON.parse(client.execute!(request).body)
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
      youtube_record = response['items'].find{|v| id == v['id']}
      video['views'] = youtube_record['statistics']['viewCount'].to_i
      days_available = Date.today - Date.parse(youtube_record['snippet']['publishedAt'])
      video['monthlyViews'] = video['views'] * 365.0 / days_available / 12      
    end
    return JSON.dump(@video_list)
  end
&lt;/pre&gt;

&lt;p class="code-remark"&gt;The language for this example is Ruby&lt;/p&gt;

&lt;p&gt;The first thing for me to say here is that there isn't much
      code in this example. If the entire codebase is just this
      script, then you don't have to worry as much about
      modularity. I need a small example, but any reader's eyes
      would glaze over if we looked at a real system. So I have to ask
      you to imagine this code as typical code in a system of tens of
      thousands of lines.&lt;/p&gt;

&lt;p class="skippable"&gt;The access to the YouTube API is mediated through a
      GoogleAuthorizer object which, for this article's purposes, I'm
      going to treat as an external API. It handles the messy
      details of connecting to a Google service (such as YouTube) and
      in particular handles authorization issues. If you want to
      understand how it works, take a look at &lt;a href="/articles/command-line-google.html"&gt;an article I wrote
      recently about accessing Google APIs&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;What's up with this code? You may not understand everything
      this code is doing, but you should be able to see that it mixes
      different concerns, which I've suggested by coloring the code
      example below. In order to make any
      changes you have to comprehend
       
how to access to YouTube's
      API,

how YouTube
      structures its data
, and 
 some domain logic.
&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json'))
    ids = @video_list.map{|v| v['youtubeID']}
&lt;p class="frag-api-access"&gt;    client = GoogleAuthorizer.new(
      token_key: 'api-youtube',
      application_name: 'Gateway Youtube Example',
      application_version: '0.1'
      ).api_client
    youtube = client.discovered_api('youtube', 'v3')
    request = {
      api_method: youtube.videos.list,
      parameters: {
        id: ids.join(","),
        part: &lt;p class="frag-youtube-data"&gt;'snippet, contentDetails, statistics',&lt;/p&gt;
      }
    }
    response = JSON.parse(client.execute!(request).body)&lt;/p&gt;
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
&lt;p class="frag-youtube-data"&gt;      youtube_record = response['items'].find{|v| id == v['id']}&lt;/p&gt;
&lt;p class="frag-youtube-data"&gt;      video['views'] = youtube_record['statistics']['viewCount'].to_i&lt;/p&gt;
      &lt;p class="frag-domain"&gt;days_available = Date.today - &lt;/p&gt;&lt;p class="frag-youtube-data"&gt;Date.parse(youtube_record['snippet']['publishedAt'])&lt;/p&gt;
&lt;p class="frag-domain"&gt;      video['monthlyViews'] = video['views'] * 365.0 / days_available / 12      &lt;/p&gt;
    end
    return JSON.dump(@video_list)
  end
&lt;/pre&gt;

&lt;p&gt;It's common for software mavens like me to talk about
      "separation of concerns" - which basically means different
      topics should be in separate modules. My primary reason for this
      is comprehension: in a well-modularized program each
      module should be about one topic, so I can remain ignorant of
      anything I don't need to understand. Should YouTube's data
      formats change, I shouldn't have to understand the domain logic
      of the application to rearrange the access code. Even if I'm
      making  a change that takes some new data from YouTube and uses
      it in some domain logic, I should be able to split my task into
      those parts and deal with each one separately, minimizing how
      many lines of code I need to keep spinning in my head.&lt;/p&gt;

&lt;p&gt;My refactoring mission is to split these concerns out into
      separate modules. When I'm done the only code in the Video
      Service should be the uncolored code - the code that coordinates
      these other responsibilities.&lt;/p&gt;

&lt;h2&gt;Putting the code under test&lt;/h2&gt;

&lt;p&gt;The first step in refactoring is always the same. You need to
      be confident that you aren't going to inadvertently break
      anything. Refactoring is all about stringing together a large
      set of small steps, all of which are behavior preserving. By
      keeping the steps small, we increase the chances that we don't
      screw up. But I know myself well enough to know I can screw up
      even the simplest change, so to get the confidence I need I have
      to have tests to catch my mistakes.&lt;/p&gt;

&lt;p&gt;But code like this isn't straightforward to test. It would be
      nice to write a test that asserts on the calculated monthly views
      field. After all if anything else goes wrong, this is going to
      give an incorrect answer. But the trouble is that I'm accessing
      live YouTube data, and people have a habit of watching videos.
      The view count field from YouTube will change regularly, causing
      my tests to go red &lt;a href="/articles/nonDeterminism.html"&gt;non-deterministically&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So my first task is to remove that piece of flakiness. I can
      do that by introducing a &lt;a href="http://martinfowler.com/bliki/TestDouble.html"&gt;Test Double&lt;/a&gt;, an object that looks like
      YouTube but instead responds in a deterministic fashion.
      Unfortunately here I run into The Legacy Code Dilemma. &lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The Legacy Code Dilemma: When we change code, we should
        have tests in place. To put tests in place, we often have to
        change code.&lt;/p&gt;

&lt;p class="quote-attribution"&gt;&lt;a href="http://www.amazon.com/gp/product/0131177052?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=0131177052"&gt;-- Michael Feathers&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Given that I have to make this change without tests, I need
      to make the smallest and simplest changes I can think of that
      will get the interaction with YouTube behind a seam where I can
      introduce a test double. So my first step is to use &lt;a href="http://refactoring.com/catalog/extractMethod.html"&gt;Extract Method&lt;/a&gt; to get the
      interaction with YouTube separated from the rest of the routine
      by into its own method.&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json'))
    ids = @video_list.map{|v| v['youtubeID']}
&lt;p class="highlight"&gt;    response = call_youtube ids&lt;/p&gt;
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
      youtube_record = response['items'].find{|v| id == v['id']}
      video['views'] = youtube_record['statistics']['viewCount'].to_i
      days_available = Date.today - Date.parse(youtube_record['snippet']['publishedAt'])
      video['monthlyViews'] = video['views'] * 365.0 / days_available / 12      
    end
    return JSON.dump(@video_list)
  end

  def call_youtube ids
    client = GoogleAuthorizer.new(
      token_key: 'api-youtube',
      application_name: 'Gateway Youtube Example',
      application_version: '0.1'
      ).api_client
    youtube = client.discovered_api('youtube', 'v3')
    request = {
      api_method: youtube.videos.list,
      parameters: {
        id: ids.join(","),
        part: 'snippet, contentDetails, statistics',
      }
    }
    return JSON.parse(client.execute!(request).body)
  end
&lt;/pre&gt;

&lt;p&gt;Doing this achieves two things. Firstly it nicely pulls out
      the google API manipulation code into its own method (mostly)
      isolating it from any other kind of code. This on its own is
      worthwhile. Secondly, and more urgently, it sets up a seam which
      I can use to substitute test behavior. Ruby's built in minitest
      library allows me to easily stub individual methods on an object.&lt;/p&gt;

&lt;pre&gt;class VideoServiceTester &amp;lt; Minitest::Test
  def setup
    vs = VideoService.new
    vs.stub(:call_youtube, stub_call_youtube) do
      @videos = JSON.parse(vs.video_list)
      @&amp;#181;S  =  @videos.detect{|v| 'wgdBVIX9ifA' == v['youtubeID']}
      @evo =  @videos.detect{|v| 'ZIsgHs0w44Y' == v['youtubeID']}
    end
  end
  def stub_call_youtube
    JSON.parse(File.read('test/data/youtube-video-list.json'))
  end
  def test_microservices_monthly_json
    assert_in_delta 5880, @&amp;#181;S ['monthlyViews'], 1
    assert_in_delta   20, @evo['monthlyViews'], 1
  end
  # further tests as needed&amp;#8230;
&lt;/pre&gt;

&lt;p&gt;By separating out the YouTube call, and stubbing it, I can
      make this test behave deterministically. Well at least for
      today, for it to work tomorrow I need to do the same thing with
      the call to &lt;code&gt;Date.today&lt;/code&gt;. &lt;/p&gt;

&lt;p class="code-label"&gt;class VideoServiceTester&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def setup
&lt;p class="highlight"&gt;    Date.stub(:today, Date.new(2015, 2, 2)) do&lt;/p&gt;
      vs = VideoService.new
      vs.stub(:call_youtube, stub_call_youtube) do
        @videos = JSON.parse(vs.video_list)
        @&amp;#181;S  =  @videos.detect{|v| 'wgdBVIX9ifA' == v['youtubeID']}
        @evo =  @videos.detect{|v| 'ZIsgHs0w44Y' == v['youtubeID']}
      end
    end
  end
&lt;/pre&gt;

&lt;h2&gt;Separating the remote call into a connection object&lt;/h2&gt;

&lt;p&gt;Separating concerns by putting code into different functions
      is a first level of separation. But when the concerns are as
      different as domain logic and dealing with an external data
      provider, I prefer to increase the level of separation into
      different classes. &lt;/p&gt;

&lt;p class="figureImage"&gt;&lt;a name="no-sep.png"&gt;&lt;/a&gt;&lt;img alt="Figure 1" src="refactoring-external-service/no-sep.png" width="400"&gt;&lt;/p&gt;

&lt;p class="figureCaption"&gt;Figure 1: 
        At the beginning, the video service class contains four responsibilities
      &lt;/p&gt;
&lt;p&gt;My first step is therefore to create a new
      class and use &lt;a href="http://refactoring.com/catalog/moveMethod.html"&gt;Move Method&lt;/a&gt;.&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def call_youtube ids
    YoutubeConnection.new.list_videos ids
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class YoutubeConnection&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def list_videos ids
    client = GoogleAuthorizer.new(
      token_key: 'api-youtube',
      application_name: 'Gateway Youtube Example',
      application_version: '0.1'
      ).api_client
    youtube = client.discovered_api('youtube', 'v3')
    request = {
      api_method: youtube.videos.list,
      parameters: {
        id: ids.join(","),
        part: 'snippet, contentDetails, statistics',
      }
    }
    return JSON.parse(client.execute!(request).body)
  end
&lt;/pre&gt;

&lt;p&gt;With that I can also change the stub so it returns a test
      double rather than simply stubbing the method.&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoServiceTester&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def setup
    Date.stub(:today, Date.new(2015, 2, 2)) do
&lt;p class="highlight"&gt;      YoutubeConnection.stub(:new, YoutubeConnectionStub.new) do&lt;/p&gt;
        @videos = JSON.parse(VideoService.new.video_list)
        @&amp;#181;S  =  @videos.detect{|v| 'wgdBVIX9ifA' == v['youtubeID']}
        @evo =  @videos.detect{|v| 'ZIsgHs0w44Y' == v['youtubeID']}
      end
    end
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class YoutubeConnectionStub&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def list_videos ids
    JSON.parse(File.read('test/data/youtube-video-list.json'))
  end
&lt;/pre&gt;

&lt;p&gt;When doing this refactoring, I have to be wary that my shiny
      new tests
      won't catch any mistakes I make behind the stub, so I have to
      manually ensure that the production code still works. (And yes,
      since you asked, I did make a mistake while doing this (leaving
      off the argument to list-videos). There's a reason I need to
      test so much.)&lt;/p&gt;

&lt;p&gt;The greater separation of concerns you get with a separate
      class also gives you a better seam for testing - I can wrap
      everything that needs to be stubbed into a single object
      creation, which is particularly handy if we need to make
      multiple calls to the same service object during the test.&lt;/p&gt;

&lt;p&gt;With the call to YouTube moved to the connection object, the
      method on the video service isn't worth having any more so I
      subject it to
      &lt;a href="http://refactoring.com/catalog/inlineMethod.html"&gt;Inline Method&lt;/a&gt;.&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json'))
    ids = @video_list.map{|v| v['youtubeID']}
&lt;p class="highlight"&gt;    response = YoutubeConnection.new.list_videos ids&lt;/p&gt;
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
      youtube_record = response['items'].find{|v| id == v['id']}
      video['views'] = youtube_record['statistics']['viewCount'].to_i
      days_available = Date.today - Date.parse(youtube_record['snippet']['publishedAt'])
      video['monthlyViews'] = video['views'] * 365.0 / days_available / 12      
    end
    return JSON.dump(@video_list)
  end

&lt;p class="deleted"&gt;  def call_youtube ids
    YoutubeConnection.new.list_videos ids
  end&lt;/p&gt;
&lt;/pre&gt;

&lt;p&gt;I don't like that my stub has to parse the json string. On
      the whole I like to keep connection objects as &lt;a href="http://xunitpatterns.com/Humble%20Object.html"&gt;Humble Objects&lt;/a&gt;, because any behavior they do isn't tested. So I prefer
      to pull the parsing out into the callers.
      &lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json'))
    ids = @video_list.map{|v| v['youtubeID']}
    response = &lt;p class="highlight"&gt;JSON.parse&lt;/p&gt;(YoutubeConnection.new.list_videos(ids))
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
      youtube_record = response['items'].find{|v| id == v['id']}
      video['views'] = youtube_record['statistics']['viewCount'].to_i
      days_available = Date.today - Date.parse(youtube_record['snippet']['publishedAt'])
      video['monthlyViews'] = video['views'] * 365.0 / days_available / 12      
    end
    return JSON.dump(@video_list)
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class YoutubeConnection&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def list_videos ids
    client = GoogleAuthorizer.new(
      token_key: 'api-youtube',
      application_name: 'Gateway Youtube Example',
      application_version: '0.1'
      ).api_client
    youtube = client.discovered_api('youtube', 'v3')
    request = {
      api_method: youtube.videos.list,
      parameters: {
        id: ids.join(","),
        part: 'snippet, contentDetails, statistics',
      }
    }
    return &lt;p class="deleted"&gt;JSON.parse&lt;/p&gt;(client.execute!(request).body)
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class YoutubeConnectionStub&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def list_videos ids
    &lt;p class="deleted"&gt;JSON.parse&lt;/p&gt;(File.read('test/data/youtube-video-list.json'))
  end
&lt;/pre&gt;

&lt;p class="figureImage"&gt;&lt;a name="sep-connection.png"&gt;&lt;/a&gt;&lt;img alt="Figure 2" src="refactoring-external-service/sep-connection.png"&gt;&lt;/p&gt;

&lt;p class="figureCaption"&gt;Figure 2: 
        The first major step separates the youtube connection code
        into a &lt;b&gt;connection&lt;/b&gt; object.
      &lt;/p&gt;

&lt;h2&gt;Separating the youtube data structure into a Gateway&lt;/h2&gt;

&lt;p&gt;Now that I have the basic connection to YouTube separated and
      stubbable, I can work on the code that delves through the
      YouTube data structures. The problem here is that a bunch of
      code needs to know that to get the view count data, you have to
      look into the "statistics" part of the result, but to get the
      published date you need to delve into "snippet" section instead.
      Such delving is common with data from remote sources, it's
      organized the way that makes sense for them, not for me. This is
      entirely reasonable behavior, they don't have insights into my
      needs, I have enough trouble doing that on my own.
      &lt;/p&gt;

&lt;p&gt;I find that a good way to think about this is Eric Evans's notion of
      &lt;a href="http://martinfowler.com/bliki/BoundedContext.html"&gt;Bounded Context&lt;/a&gt;. YouTube organizes its data
      according to its context, while I want to organize mine
      according to a different one. Code that combines two bounded
      contexts gets convoluted because it's mixing two separate
      vocabularies together. I need to separate them with what Eric
      calls an &lt;i&gt;anti-corruption layer&lt;/i&gt;, a clear boundary between them.
      His illustration of an anti-corruption layer is of the Great
      Wall of China, and as with any wall like this, we need gateways
      that allow some things to pass between them. In software terms a
      gateway allows me to reach through the wall to get the data I
      need from the YouTube bounded context. But the gateway should be
      expressed in a way that makes sense within my context rather
      than theirs.&lt;/p&gt;

&lt;p class="figureImage"&gt;&lt;a name="gateway-sketch.png"&gt;&lt;/a&gt;&lt;img alt="Figure 3" src="refactoring-external-service/gateway-sketch.png" width="400"&gt;&lt;/p&gt;
&lt;p&gt;In this, simple, example, that means a gateway object that
      can give me the published date and view counts without the
      client needing to know how that's stored in the YouTube data
      structure. The gateway object translates from YouTube's context
      into mine.&lt;/p&gt;

&lt;p&gt;I begin by creating a gateway object that I initialize with
      response I got from the connection.&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json'))
    ids = @video_list.map{|v| v['youtubeID']}
&lt;p class="highlight"&gt;    youtube = YoutubeGateway.new(YoutubeConnection.new.list_videos(ids))&lt;/p&gt;
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
      youtube_record &lt;p class="highlight"&gt;= youtube.record(id)&lt;/p&gt;
      video['views'] = youtube_record['statistics']['viewCount'].to_i
      days_available = Date.today - Date.parse(youtube_record['snippet']['publishedAt'])
      video['monthlyViews'] = video['views'] * 365.0 / days_available / 12      
    end
    return JSON.dump(@video_list)
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class YoutubeGateway&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def initialize responseJson
    @data = JSON.parse(responseJson)
  end
  def record id
    @data['items'].find{|v| id == v['id']}
  end
&lt;/pre&gt;

&lt;p&gt;I created the simplest behavior I could at this point, even
      though I don't intend to use the gateway's record method
      eventually, indeed unless I stop for a cup of tea I don't think
      it will last for half-an-hour.&lt;/p&gt;

&lt;p&gt;Now I'll move the delving logic for the views from the
      service into the gateway, creating a separate gateway item class
      to represent each video record.&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json'))
    ids = @video_list.map{|v| v['youtubeID']}
    youtube = YoutubeGateway.new(YoutubeConnection.new.list_videos(ids))
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
      youtube_record = youtube.record(id)
&lt;p class="highlight"&gt;      video['views'] = youtube.item(id)['views']&lt;/p&gt;
      days_available = Date.today - Date.parse(youtube_record['snippet']['publishedAt'])
      video['monthlyViews'] = video['views'] * 365.0 / days_available / 12      
    end
    return JSON.dump(@video_list)
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class YoutubeGateway&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def item id
    {
      'views' =&amp;gt; record(id)['statistics']['viewCount'].to_i
    }
  end
&lt;/pre&gt;

&lt;p&gt;I do the same for the published date&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json'))
    ids = @video_list.map{|v| v['youtubeID']}
    youtube = YoutubeGateway.new(YoutubeConnection.new.list_videos(ids))
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
&lt;p class="deleted"&gt;      youtube_record = youtube.record(id)&lt;/p&gt;
      video['views'] = youtube.item(id)['views']
      days_available = Date.today - &lt;p class="highlight"&gt;youtube.item(id)['published']&lt;/p&gt;
      video['monthlyViews'] = video['views'] * 365.0 / days_available / 12      
    end
    return JSON.dump(@video_list)
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class YoutubeGateway&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def item id
    {
      'views'     =&amp;gt; record(id)['statistics']['viewCount'].to_i,
&lt;p class="highlight"&gt;      'published' =&amp;gt; Date.parse(record(id)['snippet']['publishedAt'])&lt;/p&gt;
    }
  end
&lt;/pre&gt;

&lt;p&gt;Since I'm using the records in the gateway looked up by key,
      I'd like to reflect that usage better in the internal data
      structure, which I can do by replacing the list with a hash&lt;/p&gt;

&lt;p class="code-label"&gt;class                                                                  YoutubeGateway&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def initialize responseJson
&lt;p class="highlight"&gt;    @data = JSON.parse(responseJson)['items']
      .map{|i| [ i['id'], i ] }
      .to_h&lt;/p&gt;
  end
  def item id
    {
      'views' =&amp;gt;  &lt;p class="highlight"&gt;@data[id]&lt;/p&gt;['statistics']['viewCount'].to_i,
      'published' =&amp;gt; Date.parse(&lt;p class="highlight"&gt;@data[id]&lt;/p&gt;['snippet']['publishedAt'])
    }
  end
&lt;p class="deleted"&gt;  def record id
    @data['items'].find{|v| id == v['id']}
  end&lt;/p&gt;

&lt;/pre&gt;

&lt;p class="figureImage"&gt;&lt;a name="sep-gateway.png"&gt;&lt;/a&gt;&lt;img alt="Figure 4" src="refactoring-external-service/sep-gateway.png"&gt;&lt;/p&gt;

&lt;p class="figureCaption"&gt;Figure 4: 
        Separating data handling into a &lt;b&gt;gateway&lt;/b&gt; object
      &lt;/p&gt;
&lt;p&gt;With that done, I've done the key separation that I wanted to
      do. The YouTube connection object handles the calls to YouTube,
      returning a data structure that it gives to the YouTube gateway
      object. The service code is now all about how I want to see the
      data rather than how it's stored in different services. &lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json'))
    ids = @video_list.map{|v| v['youtubeID']}
    youtube = YoutubeGateway.new(YoutubeConnection.new.list_videos(ids))
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
      video['views'] = youtube.item(id)['views']
      days_available = Date.today - youtube.item(id)['published']
      video['monthlyViews'] = video['views'] * 365.0 / days_available / 12      
    end
    return JSON.dump(@video_list)
  end
&lt;/pre&gt;

&lt;h2&gt;Separating the domain logic into a Domain Object&lt;/h2&gt;

&lt;p&gt;Although all the interaction of YouTube is now parceled out
      to separate objects, the video service still mixes its domain
      logic (how to calculate monthly views) from choreographing the
      relationship between the data stored locally and the data in the
      service. If I introduce a domain object for the video, I can
      separate that out.&lt;/p&gt;

&lt;p&gt;My first step is to simply wrap the hash of video data in an
      object.&lt;/p&gt;

&lt;p class="code-label"&gt;class Video&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def initialize aHash
    @data = aHash
  end
  def [] key
    @data[key]
  end
  def []= key, value
    @data[key] = value
  end
  def to_h
    @data
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json'))&lt;p class="highlight"&gt;.map{|h| Video.new(h)}&lt;/p&gt;
    ids = @video_list.map{|v| v['youtubeID']}
    youtube = YoutubeGateway.new(YoutubeConnection.new.list_videos(ids))
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
      video['views'] = youtube.item(id)['views']
      days_available = Date.today - youtube.item(id)['published']
      video['monthlyViews'] = video['views'] * 365.0 / days_available / 12      
    end
    return JSON.dump(@video_list&lt;p class="highlight"&gt;.map{|v| v.to_h}&lt;/p&gt;)
  end
&lt;/pre&gt;

&lt;p&gt;To move the calculation logic into the new video object, I
      first need to get it into the right shape for the move - which I
      can do by parcelling it all into a single method on video service with the video domain
      object and the YouTube gateway item as arguments. The first step
      to that is to use &lt;a href="http://refactoring.com/catalog/extractVariable.html"&gt;Extract Variable&lt;/a&gt; on the
      gateway item.&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json')).map{|h| Video.new(h)}
    ids = @video_list.map{|v| v['youtubeID']}
    youtube = YoutubeGateway.new(YoutubeConnection.new.list_videos(ids))
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
&lt;p class="highlight"&gt;      &lt;p class="highlight"&gt;youtube_item&lt;/p&gt; = youtube.item(id)&lt;/p&gt;
      video['views'] = &lt;p class="highlight"&gt;youtube_item&lt;/p&gt;['views']
      days_available = Date.today - &lt;p class="highlight"&gt;youtube_item&lt;/p&gt;['published']
      video['monthlyViews'] = video['views'] * 365.0 / days_available / 12      
    end
    return JSON.dump(@video_list.map{|v| v.to_h})
  end
&lt;/pre&gt;

&lt;p&gt;With that done I can easily extract the calculation logic into its own method.&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json')).map{|h| Video.new(h)}
    ids = @video_list.map{|v| v['youtubeID']}
    youtube = YoutubeGateway.new(YoutubeConnection.new.list_videos(ids))
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
      youtube_item = youtube.item(id)
&lt;p class="highlight"&gt;      enrich_video video, youtube_item&lt;/p&gt;
    end
    return JSON.dump(@video_list.map{|v| v.to_h})
  end

&lt;p class="highlight"&gt;  def enrich_video video, youtube_item&lt;/p&gt;
      video['views'] = youtube_item['views']
      days_available = Date.today - youtube_item['published']
      video['monthlyViews'] = video['views'] * 365.0 / days_available / 12          
  end
&lt;/pre&gt;

&lt;p&gt;And then it's easy to apply &lt;a href="http://refactoring.com/catalog/moveMethod.html"&gt;Move Method&lt;/a&gt;
       to move it into the video.&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json')).map{|h| Video.new(h)}
    ids = @video_list.map{|v| v['youtubeID']}
    youtube = YoutubeGateway.new(YoutubeConnection.new.list_videos(ids))
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
      youtube_item = youtube.item(id)
&lt;p class="highlight"&gt;      video.enrich_with_youtube youtube_item&lt;/p&gt;
    end
    return JSON.dump(@video_list.map{|v| v.to_h})
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class Video&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;&lt;p class="highlight"&gt;  def enrich_with_youtube youtube_item&lt;/p&gt;
    &lt;p class="highlight"&gt;@data&lt;/p&gt;['views'] = youtube_item['views']
    days_available = Date.today - youtube_item['published']
    &lt;p class="highlight"&gt;@data&lt;/p&gt;['monthlyViews'] = @data['views'] * 365.0 / days_available / 12          
  end

&lt;/pre&gt;

&lt;p&gt;With that done, I can remove the updates to video's hash.&lt;/p&gt;

&lt;p class="code-label"&gt;class Video&amp;#8230;
&lt;/p&gt;

&lt;pre class="deleted"&gt;  def []= key, value
    @data[key] = value
  end
&lt;/pre&gt;

&lt;p&gt;Now I have proper objects, I can simplify the choreography
       with ids in the service method. I start by using &lt;a href="http://refactoring.com/catalog/inlineTemp.html"&gt;Inline Temp&lt;/a&gt; on &lt;code&gt;youtube_item&lt;/code&gt; and then
       replace the reference to the enumeration index with a method
       call on the video object.&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json')).map{|h| Video.new(h)}
    ids = @video_list.map{|v| v['youtubeID']}
    youtube = YoutubeGateway.new(YoutubeConnection.new.list_videos(ids))
    ids.each do |id|
      video = @video_list.find{|v| id == v['youtubeID']}
&lt;p class="deleted"&gt;      youtube_item = youtube.item(id)&lt;/p&gt;
      video.enrich_with_youtube(&lt;p class="highlight"&gt;youtube.item(video.youtube_id))&lt;/p&gt;
    end
    return JSON.dump(@video_list.map{|v| v.to_h})
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class Video&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;&lt;p class="highlight"&gt;  def youtube_id&lt;/p&gt;
    @data['youtubeID']
  end
&lt;/pre&gt;

&lt;p&gt;That allows me to just use the objects directly for the
       enumeration.&lt;/p&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json')).map{|h| Video.new(h)}
    ids = @video_list.map{|v| v['youtubeID']}
    youtube = YoutubeGateway.new(YoutubeConnection.new.list_videos(ids))
&lt;p class="highlight"&gt;    @video_list.each {|v| v.enrich_with_youtube(youtube.item(v.youtube_id))}&lt;/p&gt;
    return JSON.dump(@video_list.map{|v| v.to_h})
  end
&lt;/pre&gt;

&lt;p&gt;And remove the accessor for the hash in the video&lt;/p&gt;

&lt;p class="code-label"&gt;class Video&amp;#8230;
&lt;/p&gt;

&lt;pre class="deleted"&gt;  def [] key
    @data[key]
  end
&lt;/pre&gt;

&lt;p class="code-label"&gt;class VideoService&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json')).map{|h| Video.new(h)}
    ids = @video_list.map{|v| &lt;p class="highlight"&gt;v.youtube_id&lt;/p&gt;}
    youtube = YoutubeGateway.new(YoutubeConnection.new.list_videos(ids))
    @video_list.each {|v| v.enrich_with_youtube(youtube.item(v.youtube_id))}
    return JSON.dump(@video_list.map{|v| v.to_h})
  end
&lt;/pre&gt;

&lt;p&gt;I could replace the video object's internal hash with
       fields, but I don't think it's worth it as it's primarily
       loaded with a hash and its final output is a jsonified hash.
       An &lt;a href="http://martinfowler.com/bliki/EmbeddedDocument.html"&gt;Embedded Document&lt;/a&gt; is a perfectly
       reasonable form of domain object.&lt;/p&gt;

&lt;h2&gt;Musings on the final objects&lt;/h2&gt;

&lt;p class="figureImage"&gt;&lt;a name="separated-dependencies.png"&gt;&lt;/a&gt;&lt;img alt="Figure 5" src="refactoring-external-service/separated-dependencies.png"&gt;&lt;/p&gt;

&lt;p class="figureCaption"&gt;Figure 5: The objects created
       through this refactoring and their dependencies&lt;/p&gt;
&lt;p class="code-label"&gt;class VideoService...
&lt;/p&gt;

&lt;pre&gt;  def video_list
    @video_list = JSON.parse(File.read('videos.json')).map{|h| Video.new(h)}
    ids = @video_list.map{|v| v.youtube_id}
    youtube = YoutubeGateway.new(YoutubeConnection.new.list_videos(ids))
    @video_list.each {|v| v.enrich_with_youtube(youtube.item(v.youtube_id))}
    return JSON.dump(@video_list.map{|v| v.to_h})
  end
&lt;/pre&gt;

&lt;pre&gt;&lt;p class="highlight"&gt;class YoutubeConnection&lt;/p&gt;
  def list_videos ids
    client = GoogleAuthorizer.new(
      token_key: 'api-youtube',
      application_name: 'Gateway Youtube Example',
      application_version: '0.1'
      ).api_client
    youtube = client.discovered_api('youtube', 'v3')
    request = {
      api_method: youtube.videos.list,
      parameters: {
        id: ids.join(","),
        part: 'snippet, contentDetails, statistics',
      }
    }
    return client.execute!(request).body
  end
end
&lt;/pre&gt;

&lt;pre&gt;&lt;p class="highlight"&gt;class YoutubeGateway&lt;/p&gt;
  def initialize responseJson
    @data = JSON.parse(responseJson)['items']
      .map{|i| [ i['id'], i ] }
      .to_h
  end
  def item id
    {
      'views' =&amp;gt;  @data[id]['statistics']['viewCount'].to_i,
      'published' =&amp;gt; Date.parse(@data[id]['snippet']['publishedAt'])
    }
  end
end
&lt;/pre&gt;

&lt;pre&gt;&lt;p class="highlight"&gt;class Video&lt;/p&gt;
  def initialize aHash
    @data = aHash
  end
  def to_h
    @data
  end
  def youtube_id
    @data['youtubeID']
  end
  def enrich_with_youtube youtube_item
    @data['views'] = youtube_item['views']
    days_available = Date.today - youtube_item['published']
    @data['monthlyViews'] = @data['views'] * 365.0 / days_available / 12          
  end
end
&lt;/pre&gt;

&lt;p&gt;So what have I achieved? Refactoring often reduces code
       size, but in this case it's almost doubled from 26 to 54 lines.
       All else being equal, less code is better. But here I think the
       better modularity you get by separating the concerns is usually
       worth the size increase. This is also where the size of a pedagogical (i.e.
       toy) example can obscure the point. 26 lines of code isn't much
       to comprehend, but if we are talking about 2600 lines written
       in this style, then the modularity becomes well worth any code
       size increase. And usually any such increase is much smaller
       when you do this kind of thing with a larger code base since  
       you uncover more opportunities to reduce code size by
       eliminating duplication.&lt;/p&gt;

&lt;p&gt;You'll notice I've finished with four kinds of object here:
       coordinator, domain object, gateway, and connection. This is a
       common arrangement of responsibilities, although different cases
       see reasonable variations in how the dependencies are laid out.
       The best arrangement of responsibilities and dependencies
       differs due to particular needs. Code that needs to change
       frequently should be separated from code that changes rarely,
       or simply changes for different reasons. Code that is widely
       reused should not depend on code that is used only for a
       particular case. These drivers differ from circumstance to
       circumstance, and dicate the dependency patterns.&lt;/p&gt;

&lt;p&gt;One common change is to reverse the dependency between
       domain object and gateway - turning the gateway into a &lt;a href="http://martinfowler.com/eaaCatalog/mapper.html"&gt;Mapper&lt;/a&gt;.
       This allows the domain object to be independent of how it's
       populated, at the cost of the mapper knowing about the domain
       object and getting some access to its guts. If the domain
       object is used in many contexts, then this can be a valuable arrangement.&lt;/p&gt;

&lt;p&gt;Another change might be to shift the code for calling the
       connection from the coordinator to the gateway. This simplifies
       the coordinator but makes the gateway a bit more complex.
       Whether this is a good idea depends on whether the coordinator
       is getting too complex, or if many coordinators use the same
       gateway leading to duplicate code in setting up the connection.&lt;/p&gt;

&lt;p&gt;I also think it's likely I'd move some of the behavior of
       the connection out to the caller, particularly if the caller is
       a gateway object. The gateway knows what data it needs, so
       should supply the list of parts in the parameters of the call.
       But that's really only an issue once we have other clients
       calling &lt;code&gt;list_videos&lt;/code&gt;, so I'd be inclined to wait until that day. &lt;/p&gt;

&lt;p&gt;One thing that I feel is important, whatever the details of
       your case, is to have a consistent naming policy for the roles
       of the objects involved. I sometimes hear people say that you
       shouldn't put pattern names into your code, but I don't agree.
       Often pattern names help to communicate roles different
       elements play, so it's silly to spurn the opportunity.
       Certainly within a team your code will show common patterns and
       the naming should reflect that. I use "gateway" following my
       coining of the &lt;a href="http://martinfowler.com/eaaCatalog/gateway.html"&gt;Gateway&lt;/a&gt; pattern in P of
       EAA. I've used "connection" here to show the raw link to an
       external system, and intend to use that convention in my
       future writing. This naming convention isn't universal and while my
       pride would be gratifyingly inflated by you using my naming
       conventions, the important point isn't which naming convention
       you should use but that you should pick some convention.&lt;/p&gt;

&lt;p&gt;When I break a method into a group of objects like this,
       there's a natural question about the consequences for testing.
       I had a unit test for the original method in the video service,
       should I now write tests for the three new classes? My
       inclination is that, providing the existing tests cover the
       behavior sufficiently, there isn't a need to add any more right
       away. As we add more behavior, then we should add more
       tests, and if this behavior is added to the new objects then
       the new tests will focus on them. In time that may mean that
       some of tests currently targeting the video service will look
       out of place and should move. But all of this in the future and
       should be dealt with in the future.&lt;/p&gt;

&lt;p&gt;A particular concern I'd be watching for in the tests is the
       use of the stubs I put in over the YouTube connection. It's
       very easy for stubs like this to get out of hand, then they can
       actually slow down changes because a simple production code
       change leads to updating many tests. The essence here is to pay
       attention to duplication in the test code and address it as
       seriously as you do with duplication in production code.&lt;/p&gt;

&lt;p&gt;Such thinking about organizing test doubles naturally leads
       into the broader question of assembling service
       objects. Now that I have split a behavior from a single service
       object into three service objects and a domain entity (using
       the &lt;a href="http://martinfowler.com/bliki/EvansClassification.html"&gt;Evans Classification&lt;/a&gt;) there's a natural
       question about how the service objects should be instantiated,
       configured, and assembled. Currently the video service
       does this directly for its dependents, but this can easily get
       out of hand with larger systems. To handle this complexity it's
       common to use techniques such as &lt;a href="/articles/injection.html"&gt;Service Locators and
       Dependency Injection&lt;/a&gt;. I'm not going to talk about that
       right now, but that may be the topic for a follow-on article.&lt;/p&gt;

&lt;p&gt;This example uses objects, in large part because I'm far
       more familiar with object-oriented style than functional
       styles. But I would expect the fundamental division of
       responsibilities to be the same, but with boundaries set by
       function (or perhaps namespace) rather than classes and
       methods. Some other details would change, the video object
       would be a data structure and enriching it would create new
       data structures rather than modifying in place. Looking at this
       in a functional style would be an interesting article.&lt;/p&gt;

&lt;p&gt;Finally I want to re-stress an important general point about
       refactoring. Refactoring isn't a term you should use to any
       restructuring of a code base. It specifically means the
       approach that applies a series of very small
       behavior-preserving changes. We saw a couple of examples here
       where I deliberately introduced code that I knew I was going to
       remove shortly afterwards, just so I can take small steps that
       preserve behavior. This is the essence of refactoring, as I
       recently tweeted:&lt;/p&gt;

&lt;blockquote class="twitter-tweet" lang="en"&gt;Refactoring is a specific way to
           change code by a series of tiny behavior-preserving
       transformations. It is not just moving code around.&lt;a href="https://twitter.com/martinfowler/status/564813269102493696"&gt;-- Martin Fowler&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;The point here is that by taking small steps, you end up
       going faster because you don't break anything and thus avoid
       debugging. Most people find this counter-intuitive, I certainly
       did when Kent Beck first showed me how he refactored. But I quickly
       discovered how effective it is.&lt;/p&gt;


&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/articles/refactoring-external-service.html&amp;amp;text=Refactoring%20code%20that%20accesses%20external%20services" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/articles/refactoring-external-service.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/articles/refactoring-external-service.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;For articles on similar topics&amp;#8230;&lt;/h2&gt;

&lt;p&gt;&amp;#8230;take a look at the following tags:&lt;/p&gt;

 
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:43:31 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:refactoring-code-that-accesses-external-services.html</guid></item><item><title>Retreaded: ConversationalStories</title><link>http://www.ciandcd.com/retreaded-conversationalstories.html</link><description>&lt;div&gt; 

 

 

&lt;p class="tagLabel"&gt;tags:&lt;/p&gt;

&lt;p class="clear"&gt;&lt;/p&gt;

&lt;p&gt;Here's a common misconception about agile methods. It centers on
  the way user stories are created and flow through the development
  activity. The misconception is that the product owner (or business
  analysts) creates user stories and then put them in front of
  developers to implement. The notion is that this is a flow from
  product owner to development, with the product owner responsible for
  determining &lt;i&gt;what&lt;/i&gt; needs to be done and the developers
  &lt;i&gt;how&lt;/i&gt; to do it.&lt;/p&gt;
&lt;img src="images/conversationalStories/decreed.png"&gt;
&lt;p&gt;A justification for this approach is that this separates the
  responsibilities along the lines of competence. The product owner
  knows the business, what the software is for, and thus what needs to
  be done. The developers know technology and know how to do things,
  so they can figure out how to realize the demands of the product
  owner.&lt;/p&gt;

&lt;p&gt;This notion of product owners coming up with
  &lt;a href="DecreedStories.html"&gt;DecreedStories&lt;/a&gt; is a profound misunderstanding of the way
  agile development should work. When we were brainstorming names at
  &lt;a href="http://martinfowler.com/articles/agileStory.html"&gt;Snowbird&lt;/a&gt;, I
  remember Kent suggesting "conversational". This emphasized the fact
  that the heart of our thinking was of an on-going conversation
  between customers and developers about how a development project
  should proceed.&lt;/p&gt;
&lt;img src="images/conversationalStories/conversation.png"&gt;
&lt;p&gt;In terms of coming up with stories, what this means is that they
  are always something to be refined through conversation - and that
  developers should play an active role in helping that
  definition.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;spotting inconsistencies and gaps between the stories&lt;/li&gt;

&lt;li&gt;using technical knowledge to come up with new stories that
    seem to fit the product owner's vision&lt;/li&gt;

&lt;li&gt;seeing alternative stories that would be cheaper to build
    given the technological landscape&lt;/li&gt;

&lt;li&gt;split stories to make them easier to plan or implement&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is the Negotiable principle in Bill Wake's &lt;a href="http://xp123.com/xplor/xp0308"&gt;INVEST&lt;/a&gt; test for
  stories. Any member of an agile team can create stories and suggest
  modifications. It may be that just a few members of a team gravitate
  to writing most of the stories. That's up to the team's
  self-organization as to how they want that to happen. But everyone
  should be engaged in coming up and refining stories. (This
  involvement is in addition to the develpers' responsibility to
  estimate stories.)&lt;/p&gt;

&lt;p&gt;The product owner does have a special responsibility. In the end
  the product owner is the final decider on stories, particularly
  their prioritization. This reflects the fact that the product owner
  should be the best person to judge that slippery attribute of
  business value. But having a final decision maker should never stop
  others from participating, and should not lead people astray into a
  decreed model of stories.&lt;/p&gt;

&lt;p class="repost"&gt;reposted on 19 Feb 2015&lt;/p&gt;

&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/bliki/ConversationalStories.html&amp;amp;text=Bliki:%20ConversationalStories" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/bliki/ConversationalStories.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/bliki/ConversationalStories.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:43:29 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:retreaded-conversationalstories.html</guid></item><item><title>Bliki: BeckDesignRules</title><link>http://www.ciandcd.com/bliki-beckdesignrules.html</link><description>&lt;div&gt; 

 

 

&lt;p class="tagLabel"&gt;tags:&lt;/p&gt;

&lt;p class="clear"&gt;&lt;/p&gt;

&lt;p&gt;Kent Beck came up with his four rules of simple design while he
  was developing &lt;a href="ExtremeProgramming.html"&gt;ExtremeProgramming&lt;/a&gt; in the late 1990's. I express
  them like this. &lt;a href="#footnote-xp-formulation"&gt;[1]&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Passes the tests&lt;/li&gt;

&lt;li&gt;Reveals intention&lt;/li&gt;

&lt;li&gt;No duplication&lt;/li&gt;

&lt;li&gt;Fewest elements&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The rules are in priority order, so "passes the tests" takes
  priority over "reveals intention"&lt;/p&gt;

&lt;img src="images/beckDesignRules/kent.jpg" width="200px"&gt;
&lt;p&gt;&lt;i&gt;
    Kent Beck developed Extreme Programming, Test Driven Development,
    and can always be relied on for good Victorian facial hair for his
    local ballet.
  &lt;/i&gt;&lt;/p&gt;
&lt;p&gt;The most important of the rules is "passes the tests". XP was
  revolutionary in how it raised testing to a first-class activity in
  software development, so it's natural that testing should play a
  prominent role in these rules. The point is that whatever else you
  do with the software, the primary aim is that it works as
  intended and tests are there to ensure that happens.&lt;/p&gt;

&lt;p&gt;"Reveals intention" is Kent's way of saying the code
  should be easy to understand. Communication is a core value of
  Extreme Programing, and many programmers like to stress that
  programs are there to be read by people. Kent's form of expressing
  this rule implies that the key to enabling understanding is to express your
  intention in the code, so that your readers can understand what your
  purpose was when writing it.&lt;/p&gt;

&lt;p&gt;The "no duplication" is perhaps the most powerfully subtle of
  these rules. It's a notion expressed elsewhere as DRY or SPOT &lt;a href="#footnote-dry"&gt;[2]&lt;/a&gt;, Kent
  expressed it as saying everything should be said "Once and only Once."
  Many programmers have observed that the exercise of eliminating
  duplication is a powerful way to drive out good designs. &lt;a href="#footnote-dup-column"&gt;[3]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The last rule tells us that anything that doesn't serve the three
  prior rules should be removed. At the time these rules were
  formulated there was a lot of design advice around adding elements to
  an architecture in order to increase flexibility for future requirements.
  Ironically the extra complexity of all of these elements usually
  made the system harder to modify and thus less flexible in practice.&lt;/p&gt;

&lt;p&gt;People often find there is some tension between "no duplication"
  and "reveals intention", leading to arguments about which order
  those rules should appear. I've always seen their order as
  unimportant, since they feed off each other in refining the code. Such things
  as adding duplication to increase clarity is often papering over a problem,
  when it would be better to solve it. &lt;a href="#footnote-kent-empathy"&gt;[4]&lt;/a&gt;&lt;/p&gt;

&lt;img src="images/beckDesignRules/sketch.png" width="500px"&gt;
&lt;p&gt;&lt;i&gt;&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;What I like about these rules is that they are very simple to
  remember, yet following them improves code in any language or
  programming paradigm that I've worked with. They are an example of
  Kent's skill in finding principles that are generally applicable and
  yet concrete enough to shape my actions.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;At the time there was a lot of &amp;#8220;design is subjective&amp;#8221;, &amp;#8220;design is
  a matter of taste&amp;#8221; bullshit going around. I disagreed. There are
  better and worse designs. These criteria aren&amp;#8217;t perfect, but they
  serve to sort out some of the obvious crap and (importantly) you can
  evaluate them right now. The real criteria for quality of design,
  &amp;#8220;minimizes cost (including the cost of delay) and maximizes benefit
  over the lifetime of the software,&amp;#8221; can only be evaluated post hoc,
  and even then any evaluation will be subject to a large bag full of
  cognitive biases. The four rules are generally predictive.&lt;/p&gt;

&lt;p class="quote-attribution"&gt;-- Kent Beck&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2&gt;Further Reading&lt;/h2&gt;

&lt;p&gt;There are many expressions of these rules out there, here are a
    few that I think are worth exploring:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.jbrains.ca/permalink/the-four-elements-of-simple-design"&gt;J.B.
      Rainsberger's summary&lt;/a&gt;. He also has a good discussion of &lt;a href="http://blog.thecodewhisperer.com/2013/12/07/putting-an-age-old-battle-to-rest/"&gt;the
      interplay between the rules 2&amp;amp;3.&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;&lt;a href="http://xprogramming.com/classics/expemergentdesign/"&gt;Ron Jeffries&lt;/a&gt;&lt;/li&gt;

&lt;li&gt;These rules, like much else of Extreme Programming, were
      originally discussed and refined &lt;a href="http://c2.com/cgi/wiki?XpSimplicityRules"&gt;on Ward's Wiki&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;

&lt;p&gt;Kent reviewed this post and sent me some very helpful feedback,
    much of which I appropriated into the text.&lt;/p&gt;
&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/bliki/BeckDesignRules.html&amp;amp;text=Bliki:%20BeckDesignRules" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/bliki/BeckDesignRules.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/bliki/BeckDesignRules.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:43:27 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:bliki-beckdesignrules.html</guid></item><item><title>Retreaded: CodeAsDocumentation</title><link>http://www.ciandcd.com/retreaded-codeasdocumentation.html</link><description>&lt;div&gt; 

 

 

&lt;p class="tagLabel"&gt;tags:&lt;/p&gt;

&lt;p class="clear"&gt;&lt;/p&gt;

&lt;p&gt;One of the common elements of agile methods is that they raise
	 programming to a central role in software
	development - one much greater than the software engineering
	community usually does. Part of this is classifying the code as a
	major, if not the primary documentation of a software system.&lt;/p&gt;

&lt;p&gt;Almost immediately I feel the need to rebut a common
	misunderstanding. Such a principle is not saying that code is the
	only documentation. Although I've often heard this said of Extreme
	Programming - I've never heard the leaders of the Extreme
	Programming movement say this. Usually there is a need for
	further documentation to act as a supplement to the code. &lt;/p&gt;

&lt;p&gt;The rationale for the code being the primary source of
	documentation is that it is the only one that is sufficiently
	detailed and precise to act in that role - a point made so
	eloquently by Jack Reeves's famous essay &lt;a href="http://www.developerdotstar.com/mag/articles/reeves_design_main.html"&gt;"What
  is Software Design?"&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This principle comes with a important consequence - that it's
	important that programmers put in the effort to make sure that this
	code is clear and readable. Saying that code is documentation isn't
	saying that a particular code base is good documentation. Like any
	documentation, code can be clear or it can be gibberish. Code is no
	more inherently clear than any other form of documentation. (And
	other forms of documentation can be hopelessly unclear too - I've
	seen plenty of gibberish UML diagrams, to flog a popular horse.)&lt;/p&gt;

&lt;p&gt;Certainly it seems that most code bases aren't very good
	documentation. But just as it's a fallacy to conclude that declaring
	code to be documentation excludes other forms, it's a fallacy to say
	that because code is often poor documentation means that it's
	&lt;i&gt;necessarily&lt;/i&gt; poor. It is possible to write clear code, indeed I'm
	convinced that most code bases can be made much more clear.&lt;/p&gt;

&lt;p&gt;I think part of the reason that code is often so hard to read is
	because people aren't taking it seriously as documentation. If
	there's no will to make code clear, then there's little chance it
	will spring into clarity all by itself. So the first step to clear
	code is to accept that code is documentation, and then put the
	effort in to make it be clear. I think this comes down to what was
	taught to most programmers when they began to program. My teachers didn't put much emphasis on
	making code clear, they didn't seem to value it and certainly didn't
	talk about how to do it. We as a whole industry need to put much
	more emphasis on valuing the clarity of code.&lt;/p&gt;

&lt;p&gt;The next step is to learn how, and here let me offer you the
	advice of a best selling technical author - there's nothing like
	review. I would never think of publishing a book without having many
	people read it and give me feedback. Similarly there's nothing more
	important to clear code than getting feedback from others about
	what is or isn't easy to understand. So take every opportunity to
	find ways to get other people to read your code. Find out what they
	find easy to understand, and what things confuse them. (Yes, pair
	programming is a great way to do this.)&lt;/p&gt;

&lt;p&gt;For more concrete advice - well I suggest reading good books on
	programming style. &lt;a href="http://www.amazon.com/gp/product/0735619670?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=0735619670"&gt;Code Complete&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt; is the first place to look. I'll
	naturally suggest &lt;a href="http://martinfowler.com/books/refactoring.html"&gt;Refactoring&lt;/a&gt; - after all much of refactoring is
	about making code clearer. After Refactoring, &lt;a href="http://martinfowler.com/books/r2p.html"&gt;Refactoring to
	Patterns&lt;/a&gt; is an obvious suggestion. &lt;/p&gt;

&lt;p&gt;You'll always find people will disagree on various
	points. Remember that a code base is owned primarily by a team (even
	if you practice individual code ownership over bits of it). A
	professional programmer is prepared to bend her personal style to
	reflect the needs of the team. So even if you like ternary operators
	don't use them if your team doesn't find them easy to
	understand. You can program in your own style on your personal
	projects, but anything you do in a team should follow the needs of
	that team. &lt;/p&gt;

&lt;p class="repost"&gt;reposted on 25 Mar 2015&lt;/p&gt;

&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/bliki/CodeAsDocumentation.html&amp;amp;text=Bliki:%20CodeAsDocumentation" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/bliki/CodeAsDocumentation.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/bliki/CodeAsDocumentation.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:43:25 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:retreaded-codeasdocumentation.html</guid></item><item><title>Bliki: MicroservicePremium</title><link>http://www.ciandcd.com/bliki-microservicepremium.html</link><description>&lt;div&gt; 

 

 

&lt;p class="tagLabel"&gt;tags:&lt;/p&gt;

&lt;p class="clear"&gt;&lt;/p&gt;

&lt;p&gt;The &lt;a href="/articles/microservices.html"&gt;microservices
  architectural style&lt;/a&gt; has been the hot topic over the last year.
  At the recent &lt;a href="http://softwarearchitecturecon.com/sa2015"&gt;O'Reilly software
  architecture conference&lt;/a&gt;, it seemed like every session talked
  about microservices. Enough to get everyone's over-hyped-bullshit
  detector up and flashing. One of the consequences of this is that
  we've seen teams be too eager to embrace microservices, &lt;a href="#footnote-envy"&gt;[1]&lt;/a&gt; not
  realizing that microservices introduce complexity on their own
  account. This adds a premium to a project's cost and risk - one that
  often gets projects into serious trouble. &lt;/p&gt;

&lt;p&gt;While this hype around microservices is annoying, I do think it's a
  useful bit of terminology for a style of architecture which has been
  around for a while, but needed a name to make it easier to talk
  about. The important thing here is not how annoyed you feel about the
  hype, but the architectural question it raises: &lt;b&gt;is a microservice
  architecture a good choice for the system
  you're working on?&lt;/b&gt;&lt;/p&gt;

&lt;blockquote class="twitter-tweet" lang="en"&gt;
    any decent answer to an interesting question begins, "it depends..."
  &lt;a href="https://twitter.com/KentBeck/status/596007846887628801"&gt;-- Kent Beck&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;"It depends" must start my answer, but then I must shift the
  focus to what factors it depends &lt;i&gt;on&lt;/i&gt;. The fulcrum of whether
  or not to use microservices is the complexity of the system you're
  contemplating. The microservices approach is all about handling a
  complex system, but in order to do so the approach introduces its
  own set of complexities. When you use microservices you have to work
  on automated deployment, monitoring, dealing with failure, eventual
  consistency, and other factors that a distributed system introduces.
  There are well-known ways to cope with all this, but it's extra
  effort, and nobody I know in software development seems to have
  acres of free time.&lt;/p&gt;
&lt;img src="images/microservice-verdict/productivity.png"&gt;
&lt;p&gt;So my primary guideline would be &lt;b&gt;don't even consider
  microservices unless you have a system that's too complex to manage
  as a monolith&lt;/b&gt;. The majority of software systems should be built
  as a single monolithic application. Do pay attention to good
  modularity within that monolith, but don't try to separate it into
  separate services.&lt;/p&gt;

&lt;p&gt;The complexity that drives us to microservices can come from many
  sources including
  dealing with large teams &lt;a href="#footnote-conway"&gt;[2]&lt;/a&gt;, &lt;a href="http://samnewman.io/blog/2015/05/05/single-tenancy-vs-multi-tenancy/"&gt;multi-tenancy&lt;/a&gt;,
  supporting many
  user interaction models, allowing different business functions to
  evolve independently, and scaling. But the biggest factor
  is that of sheer size - people finding they have a monolith that's too big
  to modify and deploy.&lt;/p&gt;

&lt;p&gt;At this point I feel a certain frustration. Many of the problems
  ascribed to monoliths aren't essential to that style. I've heard people say that
  you need to use microservices because it's impossible to do
  &lt;a href="ContinuousDelivery.html"&gt;ContinuousDelivery&lt;/a&gt; with monoliths - yet there are plenty of
  organizations that succeed with a &lt;a href="http://paulhammant.com/2011/11/29/cookie-cutter-scaling/"&gt;cookie-cutter
  deployment&lt;/a&gt; approach: Facebook and Etsy are two well-known
  examples.&lt;/p&gt;

&lt;p&gt;I've also heard arguments that say that as a system increases in
  size, you have to use microservices in order to have parts that are
  easy to modify and replace. Yet there's no reason why you can't make
  a single monolith with well defined module boundaries. At least
  there's no reason &lt;i&gt;in theory&lt;/i&gt;, in practice it seems too easy for
  module boundaries to be breached and monoliths to get tangled as
  well as large.&lt;/p&gt;

&lt;p&gt;We should also remember that there's a substantial variation in
  service-size between different microservice systems. I've seen
  microservice systems vary from a team of 60 with 20 services to a
  team of 4 with 200 services. It's not clear to what degree service
  size affects the premium.&lt;/p&gt;

&lt;p&gt;As size and other complexity boosters kick into a project I've
  seen many teams find that microservices are a better place to be.
  But unless you're faced with that complexity, remember that the
  microservices approach brings a high premium, one that can slow down
  your development considerably. So if you can keep your system simple
  enough to avoid the need for microservices: do.&lt;/p&gt;

 

&lt;p class="acknowledgements"&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;

    I stole much of this thinking from my colleagues: James Lewis, Sam
    Newman, Thiyagu Palanisamy, and Evan Bottcher. Stefan Tilkov's
    comments on an earlier draft were instrumental in sharpening this post. Rob
    Miles, David Nelson, Brian Mason, and Scott Robinson discussed
    drafts of this article on our internal mailing list.
  &lt;/p&gt;

&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/bliki/MicroservicePremium.html&amp;amp;text=Bliki:%20MicroservicePremium" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/bliki/MicroservicePremium.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/bliki/MicroservicePremium.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:43:22 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:bliki-microservicepremium.html</guid></item><item><title>Bliki: Yagni</title><link>http://www.ciandcd.com/bliki-yagni.html</link><description>&lt;div&gt; 

 

 

&lt;p class="tagLabel"&gt;tags:&lt;/p&gt;

&lt;p class="clear"&gt;&lt;/p&gt;

&lt;p&gt;Yagni originally is an acronym that stands for "You Aren't Gonna
  Need It". It is a mantra from &lt;a href="ExtremeProgramming.html"&gt;ExtremeProgramming&lt;/a&gt;
  that's often used generally in agile software teams. It's a
  statement that some capability we presume our software needs in the future
  should not be built now because "you aren't gonna need it". &lt;/p&gt;

&lt;p&gt;Yagni is a way to refer to the XP practice of Simple Design (from
  the first edition of &lt;a href="http://www.amazon.com/gp/product/0321278658?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=0321278658"&gt;The White Book&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt;, the second edition refers to
  the related notion of "incremental design"). &lt;a href="#footnote-origin"&gt;[1]&lt;/a&gt; Like many elements of XP, it's a sharp contrast to
  elements of the widely held principles of software engineering in
  the late 90s. At that time there was a big push for careful up-front
  planning of software development.&lt;/p&gt;

&lt;p&gt;Let's imagine I'm working with a startup in Minas Tirith selling
  insurance for the shipping business. Their software system is broken
  into two main components: one for pricing, and one for sales. The
  dependencies are such that they can't usefully build
  sales software until the relevant pricing software is completed.&lt;/p&gt;

&lt;p&gt;At the moment, the team is working on updating the pricing
  component to add support for risks from storms. They know that in six
  months time, they will need to also support pricing for piracy
  risks. Since they are currently working on the pricing engine they consider
  building the presumptive feature &lt;a href="#footnote-presumptive-feature"&gt;[2]&lt;/a&gt; for piracy pricing now, since that way the pricing
  service will be complete before they start working on the sales
  software.&lt;/p&gt;

&lt;p&gt;Yagni argues against this, it says that since you won't need
  piracy pricing for six months you shouldn't build it until it's
  necessary. So if you think it will take two months to build this
  software, then you shouldn't start for another four months
  (neglecting any buffer time for schedule risk and updating the sales
  component). &lt;/p&gt;

&lt;p&gt;The first argument for yagni is that while we may now think we
  need this presumptive feature, it's likely that we will be wrong.
  After all the context of agile methods is an acceptance that we
  welcome changing requirements. A plan-driven requirements guru might
  counter argue that this is because we didn't do a good-enough job of
  our requirements analysis, we should have put more time and effort
  into it. I counter that by pointing out how difficult and costly it is to
  figure out your needs in advance, but even if you can, you can still
  be blind-sided when the Gondor Navy wipes out the pirates, thus
  undermining the entire business model.&lt;/p&gt;

&lt;p&gt;In this case, there's an obvious cost of the presumptive feature
  - the &lt;b&gt;cost of build&lt;/b&gt;: all the effort spent on analyzing, programming,
  and testing this now useless feature.&lt;/p&gt;

&lt;p&gt;But let's consider that we were completely correct with our
  understanding of our needs, and the Gondor Navy didn't wipe out the
  pirates. Even in this happy case, building the
  presumptive feature incurs two
  serious costs. The first cost is the cost of delayed value. By
  expending our effort on the piracy pricing software we didn't build
  some other feature.  If we'd instead put our energy into building
  the sales software for weather risks, we could have put a full
  storm risks feature into production and be generating revenue two
  months earlier. This &lt;b&gt;cost of delay&lt;/b&gt; due to the presumptive feature is
  two months revenue from storm insurance.&lt;/p&gt;

&lt;p&gt;The common reason why people build presumptive features is
  because they think it will be cheaper to build it now rather than
  build it later. But that cost comparison has to be made at least
  against the cost of delay,  preferably factoring in the
  probability that you're building an unnecessary feature, for which
  your odds are at least &amp;#8532;. &lt;a href="#footnote-kohavi"&gt;[3]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Often people don't think through the comparative cost of building
  now to building later. One approach I use when mentoring developers
  in this situation is to ask them to imagine any refactoring they
  would have to do later to introduce the capability when it's needed.
  Often that thought experiment is enough to convince them that it
  won't be significantly more expensive to add it later. Another
  result from such an imagining is to add something that's easy to do
  now, adds minimal complexity, yet significantly reduces the later
  cost. Using lookup tables for error messages rather than inline
  literals are an example that are simple yet
  make later translations easier to support.&lt;/p&gt;

&lt;blockquote class="twitter-tweet" lang="en"&gt;
    Reminder, any extensibility point that&amp;#8217;s never used isn&amp;#8217;t just
    wasted effort, it&amp;#8217;s likely to also get in your way as well
  &lt;a href="https://twitter.com/jeremydmiller/status/568797862441586688"&gt;-- Jeremy Miller&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;The cost of delay is one cost that a successful presumptive
  feature imposes, but another is the &lt;b&gt;cost of carry&lt;/b&gt;. The code for the
  presumptive feature adds some complexity to the software, this
  complexity makes it harder to modify and debug that software, thus
  increasing the cost of other features. The extra complexity from
  having the piracy-pricing feature in the software might add a couple
  of weeks to how long it takes to build the storm insurance sales
  component. That two weeks hits two ways: the additional cost to
  build the feature, plus the additional cost of delay since it look
  longer to put it into production. We'll incur a cost of carry on every
  feature built between now and the time the piracy insurance software
  starts being useful. Should we never need the piracy-pricing
  software, we'll incur a cost of carry on every feature built until
  we remove the piracy-pricing feature (assuming we do), together with
  the cost of removing it.&lt;/p&gt;

&lt;p&gt;So far I've divided presumptive features in two categories:
  successful and unsuccessful. Naturally there's really a spectrum
  there, and with one point on that spectrum that's worth highlighting: the right
  feature built wrong. Development teams are always learning, both
  about their users and about their code base. They learn about the
  tools they're using and these tools go through regular upgrades. They
  also learn about how their code works together. All this means that
  you often realize that a feature coded six months ago wasn't done
  the way you now realize it should be done. In that case you have
  accumulated &lt;a href="TechnicalDebt.html"&gt;TechnicalDebt&lt;/a&gt; and have to
  consider the &lt;b&gt;cost of repair&lt;/b&gt; for that feature or the on-going
  costs of working around its difficulties.&lt;/p&gt;

&lt;p&gt;So we end up with three classes of presumptive features, and four
  kinds of costs that occur when you neglect yagni for them.&lt;/p&gt;
&lt;img src="images/yagni/sketch.png"&gt;
&lt;p&gt;My insurance example talks about relatively user-visible
  functionality, but the same argument applies for abstractions to
  support future flexibility. When building the storm risk calculator,
  you may consider putting in abstractions and parameterizations now
  to support piracy and other risks later. Yagni says not to do this,
  because you may not need the other pricing functions, or if you do your
  current ideas of what abstractions you'll need will not match what
  you learn when you do actually need them. This doesn't mean to
  forego all abstractions, but it does mean any abstraction that makes
  it harder to understand the code for current requirements is
  presumed guilty.&lt;/p&gt;

&lt;p&gt;Yagni is at its most visible with larger features, but you see it
  more frequently with small things. Recently I wrote some code that
  allows me to highlight part of a line of code. For this, I allow the
  highlighted code to be specified using a regular expression. One
  problem I see with this is that since the whole regular expression
  is highlighted, I'm unable to deal with the case where I need the
  regex to match a larger section than what I'd like to highlight. I
  expect I can solve that by using a group within the regex and
  letting my code only highlight the group if a group is present. But I
  haven't needed to use a regex that matches more than what I'm
  highlighting yet, so I haven't extended my highlighting code to
  handle this case - and won't until I actually need it. For similar
  reasons I don't add fields or methods until I'm actually ready to
  use them.&lt;/p&gt;

&lt;p&gt;Small yagni decisions like this fly under the radar of project
  planning. As a developer it's easy to spend an hour adding an abstraction
  that we're sure will soon be needed. Yet all the arguments above
  still apply, and a lot of small yagni decisions add up to
  significant reductions in complexity to a code base, while speeding
  up delivery of features that are needed more urgently.&lt;/p&gt;

&lt;p&gt;Now we understand why yagni is important we can dig into a common
  confusion about yagni. &lt;b&gt;Yagni only applies to capabilities built
  into the software to support a presumptive feature, it does not
  apply to effort to make the software easier to modify.&lt;/b&gt; Yagni is
  only a viable strategy if the code is easy to change, so expending
  effort on refactoring isn't a violation of yagni because refactoring
  makes the code more malleable. Similar reasoning applies for
  practices like &lt;a href="SelfTestingCode.html"&gt;SelfTestingCode&lt;/a&gt; and
  &lt;a href="ContinuousDelivery.html"&gt;ContinuousDelivery&lt;/a&gt;. These are &lt;a href="/articles/designDead.html"&gt;enabling practices for evolutionary
  design&lt;/a&gt;, without them yagni turns from a beneficial practice into
  a curse. But if you do have a malleable code base, then yagni
  reinforces that flexibility. Yagni has the curious property that it
  is both enabled by and enables evolutionary design.
  &lt;/p&gt;

&lt;p&gt;
    Yagni is not a justification for neglecting the health of your
    code base. Yagni requires (and enables) malleable code.
  &lt;/p&gt;
&lt;p&gt;I also argue that yagni only applies when you introduce extra
  complexity now that you won't take advantage of until later. If you
  do something for a future need that doesn't actually increase the
  complexity of the software, then there's no reason to invoke
  yagni.&lt;/p&gt;

&lt;p&gt;Having said all this, there are times when applying yagni does cause
  a problem, and you are faced with an expensive change when an
  earlier change would have been much cheaper. The tricky thing here
  is that these cases are hard to spot in advance, and much easier to
  remember than the cases where yagni saved effort &lt;a href="#footnote-availability"&gt;[4]&lt;/a&gt;. My sense is that yagni-failures are relatively
  rare and their costs are easily outweighed by when yagni
  succeeds.&lt;/p&gt;

&lt;h2&gt;Further Reading&lt;/h2&gt;

&lt;p&gt;My essay &lt;a href="/articles/designDead.html"&gt;Is Design
    Dead&lt;/a&gt; talks in more detail about the role of design and
    architecture in agile projects, and thus role yagni plays as an
    enabling practice.&lt;/p&gt;

&lt;p&gt;This principle was first discussed and fleshed out on &lt;a href="http://c2.com/cgi/wiki?YouArentGonnaNeedIt"&gt;Ward's Wiki&lt;/a&gt;.&lt;/p&gt;
&lt;p class="acknowledgements"&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;

    Rachel Laycock talked through this post with me and played a
    critical role in its final organization. Chet Hendrickson and
    Steven Lowe reminded
    me to discuss small-scale yagni decisions.

    Rebecca Parsons, Alvaro Cavalcanti, Mark Taylor, Aman King, Rouan
    Wilsenach, Peter Gillard-Moss, Kief Morris, Ian Cartwright, James
    Lewis, Kornelis Sietsma, and Brian Mason participated in an insightful
    discussion about drafts of this article on our internal mailing list.
    
  &lt;/p&gt;

&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/bliki/Yagni.html&amp;amp;text=Bliki:%20Yagni" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/bliki/Yagni.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/bliki/Yagni.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:43:20 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:bliki-yagni.html</guid></item><item><title>Bliki: MonolithFirst</title><link>http://www.ciandcd.com/bliki-monolithfirst.html</link><description>&lt;div&gt; 

 

 

&lt;p class="tagLabel"&gt;tags:&lt;/p&gt;

&lt;p class="clear"&gt;&lt;/p&gt;

&lt;p&gt;As I hear stories about teams using a &lt;a href="/articles/microservices.html"&gt;microservices architecture&lt;/a&gt;, I've
    noticed a common pattern.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Almost all the successful microservice stories have started with a
      monolith that got too big and was broken up&lt;/li&gt;

&lt;li&gt;Almost all the cases where I've heard of a system that was built as a
      microservice system from scratch, it has ended up in serious trouble.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This pattern has led many of my colleagues to argue that &lt;b&gt;you
    shouldn't start a new project with microservices, even if you're
    sure your application will be big enough to make it worthwhile.
    &lt;/b&gt;.&lt;/p&gt;
&lt;img src="images/microservice-verdict/path.png"&gt;
&lt;p&gt;Microservices are a useful architecture, but even their advocates
  say that using them incurs a significant
  &lt;a href="MicroservicePremium.html"&gt;MicroservicePremium&lt;/a&gt;, which means they are only useful
  with more complex systems. This premium, essentially the cost of
  managing a suite of services, will slow down a team, favoring a
  monolith for simpler applications. This leads to a powerful argument
  for a monolith-first strategy, where you should build a new
  application as a monolith initially, even if you think it's likely
  that it will benefit from a microservices architecture later on.&lt;/p&gt;

&lt;p&gt;The first reason for this is classic &lt;a href="Yagni.html"&gt;Yagni&lt;/a&gt;. When you begin a new
  application, how sure are you that it will be useful to your users?
  It may be hard to scale a poorly designed but successful software
  system, but that's still a better place to be than its inverse. As
  we're now recognizing, often the best way to find out if a software
  idea is useful is to build a simplistic version of it and see how
  well it works out. During this first phase you need to prioritize
  speed (and thus cycle time for feedback), so the premium of
  microservices is a drag you should do without.&lt;/p&gt;

&lt;p&gt;The second issue with starting with microservices is that they
  only work well if you come up with good, stable boundaries between
  the services - which is essentially the task of drawing up the right
  set of &lt;a href="BoundedContext.html"&gt;BoundedContexts&lt;/a&gt;. Any refactoring of functionality
  between services is much harder than it is in a monolith. But even
  experienced architects working in familiar domains have great
  difficulty getting boundaries right at the beginning. By building a
  monolith first, you can figure out what the right boundaries are,
  before a microservices design brushes a layer of treacle over them.
  It also gives you time to develop the
  &lt;a href="MicroservicePrerequisites.html"&gt;MicroservicePrerequisites&lt;/a&gt; you need for finer-grained
  services. &lt;/p&gt;

&lt;p&gt;I've heard different ways to execute a monolith-first strategy.
  The logical way is to design a monolith carefully,
  paying attention to modularity within the software, both at the API
  boundaries and how the data is stored. Do this well, and it's a
  relatively simple matter to make the shift to microservices. However
  I'd feel much more comfortable with this approach if I'd heard a
  decent number of stories where it worked out that way. &lt;a href="#footnote-typical-monolith"&gt;[1]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A more common approach is to start with a monolith and gradually
  peel off microservices at the edges. Such an approach can leave a
  substantial monolith at the heart of the microservices
  architecture, but with most new development occurring in the
  microservices while the monolith is relatively quiescent. &lt;/p&gt;

&lt;p&gt;Another common approach is to just replace the monolith entirely.
  Few people look at this as an approach to be proud of, yet there are
  advantages to building a monolith as a
  &lt;a href="SacrificialArchitecture.html"&gt;SacrificialArchitecture&lt;/a&gt;. Don't be afraid of building a
  monolith that you will discard, particularly if a monolith can get
  you to market quickly.&lt;/p&gt;

&lt;p&gt;Another route I've run into is to start with just a couple of
  coarse-grained services, larger than those you expect to end up
  with. Use these coarse-grained services to get used to working with
  multiple services, while enjoying the fact that such coarse granularity
  reduces the amount of inter-service refactoring you have to do. Then
  as boundaries stabilize, break down into finer-grained services. &lt;a href="#footnote-duolith"&gt;[2]&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;While the bulk of my contacts lean toward the monolith-first
  approach, it is &lt;a href="/articles/dont-start-monolith.html"&gt;by no
  means unanimous&lt;/a&gt;. The counter argument says that starting with
  microservices allows you to get used to the rhythm of developing in
  a microservice environment. It takes a lot, perhaps too much,
  discipline to build a monolith in a sufficiently modular way that it
  can be broken down into microservices easily. By starting with
  microservices you get everyone used to developing in separate small
  teams from the beginning, and having teams separated by service
  boundaries makes it much easier to scale up the development effort
  when you need to. This is especially viable for system replacements
  where you have a better chance of coming up with stable-enough
  boundaries early. Although the evidence is sparse, I feel that you
  shouldn't start with microservices unless you have reasonable
  experience of building a microservices system in the team.&lt;/p&gt;

&lt;p&gt;I don't feel I have enough anecdotes yet to get a firm handle on
  how to decide whether to use a monolith-first strategy. These are
  early days in microservices, and there are relatively few anecdotes
  to learn from. So anybody's advice on these topics must be seen as
  tentative, however confidently they argue.&lt;/p&gt;

&lt;h2&gt;Further Reading&lt;/h2&gt;

&lt;p&gt;Sam Newman &lt;a href="http://samnewman.io/blog/2015/04/07/microservices-for-greenfield/"&gt;describes a case study&lt;/a&gt; of a team considering using
    microservices on a greenfield project.&lt;/p&gt;
&lt;p class="acknowledgements"&gt;
&lt;h2&gt;Acknowledgements&lt;/h2&gt;

    I stole much of this thinking from my coleagues: James Lewis, Sam
    Newman, Thiyagu Palanisamy, and Evan Bottcher. Stefan Tilkov's
    comments on an earlier draft played a pivotal role in clarifying
    my thoughts. Chad Currie created the lovely glyphy
    dragons. Steven Lowe, Patrick Kua, Jean Robert D'amore, Chelsea
    Komlo, Ashok Subramanian, Dan Siwiec, Prasanna Pendse, Kief
    Morris, Chris Ford, and Florian Sellmayr discussed drafts on our
    internal mailing list.
  &lt;/p&gt;

&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/bliki/MonolithFirst.html&amp;amp;text=Bliki:%20MonolithFirst" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/bliki/MonolithFirst.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/bliki/MonolithFirst.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:43:18 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:bliki-monolithfirst.html</guid></item><item><title>Don't start with a Monolith</title><link>http://www.ciandcd.com/dont-start-with-a-monolith.html</link><description>&lt;div&gt;&lt;p class="subtitle"&gt;&amp;#8230; when your goal is a microservices
  architecture&lt;/p&gt;&lt;p&gt;Stefan Tilkov is a co-founder and principal consultant at innoQ , a technology consulting company with offices in Germany and Switzerland. He has been involved in the design of large-scale, distributed systems for more than two decades, using a variety of technologies and tools ranging from C++ and CORBA over J2EE/Java EE and Web Services to REST and Ruby on Rails. He has authored numerous articles and a book (&amp;#8220; REST und HTTP &amp;#8221;, German), and is a frequent speaker at conferences around the world.&lt;/p&gt;&lt;p&gt; In the last few months, I&amp;#8217;ve heard repeatedly that the only way to
get to a successful microservices architecture is by starting with a
monolith first. &lt;a href="http://www.codingthearchitecture.com/2014/07/06/distributed_big_balls_of_mud.html"&gt;To
paraphrase Simon Brown&lt;/a&gt;: If you can&amp;#8217;t build a well-structured
monolith, what makes you think you can build a well-structured set of
microservices? The most recent &amp;#8211; and, as usual, very convincing &amp;#8211;
rendering of this argument comes from &lt;a href="/bliki/MonolithFirst.html"&gt;Martin Fowler&lt;/a&gt; on this very site. As I had
a chance to comment on an earlier draft, I had some time to think
about this. And I did, especially because I usually find myself in
agreement with him, and some others whose views I typically share
seemed to agree with him, too.
&lt;/p&gt;

&lt;p&gt;I&amp;#8217;m firmly convinced that starting with a monolith is usually exactly the
wrong thing to do. 
&lt;/p&gt;

&lt;p&gt;Starting to build a new system is exactly the time when you should
be thinking about carving it up into pieces. I strongly disagree with
the idea that you can postpone this, as expressed by Sam Newman, again
someone I agree with 95% of the time:
&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I remain convinced that it is much easier to partition an existing,
  "brownfield" system than to do so up front with a new, greenfield
  system. You have more to work with. You have code you can examine,
  you can speak to people who use and maintain the system. You also
  know what 'good' looks like - you have a working system to change,
  making it easier for you to know when you may have got something
  wrong or been too aggressive in your decision making process.&lt;/p&gt;

&lt;p class="quote-attribution"&gt;&lt;a href="http://samnewman.io/blog/2015/04/07/microservices-for-greenfield"&gt;-- Sam Newman&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the majority of cases, it will be awfully hard, if not outright
impossible, to cut up an existing monolith this way. (That doesn&amp;#8217;t
mean it&amp;#8217;s always impossible, but that&amp;#8217;s a topic for a future post.)
There is some common ground in that I agree you should know the domain
you&amp;#8217;re building a system for very well before trying to partition
it, though: In my view, the ideal scenario is one where you&amp;#8217;re building a
&lt;i&gt;second version&lt;/i&gt; of an existing system.
&lt;/p&gt;

&lt;p&gt;If you are actually able to build a well-structured monolith, you
probably don&amp;#8217;t need microservices in the first place. Which is OK! I
definitely agree with &lt;a href="/bliki/MicroservicePremium.html"&gt;Martin&lt;/a&gt;: You shouldn&amp;#8217;t introduce the
complexity of additional distribution into your system if you don&amp;#8217;t
have a very good reason for doing so.
&lt;/p&gt;

&lt;p&gt;(So what would be a good reason? There are many, but to me the most
important one is to allow for fast, independent delivery of individual parts within a larger
system. Microservices&amp;#8217; main benefit, in my view, is enabling parallel
development by establishing a hard-to-cross boundary between different
parts of your system. By doing this, you make it hard &amp;#8211; or at least
harder &amp;#8211; to do the wrong thing: Namely, connecting parts that shouldn&amp;#8217;t
be connected, and coupling those that need to be connected too
tightly. In theory, you don&amp;#8217;t need microservices for this if you
simply have the discipline to follow clear rules and establish clear
boundaries within your monolithic application; in practice, I&amp;#8217;ve found
this to be the case only very rarely.)&lt;/p&gt;

&lt;img src="dont-start-monolith/monolith-theory-practice.svg" width="100%"&gt;
&lt;p class="photoCaption"&gt;
  You might be tempted to assume there are a number of nicely
  separated microservices hiding in your monolith, just waiting to be
  extracted. In reality, though, it&amp;#8217;s extremely hard to avoid creating
  lots of connections, planned and unplanned. In fact the whole point
  of the microservices approach is to make it hard to create something
  like this.
&lt;/p&gt;
&lt;p&gt;
But if you start with a monolith, the parts will become extremely
tightly coupled to each other. &lt;i&gt;That&amp;#8217;s the very definition of a
monolith&lt;/i&gt;. The parts will rely on features of the platform they all
use. They&amp;#8217;ll communicate based on abstractions that are shared because
they all use the same libraries. They&amp;#8217;ll communicate using means that
are only available when they are hosted in the same process. And these
are only the technical aspects! Far worse than that, the parts will
(almost) freely share domain objects, rely on the same, shared
persistence model, assume database transactions are readily available
so that there&amp;#8217;s no need for compensation &amp;#8230; Even the very fact that
it&amp;#8217;s easy to refactor things and move them around &amp;#8211; all in the
convenience of your IDE&amp;#8217;s view of a single project &amp;#8211; is what makes it
extremely hard to cut things apart again. It&amp;#8217;s &lt;i&gt;extremely&lt;/i&gt; hard to
split up an existing monolith into separate pieces.
&lt;/p&gt;

&lt;p&gt;
I strongly believe &amp;#8211; and experience from a number of our recent projects
confirms this &amp;#8211; that when you start out, you should think about
the subsystems you build, &lt;i&gt;and build them as independently of each
other as possible&lt;/i&gt;. Of course you should only do this if you believe
your system is large enough to warrant this. If it&amp;#8217;s just you and one
of your co-workers building something over the course of a few weeks,
it&amp;#8217;s entirely possible that you don&amp;#8217;t.
&lt;/p&gt;

&lt;p&gt;But starting with an approach where you carve up your system into
smaller parts, and treat each of them as a clearly separated, individual
system with its own development, deployment, and delivery cycle, and (the
possibility of) its own internal architecture, is a very powerful
concept that can help you to deliver a system in the first place.
&lt;/p&gt;

&lt;p&gt;So is there any actual experience to back this up? Yes, there are a few systems we&amp;#8217;ve been
involved with recently that showed this concept to work &amp;#8211; provided you
tolerate the fact that what I&amp;#8217;m talking about is more likely bigger
than your typical microservice. The most prominent
one is Otto.de, about which I did
&lt;a href="http://www.infoq.com/presentations/modular-ecommerce-website"&gt;a talk together with their tech lead&lt;/a&gt;
 (and which you can read about (in German) in
 &lt;a href="http://www.informatik-aktuell.de/entwicklung/methoden/von-monolithen-und-microservices.html"&gt;this nice write-up by one of their lead architects&lt;/a&gt;).
 But there are quite a few others as well, and I remain convinced it&amp;#8217;s
 a good idea to start building a system using this approach &amp;#8211;
 given you know the domain you&amp;#8217;re building for really, really well.
&lt;/p&gt;

&lt;p&gt;But there is another lesson to be learned from this discussion, in my
view &amp;#8211; and it&amp;#8217;s a more general one: Beware of architectural recipes
that are too simple and too obvious. This one &amp;#8211; start by carving up
your domain into separate, independent parts &amp;#8211; is no
exception. Sometimes a monolith &lt;i&gt;is&lt;/i&gt; preferable, sometime it&amp;#8217;s
not. If you decide to build things using a microservices approach, you
need to be aware that while it will be a lot easier to make localized
decisions in each individual part, it will be much harder to change
the very boundaries that enable this. Refactoring in the small becomes
easier, refactoring in the large becomes much harder. 
&lt;/p&gt;

&lt;p&gt;As is always the case with architecture discussions, there is no way
to get around the fact that you need to make that decision on your
own, in each and every individual case.
&lt;/p&gt;



&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/articles/dont-start-monolith.html&amp;amp;text=Don%E2%80%99t%20start%20with%20a%20monolith" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/articles/dont-start-monolith.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/articles/dont-start-monolith.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;For articles on similar topics&amp;#8230;&lt;/h2&gt;

&lt;p&gt;&amp;#8230;take a look at the tag: &lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:43:17 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:dont-start-with-a-monolith.html</guid></item><item><title>Tor for Technologists</title><link>http://www.ciandcd.com/tor-for-technologists.html</link><description>&lt;div&gt;&lt;p&gt;&lt;a href="https://torproject.org"&gt;Tor&lt;/a&gt; is a technology that is cropping up in
    news articles quite often nowadays. However, there exists a lot of
    misunderstanding about it. Even many technologists don't see past its
    use for negative purposes, but Tor is much more than that. It is an
    important tool for democracy and freedom of speech - but it's also
    something that is very useful in the day-to-day life of a
    technologist. Tor is also an interesting case study in how to design a
    system that has very specific security requirements.&lt;/p&gt;

&lt;p&gt;The Internet is currently a quite hostile place. There are threats
    of all kinds, ranging from &lt;a href="http://www.catb.org/jargon/html/S/script-kiddies.html"&gt;script
    kiddies&lt;/a&gt; and drive-by &lt;a href="https://www.owasp.org/index.php/Phishing"&gt;phishing&lt;/a&gt;
    attacks to pervasive &lt;a href="https://www.eff.org/nsa-spying"&gt;dragnet
    surveillance&lt;/a&gt; by many of the major intelligence services in the
    world. The extent of these problems have only recently become clear to
    us. In this context, a tool like Tor fills a very important niche. You
    could argue that it's a sign of the times that even a company like
    Facebook encourages &lt;a href="https://www.facebook.com/notes/protect-the-graph/making-connections-to-facebook-more-secure/1526085754298237"&gt;the use of Tor&lt;/a&gt;
    to access their services. The time is right to add Tor to your tool
    belt.&lt;/p&gt;


&lt;h2&gt;How does Tor work?&lt;/h2&gt;

&lt;p&gt;The goal of Tor is to enable anonymous network traffic. The word
      Tor originally stood for The Onion Router (although Tor should not be
      capitalized like an acronym). Onion routing is the method that Tor
      uses to hide the IP address of the client. This also has the added
      benefit of making it extremely hard for anyone in between to see who
      is asking for what information on the Internet. In practice, this
      means that if someone is intercepting your traffic close to your
      computer, Tor will hide what servers you are talking to. If someone is
      intercepting traffic close to the server, they will not be able to see
      who is visiting that server.&lt;/p&gt;

&lt;p&gt;Tor works by having thousands of volunteers hosting something
      called Tor Relays. These are fundamentally servers that spend all
      their time forwarding traffic. So when you want to access a service,
      the Tor client program that is running on your computer will choose
      three different relays. It will then take the packets of information
      and wrap them in encryption using public key cryptography. It will
      encrypt the packet three times over, to each one of the public keys of
      the relays that were chosen for this communication session. It will
      then send the triple-encrypted package to the first relay on the list.
      That relay will unwrap the encryption meant for itself - and what is
      left is a packet with two layers of encryption. The first relay will
      send the packet to the second relay, which will unwrap another layer
      of encryption. It sends this to the final relay which will unwrap the
      last piece of encryption. Since there is no encryption left, the final
      relay knows the real destination of the packet - so it contacts the
      original service requested. It then receives the answer and sends the
      information back, using the same process in reverse.&lt;/p&gt;

&lt;p class="figureImage"&gt;&lt;a name="sketch.png"&gt;&lt;/a&gt;&lt;img alt="Figure 1" src="tor-for-technologists/sketch.png"&gt;&lt;/p&gt;
&lt;p&gt;The final relay is usually called an exit relay. The first relay
      is usually called an entry relay. The way this works only the entry
      relay knows the real IP address of your machine - and only the exit
      relay knows which service you actually wanted to contact. And the
      service will only see the IP address of the exit relay. All of these
      things come together to protect the anonymity of the client quite
      well.&lt;/p&gt;

&lt;p&gt;One important point is that you don't have to trust all the Tor
      relays in order to be able to trust the Tor network. Since anyone can
      set up their own relay, there is nothing that stops bad actors from
      setting up relays. Thus, Tor is designed to not be compromised even
      if this happens.&lt;/p&gt;

&lt;p&gt;Tor is explicitly designed to be a &lt;a href="https://en.wikipedia.org/wiki/Low_latency"&gt;low-latency network&lt;/a&gt; - that means it
      is possible to use it for things like Instant Messaging, audio
      conferencing or even video conferencing under good circumstances.&lt;/p&gt;

&lt;h3&gt;The Tor client&lt;/h3&gt;

&lt;p&gt;If you want to use Tor, there are two options: the
        first one is to run the client program, and the second is to
        download and run something called the &lt;a href="https://www.torproject.org/projects/torbrowser.html.en"&gt;Tor Browser
        Bundle&lt;/a&gt;. If you want to use Tor for other kinds of traffic
        than web, it's useful to run the Tor client software
        directly. Installing it is usually done through the regular package
        management systems for your operating system - and once installed
        it should be running on your machine in the background, without you
        having to do anything at all. Since Tor works as a proxy, you don't
        interact with Tor directly. Instead you ask your regular programs
        to tunnel traffic through Tor.&lt;/p&gt;

&lt;p&gt;The Tor client program by default will listen to port 9050 using
        a protocol called &lt;a href="https://www.ietf.org/rfc/rfc1928.txt"&gt;SOCKS&lt;/a&gt; - this is a
        standard proxy protocol that many applications can be configured to
        use. For example, I use a program called &lt;a href="http://gajim.org/"&gt;Gajim&lt;/a&gt; for instant messaging. In this program I
        have configured all my accounts to connect over Tor by specifying
        the correct SOCKS proxy settings.&lt;/p&gt;

&lt;p&gt;If you are using command line tools, it is usually possible to
        use a program called &lt;i&gt;usewithtor&lt;/i&gt; or &lt;i&gt;torsocks&lt;/i&gt; in order to
        transparently make the program use Tor. There are other ways of
        using Tor as well - &lt;a href="https://www.mozilla.org/en-US/thunderbird/"&gt;Thunderbird&lt;/a&gt; can
        be configured to run all its traffic over Tor if you want to make
        your email harder to track and intercept - however, there are some
        risks associated with the manual configuration of Tor in
        Thunderbird. There are many possible types of information leakage, such as DNS
        lookups, geo-location information in headers and even IP addresses
        in mail headers in some cases. Fortunately, there exists a plugin called &lt;a href="https://addons.mozilla.org/En-us/thunderbird/addon/torbirdy/"&gt;TorBirdy&lt;/a&gt; that automates many of the steps
        and tries to stop such unsafe information leakage from happening. &lt;/p&gt;
&lt;h3&gt;The Tor Browser Bundle&lt;/h3&gt;

&lt;p&gt;By far the easiest way of using Tor is to download the Tor Browser
        Bundle which doesn't even have to be installed - it can be run directly after download
        (and you can have it on a USB stick or a DVD and run it from there as
        well). The Tor Browser Bundle is basically the Tor client software
        combined with a very customized browser. Since browsers have many ways
        they can leak your anonymity, and there are many other things that can
        threaten your security, the Tor Browser Bundle is much safer than just
        configuring your regular browser to use Tor.&lt;/p&gt;

&lt;p&gt;The Tor Browser Bundle comes with several plugins that will improve
        your privacy, and it has also been configured to not leak information
        through side channels such as font configuration, Flash plugins and
        other features.&lt;/p&gt;
&lt;h3&gt;Hidden Services&lt;/h3&gt;

&lt;p&gt;Tor also has another interesting feature that has gotten a lot of
        attention lately. Tor hidden services give a service the ability to
        hide its IP address, just as the regular Tor operation allows a client
        to hide its IP address. It works by using something called a
        rendezvous protocol, which is a bit complicated. Basically, you can
        imagine that both the client and the server establishes their own full
        circuits of three relays. They then meet in the middle, somewhere in
        the Tor network - and from that point they can establish a connection.
        In general, a hidden service is found and contacted using a
        .onion-URL, which consists of 16 alphanumeric characters followed by
        .onion. The 16 characters is a representation of the public key of the
        hidden service, which means that routing of the connection is
        self-verifying. No-one can fake being a hidden service without
        stealing their private key.&lt;/p&gt;

&lt;h3&gt;Facebook and Hidden Services&lt;/h3&gt;

&lt;p&gt;There are several aspects of Facebook &lt;a href="https://www.facebook.com/notes/protect-the-graph/making-connections-to-facebook-more-secure/1526085754298237"&gt;setting up a hidden service&lt;/a&gt;
          that are interesting. It is definitely not the standard usage to set
          up a hidden service for something like Facebook where the real
          location of the service is already well known. And since Facebook
          requires logins and they have a real name policy that also makes it
          harder to put it together. However, there are several reasons why
          Facebook decided to set up a hidden service - the first is that it
          makes it easy for them to block Tor exit relays to their regular sites
          without stopping access for valid users. The second reason is that a
          hidden service still ensures that Facebook doesn&amp;#8217;t see the clients IP
          address, and enforces end-to-end encryption. So overall, the Facebook
          hidden service doesn&amp;#8217;t fill all the same functions as hidden services
          usually do, but it&amp;#8217;s still a valuable thing to have. &lt;/p&gt;
&lt;p&gt;Hidden services are extremely powerful for several reasons. First,
        they allow the service provider to be anonymous. This has of course
        been used for a lot of good and bad reasons. Secondly, if you only
        expose a service as a hidden service, you can force your clients to
        use Tor - which makes it much harder for them to accidentally expose
        their information. Third, hidden services are implemented in such a
        way that the traffic will never leave the Tor network. There are no
        exit relays for hidden services. What this means is that communication
        to a hidden service will always be encrypted all the way to the
        service. This isn't necessarily true for regular Tor connections.
        Finally, you can expose a hidden service without having a publicly
        routable IP address. A very practical example of how to use this is to
        expose SSH as a hidden service. This allows you to SSH to your own
        server wherever it may be, without having to open firewalls or having
        a public IP. I run several servers at home that are not publicly
        reachable, and being able to SSH into them and manage them anyway is
        extremely powerful.&lt;/p&gt;

&lt;p&gt;Facebook recently made a splash by exposing their servers as
        a hidden service. This is a great step forward. So you might ask
        whether this is something you should do for the projects you
        work on. In general, I would say yes - although it does require
        some extra work and testing. Making this available to your users
        is something that in general is good for everyone.&lt;/p&gt;
&lt;h3&gt;Bridges and obfuscated protocols&lt;/h3&gt;

&lt;p&gt;In some regions there is enough censorship going on that using Tor
        in the regular manner doesn't work. Most of the time this is because
        they simply block traffic to all the Tor relays. And since the Tor
        relays have to be public, it's easy for a censor to simple download
        the list of IPs to be blocked.&lt;/p&gt;

&lt;p&gt;Because of this situation, the Tor software supports something
        called bridges. These are entry relays that are not publicly known -
        if you need one you can request one on the fly. The other solution
        that is sometimes necessary is to use obfuscated protocols. Since the
        most advanced censors use deep packet inspection it's important that
        Tor traffic can masquerade as other kinds of traffic. There are
        several different plugins implemented in Tor right now for solving
        this problem - but it's an ongoing arms race to beat censorship.&lt;/p&gt;

&lt;h2&gt;Anonymity is hard - what doesn't Tor protect against?&lt;/h2&gt;

&lt;p&gt;Tor is a tool, and just like any tool it has things it does well,
      and things it doesn't do so well. It is important to know that Tor
      can't protect against every threat out there. Even when you use Tor
      correctly, there are things to keep in mind.&lt;/p&gt;

&lt;p&gt;The biggest day-to-day problem when it comes to anonymity and
      security with Tor is that if the traffic you are sending to a server
      isn't encrypted, then Tor will not help with that. Specifically, the
      traffic between you and the first two relays will be encrypted and
      hard to attack. However, the exit relay will be able to see all
      traffic that passes through. The way to protect against this is to
      either use a protocol that encrypt your information all the way to
      the endpoint (such as HTTPS or SMTP with &lt;a href="https://en.wikipedia.org/wiki/STARTTLS/"&gt;STARTTLS&lt;/a&gt;). The other way is to use services
      that expose a hidden service - which ensures you get end-to-end
      encryption of all traffic.&lt;/p&gt;

&lt;h2&gt;Browser fingerprinting&lt;/h2&gt;

&lt;p&gt;There are many different ways a user can be identified outside
        of looking at their IP address. Browser fingerprinting is a
        surprisingly effective way to uniquely identify people. It uses
        several aspects, such as what plugins are available, what fonts are
        installed and many other features that are available to the
        server. The EFF have a test service called &lt;a href="https://panopticlick.eff.org/"&gt;Panopticlick&lt;/a&gt; which shows how easy it is
        to uniquely identify someone through their browser. &lt;/p&gt;
&lt;p&gt;Tor is usually classified as a low-latency network. This has a
      consequence for anonymity. Basically, Tor is designed in such a way
      that if an attacker can observe a large percentage of the network
      traffic between relays, it is possible for them to de-anonymize much
      of that traffic. The reason is that since Tor traffic is supposed to
      go out and come back in realtime, it can be possible to do
      correlation of packet times. This could lead to a privacy
      break. However, this is the way Tor was designed. Current research
      seems to imply that it is basically impossible to get total anonymity
      if you want something that is close to real time traffic. If you are
      fine with waiting a few hours for each packet to arrive, there are
      &lt;a href="https://en.wikipedia.org/wiki/Mix_network"&gt;other kinds of techniques&lt;/a&gt; that can be
      used, but Tor is not designed for those use cases.&lt;/p&gt;

&lt;p&gt;Tor can also not protect you if you make a mistake and expose your
      IP address in any way. A typical example of something that is a bad
      idea is to use Tor for peer-to-peer file sharing - not only does it
      destroy the Tor network - it also doesn't work to hide your address,
      since most file-sharing protocols expose your real IP address. If you
      want to be sure to not expose your IP address you also need to avoid
      running plugins that can expose it. This is one of the reasons why the
      Tor Browser Bundle doesn't ship with a Flash plugin - it is just too
      dangerous from a privacy-standpoint.&lt;/p&gt;

&lt;p&gt;The same caveat is true if you expose your server as a hidden
      service. If you really want to keep it hidden you need to make sure
      that none of your server components expose your IP address in any way.
      This can be easier said than done, as the next section will talk
      about.&lt;/p&gt;

&lt;h2&gt;Is Tor broken?&lt;/h2&gt;

&lt;p&gt;When I talk to technologists about Tor, one of the more common
      questions I will get is whether I think Tor is broken or whether we
      really can trust it, whether the government has put a backdoor in it,
      etc. There are many rumors and theories about Tor out there, and in
      general they are just that - rumors and theories. That said, Tor
      doesn't protect against everything, as I mentioned above. However, I
      would be extremely surprised if Tor contains a backdoor. The code is
      open source and many, many experts have looked at it over the years.
      The protocol is also open and has been implemented more than once in
      other open source projects. It is of course not impossible that there
      exists a backdoor that no-one has noticed, but personally I wouldn't
      bet on it. I do recommend any technologist that is worried about this
      eventuality to spend a few hours and go through the source code. The
      project can always do with more eyeballs. &lt;/p&gt;

&lt;h3&gt;Research&lt;/h3&gt;

&lt;p&gt;All that said, Tor has been broken many times. The main reason for
        this is that anonymity is extremely hard, and Tor is pushing the
        boundaries of what's been done before. Thus, Tor is a favorite subject
        for researchers to poke at and see if they can figure out ways of
        getting it to do unexpected things. It's also the main way we have of
        getting an idea of where the hard limits of anonymity lie. That Tor is
        the subject of lots of research is ultimately good for the community,
        since every time a new paper comes out that talks about a weakness in
        Tor, the Tor developers fix it. Nothing is perfect at the start, so
        this way of iterating and fixing is how we get to a state where we can
        trust a tool. &lt;/p&gt;
&lt;h3&gt;Events in 2014&lt;/h3&gt;

&lt;p&gt;One reason that people are worried about the degree of security of
        Tor, is because there has been a string of suspicious events over the
        last few years. Specifically, both users of Tor and people that have
        used hidden services have been arrested under various circumstances.
        However, we have at this point no indication that any of these take
        downs were actually because of flaws in the Tor software or protocol.
        In the cases we know of, it seems that the reasons can be chalked down
        to mistakes of various kinds. In one case, there was a bug in the
        Firefox version that the Tor Browser Bundle was based on, so after the
        police took over a server, they could attack this browser and
        de-anonymize the users in question (but only those that visited that
        specific hidden service). In the case of the first Silk Road, it seems
        clear that investigators managed to hack in to the administrator
        interface and convinced it to send back its real IP address.&lt;/p&gt;

&lt;p&gt;And then we have &lt;a href="https://en.wikipedia.org/wiki/Operation_Onymous"&gt;Operation
        Onymous&lt;/a&gt;. This operation was a collaboration between several
        different law enforcement agencies with the aim to take down a
        large number of illegal online marketplaces.  What is interesting
        about this operation is all the propaganda that surrounds
        it. Initially they were claiming to have taken down over 400 hidden
        services. But after the dust settled it seems only 27 sites were
        taken down, and only 17 arrests were made. You can still argue that
        this is a lot, since they were supposed to have been protected by
        Tor and hidden services. But the truth is that apparently the
        person hosting Silk Road 2 had his name written down somewhere -
        the law enforcement agencies took him in and started questioning
        him.  Apparently he named 16 other people that were promptly
        arrested and had their computers seized. So no Tor break was
        involved in this - even though one UK law enforcement agency
        actually tweeted a taunt about Tor being broken. It was all
        propaganda. More info can be found in &lt;a href="https://www.youtube.com/watch?v=pRrFWwA-47U"&gt;this video&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Further, looking at the &lt;a href="http://www.theguardian.com/world/interactive/2013/oct/04/tor-stinks-nsa-presentation-document"&gt;documents&lt;/a&gt;
        leaked by Edward Snowden, it is still clear that the strongest
        attackers out there don't seem to have a good way of breaking
        Tor. I find this to be very encouraging.&lt;/p&gt;

&lt;h2&gt;Everyday usage of Tor&lt;/h2&gt;

&lt;p&gt;Now that you have a better idea of what Tor is, it is time for us
      to quickly talk about how you can use Tor in your day-to-day job. As
      you can probably guess from the information above, Tor can be used for
      a lot of different purposes. But for the kind of work a developer is
      doing, the first and easiest thing you can do using Tor is to test
      your system in different ways. In many cases it's hard to see what a
      system looks like without cookies and personalization. Tor makes it
      very easy.&lt;/p&gt;

&lt;p&gt;The other kind of testing you can do is to make it possible to very
      easily try a system in such a way that it looks like it comes from
      many different countries. It is possible to control which relays Tor
      will use in such a way that you can control which country your traffic
      will come out from.&lt;/p&gt;

&lt;p&gt;As I mentioned before, putting SSH behind a Tor hidden service is a
      very useful way of getting access to your systems from anywhere on the
      planet, in a way that puts at least two layers of encryption on your
      connection. However, even for a setup like this, it's important to
      have good passwords (or only use public-key logins for SSH) and to
      make it impossible to login to the root account remotely.&lt;/p&gt;

&lt;h2&gt;Tails&lt;/h2&gt;

&lt;p&gt;If you happen to be in the kind of situation where Tor is
      absolutely necessary for you and you expect to be attacked by strong
      adversaries, you are in a tricky situation. Just installing Tor on
      your regular machine isn't necessarily going to be enough. Thus,
      there is a project called &lt;a href="https://tails.boum.org/"&gt;Tails&lt;/a&gt;, which is a
      Linux distribution that runs from a CD/DVD or USB drive. It is
      preconfigured to &lt;i&gt;only&lt;/i&gt; send network traffic over Tor and also
      has many other privacy and anonymity features. It comes bundled with
      several different tools that will allow you to easily communicate
      privately, send encrypted email and also working with documents in a
      safe way.&lt;/p&gt;

&lt;p&gt;The interesting thing with Tails is that it's completely amnesiac -
      it doesn't leave a trace on your computer after you have run it. It is
      possible to have it save a small amount of data, but by default no
      traces will be left.&lt;/p&gt;

&lt;p&gt;Tails is not something you will need in your regular day-job, but
      there are certain cases where something like Tails can be extremely
      powerful. Every journalist should know how to use it - and so should
      every lawyer. It's a powerful tool and it's a good idea to be prepared
      to use it before it becomes necessary. &lt;/p&gt;

&lt;h2&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Tor is a fantastic tool that can do a lot of amazing things. It is
      one of the strongest examples of useful cryptography deployed in a
      safe way we have right now. As we have seen though, no tool is
      perfect. And in order to use Tor correctly, it's a good idea to know
      how it works. As a next step I would recommend you to download Tor and
      try it out. One of the interesting things about anonymity is that it
      loves company - even if you don't need the anonymity you are
      protecting others who do need it by using Tor. One day you might go
      forward and run your own Tor relay as well - something that improves
      the Tor network for everyone.&lt;/p&gt;


&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/articles/tor-for-technologists.html&amp;amp;text=Tor%20for%20Technologists" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/articles/tor-for-technologists.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/articles/tor-for-technologists.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;For articles on similar topics&amp;#8230;&lt;/h2&gt;

&lt;p&gt;&amp;#8230;take a look at the tag: &lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:43:14 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:tor-for-technologists.html</guid></item><item><title>A Cherry Picker's Guide to Doctor Who</title><link>http://www.ciandcd.com/a-cherry-pickers-guide-to-doctor-who.html</link><description>&lt;div&gt;&lt;p&gt;Doctor Who is a British TV series with a long history. Its
    first broadcast was in 1963, and its current incarnation has run
    for eight seasons. Most TV series require you to start at the
    beginning and watch every episode, but although that has its
    delights, you don't have to do that with Doctor Who, since many of
    its best episodes are written so that you can enjoy them as a
    self-contained film. This is my personal suggestions of how to
    watch Doctor Who by cherry picking individual episodes.&lt;/p&gt;

&lt;p&gt;For most episodes of Doctor Who, all you need to know is the
    rough premise. The Doctor is a Time Lord, a human looking alien,
    who travels through time and space with various companions
    (usually human, young, and female). This setup allows writers to
    set self-contained stories in any setting: historical, futuristic,
    and current. His spaceship is called the TARDIS, from the outside
    it looks like a blue British Police Box from the 1960s, its inside
    is a large trans-dimensional space. Each episode usually sees the
    Tardis appear and the Doctor and his companion find some sticky
    predicament focused on some malevolent alien. They resolve the
    situation, often with a notable lack of violence. The Doctor is
    strikingly not like the usual action hero, he rarely uses weapons:
    relying on his wits to defeat any threats.&lt;/p&gt;

&lt;p&gt;Doctor Who is one of the longest running TV series, it began in
    November 1963. You can divide its history so far into two broad
    periods: Classic Who runs from its beginning until 1989 when
    the series was cancelled. New Who runs from its reboot in 2005 until
    today, where it's still going strong. Classic Who was a children's
    show that appealed to many adults. The writers of New Who mostly
    fell in love with Classic Who as children (as I did) and went on
    to write a show that should appeal equally to adults and children.
    I saw my first episode of New Who with two friends my own age and
    their pre-teen daughters, who hadn't seen Classic Who as we elders
    had.&lt;/p&gt;

&lt;p&gt;If you want to explore Doctor Who from scratch, I would start
    with New Who. Classic Who has its charms, and some remarkably good
    serials. But the quality of Classic Who varies from very good to
    truly awful, and comes with special effects that vary from cheesy to
    truly awful. So although I'll suggest a couple of Classic Who serials later
    on, most new viewers should start at New Who. Although there is some
    references to Classic Who, you can appreciate New Who without
    knowing anything about Classic Who.&lt;/p&gt;

&lt;p&gt;I'm focusing on cherry picking here, but plenty of people enjoy
    starting from the beginning of New Who and taking a
    completist approach through every episode. There are certainly rewards for the
    completist approach, with some good internal references and many great
    episodes that you can't appreciate by cherry picking. You don't
    need any advice to be a completist, but I will say that you should
    give it until at least the sixth episode (&lt;a href="https://en.wikipedia.org/wiki/Dalek_(Doctor_Who_episode)"&gt;Dalek&lt;/a&gt;)
    before you decide it's not for you. In particular the fourth and
    fifth episodes (&lt;a href="https://en.wikipedia.org/wiki/Aliens_of_London"&gt;Aliens of
    London / World War Three&lt;/a&gt;) are two of the weakest
    episodes in New Who, with some annoyingly juvenile humor.&lt;/p&gt;

&lt;p&gt;For my picks I'm selecting episodes which are both my favorite
    episodes, but also episodes that don't rely on any surronding
    story arc. If it is valuable to see other picked episodes first,
    I'll mention that. There's also an argument for watching some
    individual series completely, and I'll mention which of those I
    think are worth considering for that.&lt;/p&gt;

&lt;p&gt;The first question is where does a cherry picker begin, and
    I generally advise starting with &lt;a href="https://en.wikipedia.org/wiki/Blink_(Doctor_Who)"&gt;Blink&lt;/a&gt;, which is on most
    people's short list for greatest Who episode ever. It actually
    doesn't feature the Doctor that much, and mostly ignores the
    companion of that series. But it's a clever plot, superbly acted
    by a young Carey Mulligan, and lots of wit in the script.
    Deservedly it won a BAFTA drama award, a rare event for a sci-fi
    story. &lt;a href="#footnote-purpose"&gt;[1]&lt;/a&gt; The episode was written by &lt;a href="http://en.wikipedia.org/wiki/Steven_Moffat"&gt;Stevan
    Moffat&lt;/a&gt;, who since went on to be the showrunner for Doctor Who.
    He's also the co-showrunner for &lt;a href="http://en.wikipedia.org/wiki/Sherlock_(TV_series)"&gt;Sherlock&lt;/a&gt;,
    and the writer of &lt;a href="http://en.wikipedia.org/wiki/Jekyll_(TV_series)"&gt;Jekyll&lt;/a&gt; (a
    superb six-episode miniseries).
     I rate him with Joss Whedon as one of the best writers of our
    time.&lt;/p&gt;

&lt;p&gt;So start with &lt;a href="https://en.wikipedia.org/wiki/Blink_(Doctor_Who)"&gt;Blink&lt;/a&gt;, but after that you can mostly
    pick and mix as you like. I'm going to list my picks
    chronologically series by series, but you don't have to do them in
    that order. If any episodes require you to see some others first,
    I'll point that out. You should get used to different actors
    playing the Doctor - there is a clever techno-babble reason why
    multiple actors can play the same character, which doesn't affect
    a cherry picking watcher. Each actor emphasizes different aspects
    of the same character. Companions are different people, but again
    the cherry picking choices I've made don't rely too much on their
    ongoing story.&lt;/p&gt;

&lt;p&gt;The first series was the reboot of Doctor Who, making it return
    to the screen after a silence of fifteen years. The driving force
    for the reboot was &lt;a href="http://en.wikipedia.org/wiki/Russell_T_Davies"&gt;Russel T Davies&lt;/a&gt;, who already had garnered a
    fine reputation as a TV writer. Playing the Doctor in this series
    is Christopher Eccleston, sadly in his only series as the Doctor,
    with Rose (Billie Piper) as his companion. The standout episode of
    the first series is Moffat's &lt;a href="https://en.wikipedia.org/wiki/The_Empty_Child"&gt;The Empty Child / The Doctor
    Dances&lt;/a&gt;. Most movies
    aren't this good, and this double episode introduced the Moffat
    approach of combining fright and wit. The other cherry pick I'd
    make from the first series is &lt;a href="https://en.wikipedia.org/wiki/Dalek_(Doctor_Who_episode)"&gt;Dalek&lt;/a&gt;, which reintroduces the
    Doctor's iconic enemy in a story for them that still hasn't been
    surpassed.&lt;/p&gt;

&lt;p&gt;The first series also has one of the better story arcs, so may
    be worth doing the full series, if only to really enjoy the final
    two-parter which isn't worth watching without that context. (A tip
    if you do watch the whole series: don't watch the trailer for the
    next episode at the
    end of &lt;a href="https://en.wikipedia.org/wiki/Boom_Town_(Doctor_Who)"&gt;Boom Town&lt;/a&gt;, as
    it gives away an important part of the plot of &lt;a href="https://en.wikipedia.org/wiki/Bad_Wolf"&gt;Bad Wolf&lt;/a&gt;.)&lt;/p&gt;

&lt;p&gt;The second
    series has David Tennant playing the tenth Doctor &lt;a href="#footnote-doctor-numbers"&gt;[2]&lt;/a&gt;. My main cherry pick here
    is again the Moffat episode:&lt;a href="https://en.wikipedia.org/wiki/The_Girl_in_the_Fireplace"&gt;The Girl in the Fireplace&lt;/a&gt;. I also think
    the two parter &lt;a href="https://en.wikipedia.org/wiki/The_Impossible_Planet"&gt;The Impossible Planet / The Satan
    Pit&lt;/a&gt; is worth
    watching.&lt;/p&gt;

&lt;p&gt;The third series continued with David Tenant but brought in a new
    companion: Martha. &lt;a href="https://en.wikipedia.org/wiki/Blink_(Doctor_Who)"&gt;Blink&lt;/a&gt; comes from this series, but another
    outstanding highlight of series three is &lt;a href="https://en.wikipedia.org/wiki/Human_Nature_(Doctor_Who_episode)"&gt;Human Nature / The Family
    of Blood&lt;/a&gt;. You do need to have watched a few Whos to really get into
    the Doctor's nature and character to appreciate this one, (I'd
    suggest watching the series 4 picks first). It
    was the first episode for me to reach the same heights as &lt;a href="https://en.wikipedia.org/wiki/The_Empty_Child"&gt;The Empty Child / The Doctor
    Dances&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For the fourth series Tenant was joined by the already well-known
    comedienne Catharine Tate as the companion Donna. Again Moffat came
    up with an outstanding thriller &lt;a href="https://en.wikipedia.org/wiki/Silence_in_the_Library"&gt;Silence in the Library /
    Forest of the Dead&lt;/a&gt;. But there are also other great highlights here.
    &lt;a href="https://en.wikipedia.org/wiki/The_Unicorn_and_the_Wasp"&gt;The Unicorn and the Wasp&lt;/a&gt; is probably the most out-and-out comedy in
    Who, a wonderful send-up of the Agatha Christie country house murder
    mystery (featuring Agatha herself, as only Who can). Russel Davis also
    came up with a taut character-driven thriller &lt;a href="https://en.wikipedia.org/wiki/Midnight_(Doctor_Who)"&gt;Midnight&lt;/a&gt;. I also
    really enjoyed &lt;a href="https://en.wikipedia.org/wiki/Turn_Left_(Doctor_Who)"&gt;Turn Left&lt;/a&gt;, but am not sure whether to recommend it
    here as it makes many references to non-cherry-picked episodes.
    However I think you can still enjoy it without  following
    those references, if only for some great acting from Tate and
    Bernard Cribbins. &lt;/p&gt;

&lt;p&gt;After the fourth season there was a year of specials, which
    generally isn't counted as a series. From this set, I'd pick &lt;a href="https://en.wikipedia.org/wiki/The_Waters_of_Mars"&gt;The Waters of Mars&lt;/a&gt;, which was a fine take on the "base under seige"
    style of Who plot.&lt;/p&gt;

&lt;p&gt;With series 5 there was a wholesale change. Russel Davies gave up
    the role of showrunner, handing over to Moffat. Tenant also gave up
    the role of the Doctor. Matt Smith became the eleventh Doctor, and
    we got a new companion in Amy, later joined by her husband Rory. The
    first two episodes of the Moffat era: &lt;a href="https://en.wikipedia.org/wiki/The_Eleventh_Hour_(Doctor_Who)"&gt;The
    Eleventh Hour&lt;/a&gt; and &lt;a href="https://en.wikipedia.org/wiki/The_Beast_Below"&gt;The Beast Below&lt;/a&gt; are both
    worth picking. I also would pick the two parter &lt;a href="https://en.wikipedia.org/wiki/The_Time_of_Angels"&gt;The Time of Angels /
    Flesh and Stone&lt;/a&gt;,
    although for this one it's important to have seen &lt;a href="https://en.wikipedia.org/wiki/Blink_(Doctor_Who)"&gt;Blink&lt;/a&gt; first (to
    know about The Weeping Angels) and &lt;a href="https://en.wikipedia.org/wiki/Silence_in_the_Library"&gt;Silence in the Library /
    Forest of the Dead&lt;/a&gt; to know about River Song. River Song becomes an
    important character in various episodes in series 5 and 6 after
    this. Series 5 is also another good series to watch all the way
    through, with good development up to the finale. But if you want to
    stay with cherry picking, do watch &lt;a href="https://en.wikipedia.org/wiki/Amy%27s_Choice_(Doctor_Who)"&gt;Amy's Choice&lt;/a&gt;, and perhaps &lt;a href="https://en.wikipedia.org/wiki/Vincent_and_the_Doctor"&gt;Vincent and the Doctor&lt;/a&gt;&lt;a href="#footnote-vincent"&gt;[3]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;One of the traditions of New Who is to have a Christmas special
    episode for broadcast on Christmas Day. Davis did the earlier
    christmas specials, and I don't put any of them on my pick list. But
    I do pick Moffat's first special &lt;a href="https://en.wikipedia.org/wiki/A_Christmas_Carol_(Doctor_Who)"&gt;A
    Christmas Carol&lt;/a&gt;, as a clever remix of the Scrooge
    story starring Michael Gambon.&lt;/p&gt;

&lt;p&gt;With series 6, Moffat decided to break the pattern of earlier
    series and start the series with a
    two-parter &lt;a href="https://en.wikipedia.org/wiki/The_Impossible_Astronaut"&gt;The
    Impossible Astronaut / Day of the Moon"&lt;/a&gt;, which opens
    the series with a bang. The unresolved question from this episode
    may entice you to watch this whole series, and that's not a bad choice.
    If you would rather cherry-pick, you shouldn't miss &lt;a href="https://en.wikipedia.org/wiki/The_Doctor%27s_Wife"&gt;The Doctor's Wife&lt;/a&gt; (written by Neil
    Gaiman) and &lt;a href="https://en.wikipedia.org/wiki/The_Girl_Who_Waited"&gt;The Girl Who Waited&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Series 7 is a series of two distinct parts, with a halfway
    split seeing the departure of Amy and Rory and the arrival of the new companion
    Clara. My cherry-picking suggestion here is to see &lt;a href="https://en.wikipedia.org/wiki/Asylum_of_the_Daleks"&gt;The Asylum of the Daleks&lt;/a&gt; first (which sort-of introduces Clara). Then see
    &lt;a href="https://en.wikipedia.org/wiki/The_Snowmen"&gt;The Snowmen&lt;/a&gt; which picks up the Doctor after he is sadly
    separated from Amy and Rory and sort-of introduces him to Clara.
    Moffat does some clever meta-textual stuff here, which you can
    only appreciate fully if you're a completist but those two
    episodes still stand strong even without fully getting the
    meta-text in the background. I would also pick out &lt;a href="https://en.wikipedia.org/wiki/The_Crimson_Horror"&gt;The Crimson Horror&lt;/a&gt;, which is a comedy with a fun performance from
    Diana Rigg.&lt;/p&gt;

&lt;p&gt;Series 7 finishes with the Big Event episode of Who so far, the
    50th anniversary special (&lt;a href="https://en.wikipedia.org/wiki/The_Day_of_the_Doctor"&gt;The Day of the Doctor&lt;/a&gt;), broadcast 50 years after the first
    episode. Naturally there's a huge amount of references for the fans
    in this episode, which unites both Tenant and Smith's Doctor,
    together with John Hurt's sort-of Doctor. It still stands alone for
    pickers, so don't let the lack of background stop you watching it.
    Before you do, however, catch the minisode: the &lt;a href="https://www.youtube.com/watch?v=-U3jrS-uhuo"&gt;The Night of the Doctor&lt;/a&gt;
    on youtube. It's a remarkable coup of story-writing to get so much
    into a seven minute episode (although it does help to know that Paul
    McGann played the eighth Doctor, the one that immediately preceded the New Who
    period).&lt;/p&gt;

&lt;p&gt;With series 8, Peter Capaldi takes on the role of the twelfth Doctor,
    and brings a darker, less charming Doctor to the scene. Not everyone
    liked this take on the character, but I do as it reminds me of My
    Doctor (the 3rd - Jon Pertwee). It also is my favorite full-series
    arc, with some great character development and interplay between the Doctor and
    Clara. For cherry pickers, however, I'd pick out 
    &lt;a href="https://en.wikipedia.org/wiki/Into_the_Dalek"&gt;Into the Dalek&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Listen_(Doctor_Who)"&gt;Listen&lt;/a&gt;, &lt;a href="https://en.wikipedia.org/wiki/Kill_the_Moon"&gt;Kill the Moon&lt;/a&gt;, and &lt;a href="https://en.wikipedia.org/wiki/Mummy_on_the_Orient_Express"&gt;Mummy on the Orient Express&lt;/a&gt;, but ensure you see the
    others before you see &lt;a href="https://en.wikipedia.org/wiki/Mummy_on_the_Orient_Express"&gt;Mummy on the Orient Express&lt;/a&gt; since that last episode
    gains a lot from getting familiar with Capaldi's Doctor.&lt;/p&gt;

&lt;p&gt;As I write this, the latest Doctor Who episode was &lt;a href="https://en.wikipedia.org/wiki/Last_Christmas_(Doctor_Who)"&gt;Last Christmas&lt;/a&gt;, the latest
    Christmas Special, which is also a pick for its wonderful mashup of
    Alien, Santa Claus, and another movie you'll recognize.&lt;/p&gt;

&lt;p&gt;So how about Classic Who? There's lots of Classic Who, but if
    you're going to explore it, I should pick out a couple of places to start.
    Unlike New Who which goes for single or double episodes, Classic Who
    had short serials of 4-6 half hour episodes. I would start with &lt;a href="https://en.wikipedia.org/wiki/City_of_Death"&gt;City of Death&lt;/a&gt;, which has the iconic Tom Baker as the fourth Doctor,
    Romana as a Time Lord companion, and a script that clearly shows it
    was part-written by Douglas Adams. After that I have to point you to
    some 3rd Doctor (since he's My Doctor), and pick out
    &lt;a href="https://en.wikipedia.org/wiki/Carnival_of_Monsters"&gt;Carnival of
    Monsters&lt;/a&gt;, written by Robert Holmes, generally rated as the
    greatest of the Classic Who writers, and features Jo Grant who was
    the companion I best remember. &lt;/p&gt;



&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/articles/doctor-who.html&amp;amp;text=A%20Cherry%20Picker&amp;#x27;s%20Guide%20to%20Doctor%20Who" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/articles/doctor-who.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/articles/doctor-who.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;For articles on similar topics&amp;#8230;&lt;/h2&gt;

&lt;p&gt;&amp;#8230;take a look at the tag: &lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:43:12 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:a-cherry-pickers-guide-to-doctor-who.html</guid></item><item><title>A Rational approach to integ. testing</title><link>http://www.ciandcd.com/a-rational-approach-to-integ-testing.html</link><description>&lt;div&gt;&lt;p&gt;In this challenging environment, a combination of automated integration testing and test virtualization can enable test teams to improve software quality and keep up with the rate of change. This white paper helps address these needs by describing the benefits that can be gained through a proactive and continuous approach to integration testing with IBM&amp;#174; Rational&amp;#174; test automation solutions.&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:43:03 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:a-rational-approach-to-integ-testing.html</guid></item><item><title>An Insider’s Guide to Agile Testing and Service Virtualization</title><link>http://www.ciandcd.com/an-insiders-guide-to-agile-testing-and-service-virtualization.html</link><description>&lt;div&gt;&lt;p&gt;Get the essentials about agile testing and service virtualization from several different perspectives &amp;#8211; get an analyst&amp;#8217;s take from Diego Lo Giudice of Forrester Research on how to remove agile testing bottlenecks and ways to calculate your potential return on investment for Service Virtualization. Hear about real customer implementations in the financial services, travel, and healthcare sectors. Learn from experts on how service virtualization is essential for mobile development, testing packaged applications (such as SAP), and more.&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:43:00 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:an-insiders-guide-to-agile-testing-and-service-virtualization.html</guid></item><item><title>Mobile Application Development Primer</title><link>http://www.ciandcd.com/mobile-application-development-primer.html</link><description>&lt;div&gt;&lt;p&gt;Industries of all varieties have begun to realize that the target audiences for their business applications have shifted in massive numbers from the use of traditional personal computers, such as desktops and laptops, to using mobile devices such as smart phones and tablets for accessing the internet and for obtaining the information they seek. This applies if the intended audience for the application is a direct customer of the enterprise (Business-to-Consumer apps, or &amp;#8220;B2C&amp;#8221;), or if the targeted user is an employee or business partner (&amp;#8220;B2E&amp;#8221; and &amp;#8220;B2B&amp;#8221;, or Business-to-Employee and Business-to-Business apps). Across the globe, more people are now using mobile devices that they can carry with them wherever they go, and which are more user friendly and intuitive to use, as their primary means of obtaining information and requesting services over the internet.&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:42:58 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:mobile-application-development-primer.html</guid></item><item><title>IBM Resilient Services for Telecommunication - Identifying and using the right architecture for delivering resilient services</title><link>http://www.ciandcd.com/ibm-resilient-services-for-telecommunication-identifying-and-using-the-right-architecture-for-delivering-resilient-services.html</link><description>&lt;div&gt;&lt;p&gt;The IBM Resilient Services for Telecommunication solution is designed to enable EA practitioners to identify and use the right reference architecture for developing resilient services.&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:42:57 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:ibm-resilient-services-for-telecommunication-identifying-and-using-the-right-architecture-for-delivering-resilient-services.html</guid></item><item><title>Ten Answers Regarding Mobile App Testing</title><link>http://www.ciandcd.com/ten-answers-regarding-mobile-app-testing.html</link><description>&lt;div&gt;&lt;p&gt;This white paper digs deep into the reasons testing mobile apps is fundamentally harder than traditional web or desktop applications. A collaboration by Tina Zhuo and Dennis Schultz from IBM along with Yoram Mizrachi from Perfecto Mobile and John Montgomery from uTest, these experts explore the complexities of mobile test environments, the value of the mobile device cloud, the unique role crowd sourcing can play, and how teams can leverage automation to help deliver quality apps.&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:42:51 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:ten-answers-regarding-mobile-app-testing.html</guid></item><item><title>IBM Cloudant: The Do-More NoSQL Data Layer</title><link>http://www.ciandcd.com/ibm-cloudant-the-do-more-nosql-data-layer.html</link><description>&lt;div&gt;&lt;p&gt;Cloudant represents a strategic acquisition by IBM&amp;#174; that extends the company&amp;#8217;s Big Data and Analytics portfolio to include a fully managed, NoSQL cloud service. Cloudant simplifies the development cycle for creators of fast-growing web and mobile applications, by alleviating the burdens of mundane database administration tasks. Developers are then able to focus on building the next generation of systems of engagement &amp;#8211; social and mobile applications &amp;#8211; without losing time, money, or sleep managing their database infrastructure and growth. Critically, Cloudant is an enterprise-ready service that supports this infrastructure with guaranteed performance and availability. Built atop a CouchDB-based NoSQL data layer, Cloudant&amp;#8217;s fully managed database-as-a-service (DBaaS) enables applications and their developers to be more agile. As a part of its data layer, clients have access to multi-master replication and mobile device synchronization capabilities for occasionally connected devices. Applications can take advantage of Cloudant&amp;#8217;s advanced real-time indexing for ad hoc full text search via Apache Lucene, online analytics via MapReduce, and advanced geospatial querying. Mobile applications can use a durable replication protocol for offline sync and global data distribution, as well as a geo-load balancing capability to ensure cross-data center availability and optimal performance. Cloudant&amp;#8217;s RESTful web-based API, flexible schema, and capacity to scale massively are what empower clients to deliver applications to market faster in a cost-effective, DBA-free service model. This IBM Redbooks&amp;#174; Solution Guide describes the IBM Cloudant features.&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:42:50 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:ibm-cloudant-the-do-more-nosql-data-layer.html</guid></item><item><title>Smart Planning for Smarter Infrastructure</title><link>http://www.ciandcd.com/smart-planning-for-smarter-infrastructure.html</link><description>&lt;div&gt;&lt;p&gt;A smart Infrastructure project is a large system of systems.&amp;#160; This paper looks at how a systems engineering approach can benefit an organization planning smart infrastructure projects.&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:42:48 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:smart-planning-for-smarter-infrastructure.html</guid></item><item><title>Agile For Dummies</title><link>http://www.ciandcd.com/agile-for-dummies.html</link><description>&lt;div&gt;&lt;p&gt;Confused by all the agile advice? Relax! With Agile for Dummies by your side you&amp;#8217;ll learn the fundamentals of agile and how to increase the productivity of your software teams while enabling them to produce higher-quality solutions that better fulfill customer needs much faster.&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:42:47 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:agile-for-dummies.html</guid></item><item><title>Smarter quality management: The fast track to competitive advantage</title><link>http://www.ciandcd.com/smarter-quality-management-the-fast-track-to-competitive-advantage.html</link><description>&lt;div&gt;&lt;p&gt;This paper introduces quality management (QM), a practical, multi-disciplined approach to software delivery that helps reduce time to market without sacrificing quality in the outcome.&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:42:45 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:smarter-quality-management-the-fast-track-to-competitive-advantage.html</guid></item><item><title>DevOps Leadership Series: Security at Velocity [Video]</title><link>http://www.ciandcd.com/devops-leadership-series-security-at-velocity-video.html</link><description>&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;p&gt;If it does not fit, it does not get done. &amp;#160;For many DevOps practices, application security falls into the &amp;#8220;does not get done&amp;#8221; bucket. &amp;#160;That&amp;#8217;s because for many DevOps-centric organizations, application security has historically be done somewhere else, by someone else, who is slow.&lt;/p&gt;

&lt;p&gt;Go faster. &amp;#160;Shift left. &amp;#160;Remove complexity. Reduce rework. &amp;#160;All mantras of DevOps practices. &amp;#160;And while DevOps practices have changed dramatically in recent years, many experts will tell you that application security has not changed enough.&lt;/p&gt;

&lt;p&gt;In this installment of the DevOps Leadership Series, you will hear&amp;#160;&lt;a href="https://www.linkedin.com/pub/chris-corriere/14/5a1/159"&gt;Chris Corriere&lt;/a&gt;&amp;#160;(DevOps Engineer, Autotrader) and&amp;#160;&lt;a href="https://www.linkedin.com/in/mitchellashley"&gt;Mitchell Ashley&lt;/a&gt;&amp;#160;(VP Information Technology, CableLabs) share perspectives the state of DevOps and security.&lt;/p&gt;

&lt;p&gt;First, listen to Chris&amp;#8217; perspective that security can move at DevOps speed, as long as you take a diversified approach:&lt;/p&gt;&lt;p align="center"&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;Then hear from Mitchell as he remarks on the evolution of security from a validation role (at the right) to an integrated, ingrained role that has shifted left:&lt;/p&gt;&lt;p align="center"&gt;&lt;br&gt;&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:42:37 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:devops-leadership-series-security-at-velocity-video.html</guid></item><item><title>Tips for Writing for a Tech Audience</title><link>http://www.ciandcd.com/tips-for-writing-for-a-tech-audience.html</link><description>&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;p&gt;I&amp;#8217;ve been writing articles and blog posts about web development and technology for a long time. The original version of this blog started in 2004, but by that time I&amp;#8217;d already written a couple articles for the ultra-prestigious ColdFusion Developer&amp;#8217;s Journal (it&amp;#8217;s ok to feel jealous).&lt;/p&gt;

&lt;p&gt;However, I&amp;#8217;ve also been editing articles and blog posts about web development and technology for a while too. It started when I was at Adobe helping to run the Adobe Developer Connection a few years ago and continued when I launched my own site (Flippin&amp;#8217; Awesome which is now Modern Web and not run by me anymore). I still do this on an almost daily basis running the &lt;a href="http://remotesynthesis.com/general/2015/05/18/writing-for-tech-audience/developer.telerik.com"&gt;Telerik Developer Network&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;All of this experience has taught me some things that I think help to make a really good (and potentially really popular) article or blog post for a developer or technology audience. In this post I&amp;#8217;ll share my recommendations, though I should note that I&amp;#8217;m not an expert at always following my own guidelines all the time.&lt;/p&gt;
&lt;h2&gt;Have a Style&lt;/h2&gt;

&lt;p&gt;It&amp;#8217;s important to keep in mind that you aren&amp;#8217;t writing API docs. API docs are generally dry, boring and simply stick to the facts. That&amp;#8217;s their goal. However, an article or blog post should allow some of your personality to shine through. This helps to make the content both more relatable and more enjoyable to read.&lt;/p&gt;

&lt;p&gt;Keep in mind that your goal is both to educate and to entertain, to some degree. Some developers revert to a litany of code and explanation. It&amp;#8217;s better to have a voice and have a story.&lt;/p&gt;

&lt;p&gt;Some things that can help:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Explain why you are trying to do something, not just what you are trying to do and how you are doing it.&lt;/li&gt;
	&lt;li&gt;What made you get interested in doing this?&lt;/li&gt;
	&lt;li&gt;Did you have a struggles along the way? There&amp;#8217;s no shame in admitting that you found something difficult - your readers will likely relate to the experience.&lt;/li&gt;
	&lt;li&gt;Have fun with the demo! Perhaps pick something you are interested in that isn&amp;#8217;t technical. For example. I often choose to use some cartoons I enjoy as subject matter for my demos.&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;Know Your Audience&lt;/h2&gt;

&lt;p&gt;While you should have a voice and a style, it&amp;#8217;s important to know when it&amp;#8217;s ok to be more or less casual in your voice. If I am writing something for my blog, I am often much more casual than if I am writing for the Telerik Developer Network or Sitepoint.&lt;/p&gt;

&lt;p&gt;If I am writing for my blog, proper grammar, punctuation and spelling are less important. If I am writing for a professional site, these become the difference between seeming like an amateur and not. You&amp;#8217;d be surprised how people are affected by these things, even if they do not recognize it themselves. Some sites have editors who help with this, but others don&amp;#8217;t - so don&amp;#8217;t rely on them to correct your mistakes.&lt;/p&gt;

&lt;p&gt;Try to always have a friend you trust read through the article first - this isn&amp;#8217;t critical for a personal blog post but even those can benefit. Even the best writers need a second opinion and there is nothing than can make your content better than a good, critical opinion.&lt;/p&gt;
&lt;h2&gt;Stay on Track&lt;/h2&gt;

&lt;p&gt;So many developers tend to think that every little detail is pertinent. So, they get sidetracked. Instead of traveling straight down a path, they don&amp;#8217;t just point out the detours, but take you down them.&lt;/p&gt;

&lt;p&gt;As a rule, if the information doesn&amp;#8217;t apply to most situations, don&amp;#8217;t spend time on it. These are the kind of scenarios like, if you are running an old version of X operating system and want to perform special action Y, you&amp;#8217;ll need to do this a different way - let&amp;#8217;s walk through it. Another example is getting lost in caveats, detailing every minor exception that in all likelihood doesn&amp;#8217;t apply to the reader.&lt;/p&gt;

&lt;p&gt;The best strategy in these cases is to simply point to the &amp;#8220;detour&amp;#8221; as an aside and link to the best resource to follow. Or note that there are exceptions but don&amp;#8217;t go detailed into the full list of caveats.&lt;/p&gt;

&lt;p&gt;You may feel as though you are being somehow incomplete in your coverage, but you are less likely to lose the 90% of the readers to whom the straightforward path applies by not catering your post to the 10%.&lt;/p&gt;
&lt;h2&gt;Avoid the Wall of Text&lt;/h2&gt;

&lt;p&gt;There is nothing harder to read than a post that has no headers and is filled with overly long paragraphs. Adding section headers and even subheaders for long sections not only makes your article easier to read, but also makes it easier to scan - which can help the reader determine it&amp;#8217;s value to them.&lt;/p&gt;

&lt;p&gt;Shorter paragraphs also make your content more readable and scannable. Plus, a wall of unbroken text can seem intimidating to a reader. Break up large paragraphs and, whenever possible, place key ideas into lists, which I&amp;#8217;ve found can help drive home key points and improve retention.&lt;/p&gt;
&lt;h2&gt;What Are Your Ideas?&lt;/h2&gt;

&lt;p&gt;Hopefully you&amp;#8217;ve found these ideas helpful. Do you have any strategies you use to improve your writing? Please share.&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:42:35 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:tips-for-writing-for-a-tech-audience.html</guid></item><item><title>/home When Moving from Ubuntu to Fedora</title><link>http://www.ciandcd.com/home-when-moving-from-ubuntu-to-fedora.html</link><description>&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;p&gt;After using Ubuntu (13.10) for like almost one year I decided to move back to Fedora (Fedora 21). This is going to be a short post on my experience on mounting the same /home I used in Ubuntu for Fedora.&lt;br&gt;
&lt;br&gt;
 I had a separate partition for /home in Ubuntu which I needed to be mounted as the /home in Fedora as well. In anaconda (Fedora installer) I choose to configure the partitions manually. In the manual partitioning window, it listed all the partition I had under Ubuntu (it was smart that it listed them under the label Ubuntu 13.10). I mounted the / of Ubuntu with re-formatting to be the same in Fedora. And for /home, I mounted the same /home in Ubuntu to be the same in Fedora. But /home was listed under both "New Fedora 21 Installation" and "Ubuntu 13.10" as well. I proceeded. During the installation I created the same user ("kalpa") which was there Ubuntu. It took a considerable amount of time for the "User creation" phase of the installation. This is to set the file permission for the new user. Time taken for the process may change based on the number of files that are there in home. The rest of the installation went smooth. And I have no issue with /home up to now using Fedora 21.&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:42:16 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:home-when-moving-from-ubuntu-to-fedora.html</guid></item><item><title>The Most Controversial Concept in Agile Delivery: Estimating in Story Points</title><link>http://www.ciandcd.com/the-most-controversial-concept-in-agile-delivery-estimating-in-story-points.html</link><description>&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;p&gt;This blog post is another one of those that I should have written a while ago as the topic of story point based estimation keeps coming up again and again. To really understand why story point based estimation is important for Agile delivery, I think I need to explain the idea behind it.&lt;/p&gt;

&lt;p&gt;The purpose of estimates is to get a good idea of how much work needs to be done to achieve a certain outcome. To do that, the estimate should be accurate and reasonably precise. This is where the crux of the problem is: precision. If I&amp;#8217;d asked you how long it takes to fly from Sydney to Los Angeles, you would not respond with an estimate that includes minutes and seconds because you know that it is ineffective as it is not precise. The more precise we get in estimates, the more we pretend to be able to do something that we cannot do: work at that level of precision. The other downside of precision is that each level of precision requires more work to be put in the estimation process. I have done many IT projects and can tell you that my estimates for each individual task is off by as much as +/- 100% easily, but in aggregate my estimates are pretty good.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s explore the difference between accuracy and precision a bit further:&lt;/p&gt;

&lt;p align="center"&gt;&lt;a href="https://notafactoryanymore.files.wordpress.com/2015/05/accurate-vs-precise.jpg"&gt;&lt;img class="aligncenter size-large wp-image-235" src="https://notafactoryanymore.files.wordpress.com/2015/05/accurate-vs-precise.jpg?w=625&amp;amp;h=469" alt="accurate vs precise" height="469" width="625"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It should be clear that we care more about accuracy than we care about precision and that is exactly what story points do for me. I am spending just the necessary amount of time estimating to be reasonably accurate without trying to become too precise. The usual Fibonacci sequence (1,2,3,5,8,&amp;#8230;) helps to avoid false precision as well. Now, to be honest we could call it 1,2,3,5,8 days and be done with it as that would probably achieve the same outcome as story points. The problem is that for some reason we are a lot more tempted to use the other in between numbers when we talk about days. We are also tempted to equate days of effort with schedule, and most of us can attest that a day of effort is hardly ever done in a day of schedule as we get distracted, multi-task or attend meetings. The story point concept provides us with a nice abstraction that prevents these mental shortcuts and keeps us focused on the relative nature of the estimate.&lt;/p&gt;

&lt;p&gt;The other thing that should be obvious is that a day of effort for one person is not the same as a day of effort for another person. More experienced people need less time than more junior people, so any estimate in hours or days is flawed unless you know who will do it. Story points do not suffer from this problem as they are relative to other stories and independent to the person performing the tasks associated with it.&lt;/p&gt;

&lt;p&gt;The other nice thing with Agile estimation is that it usually is a lot closer to the often recommended Delphi technique, which asks multiple independent experts to estimate tasks and then aggregate it. Planning poker is a pretty close approximation of the Delphi technique and is therefore much more accurate than estimates done by individuals.&lt;/p&gt;

&lt;p&gt;But why do we need a point system at all, why do we not just do relative sizing in t-shirt sizes or something similar. As I have explored in another blogpost (&lt;a title="Sure ways to fail #2 – Not knowing what the goal is" href="http://notafactoryanymore.com/2015/05/07/sure-ways-to-fail-2-not-knowing-what-the-goal-is/"&gt;link&lt;/a&gt;), teams need a goal line whenever there is a certain outcome to be achieved. The easiest way to do so is by tracking progress on a numerical scale (see &lt;a title="Agile Reporting at the enterprise level – where to look? (Part 1 – Status reporting)" href="http://notafactoryanymore.com/2014/09/07/agile-reporting-at-the-enterprise-level-where-to-look-part-1-status-reporting/"&gt;Agile reporting post&lt;/a&gt;). And if you work in a larger organisation you probably want to have some common currency to be able to measure the throughput (see &lt;a title="Agile Reporting at the enterprise level (Part 2) – Measuring Productivity" href="http://notafactoryanymore.com/2015/02/26/agile-reporting-at-the-enterprise-level-part-2-measuring-productivity/"&gt;productivity blog&lt;/a&gt;) and be able to swap stories between teams. Here I will go with the guidance that SAFe provides, start with a point being roughly a full day of work and estimate everything else relative to that. On a regular basis bring members of the team together to estimate an example set of stories and use this process to recalibrate the relative understanding of size.&lt;/p&gt;

&lt;p&gt;So what if things change? One thing that people are always concerned about is scope creep or inaccurate estimates. For a story by itself I don&amp;#8217;t have strong opinions on whether or not you update the size once you realise there is more or less work than expected. However, if you use larger buckets for your initial estimates (e.g. a feature that should roughly take you 100 pts), then I think it is important to measure how many points the stories of that feature actually add up to &amp;#8211; if that is different to 100 pts in this case you have some real scope change that will impact your timelines.&lt;/p&gt;

&lt;p&gt;To close off, I will provide a few helpful links to other comments/blogs about story points which you can use to learn more about this topic:&lt;/p&gt;

&lt;p&gt;&lt;a href="http://www.scruminc.com/story-points-why-are-they-better-than/"&gt;http://www.scruminc.com/story-points-why-are-they-better-than/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="http://collaboration.csc.ncsu.edu/laurie/Papers/ESEM11_SCRUM_Experience_CameraReady.pdf"&gt;http://collaboration.csc.ncsu.edu/laurie/Papers/ESEM11_SCRUM_Experience_CameraReady.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="http://www.mountaingoatsoftware.com/blog/seeing-how-well-a-teams-story-points-align-from-one-to-eight"&gt;http://www.mountaingoatsoftware.com/blog/seeing-how-well-a-teams-story-points-align-from-one-to-eight&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="https://www.scrum.org/Forums/aft/564"&gt;https://www.scrum.org/Forums/aft/564&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="http://www.mlcarey321.com/2013/08/normalized-story-points.html"&gt;http://www.mlcarey321.com/2013/08/normalized-story-points.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href="http://blogs.versionone.com/agile_management/2013/10/14/scalable-agile-estimation-and-normalization-of-story-points-introduction-and-overview-of-the-blog-series-part-1-of-5/"&gt;http://blogs.versionone.com/agile_management/2013/10/14/scalable-agile-estimation-and-normalization-of-story-points-introduction-and-overview-of-the-blog-series-part-1-of-5/&lt;/a&gt;&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:42:14 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:the-most-controversial-concept-in-agile-delivery-estimating-in-story-points.html</guid></item><item><title>Atom.io for Markdown Editing</title><link>http://www.ciandcd.com/atomio-for-markdown-editing.html</link><description>&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;p&gt;A while back, &lt;a target="_blank" href="https://atom.io/"&gt;Atom.io&lt;/a&gt; replaced &lt;a target="_blank" href="http://www.sublimetext.com/"&gt;Sublime Text&lt;/a&gt; as my go-to editor of choice for most things (I still use Visual Studio for .NET, but I work at Microsoft so that's expected I guess.) Since I work on the documentation team, a lot of what I use Atom for is Markdown authoring. Some folks on my team have asked about my recommendations on using Atom for Markdown editing, so I thought this might be interesting for those outside the documentation team also.&lt;/p&gt;
&lt;h2&gt;Markdown support&lt;/h2&gt;

&lt;p&gt;Out of the box, Atom has pretty good support for Markdown. It has syntax highlighting and rendered preview functionality right out of the box, along with support for things like GitHub flavored Markdown. However, if you are coming from a dedicated Markdown editor such as &lt;a target="_blank" href="http://www.markdownpad.com/,"&gt;Markdown Pad&lt;/a&gt; you may miss features like a preview that scrolls in sync with the Markdown view.&lt;/p&gt;

&lt;p&gt;Fortunately, Atom has a variety of community generated packages that can be installed to provide additional functionality. Here are the ones I recommend:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;
&lt;p&gt;&lt;a target="_blank" href="https://atom.io/packages/markdown-writer"&gt;Markdown-Writer&lt;/a&gt;: Adds a bunch of keyboard commands for things like text formatting and creating links &amp;amp; images, along with support for popular static site blogging platforms.&lt;/p&gt;
&lt;/li&gt;
	&lt;li&gt;
&lt;p&gt;&lt;a target="_blank" href="https://atom.io/packages/markdown-scroll-sync"&gt;Markdown-Scorll-Sync&lt;/a&gt;: Makes the rendered preview scroll in sync with the Markdown view.&lt;/p&gt;
&lt;/li&gt;
	&lt;li&gt;
&lt;p&gt;&lt;a target="_blank" href="https://atom.io/packages/markdown-format"&gt;Markdown-Format&lt;/a&gt;: Makes your Markdown pretty when you save. Things like renumbering lists so they are actually in order (vs. the 1, 2, 3, 5, 5, 5, 8, 9 I always seem to end up with,) and padding cells in GitHub Flavored Markdown tables so they are more readable.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There's more than this, so I'll leave it to you to explore the other Markdown related packages.&lt;/p&gt;
&lt;h2&gt;Installing packages&lt;/h2&gt;

&lt;p&gt;Once you've installed &lt;a target="_blank" href="https://atom.io"&gt;Atom.io&lt;/a&gt;, to install these or any other packages, perform the following steps:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
&lt;p&gt;From the &lt;strong&gt;File&lt;/strong&gt; menu, select &lt;strong&gt;Settings&lt;/strong&gt;, then &lt;strong&gt;Install&lt;/strong&gt;. Enter the name of the package you wish to install (or part of the name, such as &lt;strong&gt;Markdown&lt;/strong&gt; to see all packages that contain that word.)&lt;/p&gt;
&lt;/li&gt;
	&lt;li&gt;
&lt;p&gt;Click the &lt;strong&gt;Install&lt;/strong&gt; button beside the package. If you want to read more about the package before installing, click the title of the package and it will open your browser and display more information.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;h2&gt;Configuration&lt;/h2&gt;

&lt;p&gt;Once installed, you can use the &lt;strong&gt;Packages&lt;/strong&gt; tab from &lt;strong&gt;Settings&lt;/strong&gt; to disable, enable, or configure packages.&lt;/p&gt;

&lt;p&gt;While some packages have their own configuration, you will also want to look at &lt;strong&gt;File&lt;/strong&gt;, &lt;strong&gt;Settings&lt;/strong&gt;, &lt;strong&gt;Settings&lt;/strong&gt; to configure the following settings for Markdown authoring:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Tab length&lt;/strong&gt;: If &lt;strong&gt;Soft tabs&lt;/strong&gt; is enabled, then you should set this to 4 since Markdown expects either a single tab or 4 spaces when indenting.&lt;/p&gt;
&lt;/li&gt;
	&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Soft wrap&lt;/strong&gt;, &lt;strong&gt;Soft wrap and preferred line length&lt;/strong&gt;, and &lt;strong&gt;Preferred line length&lt;/strong&gt;: This causes the editor to wrap lines at the specified line length. Otherwise, paragraphs will scroll off the editor to the right. Makes things much more readable, especially when the preview window is open.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This last one is a matter of preference. Markdown-Writer does its own special thing with the tab key for indention. In the current version, on Windows, it only seems to indent in a list, while I want it to indent everywhere when I hit the tab key. You can override the current Markdown-Writer functionality by doing the following:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
&lt;p&gt;&lt;strong&gt;File&lt;/strong&gt;, &lt;strong&gt;Settings&lt;/strong&gt;, &lt;strong&gt;Keybindings&lt;/strong&gt;, then click the link to &lt;strong&gt;your keymap file&lt;/strong&gt; at the top of the page.&lt;/p&gt;
&lt;/li&gt;
	&lt;li&gt;
&lt;p&gt;Add the following new lines to the end of the &lt;strong&gt;keymap.cson&lt;/strong&gt; file:&lt;/p&gt;
'atom-text-editor[data-grammar~=\'gfm\']':
'tab':&amp;#160;'editor:indent'

&lt;p&gt;This restores normal tab functionality for Markdown editing. &lt;strong&gt;NOTE&lt;/strong&gt;: If you're using something like Markdown-Format, it may convert 4 spaces to tabs automatically when you save.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;&lt;h2&gt;Tips&lt;/h2&gt;

&lt;ul&gt;
	&lt;li&gt;
&lt;p&gt;To toggle the Markdown preview, use ctrl-shift-m.&lt;/p&gt;
&lt;/li&gt;
	&lt;li&gt;
&lt;p&gt;If you've opend Atom to a specific folder, and it's showing the tree view side bar, you can dismiss it with ctrl-.&lt;/p&gt;
&lt;/li&gt;
	&lt;li&gt;
&lt;p&gt;Atom.io checks for package updates automatically. If you see a blue box in the lower right-hand corner, that means some packages have been updated. Click on the blue box to install them.&lt;/p&gt;
&lt;/li&gt;
	&lt;li&gt;
&lt;p&gt;If you're working in a folder that contains a lot of Markdown files, don't even try using the tree view. It's 100x easier to use ctrl-t and then start typing the file name you want to open. This will search through the directory structure for files that match the text you've entered, and you can then click the one you want.&lt;/p&gt;
&lt;/li&gt;
	&lt;li&gt;
&lt;p&gt;If you need to change the indentation for a section of text (for example, a section of source code,) just select it and hit ctrl-[ or ] to change the indent level.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;h2&gt;So what about Visual Studio Code?&lt;/h2&gt;

&lt;p&gt;Yes, Visual Studio Code is nice, and it has some basic Markdown support. But it's brand new and the extensibility story hasn't materialized yet. So while it can be used for Markdown right now, it doesn't have all the things I need/want from a Markdown editor.&lt;/p&gt;

&lt;p&gt;Long term, it may kick Atom off my system once the community can start extending it. I'll evaluate it and do a blog on it once we get to that point.&lt;/p&gt;
&lt;h2&gt;Summary&lt;/h2&gt;

&lt;p&gt;Atom.io is a great Markdown authoring tool for people who need a generic, extensible Markdown editor. While it may not have all the bells and whistles, or speed of a dedicated Markdown tool, it has enough functionality and extensibility to get the job done. Since it also handles code authoring, it's also a nice way to reduce the number of specialized tools loaded on your workstation.&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:42:11 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:atomio-for-markdown-editing.html</guid></item><item><title>Choosing an Issue Tracker for an Online Service</title><link>http://www.ciandcd.com/choosing-an-issue-tracker-for-an-online-service.html</link><description>&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;p&gt;I&amp;#8217;ve been working on an application that I&amp;#8217;m getting close to publicly launching, and while the site is mostly functional with most of the development done (I&amp;#8217;ve been working on it in my &amp;#8216;spare time&amp;#8217; for almost a year), there&amp;#8217;s some non-development tasks I need to complete before launching, including:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;adding Google AdSense and Analytics&lt;/li&gt;
	&lt;li&gt;completing online documentation&lt;/li&gt;
	&lt;li&gt;setting up an online issue tracker&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The options for an issue tracker has taken me by surprise as there are so many options, ranging from free to varying monthly fee options. I&amp;#8217;m somewhat familiar with some&amp;#160;of the older free development project focused options, like &lt;a href="https://www.bugzilla.org/"&gt;Bugzilla&lt;/a&gt;, &lt;a href="http://trac.edgewall.org/"&gt;Trac&lt;/a&gt;, &lt;a href="http://www.redmine.org/"&gt;Redmine&lt;/a&gt;, and commercial options like Atlassian&amp;#8217;s &lt;a href="https://www.atlassian.com/software/jira"&gt;Jira&lt;/a&gt;. Part of my app is open source on GitHub and so to use the GitHub issue tracker for that part is an easy choice. I&amp;#8217;d also like to make sure whichever issue tracker I chose is easy to use from the point of view of my end users, the majority of which are unlikely to have existing GitHub accounts, or accounts on other online tracker sites, and I want to make sure it&amp;#8217;s as easy as possible for them to log issues and enhancement requests.&lt;/p&gt;

&lt;p&gt;(This article &lt;a href="http://mashable.com/2014/02/16/bug-tracking-apps/"&gt;here&lt;/a&gt; has a good list of a number of issue trackers.)&lt;/p&gt;

&lt;p&gt;Ranging from simplest to use, &lt;a href="https://trello.com/"&gt;Trello&lt;/a&gt; is a stand-out from the crowd. Although it&amp;#8217;s not a dedicated issue tracker, the approach of managing lists of items and moving items (cards) from one list (e.g. open items) to another (e.g.closed items) is trivial.&lt;/p&gt;

&lt;p&gt;Categorized in interesting-but-with-high-technical-requirements is &lt;a href="https://www.jetbrains.com/youtrack/"&gt;YouTrack&lt;/a&gt; from JetBrains, and free for upto 10 users &amp;#8230; but requiring 1GB heap to run, this would cost me some monthly runtime costs for &amp;#160;medium gear on OpenShift, so I&amp;#8217;m not sure &amp;#160;if this is worth the investment at this point.&lt;/p&gt;

&lt;p&gt;Next up in the looks-interesting category is &lt;a href="https://asana.com/product"&gt;Asana&lt;/a&gt; &amp;#8211; I&amp;#8217;m not familiar with this service, it looks like it offers much more than I would need, but could be worth investigating.&lt;/p&gt;

&lt;p&gt;I&amp;#8217;ve some decisions to make here, and right now I&amp;#8217;m thinking either using GitHub&amp;#8217;s issue tracker or Trello. What would you recommend &amp;#8211; leave me a comment! &lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:42:08 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:choosing-an-issue-tracker-for-an-online-service.html</guid></item><item><title>Know Thy MVN Plugins: Keeping One's Sanity Amidst Open Source Version Hell</title><link>http://www.ciandcd.com/know-thy-mvn-plugins-keeping-ones-sanity-amidst-open-source-version-hell.html</link><description>&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;p&gt;Problem #1: Everyone knows that keeping up with versions is tough.&amp;#160; This is the reason tools such as &lt;a href="http://www.openlogic.com/"&gt;OpenLogic&lt;/a&gt; come to exist. Some companies pay, some companies develop home grown solutions, some live with the version which are getting older every day.&lt;/p&gt;

&lt;p&gt;Problem #2:&amp;#160; When multiple open source projects once consumed by your product can bring in different versions of the same library. One never knows which will be picked up at run-time and behavior on the developer&amp;#8217;s box based on the Murphy&amp;#8217;s Law will be different from the server run-time.&lt;/p&gt;

&lt;p&gt;This does not have to be such an ordeal.&amp;#160; With the power of maven plugins this can be solved relatively easy. &lt;a href="http://www.mojohaus.org/versions-maven-plugin/"&gt;Versions Maven Plugin&lt;/a&gt; and &lt;a href="http://maven.apache.org/enforcer/maven-enforcer-plugin/"&gt;Maven Enforcer Plugin&lt;/a&gt; to the rescue! &lt;/p&gt;

&lt;p&gt;Versions Maven Plugin will keep versions up-to-date and as you probably guessed from the name Maven Enforcer plugin will be guarding against multiple versions of the maven artifact in the build package produced.&lt;/p&gt;

&lt;p&gt;Let&amp;#8217;s see how one introduced enforcer into the mix first. The code below can go in the parent pom.xml of the project, or global parent of the projects if one exists.&lt;/p&gt;&lt;pre class="as3"&gt;&amp;lt;project &amp;#8230;&amp;gt;
&amp;#8230;
&amp;lt;plugins&amp;gt;
&amp;#8230;
&amp;lt;plugin&amp;gt;
        &amp;lt;groupId&amp;gt;org.apache.maven.plugins&amp;lt;/groupId&amp;gt;
        &amp;lt;artifactId&amp;gt;maven-enforcer-plugin&amp;lt;/artifactId&amp;gt;
        &amp;lt;version&amp;gt;1.4&amp;lt;/version&amp;gt;
        &amp;lt;executions&amp;gt;
          &amp;lt;execution&amp;gt;
            &amp;lt;id&amp;gt;enforce-versions&amp;lt;/id&amp;gt;
            &amp;lt;goals&amp;gt;
              &amp;lt;goal&amp;gt;enforce&amp;lt;/goal&amp;gt;
            &amp;lt;/goals&amp;gt;
            &amp;lt;configuration&amp;gt;
              &amp;lt;rules&amp;gt;
                &amp;lt;requireMavenVersion&amp;gt;
                  &amp;lt;version&amp;gt;[2.2.*,)&amp;lt;/version&amp;gt;
                &amp;lt;/requireMavenVersion&amp;gt;
                &amp;lt;requireJavaVersion&amp;gt;
                  &amp;lt;version&amp;gt;[1.7.*,)&amp;lt;/version&amp;gt;
                &amp;lt;/requireJavaVersion&amp;gt;
                &amp;lt;DependencyConvergence/&amp;gt;
              &amp;lt;/rules&amp;gt;
            &amp;lt;/configuration&amp;gt;
          &amp;lt;/execution&amp;gt;
        &amp;lt;/executions&amp;gt;
      &amp;lt;/plugin&amp;gt;
&amp;#8230;
&amp;lt;/plugins&amp;gt;
&amp;#8230;
&amp;lt;/project&amp;gt;&lt;/pre&gt;&lt;p&gt;For each dependency version collision one has to pick the version to keep and exclude the ones that are mismatched. See maven &lt;a href="https://maven.apache.org/guides/introduction/introduction-to-optional-and-excludes-dependencies.html"&gt;help page&lt;/a&gt; for details. If one wants absolute guarantee of the version used, this is the only way to go. An example of excluded dependencies will be:&lt;/p&gt;&lt;pre class="as3"&gt;&amp;lt;project &amp;#8230;&amp;gt;
&amp;#8230;
	&amp;lt;dependencies&amp;gt;
&amp;#8230;
		&amp;lt;dependency&amp;gt;
			&amp;lt;groupId&amp;gt;com.lordofthejars&amp;lt;/groupId&amp;gt;
			&amp;lt;artifactId&amp;gt;nosqlunit-mongodb&amp;lt;/artifactId&amp;gt;
			&amp;lt;version&amp;gt;${nosqlunit.veresion}&amp;lt;/version&amp;gt;
		  &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;
		  &amp;lt;exclusions&amp;gt;
	      	&amp;lt;exclusion&amp;gt;
	      		&amp;lt;groupId&amp;gt;org.slf4j&amp;lt;/groupId&amp;gt;
	      		&amp;lt;artifactId&amp;gt;slf4j-api&amp;lt;/artifactId&amp;gt;
	      	&amp;lt;/exclusion&amp;gt;
	      	&amp;lt;exclusion&amp;gt;
	      		&amp;lt;groupId&amp;gt;com.github.fakemongo&amp;lt;/groupId&amp;gt;
	      		&amp;lt;artifactId&amp;gt;fongo&amp;lt;/artifactId&amp;gt;
	      	&amp;lt;/exclusion&amp;gt;
	      	&amp;lt;exclusion&amp;gt;
	      		&amp;lt;groupId&amp;gt;org.mongodb&amp;lt;/groupId&amp;gt;
	      		&amp;lt;artifactId&amp;gt;mongo-java-driver&amp;lt;/artifactId&amp;gt;
	      	&amp;lt;/exclusion&amp;gt;
		  &amp;lt;/exclusions&amp;gt;
		&amp;lt;/dependency&amp;gt;
&amp;#8230;
	&amp;lt;/dependencies&amp;gt;
&amp;#8230;
&amp;lt;/project&amp;gt;&lt;/pre&gt;&lt;p&gt;From the &lt;b&gt;open source developer side&lt;/b&gt;, producing two flavors of the package for consumption with and without dependencies can make world a better place. This is the difference of maven scope &amp;#8216;provided&amp;#8217; vs. default scope &amp;#8216;compile&amp;#8217;. &lt;a href="https://maven.apache.org/plugins/maven-shade-plugin/"&gt;Apache Maven Shade Plugin&lt;/a&gt; can be used to produce the version with the dependencies to be used there no conflicts can arise along with the version that is artifact version independent and can be used without version conflicts.&lt;/p&gt;

&lt;p&gt;Once all conflicts are resolved and one version of each artifact is in the final build product guaranteed, one should check if ones project is up to date. &amp;#160;It is a good practice to do the check at the beginning of the new version.&lt;/p&gt;

&lt;p&gt;To check for outdates dependencies, and plugins, and bring those up to date in the controlled manner (manually) one can execute:&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;mvn &lt;a href="http://www.mojohaus.org/versions-maven-plugin/display-dependency-updates-mojo.html"&gt;versions:display-dependency-updates&lt;/a&gt;&lt;br&gt;&lt;/li&gt;&lt;li&gt;mvn &lt;a href="http://www.mojohaus.org/versions-maven-plugin/display-plugin-updates-mojo.html"&gt;versions:display-plugin-updates&lt;/a&gt;&lt;br&gt;&lt;/li&gt;&lt;li&gt;mvn &lt;a href="http://www.mojohaus.org/versions-maven-plugin/display-property-updates-mojo.html"&gt;versions:display-property-updates&lt;/a&gt;&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;





&lt;p&gt;As an alternative one can let the plugin update the versions by executing following&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;mvn &lt;a href="http://www.mojohaus.org/versions-maven-plugin/use-latest-releases-mojo.html"&gt;versions:use-latest-releases&lt;/a&gt;&lt;br&gt;&lt;/li&gt;&lt;li&gt;mvn &lt;a href="http://www.mojohaus.org/versions-maven-plugin/update-properties-mojo.html"&gt;versions:update-properties&lt;/a&gt;&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;



&lt;p&gt;This last mvn command I am going to share is a bonus for the dedicated reader. It allows changing versions across the project and its modules painlessly:&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;mvn &lt;a href="http://www.mojohaus.org/versions-maven-plugin/set-mojo.html"&gt;versions:set&lt;/a&gt; -DgenerateBackupPoms=false&amp;#8203; -DnewVersion=&amp;lt;version&amp;gt;&lt;br&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:42:05 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:know-thy-mvn-plugins-keeping-ones-sanity-amidst-open-source-version-hell.html</guid></item><item><title>Git Simple Feature Branch Workflow</title><link>http://www.ciandcd.com/git-simple-feature-branch-workflow.html</link><description>&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;p&gt;In my&amp;#160;&lt;a href="http://madhukaudantha.blogspot.com/2015/06/workflows-for-git.html" target="_blank"&gt;previous post&lt;/a&gt;, I wrote about git work flows. Now I will going to try out simple '&lt;a href="http://madhukaudantha.blogspot.com/2015/06/workflows-for-git.html" target="_blank"&gt;Feature Branch Workflow&lt;/a&gt;'.&lt;br&gt;&lt;/p&gt;1. I pull down the latest changes from mastergit checkout mastergit pull origin master2. I make branch to make changes&amp;#160;git checkout -b new-feature3. Now I am working on the feature4. I keep my feature branch fresh and up to date with the latest changes in master, using 'rebase'Every once in a while during the development update the feature branch with the latest changes in master.git fetch origingit rebase origin/masterIn the case where other devs are also working on the same shared remote feature branch, also rebase changes coming from it:git rebase origin/new-featureResolving conflicts during the rebase allows me to have always clean merges at the end of the feature development.5. When I am ready I commit my changesgit add -pgit commit -m "my changes"6. rebasing keeps my code working, merging easy, and history clean.git fetch origingit rebase origin/new-featuregit rebase origin/masterBelow two points are optional6.1 push my branch for discussion (pull-request)git push origin new-feature6.2 feel free to rebase within my feature branch, my team can handle it!git rebase -i origin/master&lt;br&gt;&lt;i&gt;&lt;b&gt;Few point that can be happen in developing phase.&lt;/b&gt;&lt;/i&gt;&lt;br&gt;Another new feature is needed and it need some commits from my new branch 'new-feature' that new feature need new branch and few commits need to push to it and clean from my branch.&lt;br&gt;&lt;br&gt;7.1 Creating x-new-feature branch on top of 'new-feature'&lt;br&gt;git checkout -b&amp;#160;x-new-feature&amp;#160;new-feature&lt;br&gt;&lt;br&gt;7.2 Cleaning commits&lt;br&gt;//revert a commit&lt;br&gt;git revert --no-commit&amp;#160;&lt;br&gt;//reverting few steps a back from current HEAD&lt;br&gt;git reset --hard HEAD~2&lt;br&gt;&lt;br&gt;7.3 Updating the git&lt;br&gt;//Clean new-feature branch&lt;br&gt;git push origin HEAD --force	
	&lt;p&gt;&lt;/p&gt;

    &lt;p&gt;1. I pull down the latest changes from mastergit checkout mastergit pull origin master2. I make branch to make changes git checkout -b new-feature3. Now I am working on the feature4. I keep my feature branch fresh and up to date with the latest changes in master, using 'rebase'Every once in a while during the development update the feature branch with the latest changes in master.git fetch origingit rebase origin/masterIn the case where other devs are also working on the same shared remote feature branch, also rebase changes coming from it:git rebase origin/new-featureResolving conflicts during the rebase allows me to have always clean merges at the end of the feature development.5. When I am ready I commit my changesgit add -pgit commit -m "my changes"6. rebasing keeps my code working, merging easy, and history clean.git fetch origingit rebase origin/new-featuregit rebase origin/masterBelow two points are optional6.1 push my branch for discussion (pull-request)git push origin new-feature6.2 feel free to rebase within my feature branch, my team can handle it!git rebase -i origin/masterAnother new feature is needed and it need some commits from my new branch 'new-feature' that new feature need new branch and few commits need to push to it and clean from my branch.7.1 Creating x-new-feature branch on top of 'new-feature'git checkout -b x-new-feature new-feature7.2 Cleaning commits//revert a commitgit revert --no-commit//reverting few steps a back from current HEADgit reset --hard HEAD~27.3 Updating the git//Clean new-feature branchgit push origin HEAD --force&lt;/p&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:42:01 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:git-simple-feature-branch-workflow.html</guid></item><item><title>Organisation Pattern: Trunk Based Development</title><link>http://www.ciandcd.com/organisation-pattern-trunk-based-development.html</link><description>&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;p&gt;Trunk Based Development is a version control strategy in which developers commit their changes to the shared trunk of a source code repository with minimal branching. Trunk Based Development became well known in the mid 2000s as&amp;#160;&lt;a href="http://www.martinfowler.com/articles/continuousIntegration.html"&gt;Continuous Integration&lt;/a&gt;&amp;#160;became a mainstream development practice, and today it is equally applicable to centralised Version Control Systems (VCS) and Distributed Version Control Systems (DVCS).&lt;/p&gt;&lt;p&gt;In Trunk Based Development new features are developed concurrently on trunk as a series of small, incremental steps that preserve existing functionality and minimise merge complexity. Features are always released from trunk, and defect fixes are either released from trunk or a short-lived release branch.&lt;/p&gt;&lt;p&gt;When development of a feature spans multiple releases its entry point is concealed to ensure the ongoing changes do not impede release cadence. The addition of a new feature can be concealed with a&amp;#160;&lt;a href="http://www.martinfowler.com/bliki/FeatureToggle.html"&gt;Feature Toggle&lt;/a&gt;, which means a configuration parameter or business rule is used to turn a feature on or off at runtime. As shown below&amp;#160;a&amp;#160;Feature Toggle is turned off while its feature is in development (v1), turned on when its feature is in production (v2), and removed after a period of time (v3).&lt;/p&gt;&lt;p&gt;&lt;a href="http://www.alwaysagileconsulting.com/blog/wp-content/uploads/2015/04/Organisation-Pattern-Trunk-Based-Development-Feature-Toggle-Step-By-Step.png"&gt;&lt;img class="aligncenter wp-image-4456" src="http://www.alwaysagileconsulting.com/blog/wp-content/uploads/2015/04/Organisation-Pattern-Trunk-Based-Development-Feature-Toggle-Step-By-Step.png" alt="Organisation Pattern - Trunk Based Development - Feature Toggle Step By Step" width="650" height="208"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Updates to an existing feature can be concealed with a&amp;#160;&lt;a href="http://martinfowler.com/bliki/BranchByAbstraction.html"&gt;Branch By Abstraction&lt;/a&gt;, which means an&amp;#160;abstraction layer is temporarily introduced to encapsulate both the old behaviour in use and the new behaviour in development. As shown below a&amp;#160;Branch By Abstraction&amp;#160;routes requests to the old behaviour while the new behaviour is in development (v1-v2), reroutes requests to the new behaviour when it is in production (v3), and is removed after a period of time (v4).&lt;/p&gt;&lt;p&gt;&lt;a href="http://www.alwaysagileconsulting.com/blog/wp-content/uploads/2015/04/Organisation-Pattern-Trunk-Based-Development-Branch-By-Abstraction-Step-By-Step.png"&gt;&lt;img class="aligncenter wp-image-4458" src="http://www.alwaysagileconsulting.com/blog/wp-content/uploads/2015/04/Organisation-Pattern-Trunk-Based-Development-Branch-By-Abstraction-Step-By-Step.png" alt="Organisation Pattern - Trunk Based Development - Branch By Abstraction Step By Step" width="650" height="223"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Trunk Based Development is synonymous with Continuous Integration, which has been described by Jez Humble et al as &amp;#8220;&lt;a href="http://www.amazon.co.uk/dp/1449368425"&gt;the most important technical practice in the agile canon&lt;/a&gt;&amp;#8220;. Continuous Integration is a development practice where&amp;#160;all members of a team integrate and test their changes together on at least a daily basis, resulting in a shared mindset of collaboration and an always releasable codebase. This is verified by an automated build server continuously building the latest changes, and can include pre- and post-build actions such as code reviews and auto-revert on failure.&lt;/p&gt;&lt;p&gt;Consider an organisation that provides an online Company Accounts Service, with its codebase maintained by a team practicing Trunk Based Development and Continuous Integration. In iteration 1&amp;#160;two features are requested &amp;#8211; F1 Computations and F2 Write Offs &amp;#8211; so the team discuss their concurrent development&amp;#160;and decide on a&amp;#160;Feature Toggle for F1 as it is a larger change. The developers commit their changes for F1 and F2 to trunk multiple times a day, with F1 tested in its on and off states to verify its progress alongside F2.&lt;/p&gt;&lt;p&gt;&lt;a href="http://www.alwaysagileconsulting.com/blog/wp-content/uploads/2015/06/Organisation-Pattern-Trunk-Based-Development-Trunk-Based-Development-1.png"&gt;&lt;img class="aligncenter wp-image-4464" src="http://www.alwaysagileconsulting.com/blog/wp-content/uploads/2015/06/Organisation-Pattern-Trunk-Based-Development-Trunk-Based-Development-1.png" alt="Organisation Pattern - Trunk Based Development - Trunk Based Development 1" width="882" height="450"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In iteration 2 more features &amp;#8211; F3 Bank Details and F4 Accounting Periods &amp;#8211; begin development. F4 requires a different downstream submissions system, so the team design&amp;#160;a Branch By Abstraction for submissions to ensure F1 and F3 can continue with the legacy&amp;#160;submissions system until F4 is complete.&amp;#160;F2 is signed off and released into production with F1 still toggled off at runtime. Some changes for F3 break the build, which triggers an automatic revert and a team discussion on a better design for F3.&lt;/p&gt;&lt;p&gt;&lt;a href="http://www.alwaysagileconsulting.com/blog/wp-content/uploads/2015/06/Organisation-Pattern-Trunk-Based-Development-Trunk-Based-Development-2.png"&gt;&lt;img class="aligncenter wp-image-4465" src="http://www.alwaysagileconsulting.com/blog/wp-content/uploads/2015/06/Organisation-Pattern-Trunk-Based-Development-Trunk-Based-Development-2.png" alt="Organisation Pattern - Trunk Based Development - Trunk Based Development 2" width="882" height="450"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In iteration 3 a&amp;#160;production defect is found in F2, and after the defect is fixed on trunk a release branch is agreed for risk mitigation. An F2.1 release branch is created from the last commit of the&amp;#160;F2&amp;#160;release,&amp;#160;the&amp;#160;fix is merged to the branch, and F2.1 is released into production. F4 continues on trunk, with the&amp;#160;submissions Branch By Abstraction tested in both modes. F3 is signed off and released into production using the legacy submissions system.&lt;/p&gt;&lt;p&gt;&lt;a href="http://www.alwaysagileconsulting.com/blog/wp-content/uploads/2015/06/Organisation-Pattern-Trunk-Based-Development-Trunk-Based-Development-3.png"&gt;&lt;img class="aligncenter wp-image-4466" src="http://www.alwaysagileconsulting.com/blog/wp-content/uploads/2015/06/Organisation-Pattern-Trunk-Based-Development-Trunk-Based-Development-3.png" alt="Organisation Pattern - Trunk Based Development - Trunk Based Development 3" width="882" height="450"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In iteration 4&amp;#160;F1 is signed off and its Feature Toggle is turned on in production following a release. F4 is signed off and released into production, but when the Branch By Abstraction is switched to the&amp;#160;new submissions system a defect is found. As a result the Branch By Abstraction is reverted at runtime to the legacy submissions system, and a F4.1 fix is&amp;#160;released from trunk.&lt;/p&gt;&lt;p&gt;&lt;a href="http://www.alwaysagileconsulting.com/blog/wp-content/uploads/2015/06/Organisation-Pattern-Trunk-Based-Development-Trunk-Based-Development-4.png"&gt;&lt;img class="aligncenter wp-image-4467" src="http://www.alwaysagileconsulting.com/blog/wp-content/uploads/2015/06/Organisation-Pattern-Trunk-Based-Development-Trunk-Based-Development-4.png" alt="Organisation Pattern - Trunk Based Development - Trunk Based Development 4" width="882" height="450"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;In this example F1, F2, F3, and F4 clearly benefit from being developed by a team collaborating on a single shared code stream. For F1 the team agrees on the why and how of the Feature Toggle, with F1 tested in both its on and off states. For F2 the defect fix is made available from trunk and everyone is aware of the decision to use a release branch for risk mitigation. For F3 the prominence of a reverted build failure encourages people to contribute to a better design. For F4 there is a team&amp;#160;decision&amp;#160;to create a submissions Branch By Abstraction, with the new abstraction layer offering fresh insights into the legacy system and incremental commits enabling regular feedback on the new approach. Furthermore, when the new submissions system is switched on and a defect is found in F4 the ability to revert at runtime to the legacy submissions means the Company Accounts Service can remain online with zero downtime.&lt;/p&gt;&lt;p&gt;This&amp;#160;highlights the advantages of Trunk Based Development:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Continuous Integration &amp;#8211; incremental commits to trunk ensure an&amp;#160;always&amp;#160;integrated, always tested codebase with minimal integration costs and a predictable flow of features&lt;/li&gt;&lt;li&gt;Adaptive scheduling &amp;#8211; an always releasable codebase separates the release schedule from development efforts, meaning features can be released on demand&amp;#160;according to customer needs&lt;/li&gt;&lt;li&gt;Collaborative design &amp;#8211; everyone working&amp;#160;on&amp;#160;the same code encourages constant communication,&amp;#160;with team members sharing responsibility for design changes and a cohesive&amp;#160;&lt;a href="http://www.ibm.com/developerworks/java/library/j-eaed1/index.html"&gt;Evolutionary Architecture&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Operational and business empowerment&amp;#160;&amp;#8211;&amp;#160;techniques such as&amp;#160;Feature Toggle and Branch By Abstraction decouple release from launch, providing&amp;#160;the operational benefit of graceful degradation&amp;#160;on failure and the business benefit of&amp;#160;&lt;a href="http://www.informit.com/articles/article.aspx?p=1833567&amp;amp;seqNum=2"&gt;Dark Launching&lt;/a&gt;&amp;#160;features&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Breaking down features and re-architecting an existing system in incremental&amp;#160;steps requires discipline, planning, and ingenuity from an entire team on a daily basis, and Trunk Based Development can incur a development overhead for some time if multiple technologies are in play and/or the codebase is poorly structured. However, those&amp;#160;additional efforts&amp;#160;will substantially reduce integration costs and gradually push&amp;#160;the codebase in the right&amp;#160;direction &amp;#8211; as shown&amp;#160;by Dave Farley and Jez Humble praising Trunk Based Development for &amp;#8220;&lt;a href="http://www.amazon.co.uk/dp/0321601912"&gt;the gentle, subtle pressure it applies to make the design of your software better&lt;/a&gt;&amp;#8220;.&lt;/p&gt;&lt;p&gt;A common misconception of Trunk Based Development is that it is slow, as features take longer to complete and team velocity is&amp;#160;often lower than expected. However, an organisation should optimise globally for cycle time not locally for velocity, and by mandating a single code stream Trunk Based Development ensures developers work at the maximum rate of the team not the individual, with reduced integration costs resulting in lower lead times.&lt;/p&gt;&lt;p&gt;Trunk Based Development is&amp;#160;simple, but not easy. It has a steep learning curve but the continuous integration&amp;#160;of&amp;#160;small changesets into trunk&amp;#160;will minimise integration costs,&amp;#160;encourage collaborative design, empower runtime&amp;#160;operational and business decisions, and&amp;#160;ultimately drive the engine of Continuous&amp;#160;Delivery. It is for this reason Dave Farley and Jez Humble&amp;#160;declared&amp;#160;&amp;#8220;&lt;a href="http://www.amazon.co.uk/dp/0321601912"&gt;we can&amp;#8217;t emphasise enough how important this practice is in enabling continuous delivery of valuable, working software&lt;/a&gt;&amp;#8220;.&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:41:59 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:organisation-pattern-trunk-based-development.html</guid></item><item><title>Does DevOps Reduce Technical Debt – or Make it Worse?</title><link>http://www.ciandcd.com/does-devops-reduce-technical-debt-or-make-it-worse.html</link><description>&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;p&gt;DevOps can help&amp;#160;&lt;a href="http://www.informationweek.com/strategic-cio/how-devops-can-cut-innovation-crushing-technical-debt/a/d-id/1318677"&gt;reduce technical debt&lt;/a&gt;&amp;#160;in some fundamental ways.&lt;/p&gt;&lt;h2&gt;Continuous Delivery/Deployment&lt;/h2&gt;&lt;p&gt;First, building a&amp;#160;&lt;a href="http://devops.com/2014/07/29/continuous-delivery-pipeline/"&gt;Continuous Delivery/Deployment pipeline&lt;/a&gt;, automating the work of migration and deployment, will force you to clean up inconsistencies and holes in configuration and code deployment, and inconsistencies between development, test and production environments.&lt;/p&gt;&lt;p&gt;And automated Continuous Delivery and&amp;#160;&lt;a href="http://shop.oreilly.com/product/0636920039297.do"&gt;Infrastructure as Code&lt;/a&gt;&amp;#160;gets rid of dangerous one-of-a-kind&lt;a href="http://martinfowler.com/bliki/SnowflakeServer.html"&gt;snowflakes&lt;/a&gt;&amp;#160;and&amp;#160;&lt;a href="http://java.dzone.com/articles/configuration-drift"&gt;configuration drift&lt;/a&gt;&amp;#160;caused by making configuration changes and applying patches manually over time. Which makes systems easier to setup and manage, and reduces the risk of an un-patched system becoming the target of a security attack or the&amp;#160;&lt;a href="https://news.ycombinator.com/item?id=7652036"&gt;cause of an operational problem&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;A CD pipeline also makes it easier, cheaper and faster to pay down other kinds of technical debt. With Continuous Delivery/Deployment, you can test and push out patches and refactoring changes and platform upgrades faster and with more confidence.&lt;/p&gt;&lt;h2&gt;Positive Feedback&lt;/h2&gt;&lt;p&gt;The Lean feedback cycle and&amp;#160;&lt;a href="http://leankit.com/blog/2015/03/3-reasons-it-ops-uses-lean-flow-kanban-for-devops-part-2-of-3/"&gt;Just-in-Time prioritization&lt;/a&gt;&amp;#160;in DevOps ensures that you&amp;#8217;re working on whatever is most important to the business. This means that bugs and usability issues and security vulnerabilities don&amp;#8217;t have to wait until after the next feature release to get fixed. Instead, problems that impact operations or the users will get fixed immediately.&lt;/p&gt;&lt;p&gt;Teams that do&amp;#160;&lt;a href="https://codeascraft.com/2012/05/22/blameless-postmortems/"&gt;Blameless Post-Mortems&lt;/a&gt;&amp;#160;and&amp;#160;&lt;a href="http://www.kitchensoap.com/2014/11/14/the-infinite-hows-or-the-dangers-of-the-five-whys/"&gt;Root Cause(s) Analysis&lt;/a&gt;&amp;#160;when problems come up will go even further, and fix problems at the source and improve in fundamental and important ways.&lt;/p&gt;&lt;p&gt;But there&amp;#8217;s a negative side to DevOps that can add to technical debt costs.&lt;/p&gt;&lt;h2&gt;Erosive Change&lt;/h2&gt;&lt;p&gt;Michael Feathers&amp;#8217; research has shown that constant,&amp;#160;&lt;a href="http://swreflections.blogspot.ca/2012/10/bad-things-happen-to-good-code.html"&gt;iterative change is erosive&lt;/a&gt;: the same code gets changed over and over, the same classes and methods become bloated (because it is naturally easier to add code to an existing method or a method to an existing class), structure breaks down and the design is eventually lost.&lt;/p&gt;&lt;p&gt;DevOps can make this even worse.&lt;/p&gt;&lt;p&gt;DevOps and Continuous Delivery/Deployment involves pushing out lots of small changes, running experiments and iteratively tuning features and the user experience based on continuous feedback from production use.&lt;/p&gt;&lt;p&gt;Many DevOps teams work directly on the code mainline, &amp;#8220;&lt;a href="http://theagileadmin.com/2010/06/24/velocity-2010-always-ship-trunk/"&gt;branching in code&lt;/a&gt;&amp;#8221; to &amp;#8220;&lt;a href="http://agiletesting.blogspot.ca/2009/07/dark-launching-and-other-lessons-from.html"&gt;dark launch&lt;/a&gt;&amp;#8221; code changes, while code is still being developed, using conditional logic and flags to skip over sections of code at run-time. This can make the code hard to understand, and potentially dangerous: if a&amp;#160;&lt;a href="http://martinfowler.com/bliki/FeatureToggle.html"&gt;feature toggle&lt;/a&gt;&amp;#160;is turned on before the code is ready,&lt;a href="http://web.archive.org/web/20110721063430/http://jamesmckay.net/2011/07/why-does-martin-fowler-not-understand-feature-branches/"&gt;&amp;#160;bad things can happen&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Feature flags are also used to run A/B experiments and control risk on release, by rolling out a change incrementally to a few users to start. But the longer that feature flags are left in the code, the&lt;a href="http://swreflections.blogspot.ca/2014/08/feature-toggles-are-one-of-worst-kinds.html"&gt;harder it is to understand and change&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;There is a lot of housekeeping that needs to be done in DevOps: upgrading the CD pipeline and making sure that all of the tests are working; maintaining Puppet or Chef (or whatever configuration management tool you are using) recipes; disciplined,&amp;#160;&lt;a href="http://swreflections.blogspot.com/2012/04/what-refactoring-is-and-what-it-isnt.html"&gt;day-to-day refactoring&lt;/a&gt;; keeping track of features and options and cleaning them up when they are no longer needed, getting rid of dead code and trying to keep the code as simple as possible.&lt;/p&gt;&lt;h2&gt;Microservices and Technology Choices&lt;/h2&gt;&lt;p&gt;&lt;a href="http://martinfowler.com/articles/microservices.html"&gt;Microservices&amp;#160;&lt;/a&gt;are a&amp;#160;&lt;a href="http://www.infoq.com/news/2014/08/microservices-monoliths"&gt;popular architectural approach for DevOps&lt;/a&gt;&amp;#160;teams.&lt;/p&gt;&lt;p&gt;This is because loosely-coupled Microservices are easier for individual teams to independently deploy, change,&amp;#160;&lt;a href="http://jimplush.com/talk/2015/02/28/microservices-allow-for-localized-tech-debt/"&gt;refactor or even replace&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;And a Microservices-based approach provides developers with more freedom when deciding on language or technology stack: teams don&amp;#8217;t necessarily have to work the same way, they can choose the right tool for the job, as long as they support an API contract for the rest of the system.&lt;/p&gt;&lt;p&gt;In the short term there are obvious advantages to giving teams more freedom in making technology choices. They can deliver code faster, quickly try out prototypes, and teams get a chance to experiment and learn about different technologies and languages.&lt;/p&gt;&lt;p&gt;But Microservices &amp;#8220;&lt;a href="http://highscalability.com/blog/2014/4/8/microservices-not-a-free-lunch.html"&gt;are not a free lunch&lt;/a&gt;&amp;#8221;. As you add more services, system testing costs and complexity increase. Debugging and problem solving gets harder. And as more teams choose different languages and frameworks, it&amp;#8217;s harder to track vulnerabilities, harder to operate, and harder for people to switch between teams. Code gets duplicated because teams want to minimize coupling and it is difficult or impossible to share libraries in a polyglot environment. Data is often duplicated between services for the same reason, and data inconsistencies creep in over time.&lt;/p&gt;&lt;h2&gt;Negative Feedback&lt;/h2&gt;&lt;p&gt;There is a potentially negative side to the Lean delivery feedback cycle too.&lt;/p&gt;&lt;p&gt;Constantly responding to production feedback, always working on what&amp;#8217;s most immediately important to the organization, doesn&amp;#8217;t leave much space or time to consider bigger, longer-term technical issues, and to work on paying off deeper architectural and technical design debt that result from poor early decisions or incorrect assumptions.&lt;/p&gt;&lt;p&gt;Smaller, more immediate problems get fixed fast in DevOps. Bugs that matter to operations and the users can get fixed right away instead of waiting until all the features are done, and patches and upgrades to the run-time can be pushed out more often. Which means that you can pay off a lot of debt before costs start to compound.&lt;/p&gt;&lt;p&gt;But behind-the-scenes, strategic debt will continue to add up. Nothing&amp;#8217;s broke, so you don&amp;#8217;t have to fix anything right away. And you can&amp;#8217;t refactor your way out of it either, at least not easily. So you end up living with a poor design or an aging technology platform, slowly slowing down your ability to respond to changes, to come up with new solutions. Or forcing you to continue filling in security holes as they come up, or scrambling to scale as load increases.&lt;/p&gt;&lt;p&gt;DevOps can reduce technical debt. But only if you work in a highly disciplined way. And only if you raise your head up from tactical optimization to deal with bigger, more strategic issues before they become real problems.&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:41:57 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:does-devops-reduce-technical-debt-or-make-it-worse.html</guid></item><item><title>How to Monitor a Java EE DataSource</title><link>http://www.ciandcd.com/how-to-monitor-a-java-ee-datasource.html</link><description>&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;h3&gt;Introduction&lt;/h3&gt;&lt;p&gt;&lt;a href="https://github.com/vladmihalcea/flexy-pool"&gt;FlexyPool&lt;/a&gt;&amp;#160;is an open-source framework that can monitor a&amp;#160;&lt;a href="http://docs.oracle.com/javase/7/docs/api/javax/sql/DataSource.html"&gt;DataSource&lt;/a&gt;connection usage. This tool come out of necessity, since we previously lacked support for provisioning connection pools.&lt;/p&gt;&lt;p&gt;FlexyPool was initially designed for stand-alone environments and the&lt;a href="http://docs.oracle.com/javase/7/docs/api/javax/sql/DataSource.html"&gt;DataSource&lt;/a&gt;&amp;#160;proxy configuration was done programmatically. Using&amp;#160;&lt;a href="http://vladmihalcea.com/2013/12/15/why-i-like-spring-bean-aliasing/"&gt;Spring bean aliases&lt;/a&gt;, we could even substitute an already configured DataSource with the FlexyPool Metrics-aware proxy alternative.&lt;/p&gt;&lt;h3&gt;Java EE support&lt;/h3&gt;&lt;p&gt;Recently, I&amp;#8217;ve been asked about supporting Java EE environments and in the true open-source spirit, I accepted the challenge. Supporting a managed environment is tricky because the DataSource is totally decoupled from the application-logic and made available through a&amp;#160;&lt;a href="https://en.wikipedia.org/wiki/Java_Naming_and_Directory_Interface"&gt;JNDI&lt;/a&gt;&amp;#160;lookup.&lt;/p&gt;&lt;p&gt;One drawback is that we can&amp;#8217;t use automatic pool sizing strategies, since most Application Servers return a custom DataSource implementation (which is closely integrated with their in-house JTA transaction manager solution), that doesn&amp;#8217;t offer access to reading/writing the connection pool size.&lt;/p&gt;&lt;p&gt;While the DataSource might not be adjustable, we can at least monitor the connection usage and that&amp;#8217;s enough reason to support Java EE environments too.&lt;/p&gt;&lt;h3&gt;Adding declarative configuration&lt;/h3&gt;&lt;p&gt;Because we operate in a managed environment, we can no longer configure the DataSource programmatically, so we need to use the declarative configuration support.&lt;/p&gt;&lt;p&gt;By default, FlexyPool looks for the&amp;#160;flexy-pool.properties&amp;#160;file in the current Class-path. The location can be customized using the&amp;#160;flexy.pool.properties.pathSystem property , which can be a:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;URL (e.g.&amp;#160;file:/D:/wrk/vladmihalcea/flexy-pool/flexy-pool-core/target/test-classes/flexy-pool.properties)&lt;/li&gt;&lt;li&gt;File system path (e.g.&amp;#160;D:\wrk\vladmihalcea\flexy-pool\flexy-pool-core\target\test-classes\flexy-pool.properties)&lt;/li&gt;&lt;li&gt;Class-path nested path (e.g.&amp;#160;nested/fp.properties)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The properties file may contain the following configuration options:&lt;/p&gt;Parameter nameDescription&lt;p&gt;flexy.pool.data.source.unique.name&lt;/p&gt;&lt;p&gt;Each FlexyPool instance requires a unique name so that JMX domains won&amp;#8217;t clash&lt;/p&gt;&lt;p&gt;flexy.pool.data.source.jndi.name&lt;/p&gt;&lt;p&gt;The JNDI DataSource location&lt;/p&gt;&lt;p&gt;flexy.pool.data.source.jndi.lazy.lookup&lt;/p&gt;&lt;p&gt;Whether to lookup the DataSource lazily (useful when the target DataSource is not available when the FlexyPoolDataSource is instantiated)&lt;/p&gt;&lt;p&gt;flexy.pool.data.source.class.name&lt;/p&gt;&lt;p&gt;The DataSource can be instantiated at Runtime using this Class name&lt;/p&gt;&lt;p&gt;flexy.pool.data.source.property.*&lt;/p&gt;&lt;p&gt;If the DataSource is instantiated at Runtime, each flexy.pool.data.source.property.${java-bean-property} will set the java-bean-property of the newly instantiated DataSource (e.g. flexy.pool.data.source.property.user=sa)&lt;/p&gt;&lt;p&gt;flexy.pool.adapter.factory&lt;/p&gt;&lt;p&gt;Specifies the PoolAdaptorFactory, in case the DataSource supports dynamic sizing. By default it uses the generic DataSourcePoolAdapter which doesn&amp;#8217;t support auto-scaling&lt;/p&gt;&lt;p&gt;flexy.pool.metrics.factory&lt;/p&gt;&lt;p&gt;Specifies the MetricsFactory used for creating Metrics&lt;/p&gt;&lt;p&gt;flexy.pool.metrics.reporter.log.millis&lt;/p&gt;&lt;p&gt;Specifies the metrics log reported interval&lt;/p&gt;&lt;p&gt;flexy.pool.metrics.reporter.jmx.enable&lt;/p&gt;&lt;p&gt;Specifies if the jmx reporting should be enabled&lt;/p&gt;&lt;p&gt;flexy.pool.metrics.reporter.jmx.auto.start&lt;/p&gt;&lt;p&gt;Specifies if the jmx service should be auto-started (set this to true in Java EE environments)&lt;/p&gt;&lt;p&gt;flexy.pool.strategies.factory.resolver&lt;/p&gt;&lt;p&gt;Specifies a ConnectionAcquiringStrategyFactoryResolver class to be used for obtaining a list of ConnectionAcquiringStrategyFactory objects. This should be set only if the PoolAdaptor supports accessing the DataSource pool size.&lt;/p&gt;&lt;h3&gt;Hibernate ConnectionProvider&lt;/h3&gt;&lt;p&gt;Most Java EE applications already use&amp;#160;&lt;a href="https://en.wikipedia.org/wiki/Java_Persistence_API"&gt;JPA&lt;/a&gt;&amp;#160;and for those who happen to be using Hibernate, we can make use of the&amp;#160;&lt;a href="https://docs.jboss.org/hibernate/orm/4.3/manual/en-US/html/ch03.html#configuration-jdbc-properties"&gt;hibernate.connection.provider_class&lt;/a&gt;configuration property for injecting our proxy DataSource.&lt;/p&gt;&lt;p&gt;Hibernate provides many built-in extension points and the connection management is totally configurable. By providing a custom&lt;a href="https://docs.jboss.org/hibernate/orm/4.3/javadocs/org/hibernate/engine/jdbc/connections/spi/ConnectionProvider.html"&gt;ConnectionProvider&lt;/a&gt;&amp;#160;we can substitute the original DataSource with the FlexyPool proxy.&lt;/p&gt;&lt;p&gt;All we have to do is adding the following property to our&amp;#160;persistence.xml&amp;#160;file:&lt;/p&gt;&lt;pre class="xml"&gt;&amp;lt;property name="hibernate.connection.provider_class"
          value="com.vladmihalcea.flexypool.adaptor.FlexyPoolHibernateConnectionProvider"/&amp;gt;&lt;/pre&gt;&lt;p&gt;Behind the scenes, this provider will configure a FlexyPoolDataSource and use it whenever a new connection is requested:&lt;/p&gt;&lt;pre class="java"&gt;private FlexyPoolDataSource&amp;lt;DataSource&amp;gt; flexyPoolDataSource;
 
@Override
public void configure(Map props) {
    super.configure(props);
    LOGGER.debug(
        "Hibernate switched to using FlexyPoolDataSource
    ");
    flexyPoolDataSource = new FlexyPoolDataSource&amp;lt;DataSource&amp;gt;(
        getDataSource()
    );
}
 
@Override
public Connection getConnection() throws SQLException {
    return flexyPoolDataSource.getConnection();
}&lt;/pre&gt;&lt;h3&gt;Instantiating the actual DataSource at runtime&lt;/h3&gt;&lt;p&gt;If you&amp;#8217;re not using Hibernate, you need to have the FlexyPoolDataSource ready before the&amp;#160;&lt;a href="https://docs.oracle.com/javaee/7/api/javax/persistence/EntityManagerFactory.html"&gt;EntityManagerFactory&lt;/a&gt;&amp;#160;finishes bootstrapping:&lt;/p&gt;&lt;pre class="xml"&gt;&amp;lt;?xml version="1.0" encoding="UTF-8"?&amp;gt;
&amp;lt;persistence version="2.0" xmlns="http://java.sun.com/xml/ns/persistence"
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xsi:schemaLocation="
        http://java.sun.com/xml/ns/persistence
        http://java.sun.com/xml/ns/persistence/persistence_2_0.xsd"&amp;gt;
 
    &amp;lt;persistence-unit name="persistenceUnit" transaction-type="JTA"&amp;gt;
 
        &amp;lt;provider&amp;gt;org.hibernate.jpa.HibernatePersistenceProvider&amp;lt;/provider&amp;gt;
 
        &amp;lt;jta-data-source&amp;gt;java:global/jdbc/flexypool&amp;lt;/jta-data-source&amp;gt;
 
        &amp;lt;properties&amp;gt;
            &amp;lt;property
                name="hibernate.hbm2ddl.auto"
                value="update"/&amp;gt;
 
            &amp;lt;property
                name="hibernate.show_sql"
                value="true"/&amp;gt;
 
            &amp;lt;property
                name="hibernate.dialect"
                value="org.hibernate.dialect.HSQLDialect"/&amp;gt;
 
            &amp;lt;property
                name="hibernate.transaction.jta.platform"
                value="org.hibernate.service.jta.platform.internal.SunOneJtaPlatform"/&amp;gt;
        &amp;lt;/properties&amp;gt;
    &amp;lt;/persistence-unit&amp;gt;
&amp;lt;/persistence&amp;gt;&lt;/pre&gt;&lt;p&gt;While in a production Java EE environment we use an Application server specific DataSource configuration, for simplicity sake, I&amp;#8217;m going to configure the FlexyPooldataSource using the&amp;#160;&lt;a href="http://docs.oracle.com/javaee/7/api/javax/annotation/sql/DataSourceDefinition.html"&gt;DataSourceDefinition&lt;/a&gt;&amp;#160;annotation:&lt;/p&gt;&lt;pre class="java"&gt;@DataSourceDefinition(
    name = "java:global/jdbc/flexypool",
    className = "com.vladmihalcea.flexypool.FlexyPoolDataSource")
@Stateless
public class FlexyPoolDataSourceConfiguration {}&lt;/pre&gt;&lt;p&gt;We now need to pass the actual DataSource properties to FlexyPool and this is done through the flexy-pool.properties configuration file:&lt;/p&gt;&lt;pre class="shell"&gt;flexy.pool.data.source.unique.name=unique-name
flexy.pool.data.source.class.name=org.hsqldb.jdbc.JDBCDataSource
flexy.pool.data.source.property.user=sa
flexy.pool.data.source.property.password=
flexy.pool.data.source.property.url=jdbc:hsqldb:mem:test
flexy.pool.metrics.reporter.jmx.auto.start=true&lt;/pre&gt;&lt;p&gt;The actual DataSource is going to be created by the FlexyPoolDataSource on start-up.&lt;/p&gt;&lt;h3&gt;Locating the actual DataSource from JNDI&lt;/h3&gt;&lt;p&gt;If the actual DataSource is already configured by the Application Server, we can instruct FlexyPool to fetch it from JNDI. Let&amp;#8217;s say we have the following DataSource configuration:&lt;/p&gt;&lt;pre class="java"&gt;@DataSourceDefinition(
    name = "java:global/jdbc/default",
    className = "org.hsqldb.jdbc.JDBCDataSource",
    url = "jdbc:hsqldb:mem:test",
    initialPoolSize = 3,
    maxPoolSize = 5
)
@Stateless
public class DefaultDataSourceConfiguration {}&lt;/pre&gt;&lt;p&gt;To proxy the JNDI DataSource, we need to configure FlexyPool like this:&lt;/p&gt;&lt;pre class="shell"&gt;flexy.pool.data.source.unique.name=unique-name
flexy.pool.data.source.jndi.name=java:global/jdbc/default
flexy.pool.metrics.reporter.jmx.auto.start=true&lt;/pre&gt;&lt;p&gt;The FlexyPoolDataSource is defined alongside the actual DataSource:&lt;/p&gt;&lt;pre class="java"&gt;@DataSourceDefinition(
    name = "java:global/jdbc/flexypool",
    className = "com.vladmihalcea.flexypool.FlexyPoolDataSource")
@Stateless
public class FlexyPoolDataSourceConfiguration {}&lt;/pre&gt;&lt;p&gt;The JPA will have to fetch the FlexyPoolDataSource instead of the actual one:&lt;/p&gt;&lt;pre class="xml"&gt;&amp;lt;jta-data-source&amp;gt;java:global/jdbc/flexypool&amp;lt;/jta-data-source&amp;gt;
&lt;/pre&gt;&lt;p&gt;In&amp;#160;&lt;a href="https://github.com/vladmihalcea/flexy-pool/tree/master/flexy-tomee"&gt;TomEE&lt;/a&gt;, because the DataSourceDefinitions are not lazily instantiated, the actual DataSource might not be available in the JNDI registry when the FlexyPoolDataSource definition is processed.&lt;/p&gt;&lt;p&gt;For this, we need to instruct FlexyPool to dely the JNDI lookup until the DataSource is actually requested:&lt;/p&gt;&lt;pre class="shell"&gt;flexy.pool.data.source.jndi.lazy.lookup=true&lt;/pre&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;&lt;p&gt;The last time I used Java EE was in 2008, on a project that was using Java EE 1.4 with EJB 2.1. After 7 years of using Spring exclusively, I&amp;#8217;m pleasantly surprised by the Java EE experience.&amp;#160;&lt;a href="http://arquillian.org/"&gt;Arquillian&lt;/a&gt;&amp;#160;is definitely my favourite add-on, since integration testing is of paramount importance in enterprise applications.&amp;#160;&lt;a href="http://antoniogoncalves.org/2011/09/25/injection-with-cdi-part-iii/"&gt;CDI&lt;/a&gt;&amp;#160;is both easy and powerful and I&amp;#8217;m glad the dependency injection got standardised.&lt;/p&gt;&lt;p&gt;But the best asset of the Java EE platform is the community itself. Java EE has very strong community, willing to give you a hand when in need. I&amp;#8217;d like to thank&amp;#160;&lt;a href="https://twitter.com/l33tj4v4"&gt;Steve Millidge (Founder of Payara and C2B2)&lt;/a&gt;&amp;#160;for giving me some great tips on designing the FlexyPool Java EE integration,&amp;#160;&lt;a href="https://twitter.com/alexsotob"&gt;Alex Soto&lt;/a&gt;,&amp;#160;&lt;a href="https://twitter.com/agoncal"&gt;Antonio Goncalves&lt;/a&gt;&amp;#160;and all the other Java EE members whom I had some very interesting conversations on Twitter.&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:41:55 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:how-to-monitor-a-java-ee-datasource.html</guid></item><item><title>Reducing Risk Through Security Qa Automation</title><link>http://www.ciandcd.com/reducing-risk-through-security-qa-automation.html</link><description>&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;p&gt;Organizations are under constant pressure to protect their critical assets from cyberattacks that have plagued a wide variety of industries. However, there is currently no set method of how to ensure that company applications will be safe from these threats. &lt;a href="https://www.getzephyr.com/insights/how-empower-your-quality-assurance-teams"&gt;Quality assurance&lt;/a&gt; teams have implemented a wide range of approaches to ensure security, but manually executing all of these cases can be time-consuming and lead to potential vulnerabilities. For this reason, QA should look into security automation to reduce risks and &lt;a href="http://www.getzephyr.com/products/enterprise-test-management"&gt;improve overall program capabilities&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Have realistic expectations&lt;/strong&gt;&lt;br&gt;
 When building security into the software development life cycle, there are numerous benefits businesses can see, including seamless protection integration and awareness of team members. An AT&amp;amp;T white paper noted that automated vulnerability scanning can be a great first step for QA teams to implement as it can easily and&amp;#160;&lt;a href="http://www.business.att.com/content/whitepaper/Integrated_Security_QC_wp.pdf" target="_blank"&gt;quickly identify commonly occurring issues&lt;/a&gt;. At the same time, however, it's not foolproof, since it cannot detect more sophisticated defects like authentication issues or business logic vulnerabilities.&lt;/p&gt;

&lt;p&gt;That being said, security QA automation can be a major asset to development efforts and can reduce overall risk, but will still require other tools like manual testing to fully evaluate the threat landscape. After the app has been released, automation can often be essential for finding threats, while enabling QA teams to focus on current projects that are still underway. This helps lower the potential risk across the board while still ensuring that each program gets the attention it needs, no matter where it is in its life cycle.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Tools for the job&lt;/strong&gt;&lt;br&gt;
 There are a number of resources that QA teams can utilize to test the security of their projects. TechTarget contributor Michael Cobb noted that automated QA verification is often&amp;#160;&lt;a href="http://searchsecurity.techtarget.com/answer/Which-automated-quality-assurance-tools-can-be-used-to-test-software" target="_blank"&gt;executed through code analysis and vulnerability testing&lt;/a&gt;. Both of these assets can quickly find errors that may be easily missed during manual evaluations. This alone helps significantly reduce risks to app functionality and security capabilities while ensuring that QA teams are eliminating common vulnerabilities. These tools paired with human testers can effectively find issues and better protect their projects for the future.&lt;/p&gt;

&lt;p&gt;"Despite advances in computer automation, humans are still superior at ensuring applications are developed securely, probably because the best challenge is posed by humans, notably those who can think as an attacker would," Cobb wrote. "However, human work is often more effective if a framework guides it."&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relying on QA for better security&lt;/strong&gt;&lt;br&gt;
 Even if QA teams leverage automated tools for security needs, they must still have an understanding of how these tests work and be able to execute them. Chiron Professional Journal noted that while QA professionals may not often be security experts,&amp;#160;&lt;a href="http://www.chiron-solutions.com/chiron-professional-journal/2011/04/06/how-qa-teams-can-find-and-remediate-security-vulnerabilities-earlier-in-the-development-process-reducing-downstream-cost-and-corporate-risk/" target="_blank"&gt;having the tools on hand&lt;/a&gt;&amp;#160;can help them perform the necessary processes and mitigate critical risks.&lt;/p&gt;

&lt;p&gt;"Let's be clear here &amp;#8211; we're not expecting a QA analyst to be able to cobble together a complicated script to evade an anti-cross-site scripting library &amp;#8230;&amp;#160;but we should reasonably expect that the analyst can either effectively use a tool, or follow a well-documented process that has varying tests and permutations allowing the analyst to think for themselves and flag questionable results for review by the security experts," the Chiron Professional Journal stated.&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:41:53 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:reducing-risk-through-security-qa-automation.html</guid></item><item><title>Why We Need Continuous Integration</title><link>http://www.ciandcd.com/why-we-need-continuous-integration.html</link><description>&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;h2&gt;Introduction&lt;/h2&gt;&lt;p&gt;Continuous integration is a practice that helps developers deliver better software in a more reliable and predictable manner.&lt;/p&gt;&lt;p&gt;This article deals with the problems developers face while writing, testing and delivering software to end users. Through exploring continuous integration, we will cover how we can overcome these issues.&lt;/p&gt;&lt;h2&gt;The Problem&lt;/h2&gt;&lt;p&gt;First, we will take a look at the source of the problem, which lies in the software development cycle. Next, we will cover some of the change conflicts that can take place during that process, and finally we will explore the main factors that can make these problems escalate, followed by an explanation of how continuous integration solves these issues.&lt;/p&gt;&lt;h3&gt;The Source of the Problem&lt;/h3&gt;&lt;p&gt;Let's take a look at what a traditional software development cycle looks like. Each developer gets a copy of the code from the central repository. The starting point is usually the latest stable version of the application. All developers begin at the same starting point, and work on adding a new feature or fixing a bug.&lt;/p&gt;&lt;p&gt;Each developer makes progress by working on their own or in a team. They add or change classes, methods and functions, shaping the code to meet their needs, and eventually they complete the task they were assigned to do.&lt;/p&gt;&lt;p&gt;Meanwhile, the other developers and teams continue working on their own tasks, changing the code or adding new code, solving the problems they have been assigned.&lt;/p&gt;&lt;p&gt;If we take a step back and look at the big picture, i.e. the entire project, we can see that all developers working on a project are changing the context for the other developers as they are working on the source code.&lt;/p&gt;&lt;p&gt;As teams finish their tasks, they copy their code to the central repository. There are two scenarios that can take place at this point.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;The code in the central repository is unchanged&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The code is the same as the initial copy. If this is the case, things are simple, because the system is unchanged. All the ideas we had about the system still stand.&lt;/p&gt;&lt;p&gt;This is always the case if you are the only developer working on the application and if you have finished your work before the other members of your team. Either way, things are looking good for you. The system you have created and tested can be delivered to users without additional changes.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;strong&gt;The code in the central repository has changed&lt;/strong&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;The second scenario is that the application you have been working on has changed, and you discover this at the point when you try to copy your code over to the central repository. Changes in the code may or may not be in conflict with the ones you've made.&lt;/p&gt;&lt;p&gt;If there are conflicts, you need to resolve them in order to be able to successfully deliver your code to the users. In this case, things could get complicated.&lt;/p&gt;&lt;p&gt;Next, we'll explore the types of conflicts that can happen and what you may need to do to resolve them.&lt;/p&gt;&lt;h3&gt;Change Conflicts&lt;/h3&gt;&lt;p&gt;There are several types of change conflicts that can occur when integrating code. Here are some of the most common ones. We'll start with the simplest scenarios, and gradually explore the more complex ones.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;The implementation details have changed&lt;/strong&gt;&amp;#160;- You refactored a method, but so did the developer that has already integrated their code into the central repository. The behavior of the method is the same in all three implementations. You will need to pick the version that will stay, and remove the other implementations. You can even come up with a fourth implementation. This is a simple type of conflict, which you can usually resolve within a few minutes.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;The APIs you have been relying on have changed&lt;/strong&gt;&amp;#160;- For instance, the behavior of a certain method has changed. This could affect your code in a number of ways &amp;#8212; from minor changes that you might need to make, to major structural changes. There is no silver bullet in such cases. You will need to carefully study the changes and make all the fixes.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;An entire subsystem of the application behaves in a different way&lt;/strong&gt;&amp;#160;- in such cases you will almost certainly be facing a partial, if not a full rewrite of your solution. If this is the case, you will probably need to speak with all the developers working on the application, because such a significant change should not happen without letting the rest of the team know about it.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;These and a number of other issues could come up, caused by various factors. Different versions of frameworks, libraries, databases are another potential source of conflicts.&lt;/p&gt;&lt;p&gt;Once you have updated your code so it can be compiled or interpreted, you also need to remember to repeat all the tests that you have previously ran.&lt;/p&gt;&lt;p&gt;These examples show that the amount of work needed to solve a problem that was initially assigned to a developer can easily double.&lt;/p&gt;&lt;p&gt;&lt;img src="https://d2l3jyjp24noqc.cloudfront.net/uploads/image/img/21/integration-traditional.png" alt="Traditional Integration" height="417" width="563"&gt;&lt;/p&gt;&lt;h3&gt;Escalating Factors&lt;/h3&gt;&lt;p&gt;Here are some of the main factors that can make these problems escalate.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;The size of the team working on the project.&lt;/strong&gt;&amp;#160;The number of changes that are being pushed back into the main repository is proportional to the number of people on the project. This makes the process of integrating code into the main repository significantly harder.&lt;/p&gt;&lt;/li&gt;&lt;li&gt;&lt;p&gt;&lt;strong&gt;The amount of time passed since the developer got the latest version of the code from the central repository.&lt;/strong&gt;&amp;#160;As time passes, other people working on the same project are integrating more and more of their work, and changing the context in which your code needs to run. Sometimes the changes in the main repository are so big that it's easier to do a complete rewrite of your solution.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;A large number of changes in the system make integration events more complex and can have a huge effect on the productivity of the team. Such situations are even referred to as "integration hell".&lt;/p&gt;&lt;p&gt;This process has a number of other negative consequences for your business. Testing and fixing bugs can take forever. Your releases are running late. Teams are stressed out because of long and unpredictable release cycles, and morale deteriorates.&lt;/p&gt;&lt;h2&gt;Solution: Integrate Continuously&lt;/h2&gt;&lt;p&gt;The solution to the problem of managing a large number of changes in big integration events is conceptually simple. We need to split these big integration events into much smaller integration events. This way, developers need to deal with a much smaller number of changes, which are easier to understand and manage. To keep integration events small and easily manageable, we need them to happen often. A couple of times a day is ideal. The practice of doing small integrations often is called&amp;#160;&lt;strong&gt;Continuous Integration&lt;/strong&gt;.&lt;/p&gt;&lt;p&gt;The idea is simple, but at the same time it often appears to be impossible to implement in practice. This is because changing the process requires us to change some of our own habits, and changing habits is difficult.&lt;/p&gt;&lt;p&gt;&lt;img src="https://d2l3jyjp24noqc.cloudfront.net/uploads/image/img/22/integration-continuous.png" alt="Continuous Integration" height="417" width="563"&gt;&lt;/p&gt;&lt;h3&gt;The Practice of Continuous Integration&lt;/h3&gt;&lt;p&gt;In order to avoid the previously described issues, developers need to integrate their partially complete work back into the main repository on a daily basis, or even a couple of times a day. To accomplish this, they first need to pull in all the changes added to the main repository while they were working on the code. They also must make sure that their code will work once it is integrated into the main repository. The only way to ensure this is to test every feature of the application.&lt;/p&gt;&lt;p&gt;What first comes into mind when we start considering continuous integration is that the developers would need to spend half of their time every day testing the code in order not to break the code in the main repository for everyone else.&lt;/p&gt;&lt;p&gt;This is why the prerequisite for continuous integration is having an automated test suite. Automated tests take away the burden of the manual, repetitive, and error-prone testing process from the developers. They also make the entire testing process much quicker. A computer can replace hours of manual testing with just minutes of automated testing.&amp;#160;&lt;a href="https://semaphoreci.com/community/tutorials/behavior-driven-development"&gt;Behavior-driven&lt;/a&gt;&amp;#160;and test-driven development are techniques that help developers write clean, maintainable code while writing tests at the same time. Testing techniques are out of the scope of this article, and you can read more about them in&amp;#160;&lt;a href="https://semaphoreci.com/community/tutorials"&gt;other articles on Semaphore Community&lt;/a&gt;.&lt;/p&gt;&lt;p&gt;Tests make sense only if they are executed every time the source code changes, without exception. A continuous integration service&amp;#160;such as Semaphore CI is a tool which can automate this process by monitoring the central code repository and running tests on every change in the source code. Apart from running tests, they also collect test results and communicate those results to the entire team working on the project.&lt;/p&gt;&lt;p&gt;The result of continuous integration is so important that many teams have a rule to stop working on their current task if the version in the central repository is broken. They join the team which is working on fixing the code until tests are passing again. The role of a continuous integration service is to improve the communication between developers by communicating the status of a project's source code.&lt;/p&gt;&lt;h2&gt;How to Adopt Continuous Integration&lt;/h2&gt;&lt;p&gt;Continuous integration as a practice makes a big contribution to improving the development process, but also calls for essential changes in the everyday development routine. Adopting it comes with challenges that are easy to overcome if the process is introduced gradually.&lt;/p&gt;&lt;p&gt;One of the biggest challenges teams face is the lack of an automated testing suite. A good recipe for overcoming this situation is to start adding automated tests for all new features as they are being developed. At the same time, the developer working on a bug fix should also work to cover the related code with tests. Whenever a bug is reported, the team should first write a failing test to demonstrate the existence of bug. Once the fix is created, the tests should pass.&lt;/p&gt;&lt;p&gt;Over time, the automated tests suite gradually becomes more comprehensive, and the developers begin relying on it more and more. Adopting a continuous integration service to communicate the status of the tests to the entire team in the early stages of a project is also important, because it raises awareness of the project status among team members.&lt;/p&gt;&lt;h2&gt;Conclusion&lt;/h2&gt;&lt;p&gt;Introducing continuous integration and automated testing into the development process changes the way software is developed from the ground up. It requires effort from all team members, and a cultural shift in the organization.&lt;/p&gt;&lt;p&gt;Big changes in the workflow are not easy to pull off quickly. Changes have to be introduced gradually, and all team members and stakeholders need to be on board with the idea. Educating team members about the practice of continuous integration practice and building the automated tests suite needs to be done systematically. Once the first steps have been taken, the process usually continues on its own, as both developers and stakeholders begin seeing the benefits of automated testing suites and the peace of mind that this practice brings to the entire team.&lt;/p&gt;&lt;p&gt;Article originally posted on&amp;#160;&lt;a href="https://semaphoreci.com/community/tutorials"&gt;the Semaphore Community&lt;/a&gt;.&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:41:51 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:why-we-need-continuous-integration.html</guid></item><item><title>Ensure Software Security by Understanding the Attack Surface</title><link>http://www.ciandcd.com/ensure-software-security-by-understanding-the-attack-surface.html</link><description>&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;p&gt;For many organizations, it seems like cyberattacks can come from anywhere, at any time. This sense is heightened by the number&amp;#160;of endpoints in play that could be vulnerable to threats. Quality assurance teams must ensure that they &lt;a href="https://www.getzephyr.com/insights/top-5-security-threats-software-testers-need-look"&gt;have the data on hand&lt;/a&gt; to keep these risks at bay. By gathering information on current dangers, companies can better understand the attack surface and establish safeguards.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Breaking down elements in play&lt;/strong&gt;&lt;br&gt;
 The attack surface contains all possible vulnerabilities - known and unknown - that may exist across your infrastructure, and sums up your risk of exposure. While the attack surface may seem like one big scary entity, it's actually made up of several parts. Tripwire broke considerations down into&amp;#160;&lt;a href="http://www.tripwire.com/state-of-security/featured/understanding-constitutes-attack-surface-2/" target="_blank"&gt;software, network and human attack surfaces&lt;/a&gt;&amp;#160;to make this large picture easier to manage. QA professionals should approach the attack surface this way in order to ensure that all aspects are accommodated for rather than being overwhelmed by the big picture. Everything from coding to devices and human error must be considered when gathering information and preparing for potential threats.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Analyze data and act on it&lt;/strong&gt;&lt;br&gt;
 Testing results can be a critical indicator of what types of vulnerabilities may be present within a program. The Open Web Application Security Project&amp;#160;noted that an attack surface analysis will help QA and developers&amp;#160;&lt;a href="https://www.owasp.org/index.php/Attack_Surface_Analysis_Cheat_Sheet" target="_blank"&gt;better understand what they're up against&lt;/a&gt;&amp;#160;and build in security accordingly. During this evaluation, they must determine high risk areas of code, what functions should be reviewed for defects and when the attack surface has changed. This last consideration will be especially critical as further tests and adjustments will be needed to secure the software.&lt;/p&gt;

&lt;p&gt;Anything that an organization does could affect the attack surface, which means that it will have to be constantly monitored. QA teams need to ask what's changed, how it's different from before and what potential holes were opened in the process. This will help keep the attack surface visibly mapped out, making it easy to strategize how to protect the business, its employees and customers.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Reduce the noise&lt;/strong&gt;&lt;br&gt;
 While a breach is certainly possible, that doesn't mean it should be easy for attackers to gain entry into business systems. Organizations can reduce their attack surface by decreasing the amount of noise within their infrastructure. Accuvant pointed out that doing this will&amp;#160;&lt;a href="http://www.accuvant.com/blog/how-to-reduce-attack-surface" target="_blank"&gt;reduce an attack's operating surface&lt;/a&gt;, minimizing the likelihood of malicious access. QA teams can use tactics like configuration management, exploit analysis, patching, sandboxing and secure application development to effectively reduce or eliminate the impact of a vulnerability.&lt;/p&gt;

&lt;p&gt;"Integrating these strategies into your security program make it much harder for exploits to attack your organization's systems," Accuvant stated. "By reducing your adversaries' operating surface, you are effectively limiting their attack surface."&lt;/p&gt;

&lt;p&gt;The threat of a vulnerability is a very real concern for businesses. By gathering information on what types of attacks are becoming prevalent and understanding how they can affect company software, QA teams can prepare for these risks and protect their users from the growing attack surface.&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:41:50 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:ensure-software-security-by-understanding-the-attack-surface.html</guid></item><item><title>ClusterHQ and DevOps.com survey show Containers poised for mass adoption</title><link>http://www.ciandcd.com/clusterhq-and-devopscom-survey-show-containers-poised-for-mass-adoption.html</link><description>&lt;div&gt;&lt;p&gt;DevOps.com and ClusterHQ, conducted a survey on Container usage that shows an overwhelming majority of users have either already using, testing or investigating Container usage. With 285 respondents representing a wide range of organizations, it shows that Containers will be part of many production environments in the very near future.&lt;/p&gt;
&lt;p&gt;Currently, only 38 percent of respondents reported using containers in production environments, but that number is projected to increase 69 percent over the next 12 months as organizations find new ways to address important barriers to adoption. It verified that Docker is overwhelmingly the container of choice, with 92% of respondents having used or investigated it, followed by LXC (32%) a distant second, but still far ahead of Rocket (21%). To access the complete survey and report visit&amp;#160;&lt;a href="https://clusterhq.com/assets/pdfs/state-of-container-usage-june-2015.pdf"&gt;https://clusterhq.com/assets/pdfs/state-of-container-usage-june-2015.pdf&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Companies ranging in size from small organizations with 1 to 500 employees (69%), to mid-size companies with 501-2,500 personnel (12%), all the way up to large enterprises with over 2,500 employees (19%) are represented in the survey. This demonstrates that containers are being embraced by all businesses from the startup stage to Fortune 500 companies.&lt;/p&gt;
&lt;p&gt;Respondents came predominantly from Development, Operations and DevOps teams. QA and security teams were a smaller share. The survey revealed how container technologies are being used today, as well as research-based insights while providing clues as to where the industry is trending.&lt;/p&gt;
&lt;p&gt;From the ClusterHQ release on the survey:&lt;/p&gt;
&lt;p&gt;The survey also revealed insights into what is perceived to be the primary barriers to container adoption. Security seems to be emerging as a consistent concern throughout the DevOps community in these times of never ending breaches throughout the world:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Security &amp;#8212; 61%&lt;/li&gt;
&lt;li&gt;Data Management &amp;#8212; 53%&lt;/li&gt;
&lt;li&gt;Networking &amp;#8212; 51%&lt;/li&gt;
&lt;li&gt;Skills and Knowledge &amp;#8212; 48%&lt;/li&gt;
&lt;li&gt;Persistent Storage &amp;#8212; 48%&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Data Management capabilities also emerged as essential to the success of container strategies and that the vast majority of organizations want to run databases as well as additional services in containers.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;When asked to rate how important data management&amp;#160;is to container strategies, 66 percent reported it as a critical or&amp;#160;important gating factor, 29 percent ranked it as moderately important and&amp;#160;only 5 percent reported that it carries no importance.&lt;/li&gt;
&lt;li&gt;Over 70% of respondents said they would like to&amp;#160;run a database or other stateful service in their container&amp;#160;environments.&lt;/li&gt;
&lt;li&gt;Respondents were also asked which specific features of&amp;#160;container data management they considered to be most important, selecting&amp;#160;the &amp;#8220;integration of data management capabilities into existing container&amp;#160;workflows and tools&amp;#8221; as their first choice, with &amp;#8220;seamless&amp;#160;movement of data between dev, test and production environments&amp;#8221; a&amp;#160;close second.&lt;/li&gt;
&lt;li&gt;MySQL (53%), Redis (52%), PostgreSQL (50%), and&amp;#160;Elasticsearch (43%) were reported as the top four most frequently used&amp;#160;stateful services.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Containers have become known for portability and flexibility, the survey reveals that organizations are using them in different infrastructures but most frequently in on-premises data centers (57%), followed by Amazon Web Services (52%).&lt;/p&gt;
&lt;p&gt;So I think it is safe to say Containers are rapidly becoming one of the staples of development and are here to stay to stay in the foreseeable future.&lt;/p&gt;
&lt;p&gt;DevOps.com along with ElasticBox are currently &lt;a href="https://www.surveymonkey.com/r/devopsdot"&gt;conducting another survey &lt;/a&gt;on &amp;#8220;What is DevOps to you?&amp;#8221; One in 50 respondents wins a $50 dollar Amazon gift card and one grand prize winner will win a new 3DR Drone. Take a few minutes to help us with this &lt;a href="https://www.surveymonkey.com/r/devopsdot"&gt;survey&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:40:46 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:clusterhq-and-devopscom-survey-show-containers-poised-for-mass-adoption.html</guid></item><item><title>Internap’s DevOps Culture: PrivateStack + CD = ? [Read On &amp; Draw Your Own Conclusions]</title><link>http://www.ciandcd.com/internaps-devops-culture-privatestack-cd-read-on-draw-your-own-conclusions.html</link><description>&lt;div&gt;&lt;p&gt;Engineers at &lt;a href="http://www.internap.com/" target="_blank"&gt;Internap&lt;/a&gt;, a hosting company and public cloud vendor designed a new OpenStack-based cloud platform and development environment. DevOps.com tells the story of Internap&amp;#8217;s DevOps cultural evolution, which grew virally, interwoven with the company&amp;#8217;s development of PrivateStack.&lt;/p&gt;
&lt;p&gt;Challenges in Internap&amp;#8217;s Common Development Environment&lt;/p&gt;
&lt;p&gt;Challenges that slowed Internap&amp;#8217;s cloud development process triggered a hunger for change and for a new / altered development scheme. &amp;#8220;The Internap public cloud product is essentially composed of micro services that make cloud resources available to our customers,&amp;#8221; says Mathieu Mitchell, Senior Software Developer, Internap. Internap engineering teams tested their cloud services simultaneously with each affecting the other and transferring adverse effects to production or pre-production. Internap needed to let the billing team test their services, integrated with a duplicate of the production software and environment, without affecting the virtualization team.&lt;/p&gt;
&lt;p&gt;These challenges were rooted in a commonly shared development environment for all the engineering teams and team members, which introduced tedium for engineers as they worked to build and deploy stable code.&lt;/p&gt;
&lt;p&gt;When sharing a common development environment, each engineer&amp;#8217;s tests depended on environment consistency and reliability. When engineers / developers tested two changes at the same time, there was no way to tell which change introduced a regression. The most reasonable way to address this prior to PrivateStack was to push all changes into the environment and dedicate specific engineers to identify and fix the issues that occurred.&lt;/p&gt;
&lt;p&gt;The challenge with dedicating specific engineers to troubleshoot these regressions was that they had a diminished context to work with when compared to the original developer&amp;#8217;s understanding. The team that introduced a change needed feedback from the environment and to shoulder responsibility for fixing the issue. This would result in a stable codebase and the ability to write more thorough, automated tests.&lt;/p&gt;
&lt;p&gt;&amp;#8220;This is why we created PrivateStack&amp;#8211;to allow us to independently test a single change, integrated with other services, in an environment that is the equivalent of a private production setup,&amp;#8221; says Mitchell. PrivateStack enables development teams to innovate while ensuring that changes behave correctly in production. This in turn enables CD and speeds development.&lt;/p&gt;
&lt;p&gt;DevOps Culture Leads Teams to Success&lt;/p&gt;
&lt;p&gt;As Internap developed PrivateStack, engineering observed a DevOps belief system spreading across the project and the teams. &amp;#8220;At first, it was only a minority among us, mostly people with an Agile background, who advocated the Continuous Delivery approach while we worked on this project. We really believe in delivering value to our customers as quickly as possible. To achieve Continuous Delivery, we needed people to understand that automating everything was the way to go,&amp;#8221; says Mitchell.&lt;/p&gt;
&lt;p&gt;As more engineers realized that they were the solution, they became increasingly motivated to enhance processes, create the new development environment, and use CD to develop the Internap public cloud product.&lt;/p&gt;
&lt;p&gt;Results with PrivateStack&lt;/p&gt;
&lt;p&gt;PrivateStack enables Internap teams to share the same code and system configurations while individual developers have each their own private production environment to test their software without affecting others. &amp;#8220;We heavily leverage virtualization to be able to recreate our environments, since buying racks and racks of hardware to make this available to all of our developers would be cost prohibitive,&amp;#8221; says Mitchell.&lt;/p&gt;
&lt;p&gt;With PrivateStack, Internap isolates production issues during development, keeping R&amp;amp;D efficient and slashing pre-production troubleshooting time. &amp;#8220;PrivateStack also improves our time-to-market, is much less costly, and most importantly, reduces the chance of any issues reaching the customer,&amp;#8221; says Mitchell.&lt;/p&gt;
&lt;p&gt;DevOps Culture Wrap Up&lt;/p&gt;
&lt;p&gt;To finish the PrivateStack project and foster a DevOps culture at the same time, Internap&amp;#8217;s internal core of CD true believers lead by example. Mitchell and his colleagues adhered to consistent principles while hearing out other team members on their concerns. This helped them to encourage the larger engineering department including billing and virtualization teams to adopt a DevOps and CD frame of mind.&lt;/p&gt;
&lt;p&gt;&amp;#8220;We dedicated two people to drive this initiative, along with a few others who believed in the Continuous Delivery approach but were not involved directly,&amp;#8221; says Mitchell. That was enough to get the job done. Now the vast majority of the engineering department is targeting CD.&lt;/p&gt;
&lt;p&gt;&amp;#8220;We are eager to share our PrivateStack platform with the open source community to enable other developers to run production-like environments for development in their day-to-day operations,&amp;#8221; says Mitchell.&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:40:45 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:internaps-devops-culture-privatestack-cd-read-on-draw-your-own-conclusions.html</guid></item><item><title>Two paths to metal devops: cloud-like API driven &amp; cluster building</title><link>http://www.ciandcd.com/two-paths-to-metal-devops-cloud-like-api-driven-cluster-building.html</link><description>&lt;div&gt;&lt;p&gt;I&amp;#8217;ve been seeing a rising interest in &lt;a href="http://devops.com/2015/03/06/vms-vs-containers-vms-becoming-el-caminos/"&gt;metal DevOps&lt;/a&gt;&amp;#160;fueled by containers and scale-out data center platforms (like Hadoop, Ceph &amp;amp; OpenStack) that run at the metal level. While I see this is a growing general trend (&lt;a href="http://packet.net"&gt;Packet&lt;/a&gt;, &lt;a href="http://www.internap.com/bare-metal/bare-metal-cloud/"&gt;Internap&lt;/a&gt;&lt;a href="https://wiki.openstack.org/wiki/Ironic"&gt;,&lt;/a&gt;&amp;#160;&lt;a href="http://www.rackspace.com/en-us/cloud/servers/onmetal"&gt;RackSpace&lt;/a&gt;,&amp;#160;&lt;a href="https://wiki.openstack.org/wiki/Ironic"&gt;OpenStack Ironic&lt;/a&gt;, &lt;a href="https://maas.ubuntu.com/"&gt;MaaS&lt;/a&gt;), I&amp;#8217;m going to stay firmly within &lt;a href="http://rackn.com"&gt;my wheelhouse&lt;/a&gt; and use &lt;a href="http://github.com/opencrowbar/core"&gt;OpenCrowbar&lt;/a&gt; as my reference here.&lt;/p&gt;
&lt;p&gt;Building on the API-driven metal features of OpenCrowbar, this has translated into two paths for workloads to run on metal:&lt;/p&gt;
&lt;p&gt;1) &lt;b&gt;&amp;#8220;Cloudify&amp;#8221; the metal using APIs&lt;/b&gt; from tools like &lt;a href="https://github.com/chef/chef-provisioning"&gt;Chef Provision&lt;/a&gt;, &lt;a href="http://docs.saltstack.com/en/latest/topics/cloud/install/index.html"&gt;SaltStack Libcloud&lt;/a&gt;, &lt;a href="https://github.com/docker/machine"&gt;Docker Machine&lt;/a&gt;, &lt;a href="https://github.com/cloudfoundry/bosh"&gt;Cloud Foundry BOSH&lt;/a&gt;. These tools have clients that target cloud APIs like OpenStack and Amazon. These same clients work against cloud are easily ported to Crowbar&amp;#8217;s APIs. &amp;#160;Five years ago, conventional wisdom was that we&amp;#8217;d need a universal cloud API; however, practice has shown it&amp;#8217;s not very difficult to wrap APIs in a way that does not reduce every cloud to a least common denominator.&lt;/p&gt;
&lt;p&gt;2) &lt;b&gt;DevOps deploy the workload&lt;/b&gt; using hand-offs to tools like Chef, Saltstack, Puppet or Ansible. This approach leverages the community scripts (Cookbooks, Modules, Playbooks) for the workload with the critical ability to create a tuned environment and inject the needed parameters directly into the scripts. &amp;#160;A critical lesson we learned going from Crowbar v1 to v2 was for our scripts to have crisp attribute input/output boundary to avoid embedding environmental knowledge into the code.&lt;/p&gt;
&lt;p&gt;While I&amp;#8217;m casting this in Crowbar terms, I see this approach to metal as coming into the market by force fuels by a desire for containers-on-metal and devops-on-metal.&lt;/p&gt;
&lt;h2&gt;Let&amp;#8217;s look at some of the unique and shared use-cases for each approach:&lt;/h2&gt;




&lt;p&gt;&lt;strong&gt;Metal API&lt;/strong&gt;&lt;/p&gt;

&lt;strong&gt;Both&lt;/strong&gt;

&lt;p&gt;&lt;strong&gt;Metal Cluster&lt;/strong&gt;&lt;/p&gt;




&lt;ul&gt;
&lt;li&gt;Easy Cloud to Metal Migration&lt;/li&gt;
&lt;li&gt;Minimal Tool Customization&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
&lt;li&gt;Portability of DevOps Scripts&lt;/li&gt;
&lt;li&gt;Take advantage of power cycling&lt;/li&gt;
&lt;li&gt;Enables constant refresh cycles&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
&lt;li&gt;Leverage Hardware features&lt;/li&gt;
&lt;li&gt;Advanced Network topologies&lt;/li&gt;
&lt;/ul&gt;




&lt;p&gt;In either case, you have to handle bespoke (hipster word for custom) steps in the provisioning flow that are unique to the your operational needs. Our experience is that each site (even each server!) is unique in some incremental way. &amp;#160;For example, one site may require teamed networks with VLANs while another requires flat networks with an SDN layer.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;These differences are not mistakes or errors&lt;/strong&gt;: the reality of physical ops and individual operational choices mean that there are a lot of valid configurations. Rather than attempt the Sisyphean task of enforced conformity, we work to abstract differences so that they can be ignored when they are not material.&lt;/p&gt;
&lt;p&gt;In the end, the choices are &lt;b&gt;not &lt;/b&gt;mutually exclusive. Metal APIs are often faster but harder to optimize. You can use them to get started quickly and then invest time to optimize a cluster for long term operations. &lt;b&gt;The underlying physical orchestration can support both.&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Are you looking at getting closer to metal? &amp;#160;Which of the options above makes the most sense to you? &amp;#160;I&amp;#8217;d love to hear about your use-cases, architecture and configuration requirements.&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:40:43 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:two-paths-to-metal-devops-cloud-like-api-driven-cluster-building.html</guid></item><item><title>Better and Fewer Suppliers (2015 Software Supply Chain Report)</title><link>http://www.ciandcd.com/better-and-fewer-suppliers-2015-software-supply-chain-report.html</link><description>&lt;div&gt;&lt;p&gt;&lt;strong&gt;That&amp;#160;Supplier is Better For You&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Since releasing the 2015 State of the Software Supply Chain Report, there has been a lot of great discussion across the industry on best practices for managing the complexity introduced by the volume and velocity of the components used across your software supply chain.&lt;/p&gt;
&lt;p&gt;Today I want to focus on the huge ecosystem of open source projects (&amp;#8220;suppliers&amp;#8221;) that feed a steady stream of innovative components into our software supply chains. &amp;#160;In the Java ecosystem alone, there are now over &lt;a href="https://search.maven.org/#stats"&gt;108,000 suppliers&lt;/a&gt; of open source components. &amp;#160;Across all component types available to developers (e.g., RubyGems, NuGet, npm, Bower, PyPI, etc.), estimates now reach over &lt;a href="https://www.openhub.net"&gt;650,000 suppliers&lt;/a&gt; of open source projects.&lt;/p&gt;
&lt;p&gt;However, like in traditional manufacturing, not all suppliers deliver parts of comparable quality and integrity. My latest research, the &lt;a href="http://www.sonatype.com/get-it-now/new-research"&gt;2015 State of the Software Supply Chain Report&lt;/a&gt;, shows that some open source projects use restrictive licenses and vulnerable sub-components, while other projects are far more diligent at updating the overall quality of their components. Choosing the best and fewest suppliers can improve the quality and integrity of the applications we deliver &amp;#160;to our customers.&lt;/p&gt;
&lt;p&gt;While I am hosting a &lt;a href="http://go.sonatype.com/ssc-webinar"&gt;webinar&lt;/a&gt; next week to share many of the detailed report findings, I wanted to share a few of the more meaningful stats here.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Your 7,600 Suppliers&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;My research for the report revealed many new perspectives on &amp;#8220;suppliers&amp;#8221; across the software supply chains. &amp;#160;First of all, I saw that the average large development organization consumed over 240,000 open source components last year &amp;#8212; sourced from over 7,600 open source projects.&lt;/p&gt;
&lt;p&gt;On the surface, the huge reliance on open source projects is a great thing. &amp;#160;Development teams have chosen to not write those pieces themselves, but have sourced the needed components from outside suppliers. &amp;#160;This practice speeds development, enables more innovation, and ensures time-to-release goals are &amp;#160;achieved. &amp;#160;The use of open source is so prolific today, few of us could ever imagine reducing the use of those &amp;#160;components and their suppliers in the future.&lt;/p&gt;
&lt;p&gt;At the same time that we benefit from open source, our high paced, high volume consumption practices don&amp;#8217;t allow us the time needed to do the due diligence on the suppliers or open source projects where we source our component parts from.&lt;/p&gt;
&lt;p&gt;For example, of the 240,000 average component downloads in 2014, the same businesses sourced an average of 15,000 components that included known security vulnerabilities. &amp;#160;In many cases, developers were downloading vulnerable component versions, when safer versions of those same components were available from the open source projects. While no one intends to download components with known vulnerabilities, the problem is exacerbated due to the lack the visibility into a better recommended version.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Fewer Suppliers, Less Context Switching&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;Choosing an open source project supplier should be considered an important strategic decision in organizations because changing a supplier (&amp;#8220;open source project&amp;#8221;) used &amp;#160;is far more effort than swapping out a specific component. Like traditional suppliers, open source projects have good and bad practices impacting the overall quality of their component parts.&lt;/p&gt;
&lt;p&gt;Traditional manufacturing supply chains intentionally select specific parts from approved suppliers. &amp;#160;They also rely on formalized sourcing and procurement practices. &amp;#160;This practice also focuses the organization on using the best and fewest suppliers &amp;#8212; an effort that improves quality, reduces context switching, and also accelerates mean time to repair when defects are discovered. &amp;#160;One industry example from the report describes how Toyota manages 125 suppliers for their Prius to help sustain competitive advantages over GM who manages over 800 suppliers for the Chevy Volt.&lt;/p&gt;
&lt;p&gt;By contrast, development teams working with software supply chains often rely on an unchecked variety of supply, where each developer or development team can make their own sourcing and procurement decisions. &amp;#160;The effort of managing over 7,600 suppliers introduces a drag on development and is contrary to their need to develop faster as part of agile, continuous delivery and devops practices.&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Coming to Terms&lt;/b&gt;&lt;/p&gt;
&lt;p&gt;When you come to terms with the volume of consumption and the massive ecosystem of suppliers you can source your components from, you quickly realize it is impossible to address this issue with a manual review process. &amp;#160;Any organizations clutching to these outdated manual practices are will continue to be outgunned by the velocity by their software supply chains.&lt;/p&gt;
&lt;p&gt;Just as traditional manufacturing supply chains have turned to automation, software development teams need to take the same approach by further automating their software supply chains. &amp;#160;Information about suppliers and the quality of their projects needs to be made available to developers at the time they are selecting components. &amp;#160;Information about the latest versions, features, licenses, known vulnerabilities, popularity of versions being used, and the cadence of new releases should be &amp;#160;made available to developers in an automated way. &amp;#160;Automating the availability of this information about suppliers can lead to better and fewer suppliers being used.&lt;/p&gt;
&lt;p&gt;Be sure to read the full &lt;a href="http://www.sonatype.com/get-it-now/new-research"&gt;2015 State of the Software Supply Chain Report &lt;/a&gt;for more information about open source suppliers and organizations sourcing practices. &amp;#160;The report also highlights current and best practices being used in organizations that are managing their use of suppliers that feed their software supply chains.&lt;/p&gt;
&lt;p&gt;To hear more about the overall report findings and industry best practices, please join me on Wednesday, June 24th (1pm ET) for our &lt;a href="http://go.sonatype.com/ssc-webinar"&gt;webinar&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&amp;#160;&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sun, 21 Jun 2015 00:40:42 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-21:better-and-fewer-suppliers-2015-software-supply-chain-report.html</guid></item><item><title>Contract and End-to-End tests for Microservices</title><link>http://www.ciandcd.com/contract-and-end-to-end-tests-for-microservices.html</link><description>&lt;div&gt;&lt;h1&gt;Testing Strategies in a Microservice Architecture&lt;/h1&gt;

&lt;p class="abstract"&gt;
        There has been a shift in service based architectures over the last few
        years towards smaller, more focussed "micro" services. There are many
        benefits with this approach such as the ability to independently
        deploy, scale and maintain each component and
        parallelize development across multiple teams. However,
        once these additional network partitions have been introduced, the
        testing strategies that applied for monolithic in process applications
        need to be reconsidered.
      &lt;/p&gt;

&lt;p class="author"&gt;&lt;a href="http://github.com/tobyclemson" rel="author"&gt;Toby Clemson&lt;/a&gt;&lt;/p&gt;

 
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sat, 20 Jun 2015 23:44:54 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-20:contract-and-end-to-end-tests-for-microservices.html</guid></item><item><title>Videos from XConf Manchester</title><link>http://www.ciandcd.com/videos-from-xconf-manchester.html</link><description>&lt;div&gt;&lt;p&gt;I'm an author, speaker&amp;#8230; essentially a loud-mouthed pundit on the topic of software development. I've been working in the software industry since the mid-'80s where I got into the then-new world of object-oriented software. I got a call from ThoughtWorks in 1999 to consult on one of their projects and they rapidly became my favorite client. Within a year I was an employee.&lt;/p&gt;

&lt;p&gt;My main interest is to understand how to design software systems, so as to maximize the productivity of development teams. In doing this I&amp;#8217;ve looked to understand the patterns of good software design, and also the processes that support software design. I learn a lot from listening to the experiences of my fellow ThoughtWorkers: digging for useful ideas and communicating them to the wider world.&lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sat, 20 Jun 2015 23:44:53 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-20:videos-from-xconf-manchester.html</guid></item><item><title>Sony a6000 with 16-70mm lens</title><link>http://www.ciandcd.com/sony-a6000-with-16-70mm-lens.html</link><description>&lt;div&gt;&lt;p&gt;There's been a steady rise of mirrorless cameras in the last
    few years. It seems all the cool kids have been getting into the
    Fuji X series. Although I'm not that concerned with camera gear, I
    have been keeping an eye on this trend myself - although I'm happy
    with most aspects of my current Canon DSLR system, there is one area where
    it comes up short - or rather comes up heavy, since the
    problematic aspect is weight.&lt;/p&gt;

&lt;img src="images/sony-a6000/20140818-DSC00172.jpg"&gt;
&lt;p class="photoCaption"&gt;
      Cadillac Mountain, Acadia National Park, ME
    &lt;/p&gt;
&lt;p&gt;My first serious digital camera was the &lt;a href="http://www.imaging-resource.com/PRODS/A1/A1A.HTM"&gt;Minolta A1&lt;/a&gt;.
    It was primitive by modern standards, small sensor, fixed lens,
    but it weighed in at 650g. That meant that it was easy to carry
    around, usually in a case that I threaded onto my belt. When it
    died and I got my first Canon DLSR, the increased weight meant
    that I now needed a special camera belt to carry it around on.
    Although the Canon was a much better camera, I missed the weight
    of the A1.&lt;/p&gt;

&lt;p&gt;So I've been watching the mirrorless scene, keeping an eye out
    for my neo-A1. The Fujis have got very close, but a vital element
    in the camera body is a tilting LCD display. Although I use the
    viewfinder most of the time, I do find it valuable to use a
    tilting display when I want to shoot at different angles: low
    down, overhead, or to get the right angle on a flower. The
    &lt;a href="http://www.amazon.com/gp/product/B00FPKDRVY?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B00FPKDRVY"&gt;rangefinder format Fujis&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt; didn't have a tilting screen (or lacked a
    viewfinder). The &lt;a href="http://www.amazon.com/gp/product/B00HYAL88W?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B00HYAL88W"&gt;XT-1&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt; does, but it then broke my weight limit.&lt;/p&gt;

&lt;p&gt;Although a lot of photo-blog concentrates on the Fujis, they
    aren't the only mirrorless game in town. The micro 4/3 cameras
    have been around for a while, but I didn't fancy reducing the
    sensor size. About a year ago a friend showed me his &lt;a href="http://www.amazon.com/gp/product/B0096W1OKS?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B0096W1OKS"&gt;Sony NEX-6&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt;. It seemed to hit all the
    right buttons for me, but it didn't have a lens that was right for
    me. I didn't like the power zoom on the kit lens - I wanted a good
    quality normal range zoom, something cose to the &lt;a href="http://www.amazon.com/gp/product/B002NEGTTM?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B002NEGTTM"&gt;15-85mm&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt; that spends most time on &lt;a href="/bliki/Canon60D.html"&gt;my
    Canon 60D&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So the appearance of the &lt;a href="http://www.amazon.com/gp/product/B00ENZRPG0?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B00ENZRPG0"&gt;Sony-Zeiss
    16-70mm&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt; lens, really made the Sony route pretty
    attractive. I waited for the &lt;a href="http://www.amazon.com/gp/product/B00I8BICCG?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B00I8BICCG"&gt;Sony a6000
    camera&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt; (the new version of the NEX-6) to be supported by
    Apple's raw processing and then went and bought the package. I've
    now used the combo for a few months, so can make an informal
    review.&lt;/p&gt;

&lt;img src="images/sony-a6000/sony-a6000.jpg"&gt;
&lt;p class="photoCaption"&gt;
      My &lt;a href="http://www.amazon.com/gp/product/B00I8BICCG?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B00I8BICCG"&gt;Sony a6000&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt; with the &lt;a href="http://www.amazon.com/gp/product/B00ENZRPG0?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B00ENZRPG0"&gt;Sony Zeiss 16-70mm lens&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt;. (While the
      other photos in the article were taken with this camera, I used
      my iPhone and &lt;a href="http://www.amazon.com/gp/product/B00JNRW950?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B00JNRW950"&gt;nova&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt; for this one.)&lt;/p&gt;
&lt;p&gt;Number one question for me, is the ergonomics - how much do I
    enjoy using it? My answer is "a lot". The weight is back down to
    the A1 levels, so I got a case that will thread onto my belt and a
    wrist strap. The whole setup is much more fun and convenient, the
    camera is out of the way until I need it, and I can get to it
    quickly when I do. The lens zooms smoothly and covers my common
    shooting range, and the lens balances really well with the camera.
    The camera controls are very customizable, you can assign most
    functions to any button, and I've found a setup that works well
    for me. The electronic viewfinder is big and bright, indeed my
    Canon's viewfinder now feels small when I use it. I particularly
    like the fact that I can set the electronic viewfinder to show
    subtle zebra stripes on over-exposed highlights, which means I
    don't need to chimp to check highlights any more.&lt;/p&gt;

&lt;p&gt;A lot of people like the Fuji's retro controls: aperture dial
    on the lens, dedicated shutter speed and ISO dials. Although they
    appeal to my nostalgia, I suspect the mode dial approach of the
    Sony works better for me. There are times when I like being able to switch quickly
    from shooting a landscape (aperture priority at f8) to shooting
    people (shutter speed at 1/250), and being able to do that with a
    single click of the mode dial is very handy. (The Sony also has the
    ability to save settings in presets, but I haven't felt the need
    to figure that out yet.)&lt;/p&gt;

&lt;p&gt;So how about image quality? I worry less about a camera's image
    quality then many people seem to. I consider that the quality of
    your camera is, at best, third on the list of factors of getting a
    good photograph. &lt;a href="#footnote-quality"&gt;[1]&lt;/a&gt; And realistically, any
    camera I'm likely to buy is going to be a better camera than I am
    a photographer. Having said that I'm not noticing any reduction in
    quality compared to my Canon setup. In fact I've noticed a couple
    of things looking better. The rendering of clouds in some
    situations is strikingly nice. I also think I'm able to dig out
    more detail in over and under exposed areas of the RAW files,
    giving me more flexibility when tweaking the exposure in
    post-processing.&lt;/p&gt;

&lt;img src="images/sony-a6000/20140915-DSC00362.jpg"&gt;
&lt;p class="photoCaption"&gt;
      Bergh&amp;#252;tte Fluhalp, Zermatt, Switzerland
    &lt;/p&gt;
&lt;p&gt;So all in all, I'm very happy with the purchase. It's now my
    principal serious camera, and will be my usual choice for future trips.&lt;/p&gt;

&lt;p&gt;I'm not yet at the point of getting more lenses for the Sony.
    There are a few that have some appeal. There is a &lt;a href="http://www.amazon.com/gp/product/B00B20OYUO?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B00B20OYUO"&gt;nice prime pancake lens&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt;, which would
    allow me to pop the camera in a jacket pocket. &lt;a href="http://www.amazon.com/gp/product/B0096W1ONK?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B0096W1ONK"&gt;Super-wide&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt; and &lt;a href="http://www.amazon.com/gp/product/B00HNJWSDS?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B00HNJWSDS"&gt;tele
    zooms&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt; would replace those ranges on my Canon. But I'm not sure I'm
    so committed to the Sony system yet that I want to invest in many
    more lenses.&lt;/p&gt;

&lt;p&gt;One lens I would most want is the equivalent to my &lt;a href="http://www.amazon.com/gp/product/B00009USVW?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B00009USVW"&gt;Canon 100mm f2&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt;, which I find really
    handy for my informal portraiture. If I could really stretch out
    my wish list, I'd like some way to link the camera to my &lt;a href="http://www.amazon.com/gp/product/B00JNRW950?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B00JNRW950"&gt;Nova flash unit&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt;. The Nova is such a
    convenient flash to carry around, it would be lovely to use it with
    cameras other than the iPhone.&lt;/p&gt;

&lt;img src="images/sony-a6000/20141030-DSC01260.jpg" width="400"&gt;
&lt;p class="photoCaption"&gt;Xu Hao,
    ThoughtWorks Head of Technology for China&lt;/p&gt;
&lt;p&gt;Most of the time, I prefer using the Sony, but there are cases
    where the Canon still has an edge. I enjoy taking informal
    portraits of my colleagues during meetings, here I find the Sony
    is a little slower to be ready to shoot, only a second or so, but
    that's significant when you want to get someone laughing. I also
    think the Canon would be better for wildlife or sports - although
    I rarely do those. I still have flash gear and more lenses for the
    Canon, and if I'm not concerned about weight I'm happy to keep
    using it.&lt;/p&gt;

&lt;p&gt;But generally I would recommend this combination. If you have
    an SLR and are looking for less weight, then it certainly fit the
    bill for me. If you're looking to get into serious cameras then,
    unless you are expecting to do wildlife or sports, I would
    recommend going mirrorless, and this is a fine option.&lt;/p&gt;



&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/articles/sony-a6000.html&amp;amp;text=Sony%20a6000%20with%2016-70mm%20lens" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/articles/sony-a6000.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/articles/sony-a6000.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;For articles on similar topics&amp;#8230;&lt;/h2&gt;

&lt;p&gt;&amp;#8230;take a look at the following tags:&lt;/p&gt;

 
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sat, 20 Jun 2015 23:44:31 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-20:sony-a6000-with-16-70mm-lens.html</guid></item><item><title>Garmin Oregon 600</title><link>http://www.ciandcd.com/garmin-oregon-600.html</link><description>&lt;div&gt;&lt;p&gt;Not long ago, a GPS unit (I rather like the British word
    "satnav") that you carry around when hiking, was a geeky item for
    gadget freaks. These days, most fairly well-off people carry a
    smart phone with that capability and take it for granted, so much
    so that it's reasonable to wonder if there's any value left in a
    dedicated handheld satnav. I still rather like having one, I use
    it either when hiking, or mounted on my handlebars when cycling. I
    prefer it because it's more rugged in poor weather, and also using
    the GPS won't drain my phone battery. For the last few years I've
    used the &lt;a href="http://www.amazon.com/gp/product/B000CSOXTO?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B000CSOXTO"&gt;Garmin 60CSx&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt;. It's a
    nice unit, but my device has a worsening fault with it freezing on
    startup. &lt;a href="#footnote-60-fault"&gt;[1]&lt;/a&gt;&lt;/p&gt;

&lt;img src="images/garmin-oregon-600/garmin-oregon-600.jpg" width="300"&gt;
&lt;p class="photoCaption"&gt;&lt;/p&gt;
&lt;p&gt;So I fancied getting something new, and I settled on the
    &lt;a href="http://www.amazon.com/gp/product/B00AXUXRUC?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B00AXUXRUC"&gt;Garmin
    Oregon 600&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt;. So far I haven't had a chance to use it on the bike,
    and the winter means I won't get such a chance for several more
    months. But I did take it for a few days hiking in Switzerland,
    and thought I'd share my experiences.&lt;/p&gt;

&lt;p&gt;The Oregon has a touch screen, rather than the dedicated
    buttons of the 60CSx. I was a bit concerned about that, since when I
    bought the 60CSx I also looked at the Oregon of the time and
    concluded that the screen wasn't bright enough. But things have
    improved a lot over the few years since, and the screen looks
    lovely and bright now. The touch screen generally makes it easier
    to operate. I have some concern about what I'll do in the winter
    when I have gloves on, but we don't do much winter hiking, so I
    think we'll not be too worried about it.&lt;/p&gt;

&lt;p&gt;The Oregon comes in several variants. I bought the basic 600
    model. There's also a 650, which adds a built in camera. Since
    I'll have my smart phone with me, and usually a more serious
    camera, I didn't feel that was worthwhile. The other variation in
    the line is the 600t (and 650t) which adds a complete set of topo
    maps for the USA, but these are worthless as I'll elaborate on
    later.&lt;/p&gt;

&lt;p&gt;For our hiking trip I found the Oregon 600 worked out really
    well. The unit fits nicely in the hand, and the display is bright
    and clear. The touch screen makes using the map display much
    easier, with the usual pan and pinch operations that we're used to
    with smartphones. I found the battery life to be excellent, a pair
    of rechargeable batteries easily lasted for a couple of days
    hiking.&lt;/p&gt;

&lt;p&gt;The unit is extremely customizable, with every slot on the UI
    customizable to a particular application or screen within an
    application. This can be a bit of a problem, as you can invest a
    lot of time setting up the unit to way you'd like it to work (and
    it's confusing should you accidentally remove a key application
    from the main screen, can't figure out how to get the unit to find
    locations, and have to resort to googling the web to find you need
    to dig the application out from setup menus). A nice feature is
    that you can save your customizations for different modes of use -
    so I can have one set of customizations for hiking and another for
    biking. This looks very useful, although I haven't tried switching
    modes yet. &lt;/p&gt;

&lt;p&gt;The accuracy of the unit seems pretty good. I tend to treat a
    satnav as unit to use in combination with map and compass, and I
    don't do things like geocaching, so I'm more tolerant of lower
    accuracy than some would be. But it seemed to track our hiking
    well. That said, we tended to be in open country, not in woods, or
    in canyons that are a challenge for a satnav.&lt;/p&gt;

&lt;p&gt;The great plus on modern satnavs is how they work with maps,
    and when it comes to maps and Garmin there is both bad news and
    good news. For biking with the 60CSx I bought a set of Garmin's
    &lt;a href="http://www.amazon.com/gp/product/B003FXWX54?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B003FXWX54"&gt;routable
    street maps for North America&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt;. This allowed me to plot a route out on my
    computer before the ride and then follow it, an approach which
    works really well for me. But the Garmin maps are locked to a
    single physical device, so I can't transfer them from my now
    defunct 60CSx to my new Oregon. This is ludicrous, it actually acts
    as a disincentive for upgrading a Garmin unit, and means there's
    little incentive to stay within the Garmin brand. If you've
    invested money in maps, it's very frustrating to not be able to
    use them with another device, particularly when maps are so easily
    available for free on your smartphone &lt;a href="#footnote-maps-provider"&gt;[2]&lt;/a&gt;. Indeed this, plus my annoyance with the startup
    fault on the 60CSx, were reasons to move away from Garmin when I
    wanted to replace the unit.&lt;/p&gt;

&lt;p&gt;But there's good news on the maps front - a vibrant ecosystem
    of open-source maps (see below for links). I had discovered
    these with the 60CSx, which is why I'd never bothered with buying
    topo maps (and why the "t" units are a waste of money). I'd
    downloaded excellent topo maps for hiking trips in the US and
    Europe. There are also routable street maps based on open street
    maps, although I don't yet know how well they will work for me
    until spring gets us back on our saddles. But at least for the
    swiss trip, I found &lt;a href="http://download.freizeitkarte-osm.de/garmin/latest/CHE_de_gmapsupp.img.zip"&gt;excellent
    open source maps&lt;/a&gt; that I could easily install and use. And I
    appreciate that, unlike the 60CSx, the Oregon allows me to easily
    switch between different map images without faffing around with
    the file system on the micro SD card. Garmin's &lt;a href="http://www.garmin.com/en-US/shop/downloads/basecamp"&gt;BaseCamp
    application&lt;/a&gt;  &amp;#8212; for working with maps, waypoints, and tracks &amp;#8212; has
    steadily improved over the years on the mac. I use it both for
    pre-planning routes and waypoints and also for looking at tracks
    when we're done.&lt;/p&gt;

&lt;p&gt;All in all, I like my upgrade. The Oregon 600 is all in all a
    better experience to work with than the old one. If you're
    considering a dedicated hand-held satnav, then this is a good
    choice (with the proviso that I'm only basing it on a single
    hiking trip so far.)&lt;/p&gt;



&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/articles/garmin-oregon-600.html&amp;amp;text=Garmin%20Oregon%20600%20-%20a%20brief%20review" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/articles/garmin-oregon-600.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/articles/garmin-oregon-600.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;For articles on similar topics&amp;#8230;&lt;/h2&gt;

&lt;p&gt;&amp;#8230;take a look at the tag: &lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sat, 20 Jun 2015 23:44:28 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-20:garmin-oregon-600.html</guid></item><item><title>Replacing Throwing Exceptions with Notification in Validations</title><link>http://www.ciandcd.com/replacing-throwing-exceptions-with-notification-in-validations.html</link><description>&lt;div&gt;&lt;p class="abstract"&gt;&lt;i&gt;
    If you're validating some data, you usually shouldn't be using
    exceptions to signal validation failures. Here I describe how I'd
    refactor such code into using the Notification pattern.
  &lt;/i&gt;&lt;/p&gt;&lt;p&gt;I was recently looking at some code to do some basic
    validation of some incoming JSON messages. It looked something like this. &lt;/p&gt;

&lt;pre&gt;public void check() {
   if (date == null) throw new IllegalArgumentException("date is missing");
   LocalDate parsedDate;
   try {
     parsedDate = LocalDate.parse(date);
   }
   catch (DateTimeParseException e) {
     throw new IllegalArgumentException("Invalid format for date", e);
   }
   if (parsedDate.isBefore(LocalDate.now())) throw new IllegalArgumentException("date cannot be before today");
   if (numberOfSeats == null) throw new IllegalArgumentException("number of seats cannot be null");
   if (numberOfSeats &amp;lt; 1) throw new IllegalArgumentException("number of seats must be positive");
 }&lt;/pre&gt;

&lt;p class="code-remark"&gt;The code for this example is Java&lt;/p&gt;

&lt;p&gt;This is a common way to approach validation. You run a series
    of checks on some data (here just some fields within the class in
    question). If any of these checks fails, you throw an exception
    with an error message.&lt;/p&gt;

&lt;p&gt;I have a couple of problems with this approach. Firstly I'm not
    happy with using exceptions for something like this. Exceptions
    signal something outside the expected bounds of behavior of the
    code in question. But if you're running some checks on outside input,
    this is because you expect some messages to fail - and if a
    failure is expected behavior, then you shouldn't be
    using exceptions.&lt;/p&gt;

&lt;p&gt; if a failure is expected behavior, then you shouldn't
    be using exceptions &lt;/p&gt;
&lt;p&gt;The second problem with code like this is that it fails with
    the first error it detects, but usually it's better to report all
    errors with the incoming data, not just the first. That way a
    client can choose to display all errors for the user to fix in a
    single interaction rather than give her the impression she's
    playing a game of whack-a-mole with the computer.&lt;/p&gt;

&lt;p&gt;My preferred way to deal with reporting validation issues like
    this is the &lt;a href="http://martinfowler.com/eaaDev/Notification.html"&gt;Notification
    pattern&lt;/a&gt;. A notification is an object that collects errors,
     each validation failure adds an error to the notification. A
    validation method returns a notification, which you can then
    interrogate to get more information. A simple usage looks
    has code like this for the checks.&lt;/p&gt;

&lt;pre&gt;private void validateNumberOfSeats(Notification note) {
  if (numberOfSeats &amp;lt; 1) note.addError("number of seats must be positive");
  // more checks like this
}&lt;/pre&gt;

&lt;p&gt;We can then have a simple call such as
    &lt;code&gt;aNotification.hasErrors()&lt;/code&gt; to react if there are any
    errors. Other methods on the notification can drill into more
    details about the errors. &lt;a href="#footnote-boolean"&gt;[1]&lt;/a&gt;&lt;/p&gt;

&lt;img src="replaceThrowWithNotification/sketch.png" width="600"&gt;
&lt;p class="photoCaption"&gt;&lt;/p&gt;

&lt;h2&gt;When to use this refactoring&lt;/h2&gt;

&lt;p&gt;I need to stress here, that I'm not advocating getting rid of
      exceptions throughout your code base. Exceptions are a very
      useful technique for handling exceptional behavior and getting
      it away from the main flow of logic. This refactoring is a good
      one to use only when the outcome signaled by the exception
      isn't really exceptional, and thus should be handled through the
      main logic of the program. The example I'm looking at here,
      validation, is a common case of that.&lt;/p&gt;

&lt;p&gt;A useful rule of thumb when considering exceptions comes
      from the Pragmatic Programmers:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;We believe that exceptions should rarely be used as part of
        a program's normal flow: exceptions should be reserved for
        unexpected events. Assume that an uncaught exception will
        terminate your program and ask yourself, "Will this code still
        run if I remove all the exception handlers?" If the answer is
        "no", then maybe exceptions are being used in nonexceptional
        circumstances.&lt;/p&gt;

&lt;p class="quote-attribution"&gt;&lt;a href="http://www.amazon.com/gp/product/020161622X?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=020161622X"&gt;-- Dave Thomas and Andy Hunt&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;An important consequence of this is that whether to use
      exceptions for a particular task is dependent on the context.
      So, as the prags go on to say, reading from a file that isn't
      there may or may not be an exception depending on the
      circumstances. If you are trying to read a well known file
      location, such as &lt;code&gt;/etc/hosts&lt;/code&gt; on a unix system, then
      it's likely you can assume the file should be there, so
      throwing an exception is reasonable. On the other hand if you
      are trying to read a file from a path that the user has typed in
      on the command-line, then you should expect that it's likely the
      file isn't there, and should use another mechanism - one that
      communicates the unexceptional nature of the error.&lt;/p&gt;

&lt;p&gt;There is a case when it may be sensible to use exceptions
      for validation failures. This would be situations where you have
      data that you expect to have already been validated earlier in
      processing, but you want to run the validation checks again to
      guard against a programming error letting some invalid data slip
      through.&lt;/p&gt;

&lt;p&gt;This article is about replacing exceptions for notification
      in the context of validating raw input. You may also find this
      technique useful in other situations where a notification is a
      better choice than throwing an exception, but I'm focusing on
      the validation case here, as it is a common one.&lt;/p&gt;

&lt;h2&gt;Starting Point&lt;/h2&gt;

&lt;p&gt;I've not mentioned the example domain so far, since I was just
      interested in the broad shape of the code. But as we explore the
      example further, I'll need to engage with the domain. In this
      case it's some code that receives JSON messages booking seats at a
      theater. The code is in a booking request class that's populated
      from the JSON using the gson library.&lt;/p&gt;

&lt;pre&gt;gson.fromJson(jsonString, BookingRequest.class)
&lt;/pre&gt;

&lt;p class="code-remark"&gt;Gson takes a class, looks for any fields that match a key in
      the JSON document, and then populates the matching fields.&lt;/p&gt;

&lt;p&gt;The booking request contains just two elements that we
      are validating here, the date of the performance and how many
      seats are being requested&lt;/p&gt;

&lt;p class="code-label"&gt;class BookingRequest&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  private Integer numberOfSeats; 
  private String date;
&lt;/pre&gt;

&lt;p&gt;The validation checks are the ones I showed above&lt;/p&gt;

&lt;p class="code-label"&gt;class BookingRequest&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  public void check() {
     if (date == null) throw new IllegalArgumentException("date is missing");
     LocalDate parsedDate;
     try {
       parsedDate = LocalDate.parse(date);
     }
     catch (DateTimeParseException e) {
       throw new IllegalArgumentException("Invalid format for date", e);
     }
     if (parsedDate.isBefore(LocalDate.now())) throw new IllegalArgumentException("date cannot be before today");
     if (numberOfSeats == null) throw new IllegalArgumentException("number of seats cannot be null");
     if (numberOfSeats &amp;lt; 1) throw new IllegalArgumentException("number of seats must be positive");
   }&lt;/pre&gt;

&lt;h2&gt;Building a Notification&lt;/h2&gt;

&lt;p&gt;In order to use a notification, you have to create the
      notification object. A notification can be really simple,
      sometimes just a list of strings will do the trick. &lt;/p&gt;

&lt;p&gt;A Notification collects together errors&lt;/p&gt;
&lt;pre&gt;List&amp;lt;String&amp;gt; notification = new ArrayList&amp;lt;&amp;gt;();
if (numberOfSeats &amp;lt; 5) notification.add("number of seats too small");
// do some more checks

// then later&amp;#8230;
if ( ! notification.isEmpty()) // handle the error condition
&lt;/pre&gt;

&lt;p&gt;Although a simple list idiom makes a lightweight implementation of
      the pattern, I usually like to do a bit more than this, creating
      a simple class instead. &lt;/p&gt;

&lt;pre&gt;public class Notification {
  private List&amp;lt;String&amp;gt; errors = new ArrayList&amp;lt;&amp;gt;();

  public void addError(String message) { errors.add(message); }
  public boolean hasErrors() {
    return ! errors.isEmpty();
  }
  &amp;#8230;
&lt;/pre&gt;

&lt;p&gt;By using a real class, I can make my intention clearer - the
      reader doesn't have to perform the mental map between the idiom
      and its full meaning.&lt;/p&gt;

&lt;h2&gt;Splitting the check method&lt;/h2&gt;

&lt;p&gt;My first step is to split the check method into two parts, an
      inner part that will eventually deal only with notifications and
      not throw any exceptions, and an outer part that will preserve
      the current behavior of the check method, which is to throw an
      exception is there are any validation failures.&lt;/p&gt;

&lt;p&gt;My first step to do this is to use &lt;a href="http://refactoring.com/catalog/extractMethod.html"&gt;Extract Method&lt;/a&gt; in an unusual way in that I'm extracting the
      entire body of the check method into a validation method.&lt;/p&gt;

&lt;p class="code-label"&gt;class BookingRequest&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  public void check() {
&lt;p class="highlight"&gt;    validation();&lt;/p&gt;
  }

  public void validation() {
    if (date == null) throw new IllegalArgumentException("date is missing");
    LocalDate parsedDate;
    try {
      parsedDate = LocalDate.parse(date);
    }
    catch (DateTimeParseException e) {
      throw new IllegalArgumentException("Invalid format for date", e);
    }
    if (parsedDate.isBefore(LocalDate.now())) throw new IllegalArgumentException("date cannot be before today");
    if (numberOfSeats == null) throw new IllegalArgumentException("number of seats cannot be null");
    if (numberOfSeats &amp;lt; 1) throw new IllegalArgumentException("number of seats must be positive");
  }&lt;/pre&gt;

&lt;p&gt;I then adjust the validation method to create and return a
      notification.&lt;/p&gt;

&lt;p class="code-label"&gt;class BookingRequest&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  public Notification validation() {
&lt;p class="highlight"&gt;    Notification note = new Notification();&lt;/p&gt;
    if (date == null) throw new IllegalArgumentException("date is missing");
    LocalDate parsedDate;
    try {
      parsedDate = LocalDate.parse(date);
    }
    catch (DateTimeParseException e) {
      throw new IllegalArgumentException("Invalid format for date", e);
    }
    if (parsedDate.isBefore(LocalDate.now())) throw new IllegalArgumentException("date cannot be before today");
    if (numberOfSeats == null) throw new IllegalArgumentException("number of seats cannot be null");
    if (numberOfSeats &amp;lt; 1) throw new IllegalArgumentException("number of seats must be positive");
&lt;p class="highlight"&gt;    return note;&lt;/p&gt;
  }&lt;/pre&gt;

&lt;p&gt;I can now test the notification and throw an exception if it
      contains errors.&lt;/p&gt;

&lt;p class="code-label"&gt;class BookingRequest&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  public void check() {
&lt;p class="highlight"&gt;    if (validation().hasErrors()) 
      throw new IllegalArgumentException(validation().errorMessage());&lt;/p&gt;
  }&lt;/pre&gt;

&lt;p&gt;I made the validation method public, because I'm expecting
      that most callers in the future will prefer to use this method,
      rather than the check method.&lt;/p&gt;

&lt;p&gt;Splitting the original method allows me to separate the
      validation check from the decision about how to respond to failure.&lt;/p&gt;
&lt;p&gt;At this point I haven't changed the behavior of the code at
      all, the notification won't contain any errors and any
      validation checks that fail will continue to throw an exception
      and ignore the new machinery I've put in. But I've now set
      things up ready to start replacing exception throws with
      manipulating the notification.&lt;/p&gt;

&lt;p&gt;Before I go on to that, however, I need to say something
      about error messages. When we're doing a refactoring, the rule
      is to avoid changes in observable behavior. In situations like
      this, such a rule leads immediately to the question of what
      behavior is observable. Obviously the throwing of the correct
      exception is something the outer program will observe - but to
      what extent do they care about the error message? The
      notification will eventually collect multiple errors and could
      summarize them together into a single message with something
      like&lt;/p&gt;

&lt;p class="code-label"&gt;class Notification&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  public String errorMessage() {
    return errors.stream()
      .collect(Collectors.joining(", "));
  }
&lt;/pre&gt;

&lt;p&gt;But that would be a problem if the higher levels of the
      program was relying on only getting the message from the first
      error that's detected, in which case you'd need something like&lt;/p&gt;

&lt;p class="code-label"&gt;class Notification&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  public String errorMessage() { return errors.get(0); }
&lt;/pre&gt;

&lt;p&gt;You have to look not just at the calling function, but also
      any exception handlers to figure out what the right response is
      in this situation.&lt;/p&gt;

&lt;p&gt;Although there's no way I should have introduced any problems
      at this point, I would certainly compile and test before making
      the next changes. Just because there's no chance any sensible
      person could have messed those changes up doesn't mean I can't
      mess it up.&lt;/p&gt;

&lt;h2&gt;Validating the number&lt;/h2&gt;

&lt;p&gt;The obvious thing to do now is to replace the first
      validation&lt;/p&gt;

&lt;p class="code-label"&gt;class BookingRequest&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  public Notification validation() {
    Notification note = new Notification();
    if (date == null) &lt;p class="highlight"&gt;note.addError&lt;/p&gt;("date is missing");
    LocalDate parsedDate;
    try {
      parsedDate = LocalDate.parse(date);
    }
    catch (DateTimeParseException e) {
      throw new IllegalArgumentException("Invalid format for date", e);
    }
    if (parsedDate.isBefore(LocalDate.now())) throw new IllegalArgumentException("date cannot be before today");
    if (numberOfSeats == null) throw new IllegalArgumentException("number of seats cannot be null");
    if (numberOfSeats &amp;lt; 1) throw new IllegalArgumentException("number of seats must be positive");
    return note;
  }
&lt;/pre&gt;

&lt;p&gt;An obvious move, but a bad one, as this will break the code.
      If we pass a null date into the function, it will add an error
      to the notification, but then merrily attempt to parse it and
      throw a null pointer exception - which isn't the exception we
      were looking for.&lt;/p&gt;

&lt;p&gt;So the non-obvious, but more effective thing to do in this
      case is to go backwards. &lt;/p&gt;

&lt;p class="code-label"&gt;class BookingRequest&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  public Notification validation() {
    Notification note = new Notification();
    if (date == null) throw new IllegalArgumentException("date is missing");
    LocalDate parsedDate;
    try {
      parsedDate = LocalDate.parse(date);
    }
    catch (DateTimeParseException e) {
      throw new IllegalArgumentException("Invalid format for date", e);
    }
    if (parsedDate.isBefore(LocalDate.now())) throw new IllegalArgumentException("date cannot be before today");
    if (numberOfSeats == null) throw new IllegalArgumentException("number of seats cannot be null");
    if (numberOfSeats &amp;lt; 1) &lt;p class="highlight"&gt;note.addError&lt;/p&gt;("number of seats must be positive");
    return note;
  }
&lt;/pre&gt;

&lt;p&gt;The previous check is a null check, so we need to use a
      conditional to avoid creating a null pointer exception.&lt;/p&gt;

&lt;p class="code-label"&gt;class BookingRequest&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  public Notification validation() {
    Notification note = new Notification();
    if (date == null) throw new IllegalArgumentException("date is missing");
    LocalDate parsedDate;
    try {
      parsedDate = LocalDate.parse(date);
    }
    catch (DateTimeParseException e) {
      throw new IllegalArgumentException("Invalid format for date", e);
    }
    if (parsedDate.isBefore(LocalDate.now())) throw new IllegalArgumentException("date cannot be before today");
&lt;p class="highlight"&gt;    if (numberOfSeats == null) note.addError("number of seats cannot be null");
    else if (numberOfSeats &amp;lt; 1) note.addError("number of seats must be positive");&lt;/p&gt;
    return note;
  }
&lt;/pre&gt;

&lt;p&gt;I see the next check involves a different field. Together
      with having to introduce a conditional with the previous
      refactoring, I'm now thinking this validation method is getting
      too complex and could do with being decomposed. So I extract the
      number validation parts.&lt;/p&gt;

&lt;p class="code-label"&gt;class BookingRequest&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  public Notification validation() {
    Notification note = new Notification();
    if (date == null) throw new IllegalArgumentException("date is missing");
    LocalDate parsedDate;
    try {
      parsedDate = LocalDate.parse(date);
    }
    catch (DateTimeParseException e) {
      throw new IllegalArgumentException("Invalid format for date", e);
    }
    if (parsedDate.isBefore(LocalDate.now())) throw new IllegalArgumentException("date cannot be before today");
&lt;p class="highlight"&gt;    validateNumberOfSeats(note);&lt;/p&gt;
    return note;
  }

  private void validateNumberOfSeats(Notification note) {
    if (numberOfSeats == null) note.addError("number of seats cannot be null");
    else if (numberOfSeats &amp;lt; 1) note.addError("number of seats must be positive");
  }
&lt;/pre&gt;

&lt;p&gt;Looking at the extracted validation for the number, I don't
      really like its structure. I don't like using if-then-else
      blocks in validation, since it can easily lead to overly nested
      code. I prefer linear code that aborts once it can't go on any
      further, which we can do with a guard clause. So I apply
      &lt;a href="http://refactoring.com/catalog/replaceNestedConditionalWithGuardClauses.html"&gt;Replace Nested Conditional with Guard Clauses&lt;/a&gt;.&lt;/p&gt;

&lt;p class="code-label"&gt;class BookingRequest&amp;#8230;
&lt;/p&gt;

&lt;pre&gt;  private void validateNumberOfSeats(Notification note) {
    if (numberOfSeats == null) {
      note.addError("number of seats cannot be null");
      return;
    }
    if (numberOfSeats &amp;lt; 1) note.addError("number of seats must be positive");
  }
&lt;/pre&gt;

&lt;p&gt;
        when we refactor we
      should always try to take the smallest steps we can that
      preserve behavior
      &lt;/p&gt;
&lt;p&gt;My decision to go backwards in order to keep the code green
      is an example of a crucial element of refactoring. Refactoring
      is a specific technique to restructure code through a series of
      behavior-preserving transformations. So when we refactor we
      should always try to take the smallest steps we can that
      preserve behavior. By doing this we reduce the chances of an
      error that will trap us in the debugger&lt;/p&gt;

&lt;h2&gt;Moving up the stack&lt;/h2&gt;

&lt;p&gt;Once we have the new method, the next task is to look at the
      callers of the original check method and consider adjusting them
      to make use of the new validate method instead. This will entail
      a broader look at how validation fits into the flow of the
      application, so it's outside the scope of this refactoring. But
      the medium-term target should be to eliminate the use of
      exceptions in any circumstance where we might expect validation failures. &lt;/p&gt;

&lt;p&gt;In many cases this should lead to being able to get rid of
      the check method entirely. In which case any tests on that
      method should be reworked to use the validate method. We might
      also want to adjust the tests to probe for proper collection of
      multiple errors using the notification.&lt;/p&gt;


&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/articles/replaceThrowWithNotification.html&amp;amp;text=Replacing%20Throwing%20Exceptions%20with%20Notification%20in%20Validations" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/articles/replaceThrowWithNotification.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/articles/replaceThrowWithNotification.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;For articles on similar topics&amp;#8230;&lt;/h2&gt;

&lt;p&gt;&amp;#8230;take a look at the tag: &lt;/p&gt;
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sat, 20 Jun 2015 23:44:26 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-20:replacing-throwing-exceptions-with-notification-in-validations.html</guid></item><item><title>APIs should not be copyrightable</title><link>http://www.ciandcd.com/apis-should-not-be-copyrightable.html</link><description>&lt;div&gt;&lt;p&gt;Last month, the Electronic Frontier Foundation (EFF)
    &lt;a href="https://www.eff.org/press/releases/computer-scientists-ask-supreme-court-rule-apis-cant-be-copyrighted"&gt;filed
    an amicus brief with the Supreme Court of the United States&lt;/a&gt;, asking the
    justices to review an earlier lower court decision that allows
    APIs (Application Programming Interfaces) to be copyrightable. I'm
    one of the 77 software professionals who signed the brief,
    although rather intimidated by a group that includes Abelson &amp;amp;
    Sussman, Aho &amp;amp; Ullman, Josh Bloch, Fred Brooks, Vint Cerf,
    Peter Deutsch, Mitch Kapor, Alan Kay, Brian Kernighan, Barbara
    Liskov, Guido van Rossum, Bruce Schneier, and Ken Thompson.&lt;/p&gt;

&lt;img src="images/copyright-api/sketch.png" width="400"&gt;
&lt;p class="photoCaption"&gt;&lt;/p&gt;
&lt;p&gt;The original lawsuit was brought by Oracle against Google,
    claiming that Oracle held a copyright on the Java APIs, and that
    Google infringed these APIs when they built Android. My support in
    this brief has nothing to do with the details of the dispute
    between these two tech giants, but everything to do with the
    question of how intellectual property law should apply to
    software, particularly software interfaces.&lt;/p&gt;

&lt;p&gt;I'm not part of the thinking that asserts that nothing in
    software should be intellectual property. While I do think that
    &lt;a href="/bliki/SoftwarePatent.html"&gt;software patents are inherently
    broken&lt;/a&gt;, copyright is a good mechanism to allow software
    authors to have some degree of control over of what happens with their hard work.&lt;/p&gt;

&lt;p&gt;Software has always been a tricky source of income, because
    it's trivial to copy. Copyright provides a legal basis to control at least
    some copying. Without something like this, it
    becomes very hard for someone to work on creating things and still
    be able to pay the mortgage. While we all like free stuff, I think
    it's only fair to give people the chance to earn a living from the
    work they do.&lt;/p&gt;

&lt;p&gt;But any intellectual property mechanism has to balance this
    benefit with the danger that excessive intellectual property
    restrictions can impede further innovation, whether that be
    extending an invention, or reimagining a creative work. As a
    result, patent and copyright regimes have some form of limitation
    built in. One limitation is one of time: patents and copyrights
    expire (although the &lt;a href="http://artlawjournal.com/mickey-mouse-keeps-changing-copyright-law//"&gt;Mickey Mouse
    discontinuity&lt;/a&gt; is threatening that).&lt;/p&gt;

&lt;p&gt;Interfaces are how things plug together. An example from the
    physical world is cameras with interchangeable lenses. Many camera
    makers don't encourage other companies to make lenses for their
    cameras, but such third-party companies can reverse-engineer how
    the interface works and build a lens that will mount on a camera.
    We regularly see this happen with third-party parts providers -
    and these third parties do a great deal to provide lower costs and
    features that the main company doesn't support. I used a Sigma
    lens with my Canon camera because Canon didn't (at the time)
    make an 18-200mm lens. I've bought third party batteries for
    cameras because they're cheaper. Similarly I've repaired my car with third party
    parts again to lower costs or get an audio system that better
    matched my needs.&lt;/p&gt;

&lt;p&gt;Software interfaces are much the same, and the ability to
    extend open interfaces, or reverse-engineer interfaces, has played
    a big role in advancing software systems. Open interfaces were a
    vital part of allowing the growth of the internet, nobody has to
    pay a copyright licence to build a HTTP server, nor to connect to
    one. The growth of Unix-style operating systems relied greatly on
    the fact that although much of the source code for AT&amp;amp;T's Unix
    was copyrighted, the interfaces were not. This allowed offshoots
    such as BSD and Linux to follow Unix's interfaces, which helped
    these open-source systems to get traction by making it easier for
    programs built on top of Unix to interact with new
    implementations.&lt;/p&gt;

&lt;blockquote class="twitter-tweet" lang="en"&gt;A picture is worth a 1000 words, so here's a picture of some books written by signatories of the EFF amicus brief &lt;a href="https://twitter.com/joshbloch/status/531937881703452673"&gt;-- Josh Bloch&lt;/a&gt;&lt;/blockquote&gt;
&lt;p&gt;The story of SMB and Samba is a good example of how
    non-copyrightable APIs spurred competition. When Windows became a
    dominent desktop operating system, its SMB protocol dominated
    simple networks. If non-windows computers wanted to communicate
    effectively with the dominant windows platform, they needed to
    talk to SMB. Microsoft didn't provide any documentation to help
    competitors do this, since an inability to communicate with SMB
    was a barrier to their competitors. However, Andrew Tridgell was
    able to deduce the specification for SMB and build an
    implementation for Unix, called Samba. By using Samba non-windows
    computers could collaborate on a network, thus encouraging the
    competition from Mac and Linux based systems. A similar story
    happened years before with the IBM BIOS, which was
    reverse-engineered by competitors.&lt;/p&gt;

&lt;p&gt;The power of a free-market system comes from competition, the
    notion that if I can find a way to bake bread that's either
    cheaper or tastier than my local bakers, I can start a bakery and
    compete with them. Over time my small bakery can grow and compete
    with the largest bakers. For this to work, it's vital that we
    construct the market so that existing players that dominate the
    market cannot build barriers to prevent new people coming in with
    innovations to reduce cost or improve quality. &lt;/p&gt;

&lt;p&gt;Software interfaces are critical points for this with software.
    By keeping interfaces open, we encourage a competitive
    marketplace of software systems that encourage innovation to
    provide more features and reduce costs. Closing this off will
    lead to incompatible islands of computer systems, unable to
    communicate.&lt;/p&gt;

&lt;p&gt;Such islands of incompatibility present a considerable barrier
    to new competitors, and are bad for that reason alone. But it's
    they are bad for users too. Users value software
    that can work together, and even if the various vendors of
    software aren't interested in communication, we should encourage
    other providers to step in and fill the gaps. Tying systems
    together requires open interfaces, so that integrators can safely
    implement an interface in order to create communication links. We
    value standard connectors in the physical world, and while
    software connections are often too varied for everything to be
    standardized, we shouldn't use copyright law to add further hurdles.&lt;/p&gt;

&lt;p&gt;The need to implement interfaces also goes much deeper than
    this. As programmers we often have to implement interfaces defined
    outside our code base in order to do our jobs. It's common to have
    to modify software that was written with one library in mind to
    work with another - a useful way to do this is to write &lt;a href="http://www.amazon.com/gp/product/0201634988?ie=UTF8&amp;amp;tag=martinfowlerc-20&amp;amp;linkCode=as2&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=0201634988"&gt;adapters&lt;/a&gt;&lt;img src="http://www.assoc-amazon.com/e/ir?t=martinfowlerc-20&amp;amp;l=as2&amp;amp;o=1&amp;amp;a=0321601912" width="1" height="1" border="0" alt=""&gt; that implement the interface of the
    first library by forwarding the second. Implementing interfaces is
    also vital in testing, as it allows you to create &lt;a href="http://martinfowler.com/bliki/TestDouble.html"&gt;Test Doubles&lt;/a&gt;.
    &lt;/p&gt;

&lt;p&gt;So for the sake of our ability to write programs properly, our
    users' desire to have software work together, and for society's
    desire for free markets that spur competition &amp;#8212; copyright should
    not be used for APIs.&lt;/p&gt;



&lt;p class="shares"&gt;Share: &lt;a href="https://twitter.com/intent/tweet?url=http://martinfowler.com/articles/copyright-api.html&amp;amp;text=APIs%20should%20not%20be%20copyrightable" title="Share on Twitter"&gt;&lt;img src="/t_mini-a.png"&gt;&lt;/a&gt;&lt;a href="https://facebook.com/sharer.php?u=http://martinfowler.com/articles/copyright-api.html" title="Share on Facebook"&gt;&lt;img src="/fb-icon-20.png"&gt;&lt;/a&gt;&lt;a href="https://plus.google.com/share?url=http://martinfowler.com/articles/copyright-api.html" title="Share on Google Plus"&gt;&lt;img src="/gplus-16.png"&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h2&gt;For articles on similar topics&amp;#8230;&lt;/h2&gt;

&lt;p&gt;&amp;#8230;take a look at the following tags:&lt;/p&gt;

 
&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sat, 20 Jun 2015 23:44:22 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-20:apis-should-not-be-copyrightable.html</guid></item><item><title>Agile Architecture</title><link>http://www.ciandcd.com/agile-architecture.html</link><description>&lt;div&gt;&lt;p&gt;











Rethink featured three talks, examining the last 13 years and what's evolved since the Agile Manifesto was published.
[MUSIC PLAYING] The word "agile" is actually an adjective. [LAUGHING] You can't sell adjectives. You can only sell nouns. So the first thing that we had to do was totally corrupt the English language and to put "agile" from being an adjective into a noun. When you think about Agile, there is usually some sort of plan. The benefit is that it can change. I value building cathedrals more than I value cutting stone, and I think a lot of this is driven is around the notion that we, as software professionals, have taken a lot of craft back, but we still haven't gotten to the point where we're building cathedrals. With our Agile approach, what you're doing instead is you're trying to break down by pieces of functionality. Let's build a little piece of the system over the course of these days on the most valuable product in the thinnest slice that we can. But we're basically breaking it down by complete slices. I'm calling it food that way. Now, an interesting tie-in down here is, well, how does this lie with the notion of this kind of architecture thing? You cannot do good architecture without programming, and you cannot do good programming without thinking about architecture. [MUSIC PLAYING]
&lt;/p&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sat, 20 Jun 2015 23:44:21 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-20:agile-architecture.html</guid></item><item><title>3 Reasons Why Testing Software Security Should Start Early</title><link>http://www.ciandcd.com/3-reasons-why-testing-software-security-should-start-early.html</link><description>&lt;div&gt;&lt;p class="print-link"&gt;&lt;/p&gt;&lt;p&gt;The software development life cycle&amp;#160;is an extremely intensive process for developers and quality assurance professionals alike. If even one element is neglected, it can delay project schedules and affect user performance. Security is one &lt;a href="https://www.getzephyr.com/products/enterprise-test-management/zephyr-enterprise-edition"&gt;aspect that must be built in&lt;/a&gt; from the inception of any app, and here are a few reasons&amp;#160;why:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Breaches can cost&amp;#160;your business&lt;/strong&gt;&lt;br&gt;
 Let's say that an organization uses its application&amp;#160;to&amp;#160;order and manage inventory, payroll and other operational&amp;#160;needs. If a malicious entity were&amp;#160;to&amp;#160;access this information, it could easily make fraudulent transactions, costing the company more than what was intended. Not&amp;#160;to&amp;#160;mention it will create a massive headache&amp;#160;to&amp;#160;set the record straight. TechTarget contributor Peter Gregory noted that this can happen when&amp;#160;&lt;a href="http://searchsecurity.techtarget.com/tip/Security-in-the-software-development-life-cycle" target="_blank"&gt;programs lack audit trails&lt;/a&gt;&amp;#160;and processes required for secure purchasing. By building in this functionality early on, this type of situation can be avoided, allowing organizations&amp;#160;to&amp;#160;retain customer trust and money.&lt;/p&gt;

&lt;p&gt;"Organizations that fail&amp;#160;to&amp;#160;involve information security in the life cycle will pay the price in the form of costly and disruptive events," Gregory wrote. "Many bad things can happen&amp;#160;to&amp;#160;information systems that lack the required security interfaces and characteristics."&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Access&amp;#160;to&amp;#160;confidential data can be damaging&lt;/strong&gt;&lt;br&gt;
 If a business aims&amp;#160;to&amp;#160;use an app for information sharing and availability, protection must be at the forefront of this project throughout its life cycle. While some data may not be as costly&amp;#160;to&amp;#160;leak, the&amp;#160;loss&amp;#160;of confidential reports and documents can severely affect the organization's ability&amp;#160;tofunction.&lt;/p&gt;

&lt;p&gt;QA teams must ensure that security practices are implemented and built upon constantly. TechTarget contributor Nick Lewis noted that&amp;#160;&lt;a href="http://searchsecurity.techtarget.com/tip/How-to-negate-business-logic-attack-risk-Improve-security-in-the-SDLC" target="_blank"&gt;firewalls and traditional methods will not be enough&lt;/a&gt;&amp;#160;to&amp;#160;keep targeted attacks at bay. Instead, testing the app for insufficient process validation, abuse of functionality, weak password recovery validation and information leakage will be critical&amp;#160;toguarding the program.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Analyze&amp;#160;initial risk before jumping in&lt;/strong&gt;&lt;br&gt;
 One SDLC security practice&amp;#160;to&amp;#160;observe is a primary risk assessment before the start of a new project. Not all applications are equal, which means each program will be labeled with a different risk level. Some software will be publicly accessible, whereas others will be more business-critical and involve processing sensitive data. These uses will largely determine how much risk would be involved with a breach on such activities. This information will give QA teams a clear picture of the security roadmap&amp;#160;needed, and can be implemented.&amp;#160;&lt;/p&gt;

&lt;p&gt;"Doing the preliminary risk assessment&amp;#160;to&amp;#160;establish the&amp;#160;need&amp;#160;for the system helps&amp;#160;&lt;a href="http://software-security.sans.org/resources/paper/cissp/defining-understanding-security-software-development-life-cycle" target="_blank"&gt;identify any security show stoppers&lt;/a&gt;&amp;#160;before&amp;#160;too much time and effort goes into the next SDLC phases," a SANS white paper stated. "It also gets the design team thinking about security issues early in the design process."&lt;/p&gt;

&lt;p&gt;Cyberattacks and malware in the headlines have made security more prominent than ever before. By building in protections early in the SDLC, QA teams can ensure that they will be better able&amp;#160;tohandle these threats without interruptions&amp;#160;to&amp;#160;regular business activities.&lt;/p&gt;	
	&lt;p&gt;&lt;/p&gt;

    &lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">itech001</dc:creator><pubDate>Sat, 20 Jun 2015 23:37:08 +0800</pubDate><guid>tag:www.ciandcd.com,2015-06-20:3-reasons-why-testing-software-security-should-start-early.html</guid></item></channel></rss>