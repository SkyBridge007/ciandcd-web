{"pages":[{"url":"http://www.ciandcd.com/pages/guan-yu.html","title":"关于","text":"软件持续集成和持续发布 contact QQ群：172758282，437085002 thanks https://www.python.org/ https://pythonhosted.org/feedparser/index.html https://github.com/codelucas/newspaper https://github.com/getpelican/pelican http://twitter.github.com/bootstrap","tags":"pages"},{"url":"http://www.ciandcd.com/pages/awesome.html","title":"Awesome","text":"ciandcd Continuous Integration and Continuous Delivery Written by ciandcd.com A curated list of awesome tools for: continuous integration continuous delivery software integration devops ciandcd Theory Build And Release System Online Build System Infrastructure dev env Source Code Management Code Review Build Static Check Dynamic Check Performance Analysis Coverage Testing Package Deploy Delivery Provisioning Tools Web Server Applications And Container OS And Shell Database Version Control Useful Sites Conference And Submit Other Awesome Lists Contact Theory The theory for continuous integration and continuous deliver continuousIntegration continuousdelivery software integration devopsdays * ci cheatsheet Build And Release System The system for software build and release, continuous integration and continuous delivery Jenkins An extendable open source continuous integration server BuildForge Automate and accelerate build and release processes ElectricCommander ElectricCommander gives distributed teams shared control and visibility into infrastructure, tool chains and processes. It accelerates and automates the software delivery process to enable agility, predictability and security across many build-test-deploy pipelines Teamcity Ready to work, extensible and developer-friendly build server out of the box bamboo Bamboo does more than just run builds and tests. It connects issues, commits, test results, and deploys so the whole picture is available to your entire product team go Automate and streamline the build-test-release cycle for worry-free, continuous delivery of your product hudson the previous one of Jenkins openbuildservice The Open Build Service (OBS) is a generic system to build and distribute binary packages from sources in an automatic, consistent and reproducible way. You can release packages as well as updates, add-ons, appliances and entire distributions for a wide range of operating systems and hardware architectures buildbot Buildbot is a continuous integration system designed to automate the build/test cycle. By automatically rebuilding and testing the tree each time something has changed, build problems are pinpointed quickly, before other developers are inconvenienced by the failure Parabuild Parabuild is an enterprise software build and release management system that helps software teams to release on time by providing them practically unbreakable release builds and Continuous Integration FinalBuilder Automating your Build process is simple with FinalBuilder. With FinalBuilder you don't need to edit xml, or write scripts. Visually define and debug your build scripts, then schedule them with windows scheduler, or integrate them with Continua CI, Jenkins or any other CI Server VisualBuild Visual Build enables developers and build masters to easily create an automated, repeatable build process cruisecontrol CruiseControl.NET, an Automated Continuous Integration server, implemented using the .NET Framework continuum Apache Continuum™ is an enterprise-ready continuous integration server with features such as automated builds, release management, role-based security, and integration with popular build tools and source control management systems quickbuild GitHub integration. Perforce shelve support. Coverity report rendering. Subversion external change retrieval. Resource access info. Display reasons for waiting steps. Custom build and request columns. Favorite dash board list. Inheritable environment variables.And much more... rexify perl Deployment & Configuration Management Online Build System Online build release system travis-ci ci server for github and bitbuckets cloudbees the Enterprise Jenkins Company elasticbox A DevOps approach that focuses on reusable application components as a service, and enables operations to provide IT as a Service coveralls Track your project's code coverage over time, changes to files, and badge your GitHub repo shippable Hosted continuous integration and deployment service built on docker circleci Continuous Integration for web apps. buildbox Simple self-hosted Continuous Integration drone Open source continuous integration platform built on Docker appveyor Continuous Integration and Deployment service for busy Windows snap-ci Easy builds, deployed when you want codeship Continuous Integration and Delivery made simple solanolabs Hosted continuous integration and deployment githost Painless GitLab CE & CI Hosting testling Automatic browser tests on every push magnum-ci Hosted Continuous Integration and Delivery Platform for private repositories wercker Test and deploy your applications with ease coveralls Track your project's code coverage over time, changes to files, and badge your GitHub repo ship.io Simple, powerful CI for iOS and Android. Re-build, Re-test, Re-deploy. GitLab CI - Based off of ruby. They also provide GitLab, which manages git repositories. IBM DevOps Services - Develop, track, plan, and deploy software onto the IBM Bluemix cloud platform. Infrastructure The hardware,virtual machines, fram management, docker GridWiki wiki page for Grid UGE Univa workload management solutions maximize the value of existing computing resources by efficiently sharing workloads across thousands of servers SGE Grid Engine is typically used on a computer farm or high-performance computing (HPC) cluster and is responsible for accepting, scheduling, dispatching, and managing the remote and distributed execution of large numbers of standalone, parallel or interactive user jobs. It also manages and schedules the allocation of distributed resources such as processors, memory, disk space, and software licenses LSF Platform Load Sharing Facility (or simply LSF) is a workload management platform, job scheduler, for distributed HPC environments. It can be used to execute batch jobs on networked Unix and Windows systems on many different architectures vmwarevshpere VMware vSphere (formerly VMware Infrastructure 4) is VMware's cloud computing virtualization operating system ctrixserver XenServer is the best server virtualization platform for public and private clouds, powering 4 of the 5 largest hosting provider clouds. Built with scale, security and multi-tenancy in mind, XenServer allows for even greater flexibility and cost efficiency miscrosofthyperv microsoft virtualization amazon Scalable, pay-as-you-go compute capacity in the cloud Dev env boxstarter Repeatable, reboot resilient windows environment installations made easy using Chocolatey packages. vagrantup Create and configure lightweight, reproducible, and portable development environments. veewee Easing the building of vagrant boxes Source Code Management Version control and source code management tools git Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency perforce Industry's most reliable and proven platform for versioning code, images, documents... everything clearcase IBM Rational ClearCase is a software configuration management solution that provides version control, workspace management, parallel development support, and build auditing mercurial Mercurial is a free, distributed source control management tool. It efficiently handles projects of any size and offers an easy and intuitive interface svn Subversion is an open source version control system gitlab Open source self-hosted Git management software github Powerful collaboration, review, and code management for open source and private development projects. bitbuckets Plant your code in the cloud. Watch it grow. teamfoundationservice Visual Studio Online, based on the capabilities of Team Foundation Server with additional cloud services, is the online home for your development projects. Get up and running in minutes on our cloud infrastructure without having to install or configure a single server. Visual Studio Online connects to Visual Studio, Eclipse, Xcode, and other Git clients to support development for a variety of platforms and languages phabricator Phabricator is a collection of open source web applications that help software companies build better software. * IBM DevOps Services - Store, manage, edit, and collaborate on your source code. Then deploy onto the IBM Bluemix cloud platform. Code Review Code review tools codecollaborator Collaborator helps development, testing and management teams work together to produce high quality code crucible Code reviews = quality code. Review code, discuss changes, share knowledge, and identify defects with Crucible's flexible review workflow. It's code review made easy for Subversion, CVS, Perforce, and more reviewboard Review Board takes the pain out of code review, saving you time, money, and sanity so you can focus on making great software codestriker Codestriker is an open-sourced web application which supports online code reviewing. Traditional document reviews are supported, as well as reviewing diffs generated by an SCM (Source Code Management) system and plain unidiff patches getbarkeep a fast, fun way to review code gerrit Gerrit is a web based code review system, facilitating online code reviews for projects using the Git version control system * Codebrag Codebrag is a simple code review tool that makes the process work for your team. Build Build tools gnumake GNU Make is a tool which controls the generation of executables and other non-source files of a program from the program's source files gnuautoconf Autoconf is an extensible package of M4 macros that produce shell scripts to automatically configure software source code packages mozillabuildtools The Mozilla build system, like the rest of the Mozilla codebase, is cross-platform. It uses traditional Unix-style autoconf and make tools to build the various applications (even on non-unix operating systems) scons SCons is an Open Source software construction tool—that is, a next-generation build tool. Think of SCons as an improved, cross-platform substitute for the classic Make utility with integrated functionality similar to autoconf/automake and compiler caches such as ccache. In short, SCons is an easier, more reliable and faster way to build software cmake cmake offers robust, cross-platform software development solutions. Find out how we can help your team efficiently manage the build, test, and package process for your software project msbuild The Microsoft Build Engine is a platform for building applications. This engine, which is also known as MSBuild, provides an XML schema for a project file that controls how the build platform processes and builds software. Visual Studio uses MSBuild, but it doesn't depend on Visual Studio. By invoking msbuild.exe on your project or solution file, you can orchestrate and build products in environments where Visual Studio isn't installed ant Ant can be used to pilot any type of process which can be described in terms of targets and tasks. The main known usage of Ant is the build of Java applications. maven Apache Maven is a software project management and comprehension tool. Based on the concept of a project object model (POM), Maven can manage a project's build, reporting and documentation from a central piece of information. gradle Gradle is build automation evolved. Gradle can automate the building, testing, publishing, deployment and more of software packages or other types of projects such as generated static websites, generated documentation or indeed anything else. ElectricacCelerator Continuous Delivery isn't continuous if builds and tests take too long to complete. ElectricAccelerator speeds up builds and tests by up to 20X, improving software time to market, infrastructure utilization and developer productivity Static Check Software static check tools static tools wiki wiki page coverity Continually measure and improve code quality and security across your development organization fxcop FxCop is an application that analyzes managed code assemblies (code that targets the .NET Framework common language runtime) and reports information about the assemblies, such as possible design, localization, performance, and security improvements cpd Duplicate code can be hard to find, especially in a large project. But PMD's Copy/Paste Detector (CPD) can find it for you sonar SonarQube is an open platform to manage code quality findbugs Find Bugs in Java Programs * checkstyle Checkstyle is a development tool to help programmers write Java code that adheres to a coding standard Dynamic Check Software dynamic check tools * dynamic tools wiki wiki page Performance Analysis Software performance analysis tools * performance tools wiki wiki page Coverage Software testing coverage tools * code coverage wiki wiki page Testing Software testing framework and tools Testingautomation test automation wiki page softwareqatest FAQ page qaforums SQA Forums opensourcetesting open source software testing tools, news and disccussions * selenium Selenium automates browsers Package The tools for software package and installation installshield World's #1 Software Installation Solution-Build Reliable MSI Installers for Windows Applications NSIS NSIS (Nullsoft Scriptable Install System) is a professional open source system to create Windows installers. It is designed to be as small and flexible as possible and is therefore very suitable for internet distribution rpm The RPM Package Manager (RPM) is a powerful command line driven package management system capable of installing, uninstalling, verifying, querying, and updating computer software packages yum Yum is an automatic updater and package installer/remover for rpm systems fpm Effing package management! Build packages for multiple platforms (deb, rpm, etc) with great ease and sanity. wix The most powerful set of tools available to create your Windows installation experience. * packer Packer is a tool for creating identical machine images for multiple platforms from a single source configuration. Deploy The tools for web site deploy jfrog s the first Binary Repository Management solution, Artifactory has changed the way binaries are controlled, stored and managed throughout the software release cycle xl-deploy Agentless, Model-based App Deployment Jenkinsdeployplugin deploy to tomcat bintray The fastest and most reliable way to automate the distribution of your software releases Delivery The tools for software delivery sl-release Orchestrate your Continuous Delivery pipelines. Simple. Flexible. End-to-End archiva Apache Archiva™ is an extensible repository management software that helps taking care of your own personal or enterprise-wide build artifact repository. It is the perfect companion for build tools such as Maven, Continuum, and ANT nexus The use of repository managers (also known as component managers) is helping software development teams achieve simple gains in speed, efficiency, and quality of their operations chocolatey Chocolatey NuGet is a Machine Package Manager, somewhat like apt-get, but built with Windows in mind pulp Pulp is a platform for managing repositories of content, such as software packages, and pushing that content out to large numbers of consumers. herd A single-command bittorrent distribution system, based on Twitter's Murder * murder Large scale server deploys using BitTorrent and the BitTornado library from twitter.com Provisioning Tools Provision tools Puppet Build, destroy and rebuild servers on any public or private cloud Chef Fast, scalable and flexible software for data center automation SaltStack Radically simple configuration-management, application deployment, task-execution, and multi-node orchestration engine ansible Web Server Common used web server apache Apache httpd has been the most popular web server on the Internet since April 1996 nginx A high performance free open source web server powering busiest sites on the Internet tomcat An open source software implementation of the Java Servlet and JavaServer Pages technologies jetty Jetty provides a Web server and javax.servlet container, plus support for SPDY, WebSocket, OSGi, JMX, JNDI, JAAS and many other integrations * HAProxy - Software based load Balancing, SSL offloading and performance optimization, compression, and general web routing. OS And Shell Linux shell, perl, python awesome-shell awesome-python awesome-perl awesome-sysadmin Applications And Container VM application and container docker Docker - An open platform for distributed applications for developers and sysadmins suseapplication tools to create suse applications Database Version Control Database version control system liquibase source control for your database flywaydb Database Migrations Made Easy nextep NeXtep Softwares provides software solutions for the industrialization of your database developments and deployments. Our goal is to increase the productivity of your development teams by taking control of your developments' lifecycle and by automating your deployment and test processes dbdeploy dbdeploy is a Database Change Management tool. It's for developers or DBAs who want to evolve their database design – or refactor their database – in a simple, controlled, flexible and frequent manner * dbmaestro Controlled Database Continuous Delivery is Our Business Useful Sites Other useful pages infoq stackoverflow Conference And Submit Conference and submit * devops submit Other Awesome Lists Other amazingly awesome lists can be found in awesome awesome-awesome awesome-awesomeness sysadmin Contact To add new items about continuous integration and continuous delivery: 1) pull; 2) add issue; 3) send me by email itech001@126.com; 4) qq group 172758282;","tags":"pages"},{"url":"http://www.ciandcd.com/sshfan-xiang-lian-jie-ji-autossh.html","title":"SSH反向连接及Autossh","text":"From: http://www.cnblogs.com/itech/p/4572275.html 转自： http://www.cnblogs.com/eshizhan/archive/2012/07/16/2592902.html 0. 接触Linux恐怕对SSH再熟悉不过了，还有scp，sftp各种方便的功能，一般的使用都需要ip:port（如果不是默认22的话），但有些情况比较特殊，就是想连接一台内网主机（比如公司内网，当然你肯定做不了Port Forwarding，除非你想在公司防火墙上拆个洞）。稍懂一点网络的童鞋会明白，Internet上去主动连接一台内网是不可能的，一般的解决方案分两种，一种是端口映射（Port Forwarding），将内网主机的某个端口Open出防火墙，相当于两个外网主机通信；另一种是内网主机主动连接到外网主机，又被称作反向连接（Reverse Connection），这样NAT路由/防火墙就会在内网主机和外网主机之间建立映射，自然可以相互通信了。但是，这种映射是NAT路由自动维持的，不会持续下去，如果连接断开或者网络不稳定都会导致通信失败，这时内网主机需要再次主动连接到外网主机，建立连接。 1.理论的介绍完了，下面实际操作： A要控制B A主机：外网，ip：123.123.123.123，sshd端口：2221 B主机：内网，sshd端口：2223 无论是外网主机A，还是内网主机B都需要跑ssh daemon 1.1.首先在B上执行 $ ssh -NfR 1234:localhost:2223 user1@123.123.123.123 -p2221 这句话的意思是将A主机的1234端口和B主机的2223端口绑定，相当于远程端口映射（Remote Port Forwarding）。 这里每次需要输入A主机user1的登陆密码，后面会讲到解决办法。 1.2.这时在A主机上sshd会listen本地1234端口 $ ss -ant State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 127.0.0.1:1234 *:* 1.3.像平时一样连接到A主机的1234端口就可以控制内网B主机了 $ ssh localhost -p1234 2.一开始提到，这种反向连接（Reverse Connection）不稳定，可能随时断开，需要内网主机B再次向外网A发起连接，这时需要个\"朋友\"帮你在内网B主机执行这条命令。它就是Autossh。 在此之前还要解决之前的一个问题，那就是每次内网主机B连接外网主机A时都需要输入密码，这个问题ssh本身是提供另外一种验证方式——通过密钥验证用户身份，实现自动登录。 2.1.在内网B主机上生产公钥和私钥 $ ssh-keygen ...(一直按Enter，最后在~/.ssh/下生成密钥) $ ls ~/.ssh/ id_rsa id_rsa.pub known_hosts 2.2.复制B主机上生成的id_rsa.pub公钥到外网A主机上，并将内容加入到~/.ssh/authorized_keys中 $ cat id_rsa.pub >> ~/.ssh/authorized_keys 试下，内网B主机连接外网A主机，就不再输入密码验证了 补充：今天了解到ssh-copy-id这个命令，上面这个操作就变的简单了 $ ssh-copy-id user1@123.123.123.123 2.3.再来看看Autossh的用法 $ autossh -M 5678 -NR 1234:localhost:2223 user1@123.123.123.123 -p2221 比之前的命令添加的一个-M 5678参数，负责通过5678端口监视连接状态，连接有问题时就会自动重连，去掉了一个-f参数，因为autossh本身就会在background运行。 3.终极方案：当重启内网B主机，谁来自动Autossh呢，加入daemon吧 以daemon方式执行，相当于root去执行autossh, ssh，这时刚才普通用户目录下的.ssh/authorized_keys文件会不起效。有两种办法解决，一种是用autossh的参数指定.ssh路径；另外一种是以普通用户身份执行daemon，下面是第二种方式。 /bin/su -c '/usr/bin/autossh -M 5678 -NR 1234:localhost:2223 user1@123.123.123.123 -p2221' - user1 autossh还有很多参数，用来设置重连间隔等等。 将上面命令放入下面各启动方式中，根据自己系统自己配置： SysV：/etc/inid.d/autossh Upstart: /etc/init/autossh.conf systemd: /usr/lib/systemd/system/autossh.service P.S. 1.家里是ADSL的话，用DDNS，解决ip问题 2.外网有路由的可设下端口映射 3.虽然有密钥和密码保护，但还请小心使用","tags":"中文"},{"url":"http://www.ciandcd.com/managing-change-in-sap-reducing-cost-and-risk-with-ibm-devops.html","title":"Managing change in SAP: reducing cost and risk with IBM DevOps","text":"From: http://devops.linuxjournal.com/collaborative-development/managing-change-sap-reducing-cost-and-risk-ibm-devops IBM DevOps provides a flexible, collaborative approach to SAP delivery that supports continuous innovation and improvement across the SAP landscape. It provides time-to-value from change request to delivery is accelerated, helping achieve your business and IT objectives more quickly. The visibility is enhanced and change management is unified across both SAP and non-SAP landscapes. The solution Co-development by IBM and SAP ensures tight integration and adoption of joint best practices.","tags":"devops"},{"url":"http://www.ciandcd.com/customer-feedback-drives-continuous-delivery-planning-for-devops.html","title":"Customer Feedback drives Continuous Delivery - Planning for DevOps","text":"From: http://devops.linuxjournal.com/urban-code/customer-feedback-drives-continuous-delivery-planning-devops Businesses need an approach that goes beyond Application Lifecycle Management (ALM) in order to achieve sustained innovation. The basis for that approach is DevOps, a contraction of \"development and operations,\" the two teams that form the core of an organization's technology department","tags":"devops"},{"url":"http://www.ciandcd.com/test-early-test-often-continuous-testing-as-part-of-the-devops-lifecycle.html","title":"Test early, test often. Continuous testing as part of the DevOps lifecycle","text":"From: http://devops.linuxjournal.com/develop-deploy/test-early-test-often-continuous-testing-part-devops-lifecycle Continuous testing is an effective quality management method that can help testing teams keep up with agile development. This paper explains the methods and approaches that can enable continuous testing as part of the DevOps lifecycle.","tags":"devops"},{"url":"http://www.ciandcd.com/devops-for-enterprise-systems-a-quick-intro.html","title":"DevOps for Enterprise Systems - a quick intro","text":"From: http://devops.linuxjournal.com/collaborative-development/devops-enterprise-systems-quick-intro Established IT organizations are challenged by long software delivery cycles and the complexity surrounding connecting Systems of Record to new Systems of Engagement. Tools, processes and culture typically contribute to prolonged delivery cycles, so IT organizations are moving to DevOps. How do you, as a practitioner, address the siloes involved in modern enterprise development and leverage DevOps to make your job easier?","tags":"devops"},{"url":"http://www.ciandcd.com/ibm-mobile-testing-point-of-view.html","title":"IBM mobile testing point of view","text":"From: http://devops.linuxjournal.com/develop-deploy/ibm-mobile-testing-point-view-0 Determining the quality of mobile applications requires you to look beyond the application itself and take into account all the systems and services the application makes use of. This paper describes the IBM point-of-view on critical mobile application testing scenarios and execution engines that are necessary and appropriate for a comprehensive mobile application testing solution.","tags":"devops"},{"url":"http://www.ciandcd.com/rancher-labs-hopes-to-help-devops-teams-rope-in-container-management.html","title":"Rancher Labs hopes to help DevOps teams rope in container management","text":"From: http://devops.com/2015/06/25/rancher-labs-hopes-help-devops-teams-rope-container-management/ DevOps teams have turned to containerization in a big way in order to help improve their efficiency. But many enterprises have run into problems scaling containers across their infrastructure to account for issues like networking and load balancing. Rancher Labs, a Cupertino-based cloud software startup, is hoping to help organizations address the scalability with a new open-source Docker platform. Eponymously dubbed Rancher, the beta version for the platform went live this week. It's designed to be a full set of tools to help DevOps teams manage Docker containers in production. The platform folds in infrastructure services built specifically for containers, including networking, storage management, load balancing, service discovery, monitoring, and resource management. The goal of containers is efficiency and full portability, but Sheng Liang, co-founder and CEO at Rancher Labs, notes that the implementation can be hard to get perfect. \"As users deploy containers across different infrastructures, they quickly realize that different clouds, virtualization platforms and bare metal servers have dramatically different infrastructure capabilities,\" said Liang. It's these variables that Rancher seeks to limit. \"By building a common infrastructure backplane across any resource, Rancher implements an entirely new approach to hybrid cloud computing,\" said Liang. The company is going long on containers—an assessment that backers agree with. The company also recently announced that it has received $10 million in series A funding from Mayfield and Nexus Venture Partners. In addition to its Rancher suite, the company also produces RancherOS, a minimalist Linux distribution tightly focused on running Docker containers. The management team behind Rancher is one that is familiar to many cloud practitioners. Co-founder and CEO Sheng Liang was co-founder and CEO of Cloud.com, and continued on at Citrix as CTO of Cloud Platforms Group after the company's 2011 acquisition. Also on board are Cloud.com and Citrix veterans Shannon Williams as vice president of sales and marketing, Darren Shepherd as chief architect and Will Chan as vice president of engineering. According to industry analyst Paul Burns, Rancher Labs' niche is ripe for new innovation. \"In its current state, running any containerization solution at scale and in production requires a significant amount of effort from those adopting the solution,\" says Burns, who is president of analyst firm Neovise. \"Key functions such as networking, service discovery, storage and load balancing can cause integration issues, keeping enterprises from truly reaping the benefits desired. By providing a unique solution that eases and simplifies the management of container software, Rancher is allowing companies to better utilize and benefit from the technology.\"","tags":"devops"},{"url":"http://www.ciandcd.com/open-source-and-samsung-take-center-stage-at-red-hat-summit.html","title":"Open source and Samsung take center stage at Red Hat Summit","text":"From: http://devops.com/2015/06/25/open-source-samsung-take-center-stage-red-hat-summit/ While much of the DevOps world was focused on DockerCon in San Francisco this week, there was also big news happening on the east coast. While DockerCon was shaking things up with the announcement of the Open Container Project the Red Hat Summit in Boston held its own share of big news and much of it spotlighted the important role of open source. \"Red Hat is really the independent voice of Linux focused entirely on business,\" said Rob Enderle, principal analyst with the Enderle Group . \"It is also a showcase of how to make money on the platform without taking advantage of anyone in the process and largely remaining true to the core tenets that created Linux in the first place.\" Focus on Open Source One of the primary core tenets—if not the defining core tenet—of Linux is open source. The shared development and collaborative nature of the operating system as an open source project is its main strength. At the Red Hat Summit open source played an even larger role than normal. \"The company continues to show strong momentum for OpenShift Commons as the re-write of the technology is moving forward showing that the platform-as-a-service (PaaS) battles are alive and well,\" explained Al Hilwa, program director of software development research for IDC . \"Red Hat also rebranded and updated its FeedHenry platform as it continues to turn it fully into open source and integrate it with OpenShift.\" Samsung Partnership Aside from the focus on open source, the other big news out of the Red Hat Summit was a new strategic alliance with Samsung. Samsung will push Red Hat's mobile platform and potentially other software into the enterprise in a more integrated fashion as a function of the new partnership. The relationship has potential, but according to Enderle there are both positive and negative outcomes possible. \"Samsung needs Red Hat far more than Red Hat needs Samsung because Red Hat provided the enterprise brand that Samsung needs to penetrate the space. Samsung is a firm that has never found a platform they didn't like and have historically been unable to focus on any one of them for extended periods of time,\" cautions Enderle. \"Red Hat alone won't be able to fix that and Samsung's behavior could actually tarnish Red Hat if they aren't careful.\" Strong Momentum Hilwa also made another observation about the strength of Red Hat's business model. \"That the company will cruise through the two billion dollar annual revenue barrier sometime this year is all but certain with its posted rates of growth. This is an enormous milestone for the open source world.\" We'll have to wait and see how the Red Hat / Samsung partnership pans out for either company, but the Red Hat Summit highlighted the strength of Red Hat's momentum and the unstoppable juggernaut of open source development.","tags":"devops"},{"url":"http://www.ciandcd.com/there-is-only-one-devops-connect.html","title":"There is only one DevOps Connect","text":"From: http://devops.com/2015/06/25/there-is-only-one-devops-connect/ They say that imitation is the sincerest form of flattery. If that is true I guess the folks over Informa Telecoms and Media Ltd are very, very sincere. Why else would they change the name of their DevOps Conference from \"DevOps Summit\" (which name they \"borrowed\" from the Sys-Con folks) to DevOps Connect Europe. No matter that we here at DevOps.com have already held several events in Europe and London specifically with our DevOps Connect name. No matter that we have a website and own the domain http://www.devopsconnect.com . Why shouldn't they just use our name and employ the \"colorful\" technique of registering a hyphenated play on the domain name? I guess that alone should tell you all you need to know about this company and the events they produce. Now of course we can go hire some International copyright attorneys and litigate first usage and ask for injunctions and damages. But why bother? I am a big believer in letting the market be the judge, jury and executioner. I am perfectly fine with using DevOps.com to get out the word about these kinds of business practices and letting the market act. The event these people want to put on will probably be no better than the several DevOps Summit events they put on previously. I have heard from exhibitors, speakers and attendees of those events that they were less than successful. Mind you I haven't attended any, just reporting on what I was told. Ultimately though if they were successful they wouldn't be trying to rebrand using someone else's name. Hard to argue with that logic. I am asking our readers (and there are many of you in London, as it is our single biggest city for viewers) to let this company know that copying others business names will not stand. The best way to do this is to not attend, don't participate and get the word out that this is not the way business is done. Don't be fooled by the attempt to piggyback our brand and success, we have nothing to do with that event. In fact we will be holding several DevOps Connect events in London and elsewhere in Europe in the fall. Details will be announced shortly but there will be lots of opportunities to participate. Some of you may ask, \"you haven't given us the link to this event?\" You're right, I have not and for good reason. I have already given this company and their event more ink than they are probably due. So be aware of imitators, remember there is only one DevOps Connect and those are the events put on by us at DevOps.com. We hope to see you at them and will be sure to let everyone know when we do. Until then don't reward imitators.","tags":"devops"},{"url":"http://www.ciandcd.com/visualizing-and-defining-requirements-comes-to-devops.html","title":"Visualizing and defining requirements comes to DevOps","text":"From: http://devops.com/2015/06/26/visualizing-and-defining-requirements-comes-to-devops/ We have all seen the nice circular diagrams of a DevOps methodology for application lifecycle management (ALM) like the one above. We interject some automation, make the two halves work closer together, get some feedback loops going and go as fast as we can. But lets look at those first two steps in the diagram, the ones labeled consult and design. What and how do we accomplish this? There is a company called iRise that has built a great business by allowing for rapidly visualizing and defining requirements for building applications. It is high time we recognize this as the important piece of the puzzle that it is. I recently had a chance to meet with some of the executive team of iRise while I was in London for the CD Summit. The premise of iRise is so simple I was thinking to myself, \"why hasn't anyone else thought of this?\" If the old adage of crap in is crap out is true, why wouldn't we take the time to define and plan our software better early on? I know everyone thinks they do, but do they really? Do we quickly visualize to help define the requirements and to help the team buy in and sign off? If it is set up right to begin with, it flows a lot faster later. Now the folks at Tasktop creators of the Eclipse Mylyn open source tool have teamed with iRise to OEM iRise into the Tasktop Software Lifecycle Integration system. In speaking with Stephen Brickley, EVP of iRise, he thinks this partnership will be a key factor in bringing iRise into the mainstream DevOps tools market. iRise's customers, like software departments everywhere are adopting new ways of developing and operating software. New times and new ways don't mean that you throw out what works though. To me it is a no brainer. If you have a tool that lets you quickly visualize, prototype and communicate to the rest of the team before investing a lot of effort into development, it really has to help. It is the ultimate shift left. I guess the issue is how quickly can you do this without adding extra steps to the development process. From what I saw of iRise it makes it really easy and fast. Of course I would be interested to hear from anyone who has used iRise or even a similar tool on your own experience. But if it all is as it seems, I would expect iRise to be a welcome addition to the DevOps toolbox. What do you think? What tool do you use for this now or do you just skip this in planning and initial requirements? We are always on the lookout for new tools to report on at DevOps.com, but ultimately we depend on our readers to tell us what is really useful or not. So let us know.","tags":"devops"},{"url":"http://www.ciandcd.com/integrating-dunitx-unit-testing-with-continua-ci.html","title":"Integrating DUnitX Unit Testing with Continua CI","text":"From: https://www.finalbuilder.com/resources/blogs/postid/699/integrating-dunitx-unit-testing-with-continua-ci Continua CI includes support for running and reporting on Unit Tests, in this post we will take a look at running DUnitX Unit Tests. If you not familiar with DUnitX , it's a newish Delphi Unit Test framework, I blogged about it recently . I'm not going to cover how to get up and running in Continua CI, but rather I'll focus on the Unit Testing support. If you are not familiar with Continua CI, take a look at this recent post which describes how to build Delphi projects with Continua CI. Assuming you have you Continua CI Configuration all set up, lets take a quick look at how to integrate our unit tests into the build process. You need to build a console application, and make use of the xml logger. This is how the dpr of our typical DUnitX console test application might look : var runner : ITestRunner; results : IRunResults; logger : ITestLogger; xmlLogger : ITestLogger; begin try //Create the runner runner := TDUnitX.CreateRunner; //add a console logger, pass in true to specify quiet mode //as we don't need detailed console output. logger := TDUnitXConsoleLogger.Create(true); runner.AddLogger(logger); //add an nunit xml loggeer xmlLogger := TDUnitXXMLNUnitFileLogger.Create; runner.AddLogger(nunitLogger); //Run tests results := runner.Execute; {$IFDEF CI} //Let the CI Server know that something failed. if not results.AllPassed then System.ExitCode := 1; {$ELSE} //We don;t want this happening when running under CI. System.Write('Done.. press key to quit.'); System.Readln; {$ENDIF} except on E: Exception do begin System.Writeln(E.ClassName, ': ', E.Message); System.ExitCode := 2; end; end; end. The key thing to remember, is that this application is going to be running unattended, so never use ReadLn or any sort of interaction/prompting for input etc. If I had a dollar for every \"Finalbuilder/Continua CI hangs during my build\" bug report in the last 13 years, I'd be a... well not rich, but a few hundred dollars better off! Notice I used $IFDEF CI above to set the exit code to 1 if not all tests pass. So the next thing we need to is actually get the console application building in Continua CI. I covered building delphi applications with Continua CI in earlier post, so I'll just highlight a few things specific items that we need. Firsly, if you don't have the DUnitX source code in your repository, and have configured a repositoroy for it in Continua CI, then you need to update the Search Path for your console application. If you are using MSBuild to build the console app, then it's done on the Properties tab of the MSBuild Action : I have DUnitX in a Continau CI repository named DUnitX, and I've used the default path to the repository in the workspace ( $Source.DUnitX$ translates to \"/Source/DUnitX\" in the build workspace). If you are using FinalBuilder, you need to pass that to a FinalBuilder variable in the FinalBuilder Action, I'll cover that in more detail in a future post. Notice I also set the CI define I used in my code. The other important setting, is the ExeOutput path, which much be somewhere inside the build's workspace, so I set it to $Workspace$\\Output - Continua CI will translate $Workspace$ at build time to be the workspace folder for the build (each Continua CI build gets a clean unique workspace folder). Now it's time to add our DUnitX action, somewhere in the stage workflow after we have built the test application. Setting up the DUnitX action is quite simple : We set the Test Executable to our test console app, which in the MSBuild action we configured to be output to $Workspace$\\Output - and we specify where to put the xml file that DUnitX will generate (because we added the NUnit logger). The other two options control whether the to fail the Action/Build if any tests fail or error. If you have more than one Unit Test action to run in your build process, then it's bests to leave these unchecked and use the Stage Gate feature (on the Stage Options dialog) to fail the build (more on this later). After running the build, the Unit Test results appear in a two places, firstly the Build Details page, which shows the totals for the build (for all unit tests run) : You can drill into the tests by clickong on the numbers, or by clicking on the Unit Tests tab : This page allows you to filter by status (click on the numbers), and filter by Fixture, Namespacve etc. The first time a test fails, it will show up inder the New Failures category and under Failures. Clicking on a Failed or Errored test expands the row to show the error message logged by the test framework : Notice the Shelve button next to each test. If you have a test that always fails, and you don't want it to cause the build to fail, shelving the tests tells Continua CI to ignore those failures in the future, until you unshelve them. One last feature which I touched on, is the use of Stage Gates to fails the build. Continua CI collects a bunch of metrics during the build, and we can use those metrics in Stage Gate Conditions to fail the build if for example, there are any failing tests. Each Stage has it's own Gate Conditions, which are evaluated once the stage completes.. When the build fails at the Stage Gate, you will see the output of the condition expressions (note, the expressions below are the defaults on all stages) :","tags":"ciandcd"},{"url":"http://www.ciandcd.com/building-github-pull-requests-with-continua-ci.html","title":"Building GitHub Pull Requests with Continua CI","text":"From: https://www.finalbuilder.com/resources/blogs/postid/700/building-github-pull-requests-with-continua-ci GitHub makes it relatively simple to contribute to open source projects, just fork the repository, make your changes, submit a pull request. Couldn't be simpler. Accepting those Pull requests, is dead simple too, most of the time. But what if you want to build and test the pull request first, before accepting the request. Fortunately the nature of GitHub Pull requests (or more to the point, Git itself) makes this possible. Git References Git References are a complex topic all on it's own, but lets take a quick look at a typical cloned repository. In the .git folder, open config file in notepad and take a look at the [remote \"origin\"] section, here's what mine looks like : [remote \"origin\"] url = https://github.com/VSoftTechnologies/playground.git fetch = +refs/heads/*:refs/remotes/origin/* The key entry here is the fetch. Quoting from the git documentation : \"The format of the refspec is an optional + , followed by <src>:<dst> , where <src> is the pattern for references on the remote side and <dst> is where those references will be written locally. The + tells Git to update the reference even if it isn't a fast-forward.\" The default fetch refspec will pull any branches from the original repository to our clone. But where are our pull requests? Anatomy of a pull request When a pull request is submitted, GitHub make use of Git References to essentially \"attach\" your pull request to the original repository. But in my local clone, I won't see them because the default fetch refspec doesn't include them. You can see the pull requests by using the git ls-remote command on the origin : $ git ls-remote origin $ git ls-remote origin 27dfaaf83f60ac26a6fe465042f2ddb515667ff1 HEAD 654b98d6eb862e247e5c043460e9f9a64b2f0972 refs/heads/Test 27dfaaf83f60ac26a6fe465042f2ddb515667ff1 refs/heads/master b333438310a56823f1938071af8c697b202bf855 refs/pull/1/head 95cb80af1330e73188ea32659d7744dcfe37ab43 refs/pull/2/head 90ba13b8edaab04505396dbcb1853f6f9bdaed64 refs/pull/2/merge Notice something odd there. There are two pull requests, but pull request 2 has two entries in the list, whilst pull request 1 has only 1 entry. refs/pull/2/head is a reference to the head commit of your pull request, whilst refs/pull/2/merge is a reference to the result of the automatic merge that GitHub does. On pull request 1, there was a merge conflict, so the the /merge reference was not created, on pull request 2, the merge succeeded. On the pull request page, you would typically see something like this if the merge succeeded : Getting Continua CI to see the Pull Requests The main reason for building pull requests on your CI server is to see if they build, and to run your unit tests against that build. You can chose to build the original pull request, or the result of the automatic merge, or both. In reality, if the automatic merge failed, then the person who submitted the pull request has some more work to do, so there's really no point building/testing the original pull request. What you really want to know, is \"if I accept this request, will it build and the tests pass\", so it's generally best to only build the automatic merge version of the pull request. Continua CI makes this quite simple. On the Git Repository settings, check the \"Fetch other Remote Refs\" option. This will show the Other Refs text area, which already has a default RefSpec that will fetch the pull requests (the merged versions), and create local (to Continua CI) branches with the name pr/#number - so pull request 1 becomes branch pr/1. You can modify this to taste, for example if you are fetching both the merge and the head versions of the pull requests, you might use a refespec like this : +refs/pull/*/merge:refs/remotes/origin/pr-merge/* +refs/pull/*/merge:refs/remotes/origin/pr-head/* Building the pull Requests Now we have gotten this far (which is to say, you enabled one option and clicked on save!) we can build the pull requests (it may take a few minutes to fetch the pull requests). If you manually start a build, you can select the pull request from the branch field for the github repository using the intellsense, just start typing pr/ and you will see a list : Now we can add a trigger to build pull requests (we are talking continuous integration after all). Using the Pattern Matched Branch feature on Continua CI Triggers you can make your trigger start builds when a pull request changeset is fetched from Github. The pattern is a regular expression, so &#94;pr/.* would match our pull request branches (assuming you we use the default refspec) Adding a trigger specific to the pull requests allows you to set variables differently from other branches, and you will then be able to tailor your stages according to whether you are building a pull request or not. For example, you probably don't want to run your deploy stage when building a pull request). Updating GitHup Pull Request Status One last thing you might like to add, is to update the Pull Request Status . This can be done using the Update GitHub Status Action in Continua CI (In a future update this will done via build event handlers, a new feature currently in development). This is what the pull request might look like after the status is updated by Continua CI :","tags":"ciandcd"},{"url":"http://www.ciandcd.com/adding-custom-reports-to-continua-ci-build-results.html","title":"Adding custom reports to Continua CI Build Results","text":"From: https://www.finalbuilder.com/resources/blogs/postid/701/adding-custom-reports-to-continua-ci-build-results It's not uncommon for tools run during a build process to create log files, xml or html files etc that you might want to keep, in other words Artifacts . Continua CI already has a mechanism for registering Artifacts, which enables them to be viewed/downloaded from the Build Artifacts page. Continua CI also has another way of viewing those files, Reports. Reports are viewed in an iframe, which allows you to stay within the Continua CI UI, but still naviate within the report files. A typical use of the Report feature is showing exported FinalBuilder html logs, or Code Coverate html reports (for example those produced by OpenCover). Defining a Report Defining a report is relatively simple. Just point it at a file you expect to appear in the Builds Workspace on the Server. In this case we're expecting our build process to place a file named FinalBuilderReport.html in the $Workspace$\\Reports folder. $Workspace$ will be replaced at run time with the builds workspace path on the server. The next step is to make sure that the file actually gets copied to where we said it would be. We define a Workspace Rule to copy the file back to the build workspace on the server when the Stage completes. If your tool generates .css and image files then don't forget to add rules to copy those files into place too. The final step is to make sure that the report file is actually produced. In this example, we are using an exported FinalBuilder Log file, which is created in $Workspace$\\Output\\FB7 - we tell FinalBuilder where the workspace is by setting a FinalBuilder variable (along with a buch of other stuff like version numbering etc) in the FinalBuilder Action . Viewing the Report All thats left to do now is run the build, and view the report, If everything went to plan, then when you click on the builds Report Tab, you should see something like this : Notice the Home/Back/Forward Buttons. If your report comprises of multiple pages, and all the links in the html files are relative, then you get full history support in the iframe. FWIW, Open Cover with ReportGenerator does just that. We'll take a look at using Open Cover with Continua CI in a future post.","tags":"ciandcd"},{"url":"http://www.ciandcd.com/dunitx-has-a-wizard.html","title":"DUnitX has a Wizard!","text":"From: https://www.finalbuilder.com/resources/blogs/postid/702/dunitx-has-a-wizard Thanks to a contribution from Robert Love, DUnitX now sports a shiny new IDE Wizard for creating Test projects and Test Units. Before you install and use the wizard, there is one thing I recommend you do. In your Delphi IDE, add and Environment variable DUNITX and point it at your copy of the DUnitX source. The reason for doing this, is that when the wizard creates a project, it adds $(DUNITX) to the project search path. This avoids hard coding the DUnitX folder in yhe project search path, and also avoids installing it in your global library path (I have nothing other than the defaults installed there, I always use the project search path, makes it easier to share projects with other devs). Once you have that done (I'm assuming you have pulled down the latest source from GitHub), open the project group (.grouproj) for your IDE version and build the project group. Then right click on the wizard project and click on install : If the package installs successfully then we are ready to use the wizard. Close the project group, and invoke the File\\New\\Other dialog, you will see the DUnitX Project listed You might like to Customize your File\\New menu, I made DUnitX prominent on mine (in part to remind myself to create unit tests first!) : Invoking the wizard will show a simple dialog : The options are pretty self explainatory, so I won't go into them here. The wizard generates a console application, once we have a gui runner (being worked on) we'll update the wizard to add options for that. DUnitX is open source, get it from GitHub - contributions are welcome. We also have a Google Plus Community for DUnitX.","tags":"ciandcd"},{"url":"http://www.ciandcd.com/automated-builds-vs-continuous-integration.html","title":"Automated Builds vs Continuous Integration","text":"From: https://www.finalbuilder.com/resources/blogs/postid/703/automated-builds-vs-continuous-integration Roy Osherove posted an interesting video on youtube recently, talking about the difference between Automated Builds and Continuous Integration. This is a subject that comes up often in emails from existing and potential customers. Why do I need FinalBuilder if I have Continua CI, or, why haven't we added all the functionality of FinalBuilder to Continua CI? Roy sums up the differences and reasoning in this video quite nicely. Roy goes into more detail in his \"newish\" book , Beautiful Builds . Lots of interesting food for thought. Lets face it, most developers probably don't spend too much time \"deep thinking\" how the build side of things should be done. Roy's book is full of \"deep thinking\", but Roy sums it all up quite nicely! It's not a long book, around 40 pages and definitely worth a read. FinalBuilder and Continua CI The workflow functionality in Continua CI is inspired by FinalBuilder (which is probably stating the obvious to FinalBuilder users), but the functionality that is there is fairly high level. Your Continua CI Stage workflow can be as simple as a single action (for example a FinalBuilder Action or MSBuild, or Rake or Powershell....), or you can make use of the flow control, logic, file operation actions etc and keep your build scripts as simple as your .sln file (or .dproj for delphi developers). That's for you to decide. Of course our recommendation is to use FinalBuilder (and I say \"of course\" because I have wages to pay!) but there are obvious benefits to using FinalBuilder for your automated build scripts. Develop and debug your build scripts on your machine Using FinalBuilder, you can develop and debug your build script on your machine. FinalBuilder allows you to step through the build, set breakpoints, see what is happening, how variables are changing etc. The value of this shouldn't be underestimated; developing a FinalBuilder script is fast, easy and provides immediate feedback. No Web based CI Server (is there any other kind?) can give you this level of immediate feedback. This also has the added benefit in that the FinalBuilder project will be runnable on other developers machines (assuming they have the required tools installed) All of your build script is versioned with your source code. If all of your build script functionality is in a FinalBuilder script, and that script is checked into version control (alongside your source code), then if the structure of your source code changes, so can your FinalBuilder script. The CI server will checkout the correct version of the build script for the version of source code it's checking out. Better Integration We're currently working on better integration between Continua CI and FinalBuilder. Some of our FinalBuilder Server customers have complained that they lose functionality when moving to Continua CI. We understand that, FinalBuilder Server and FinalBuilder are tightly coupled. When we started working on Continua CI, we made a conscious decision to not do that. That means Continua CI doesn't know about the internals of a FinalBuilder project, doesn't know what variables are declared. I still stand by that decision; it made Continua CI a better product. That said, there is more we can do to improve how the two products work together. This work will show up in an update over the coming weeks (when it's ready!).","tags":"ciandcd"},{"url":"http://www.ciandcd.com/finalbuilder-and-team-foundation-server-2013.html","title":"FinalBuilder and Team Foundation Server 2013","text":"From: https://www.finalbuilder.com/resources/blogs/postid/705/finalbuilder-and-team-foundation-server-2013 FinalBuilder offers you tight integration into TFS with an easy to understand IDE. In this post I will go into how to integrate FinalBuilder into your TFS build process. The post will cover; To make sure this post doesn't rival \"war and peace\" in size, I will assume a few things during the post. Namely that you have used TFS in some fashion, and/or have gotten a solution building under TFS with the default TFS template. For more information on getting up and running with TFS and common issues with TFS I suggest taking a look at the TFS ranger books and blog . Prerequisites To follow along with this post you will need the following installed: Team Foundation Server 2013, with build agent (12.0.21005.1 or later) FinalBuilder 7 (7.0.0.2745 or later) Required Visual Studio or MSBuild version available on agent Agents for Microsoft Visual Studio 2013 Team Explorer 2013 The Team Foundation Server can be of any configuration as long as it has at least one agent and a reporting service. FinalBuilder 7 is required on the agent machine, and could also be installed on the developer's machine for editing of scripts. Agents for Microsoft Visual Studio 2013 will provide access to VSTest.Console which we require for testing on the build agent. Lastly, Team Explorer is required for interaction with source control from FinalBuilder's IDE. Without this installed source control will need to be done manually. To start I have a solution under source control on a TFS 2013 collection. The solution I will be using is under a team project called VSoftSFTPLibrary under the collection \"TFS2013\\DefaultCollection\". The layouts of the team project looks like the following: I have a directory for the SFTP projects source, library files and its tests. The layout of your source control may vary from mine, however the main point to note here is that separating out directories isn't an issue. With the solution, all related files, and binaries under source control I now add a build definition to the team project. Next the folders to be used in the FinalBuilder script need to be mapped from the server to an agent location. It's important to map all folders which contain files required for the build process. At this stage we only really know of the Source directory we are going to be build and the Library folder which the source relies on. Note that if your solution relies on a certain structure of folders this structure should be the same when it arrives on the agent. In the example the source and library paths are at the same folder depth, therefore this is reflected on the agent side. When the build is finished usually we want them \"dropped\" somewhere on the network. A FinalBuilder script can use this drop location, which we will go into later on in the artcile. For now I set this to a server location accessible by the build agent service user. By default the build process is set to the default build process which comes with TFS2013. The template is called \"TfvcTemplate.12.xaml\". When used the default template will enable the building, testing, impact testing, and deployment of a solution and its projects. Our aim is to perform all the same build activities with the additon of a FinalBuilder project. To this end, the build process template needs to be changed over to one of the two supplied with FinalBuilder. In the [%ProgramFiles(x86)%\\FinalBuilder 7\\TFS Templates\\2013] folder there are two TFS 2013 build workflow templates. \"FinalBuilderTFS2013Build.xaml\" performs all the same steps as the default build workflow and adds a FinalBuilder step just after the MSBuild activity and just before the optional \"after MSBuild script\". This script is typically used by those looking to convert current default script builds to FinalBuilder in a simple and piece-meal fashion. The \"FinalBuilderOnlyTFS2013Build.xaml\" template is used when the FinalBuilder script is to take over the entire build process. For the moment I will use the \"FinalBuilderTFS2013Build.xaml\" script. Add these two scripts to the team project, typically under a BuildTemplates folder to separate them from the source and library code. Copy these templates into the local mapped location for this team project, and check them into source control. Now the \"FinalBuilderTFS2013Build.xaml\" template can be selected as the build process template. To do this add a new new build in the process section of the build definition. Navigate to the source control location where the template is stored and select it. The next step is to fill in the parameters for the build process. The majority of these are the exact same as the default TFS 2013 template with the addition of some FinalBuilder specific settings. The FinalBuilder section of settings allow for the specification of the project file, and custom arguments for the FinalBuilder project. Note that the \"2. Build | Projects\", and the \"6. FinalBuilder | Project File\" are both required by the build template. These are used to determine what should be built and what FinalBuilder project should be run. To run a FinalBuilder project we need to create one. Therefore create a FinalBuilder project with just an [Action Group] action for the moment. Save this project to the team projects FinalBuilderScripts folder and check it into source control. Once the FinalBuilder project is in source control select it in the build process using the file selector provided through the ellipse selector. You should end up with something reading like this \"$/VSoftSFTPLibrary/FinalBuilderScripts/BuildVSoftSFTPLibrary.fbp7\" Now that we have included the FinalBuilder project into our build process we need to make sure its folder is mapped to the agent. Open the source settings section, and add a mapping for the folder in which the FinalBuilder project resides. Something like the following should be what results. Queue the build and the log should read like the following excerpt. The [Action Group] line is the action group you added in the FinalBuilder project. So now we have a TFS build process which is able to call a FinalBuilder script. In the next section we will delve more into how to extract information from TFS about the build inside our FinalBuilder script. Retrieving information from TFS in a FinalBuilder Script For the FinalBuilder script to be of use in the TFS build process, it requires information about the TFS build currently running. To provide this we offer a number of actions that can extract this information during the TFS build run. To get you started, there is an example project in <FinalBuilderInstallDir>\\TFS Templates called TFSExample.fbp7. This sample project contains examples of the actions to use during a TFS build process. The [Get Team Foundataion Build Parameters] action is a special action that is only useful when a project is launched from TFS. It assigns TFS data to the specified project variables. Each of the variables in the [Get Team Foundation Build Parameters] action are as follows: Team Server URL: This is the URL of the team foundation server that queued the build. Team Project: Is the name of the team project the build belongs to. Build Id: Is the unique number allocated to this build. Platform/Flavor: These are the build parameters for the compilation of the solution to be built. Default Solution File: The solution file listed as the primary for the team project. Solution File List: The list of solutions to be built by the build process. Deployment Folder: The drop folder configured for the build process. It is blank if it is not set. Source Root: The first listed solutions root folder. Working Directory: A working directory on the agent for the current build definition. Typically space which is shared between builds made on the same agent. On the Custom Arguments tab is a list of ten variables which can be passed from the TFS build process to FinalBuilder. These are passed as plain text and converted to the variable types used to read them in the FinalBuilder script. For my script I only wanted to get the Team Foundation Build Parameters, and the variables that it used. So first I checked out the \"BuildVSoftSFTPLibrary.fbp7\" project using the built in source control features of FinalBuilder. First I need to make sure that it is indeed added to source control. If it hasn't detected that it is part of the TFS source control at this stage I add it to source control. I make sure to select the \"Microsoft Team Foundation Server MSSCCI provider\" which will use the Team Explorer 2013 installed on the machine. I select the TFS 2013 server and collection I am working with, also making sure the team project is correct. Once this is done I am then able to use the file menu to check the project out ready for editing. Next I copy all the variables I want from the TFSExample project, and paste them into my BuildVSoftSFTPLibrary.fbp7 project. To paste the variables I open the Variables Editor, right click and select paste. Last I copy over the [Get Team Foundation Build Parameters] and [Trigger Files Iterator] actions. These use the variables we just copied over and will hook themselves up as they appeared in the example project. Now we can check in these changes and queue the build. In the log for the build you will see the following: By default the [Get Team Foundation Build Parameters] action will write what it has retrieved from TFS to the build log. Something to keep in mind when debugging a FinalBuilder script in TFS. So now we want to make the FinalBuilder process take over the build completely. The first thing to do here is to stop the TFS build process from building, testing, and publishing results of the build. This requires changing the build workflow to remove the activities which do this (otherwise we would be doing things twice), and updating the FinalBuilder script to perform these tasks. Instead of working out which activities to remove from the build template we provide a build template with all the build and testing activities removed. The \"FinalBuilderOnlyTFS2013Build.xaml\" template which was copied into the BuildTemplates folder is this template. In the process section of the build definition, create a new build process using the \"FinalBuilderOnlyTFS2013Build.xaml\" file. You will notice that nearly everything in the build parameters is the same except now the before and after script events have been removed. Also the testing section is no longer present. All of this will be handled by the FinalBuilder script. Once again open your FinalBuilder project for this build and check it out. Also open up the TFSExample.fbp7 project and take a look at the [Build VS.Net Solution] action. Copy this action to the project used to build your solution. The [Build VS.NET Solution] action builds a Visual Studio.NET solution. On the [Solution] tab you will see that the Solution File is set to \"%SourceRoot%\\<YourSolution>.sln\". Replace <YourSolution> with the name of the solution that you wish to build. Note that \"%SourceRoot%\" will be the directory of only the first solution in the list of projects to build. In my project I ended up with a Solution File value of \"%SourceRoot%\\VSoftSFTPLibrary.sln\" for the [Build VS.NET Solution] action. On the [Paths] tab you will see that the Output Directory is set to %SourceRoot%\\Binaries. You may change this if you wish, but it's it not necessary. Note because the drop location may be on a different server to the build agent, it is important that VSTest runs on files located on the build agent machine. Unless you explicitly set up the trust relationship, .NET will not allow executing of assemblies on remote machines. This is why we build and test in a directory under %SourcesRoot% and then move the files to the drop location after testing. This will be covered more in the next section. On the agent the TFS agent the FinalBuilder options for Visual Studio will need to be set. If not and DEVENV.COM is required, the build will fail about the build tool location being unknown. Now are right to run the build with using just FinalBuilder. Queue the build again and now the project will build, not from a template activity but from the FinalBuilder script it is running. To perform testing we need to add a [Run VSTest.Console] action to the FinalBuilder project. The [Run VSTest.Console] uses VSTest.Console to run your test assemblies. On the [Settings] tab add the name of your test assembly to the list. You should end up with something along the lines of \"%SourceRoot%\\Binaries\\<YourTestAssembly>.dll\". On the [Publish Results] tab the action should be set to automatically publish the results to the TFS server. This means that after the tests are run they will automatically be stored with the build on the TFS Server. Note that the [Ignore Failure] option, located on the [Options] tab, is important. If any unit tests fail, the [Run VSTest.Console] action will fail, and setting [Ignore Failure] allows the FinalBuilder and TFS builds to continue. Un-check [Ignore Failure] if you would prefer the build to stop on failed tests. In either case, test failures will appear in the TFS build log and in TFS reports. On the agent the TFS agent the FinalBuilder options for VSTest.Console will need to be set. If not, the build will fail with an error about the VSTest.Console location being unknown. The last step to complete the process is to move all the files from the agent to your drop location. The simplest way to achieve this is by a [Move File(s)] action. We already have the drop folder location stored in the %DropShare% variable. It is this value which we then use in the [Move File(s)] action. Once this is run we will have a built solution, with tests run, and the binaries copied into the specified drop folder.","tags":"ciandcd"},{"url":"http://www.ciandcd.com/continua-ci-and-the-openssl-heartbleed-vulnerability.html","title":"Continua CI and the OpenSSL Heartbleed Vulnerability","text":"From: https://www.finalbuilder.com/resources/blogs/postid/706/continua-ci-and-the-openssl-heartbleed-vulnerability The short answer is No . Continua CI itself does not use Open SSL directly, but the default database engine, PostgreSQL, does. The version of PostgreSQL we ship with Continua CI is 9.1.3 .1254 and it does include a version of OpenSSL with the vulnerability, however ssl support is turned off by default and is not used by Continua CI. We also update the pg_hba.conf during install to only allow connections from localhost, however it turns out that if ssl is enabled, the ssl negotiation happens before the rules in pb_hba.conf are matched and this alone does not protect the server. If you are using your own install of PostgreSQL (or you want to be sure that what I say is correct) then I suggest you check your PostgreSQL server. You and easily check if ssl is enabled by running the following query in PGAdmin: show ssl Another option is to try the testing tool here : https://github.com/titanous/heartbleeder heartbleeder -pg yourciserver:9001 here's the output from testing one of our CI servers : heartbleeder.exe -pg pilatus:9001 Error connecting to pilatus:9001: dial tcp 10.0.0.80:9001: ConnectEx tcp: No connection could be made because the target machine actively refused it. If you are using SQLServer, then you (for once) are ok, SQL Server doesn's use OpenSSL. We will issue an update in the next few days with an updated PostgreSQL version once we have completed testing By now you have probably heard or read about the OpenSSL Heartbleed Vulnerability (unless you have been living under a rock for the last week)! We have had a few customers ask us whether Continua CI is vulnerable to this exploit.The short answer isContinua CI itself does not use Open SSL directly, but the default database engine, PostgreSQL, does. The version of PostgreSQL we ship with Continua CI is 9.1.3 .1254 and it does include a version of OpenSSL with the vulnerability, however ssl support is turned off by default and is not used by Continua CI.We also update the pg_hba.conf during install to only allow connections from localhost, however it turns out that if ssl is enabled, the ssl negotiation happens before the rules in pb_hba.conf are matched and this alone does not protect the server.If you are using your own install of PostgreSQL (or you want to be sure that what I say is correct) then I suggest you check your PostgreSQL server. You and easily check if ssl is enabled by running the following query in PGAdmin:Another option is to try the testing tool here :here's the output from testing one of our CI servers :If you are using SQLServer, then you (for once) are ok, SQL Server doesn's use OpenSSL.We will issue an update in the next few days with an updated PostgreSQL version once we have completed testing","tags":"ciandcd"},{"url":"http://www.ciandcd.com/refuge-for-automated-build-studio-users.html","title":"Refuge for Automated Build Studio Users","text":"From: https://www.finalbuilder.com/resources/blogs/postid/708/refuge-for-automated-build-studio-users Refuge for Automated Build Studio Users By Vincent Parrett On April 13, 2014 SmartBear recently discontinued development of Automated Build Studio and deleted pretty much all references to it from their website. Within hours of the smartbear email going out to ABS users, we started getting questions about crossgrade discounts, and we're more than happy to help. If you are an ABS user, contact us (sales @ finalbuilder.com ) with proof of purchase (invoice) for ABS and we'll provide you with a 50% off discount coupon for FinalBuilder 7 Professional Edition. This is a limited time once only offer, valid until May 15th 2014. Spread the word to your fellow ABS users. While you you are checking out FinalBuilder, be sure to take a look at our Continuous Integration Server product, Continua CI. It's vastly superior to the CI features in ABS, but still allows you to create your build process using a visual build tool (FinalBuilder).","tags":"ciandcd"},{"url":"http://www.ciandcd.com/automated-ui-testing-done-right-with-continuaci.html","title":"Automated UI Testing Done Right with ContinuaCI","text":"From: https://www.finalbuilder.com/resources/blogs/postid/709/automated-ui-testing-done-right-with-continuaci Automated UI Testing Done Right with ContinuaCI You have just completed an awesomely complex change to your shinny new webapp! After running all your unit tests things are in the green and looking clean. Very satisfied at the quality of your work you fire up the application to verify that everything is still working as advertised. Below is what greets you on the root path of your app: We have all been here at some time or another! What happened! Perhaps it was not your code that broke it! Maybe the issue originated from another part of your organisation, or maybe it came from somewhere on the \"inter-webs\". Its time to look at the underlying problem however ..... testing web user interfaces is hard! Its time consuming and difficult to get right. Manual clicks, much typing, cross referencing client specifications etc, surely there must be an easier way. At the end of the day we DO need to test our user interfaces Automated Web UI Testing Thankfully UI testing today can be Automated, running real browsers in real end to end functional tests, to ensure our results meet (and continue to meet) expectations. You have just completed an awesomely complex change to your shinny new webapp! After running all your unit tests things are in the green and looking clean.Very satisfied at the quality of your work you fire up the application to verify that everything is still working as advertised. Below is what greets you on the root path of your app:We have all been here at some time or another! What happened! Perhaps it was not your code that broke it! Maybe the issue originated from another part of your organisation, or maybe it came from somewhere on the \"inter-webs\".Its time to look at the underlying problem however ..... testing web user interfaces is hard! Its time consuming and difficult to get right. Manual clicks, much typing, cross referencing client specifications etc, surely there must be an easier way. At the end of the day we DO need to test our user interfacesThankfully UI testing today can be Automated, running real browsers in real end to end functional tests, to ensure our results meet (and continue to meet) expectations. For the sake of brevity and clarity in this demonstration we will focus on testing an existing endpoint. It is considered common place to find functional tests included as part of a wider build pipeline, which may consist of such steps as: Build Unit Test Deploy to Test Environment Perform Functional Tests Deploy to Production In this article we will be focusing on the functional testing component of this pipeline. We will proceed on the assumption that your code has already been, built unit tested and deployed to a Functional Test environment. Today we will; Add Automated UI testing to an existing endpoint google.com Configure ContinuaCI to automatically build our project, and perform the tests Software Requirements: Visual Studio 2010 Express Edition SP1 or greater ( visualstudio.com ) Microsoft Dot Net Framework version 4 or greater Java JRE ( http://www.oracle.com/technetwork/java/javase/downloads/index.html ) Mercurial ( mercurial.selenic.com ) Mozilla Firefox ( getfirefox.com ) Nuget ( docs.nuget.org/docs/start-here/installing-nuget ) Step1: Prepare a Selenium endpoint Firstly we will prepare for our UI tests by setting up a Selenium server. Selenium is a browser automation framework which will be used to 'remote control' a real browser. Log into the machine you have chosen for the Selenium server with administrator privileges Download and install Mozilla Firefox (getfirefox.com), this will be the browser that we target as part of this example, however Selenium can target lots of other browsers. For a full breakdown please Download Selenium Server ( Place it into a permanent location of you choosing, in our example (\"C:\\Program Files (x86)\\SeleniumServer\") Download NSSM ( Ensure that port 4444 is set to allow traffic (this is the default communications port for Selenium) Open a console and run the following commands: \"C:\\Program Files (x86)\\nssm-2.22\\win64\\nssm.exe\" install Selenium-Server \"java\" \"-jar \\\"C:\\Program Files (x86)\\SeleniumServer\\selenium-server-standalone-2.41.0.jar\\\"\" net start Selenium-Server In order to uninstall the Selenium server service, the following commands can be run: net stop Selenium-Server \"C:\\Program Files (x86)\\nssm-2.22\\win64\\nssm.exe\" remove Selenium-Server Step2: Create a test class and add it to source control Create a new class library project in Visual Studio, lets call it 'tests' Open the Nuget Package Manager Console window (tools menu-> library package manager -> package manager console), select the test project as the default project and run the following script: Install-Package Selenium.Automation.Framework Install-Package Selenium.WebDriver Install-Package Selenium.Support Install-Package NUnit Install-Package NUnit.Runners Create a new class called within the tests project (lets call it tests) and place the below code (Note: line 23 should be changed with location to the Selenium-Server we setup in the previous step): using System; using System.Text; using NUnit.Framework; using OpenQA.Selenium.Firefox; using OpenQA.Selenium; using OpenQA.Selenium.Remote; using OpenQA.Selenium.Support.UI; namespace SeleniumTests { [TestFixture] public class test { private RemoteWebDriver driver; [SetUp] public void SetupTest() { // Look for an environment variable string server = null; server = System.Environment.GetEnvironmentVariable(\"SELENIUM_SERVER\"); if (server == null) { server = \"http:// *** PUT THE NAME OF YOUR SERVER HERE ***:4444/wd/hub\"; } // Remote testing driver = new RemoteWebDriver(new Uri(server), DesiredCapabilities.Firefox()); } [TearDown] public void TeardownTest() { try { driver.Quit(); } catch (Exception) { // Ignore errors if unable to close the browser } } [Test] public void FirstSeleniumTest() { driver.Navigate().GoToUrl(\"http://www.google.com/\"); IWebElement query = driver.FindElement(By.Name(\"q\")); query.SendKeys(\"a test\"); Assert.AreEqual(driver.Title, \"Google\"); } } } Step3: Test the test! Build the solution Open NUnit build runner (by default this is located at ~\\packages\\NUnit.Runners.2.6.3\\tools\\nunit.exe) , Select file -> Open Project, and locate the tests dll that you have build in the previous step click the run button ~ 15 seconds or so you should have one green test! So what just happened? Behind the scenes an instance of firefox was opened (on the Selenium Server), perform a simple google search query and undertook a simple Nunit assertion has verified the name of the window was equal to \"Google\", very cool!. Now lets make the test fail, go ahead and change line 78, lets say \"zzGoogle\", build, and rerun the test. We now have a failing test. Go ahead and change it back so that we have a single passing test. Create a source control repository In this article we will be focusing on the functional testing component of this pipeline. We will proceed on the assumption that your code has already been, built unit tested and deployed to a Functional Test environment. Today we will;Firstly we will prepare for our UI tests by setting up a Selenium server.This machine will be designated for performing the UI tests against, preferably this will be a machine separate from your ContinuaCI server.Log into the machine you have chosen for the Selenium server with administrator privilegesDownload and install Mozilla Firefox (getfirefox.com), this will be the browser that we target as part of this example, however Selenium can target lots of other browsers. For a full breakdown please see the docs page : .Download Selenium Server ( docs.seleniumhq.org/download ), at the time of writing the latest version is 2.41.0.Place it into a permanent location of you choosing, in our example (\"C:\\Program Files (x86)\\SeleniumServer\")Download NSSM ( nssm.cc/download ), unzip it and place into a permanent location of you choosing \"C:\\Program Files (x86)\\nssm-2.22\\\"Ensure that port 4444 is set to allow traffic (this is the default communications port for Selenium)Open a console and run the following commands:In order to uninstall the Selenium server service, the following commands can be run:Create a new class library project in Visual Studio, lets call it 'tests'Open the Nuget Package Manager Console window (tools menu-> library package manager -> package manager console), select the test project as the default project and run the following script:Install-Package Selenium.Automation.FrameworkInstall-Package Selenium.WebDriverInstall-Package Selenium.SupportInstall-Package NUnitInstall-Package NUnit.RunnersCreate a new class called within the tests project (lets call it tests) and place the below code (Note: line 23 should be changed with location to the Selenium-Server we setup in the previous step):Build the solutionOpen NUnit build runner (by default this is located at ~\\packages\\NUnit.Runners.2.6.3\\tools\\nunit.exe) , Select file -> Open Project, and locate the tests dll that you have build in the previous stepclick the run button~ 15 seconds or so you should have one green test!So what just happened? Behind the scenes an instance of firefox was opened (on the Selenium Server), perform a simple google search query and undertook a simple Nunit assertion has verified the name of the window was equal to \"Google\", very cool!.Now lets make the test fail, go ahead and change line 78, lets say \"zzGoogle\", build, and rerun the test. We now have a failing test. Go ahead and change it back so that we have a single passing test. In this example, we're using mercurial open a command prompt at ~\\ type \"hg init\" add a .hgignore file into the directory. For convenience we have prepared one for you here type \"hg add\" type \"hg commit -m \"initial commit\"\" Step 4: Setting up Automated UI testing in ContinuaCI Navigate to the ContinuaCI web interface Create a project Open ContinuaCI Select \"Create Project\" from the top tasks dropdown menu Name the project something memerable; In our case: \"pete sel test 1\" Click the \"Save & Complete Wizard\" button Create a configuration for this project Click \"Create a Configuration\" Name the config something memorable; in our case \"sel-testconfig-1\" Click save & Continue Click the 'Enable now' link at the bottom of the page to enable this configuration Point to our Repository under the section \"Configuration Repositories\", select the \"Create\" link Name the repository \"test_repo\" Select \"Mercurial\" from the \"type\" dropdown list Select the Mercurial\" tab from the top of the dialogue box Enter the repository location under \"source path\" in our case '\\\\machinename\\c$\\sel-blog-final' Click validate to ensure all is well Click save, your repository is now ready to go! Add actions to our build Click on the Stages tab We will add a nuget restore action, click on the \"Nuget\" section from the categories on the left Drag and drop the action \"Nuget Restore\" onto the design surface Enter the location of the solution file: \"$Source.test_repo$\\tests.sln\" Click Save Build our tests Click on the \"Build runners\" category from the categories on the left hand menu Drag and drop a Visual Studio action onto the design surface (note that the same outcome can be achieved here with an MSBuild action). Enter the name of the solution file: \"$Source.test_repo$\\tests.sln\" Specify that this should be a 'Release' build under the configuration option Click save Setup ContinuaCI to run our Nunit tests Select the 'unit testing' category from the left hand menu Drag and drop an NUnit action onto the design surface Name our action 'run UI tests' Within the files: option, specify the name of the tests project '$Source.test_repo$\\tests\\tests.csproj' Within the Project Configuration section specify 'Release' Specify which version of NUnit In order to provide greater configuration flexibility we can pass in the location of our Selenium server to the tests at runtime. This is done within the 'Environments' tab. In our case the location of the Selenium server is http://SELSERVER:4444/wd/hub . Click Save Click save and complete Wizard We are now ready to build! Start a build immediately by clicking the top right hand side fast forward icon A build will start, and complete! When viewing the build log (this can be done by clicking on the green build number, then selecting the log tab) we can see that our UI tests have been run successfully. They are also visible within the 'Unit Tests' tab which displays further metrics around the tests. Step 5: Getting more advanced Navigate to the ContinuaCI web interfaceOpen ContinuaCISelect \"Create Project\" from the top tasks dropdown menuName the project something memerable; In our case: \"pete sel test 1\"Click the \"Save & Complete Wizard\" buttonClick \"Create a Configuration\"Name the config something memorable; in our case \"sel-testconfig-1\"Click save & ContinueClick the 'Enable now' link at the bottom of the page to enable this configurationunder the section \"Configuration Repositories\", select the \"Create\" linkName the repository \"test_repo\"Select \"Mercurial\" from the \"type\" dropdown listSelect the Mercurial\" tab from the top of the dialogue boxEnter the repository location under \"source path\" in our case '\\\\machinename\\c$\\sel-blog-final'Click validate to ensure all is wellClick save, your repository is now ready to go!Click on the Stages tabWe will add a nuget restore action, click on the \"Nuget\" section from the categories on the leftDrag and drop the action \"Nuget Restore\" onto the design surfaceEnter the location of the solution file: \"$Source.test_repo$\\tests.sln\"Click SaveClick on the \"Build runners\" category from the categories on the left hand menuDrag and drop a Visual Studio action onto the design surface (note that the same outcome can be achieved here with an MSBuild action).Enter the name of the solution file: \"$Source.test_repo$\\tests.sln\"Specify that this should be a 'Release' build under the configuration optionClick saveSelect the 'unit testing' category from the left hand menuDrag and drop an NUnit action onto the design surfaceName our action 'run UI tests'Within the files: option, specify the name of the tests project '$Source.test_repo$\\tests\\tests.csproj'Within the Project Configuration section specify 'Release'Specify which version of NUnitIn order to provide greater configuration flexibility we can pass in the location of our Selenium server to the tests at runtime. This is done within the 'Environments' tab. In our case the location of the Selenium server isClick SaveClick save and complete WizardWe are now ready to build!Start a build immediately by clicking the top right hand side fast forward iconA build will start, and complete!When viewing the build log (this can be done by clicking on the green build number, then selecting the log tab) we can see that our UI tests have been run successfully. They are also visible within the 'Unit Tests' tab which displays further metrics around the tests. Lets try a slightly more advanced example. This time we will examine a common use case. A physical visual inspection test needs to be conducted before a release can progress in the pipeline. Place the following code within our test class. using System; using System.Text; using NUnit.Framework; using OpenQA.Selenium.Firefox; using OpenQA.Selenium; using OpenQA.Selenium.Remote; using OpenQA.Selenium.Support.UI; namespace SeleniumTests { [TestFixture] public class test { private RemoteWebDriver driver; [SetUp] public void SetupTest() { // Look for an environment variable string server = null; server = System.Environment.GetEnvironmentVariable(\"SELENIUM_SERVER\"); if (server == null) { server = \"http:// *** PUT THE NAME OF YOUR SERVER HERE ***:4444/wd/hub\"; } // Remote testing driver = new RemoteWebDriver(new Uri(server), DesiredCapabilities.Firefox()); } [TearDown] public void TeardownTest() { try { driver.Quit(); } catch (Exception) { // Ignore errors if unable to close the browser } } [Test] public void FirstSeleniumTest() { driver.Navigate().GoToUrl(\"http://www.google.com/\"); IWebElement query = driver.FindElement(By.Name(\"q\")); query.SendKeys(\"a test\"); Assert.AreEqual(driver.Title, \"Google\"); } [Test] public void MySecondSeleniumTest() { // Navigate to google driver.Navigate().GoToUrl(\"http://www.google.com/\"); IWebElement query = driver.FindElement(By.Name(\"q\")); // Write a query into the window query.SendKeys(\"a test\"); // wait at maximum ten seconds for results to display var wait = new WebDriverWait(driver, TimeSpan.FromSeconds(10)); IWebElement myDynamicElement = wait.Until< IWebElement >((d) => { return d.FindElement(By.Id(\"ires\")); }); // take a screenshot of the result for visual verification var fileName = TestContext.CurrentContext.Test.Name + \"-\" + string.Format(\"{0:yyyyMMddHHmmss}\", DateTime.Now) + \".png\"; driver.GetScreenshot().SaveAsFile(fileName, System.Drawing.Imaging.ImageFormat.Png); // perform an code assertion Assert.AreEqual(driver.Title, \"Google\"); } } } Build, and run the test. In this example we added an additional test to perform a google search, wait at maximum 10 seconds for results to display, take a screenshot (stored it to disk), and perform an NUnit assertion. The screenshot output from the test can be made available as an artifact within Continua! Firstly lets commit our changes; \"hg commit -m \"added a more advanced test\"\" Open the configuration in Continua CI (clicking the pencil icon) Navigate to the stages section Double click on the stage name (which will bring up the edit stage Dialogue box) Click on the Workspace rules tab Add the following line to the bottom of the text area: \"/ < $Source.test_repo$/tests/bin/Release/**.png\". This will tell Continua to return any .png files that we produced from this test back to the ContinuaCI Server. Click on the artifacts tab. Add the following line : **.png\" This will enable any .png files within the workspace to be picked up and displayed within the Artifacts tab. **.png Click save Click Save & Complete Wizard Start a new build Sweet! A screenshot of our test was produced, and can be seen within the Artifacts tab! Clicking on 'View' will display the image: We have put the sourcecode of this article up on Github . Please subscribe and comment! We are very excited to see what you guys come up with on Continua, happy testing! Some additional considerations: The user which the Selenium service runs under should have correct privileges The machine designated as the Selenium server may require access to the internet if your webapp has upstream dependencies (eg third party API's like github) Build, and run the test.In this example we added an additional test to perform a google search, wait at maximum 10 seconds for results to display, take a screenshot (stored it to disk), and perform an NUnit assertion. The screenshot output from the test can be made available as an artifact within Continua!Firstly lets commit our changes; \"hg commit -m \"added a more advanced test\"\"Open the configuration in Continua CI (clicking the pencil icon)Navigate to the stages sectionDouble click on the stage name (which will bring up the edit stage Dialogue box)Click on the Workspace rules tabAdd the following line to the bottom of the text area: \"/","tags":"ciandcd"},{"url":"http://www.ciandcd.com/continuaci-version-15.html","title":"ContinuaCI Version 1.5","text":"From: https://www.finalbuilder.com/resources/blogs/postid/711/continua-15-new-dashboards With the upcoming 1.5 release of ContinuaCI we have made a number of enhancements to the dashboards, we think you'll love them! Here is a peek at what's coming soon. The New List View Improvements: - Stage indicator blocks provide quick drill-down into the build log. - Improved viability and responsiveness of build actions buttons. - Build action buttons moved to the left for easier access. - Viability enhancements around projects. - Improved Responsiveness and performance tweeks. List View: With Builds Completed List View: With Builds Running The New Details View Improvements: - Build and Queue times now have graphs! - Build status card on the left hand side displays the status of the build as it progresses. - Build action buttons are more obvious and responsive. - Stage indicator blocks (present on the build status cards) provide quick drill-down into the build log. - Improved Responsiveness and performance tweeks. Details View: with Builds Queued Details View: with Builds Executing Details View: with Builds Finished Stay tuned for more exciting details regarding the version 1.5 release! With the upcoming 1.5 release of ContinuaCI we have made a number of enhancements to the dashboards, we think you'll love them! Here is a peek at what's coming soon.Stay tuned for more exciting details regarding the version 1.5 release!","tags":"ciandcd"},{"url":"http://www.ciandcd.com/deployment-with-continua-ci-15-and-octopus-deploy.html","title":"Deployment with Continua CI 1.5 and Octopus Deploy","text":"From: https://www.finalbuilder.com/resources/blogs/postid/712/deployment-with-continua-ci-and-octopus-deploy So you've got your Continua CI server set up to automatically build, run unit tests and produce reports for your awesome new web application. Now you're ready to try out your project in its natural environment and then eventually release it to the wild for well-deserved public applause. Up until now, your options were either to use a Copy action to push the files up to test server and a PowerShell action to set up web services, or preferably run a FinalBuilder script utilising the plethora of actions available for transferring files and interacting with web servers. As of version 1.5, Continua CI can also work together with This post will walk through the steps required to push a .Net web application built in Continua to Octopus Deploy and trigger a deployment process to effortlessly get your application running on your test and production servers. Preparing your solution Octopus Deploy requires that you provide your applications as Lets go with the recommended OctoPack option. First prepare your Visual Studio solution - use the NuGet package manager to install the OctoPack package into the projects you want to deploy. This will include web application projects, console application projects and Windows service projects but not class libraries and unit test projects. You can now optionally add a Setting up the deployment process Next head over to your Octopus server and set up a deployment project. This should include a \"Deploy a NuGet package\" process step as below. We will set this to retrieve the application package from the built-in NuGet feed. Note that the NuGet package id should match the id element in your .nuspec file - this will default to the name of your assembly. We added a few more steps: And some variables: Setting up the build process You can now get back to Continua and set up a configuration for building your project. Once you have entered the configuration details and linked up the repository containing your project, move on over to the Stages page: For this simple example you'll need two actions: a NuGet Restore action to ensure that the OctoPack package is available for the build and an MSBuild action to build and push the application to your Octopus Deploy server. Just enter the path to your solution for the NuGet Restore action (the other fields can be left as is) and complete the main tab of the MSBuild action as required for your project. You then need to enter some additional properties to tell MSBuild to run OctoPack and tell it where to send your package. Set the RunOctoPack property to true and the OctoPackPublishPackageToHttp property to the URL for the NuGet feed on the Octopus Deploy server e.g. You will also need to provide an OctoPackPublishAPIKey property – you can generate an API key on your profile page on the Octopus Deploy server. Optionally, y ou can use the OctoPackPackageVersion to specify up the package version. Here we use Continua expressions to set this based on the build version. If you leave this out then OctoPack will get this value from the AssemblyVersion property in your AssemblyInfo.cs file. Once the actions are set up and saved, run a build and check that your package gets uploaded to the Octopus Deploy server. Then create a release for your deployment project and test that it deploys ok. Now we are ready to look into how to run this process automatically from Continua CI. Build event handler On the Continua CI configuration wizard after Stages, we have a new area titled Events. Here you can add Create a new build event handler, give it a name and select the Octopus Deploy as the Type. You can now provide general project details under the Octopus Deploy tab. The Octopus Deploy URL should end with '/api' e.g. http://octopusserver/api. Enter the API key generated under your Octopus Deploy profile and the name of your deployment project. You can then choose one or more actions to run. The available options are Create , Deploy and Promote and are used to create a new deployment release, deploy a release to an environment and promote a release from one environment to another. As you select each action, new tabs open so you can provide further details and hook the action to a build event. Creating a release Before you can deploy an application you need to create a Octopus Deploy release When creating a release you can specify the Release Version or leave this blank to automatically create a number based on the highest package version. You must provide either a Default Package Version or Step Package Versions for each step which requires one e.g. Flip over to the Create Options tab to tell Continua when to create the release. There are six On Before Stage Start On Sending Stage To Agent On Stage Completed On Build Pending Promotion On After Build Continued On Build Completed Generally we want to create the release at the start of the build before the first stage starts. Deploying to an environment Now on to the crux of this whole process - deploying your application. We generally deploy to a Test environment first and then, once we are happy with the outcome, promote to a User Acceptance environment or directly to Production. Continua CI allows you to deploy a release previously created by a Create action in the same build event handler, the highest release version in the project or a specific release version. It's up to you to ensure that the release version exists before the deploy action is run. An environment can consist of multiple machines - you can specify which machines you want to deploy to. If no machines are specified then the release will be deployed to all machines in the environment. When selecting the Build Event for deployment, ensure that it is triggerred after the package has been built and pushed to the Octopus Deploy server. Here we have set this to be run once the Build stage has completed successfully. Promoting a release You can promote the latest release from one environment to another. Ideally this would be linked to the promotion of a stage e.g. a testing stage, so that the application can be promoted from a test environment to production environment. We have set our test stage to require manual promotion; and set our promote action to run when a build is continued after waiting for promotion. Variables You can also pass variables from Continua CI to your deployment, these will be sent to the Octopus Deploy server before each action is run, updating the variables for the deployment project. We have used expressions is this example to send the build versions number and branch name. These variables can then be used to update project files with details for display or configure services differently depending on the source of the project. Running the configuration Once your build event handler dialog has been completed and saved, its time to start running the configuration. As the build processes Continua CI will display status information mirroring the process running on Octopus Deploy. You can also see full details of the deployment process in the build log. And all going well you will now see a successful deployment on your Octopus Deploy server! So you've got your Continua CI server set up to automatically build, run unit tests and produce reports for your awesome new web application. Now you're ready to try out your project in its natural environment and then eventually release it to the wild for well-deserved public applause.Up until now, your options were either to use a Copy action to push the files up to test server and a PowerShell action to set up web services, or preferably run a FinalBuilder script utilising the plethora of actions available for transferring files and interacting with web servers.As of version 1.5, Continua CI can also work together with Octopus Deploy server to provide an end-to-end continuous delivery mechanism. Using the new build event handlers feature, Continua CI builds can now be set up to create Octopus Deploy releases and initiate deployment to test and production environments, at key points in the build process.This post will walk through the steps required to push a .Net web application built in Continua to Octopus Deploy and trigger a deployment process to effortlessly get your application running on your test and production servers.Octopus Deploy requires that you provide your applications as NuGet packages . You can create and push the package to the Octopus Deploy server using Nuget Pack and Push actions, or create and push an OctoPack from MSBuild or VisualStudio build runner actions.Lets go with the recommended OctoPack option. First prepare your Visual Studio solution - use the NuGet package manager to install the OctoPack package into the projects you want to deploy. This will include web application projects, console application projects and Windows service projects but not class libraries and unit test projects.You can now optionally add a .nuspec file to the root folder of your project to describe the contents of your package. If you don't provide a .nuspec file, OctoPack will automatically create one based on your project settings.Next head over to your Octopus server and set up a deployment project. This should include a \"Deploy a NuGet package\" process step as below.We will set this to retrieve the application package from the built-in NuGet feed. Note that the NuGet package id should match the id element in your .nuspec file - this will default to the name of your assembly.We added a few more steps:And some variables:You can now get back to Continua and set up a configuration for building your project. Once you have entered the configuration details and linked up the repository containing your project, move on over to the Stages page:For this simple example you'll need two actions: a NuGet Restore action to ensure that the OctoPack package is available for the build and an MSBuild action to build and push the application to your Octopus Deploy server.Just enter the path to your solution for the NuGet Restore action (the other fields can be left as is) and complete the main tab of the MSBuild action as required for your project.You then need to enter some additional properties to tell MSBuild to run OctoPack and tell it where to send your package.Set the RunOctoPack property to true and the OctoPackPublishPackageToHttp property to the URL for the NuGet feed on the Octopus Deploy server e.g. http://octopusserver/nuget/packages You will also need to provide an OctoPackPublishAPIKey property – you can generate an API key on your profile page on the Octopus Deploy server.ou can use the OctoPackPackageVersion to specify up the package version. Here we use Continua expressions to set this based on the build version. If you leave this out then OctoPack will get this value from the AssemblyVersion property in your AssemblyInfo.cs file.Once the actions are set up and saved, run a build and check that your package gets uploaded to the Octopus Deploy server. Then create a release for your deployment project and test that it deploys ok. Now we are ready to look into how to run this process automatically from Continua CI.On the Continua CI configuration wizard after Stages, we have a new area titled Events. Here you can add Build Event Handlers for tagging repository changesets, updating the GitHub status and interacting with Octopus Deploy.Create a new build event handler, give it a name and select the Octopus Deploy as the Type.You can now provide general project details under the Octopus Deploy tab.The Octopus Deploy URL should end with '/api' e.g. http://octopusserver/api. Enter the API key generated under your Octopus Deploy profile and the name of your deployment project.You can then choose one or more actions to run. The available options areandand are used to create a new deployment release, deploy a release to an environment and promote a release from one environment to another. As you select each action, new tabs open so you can provide further details and hook the action to a build event.Before you can deploy an application you need to create a Octopus Deploy releaseWhen creating a release you can specify the Release Version or leave this blank to automatically create a number based on the highest package version. You must provide either a Default Package Version or Step Package Versions for each step which requires one e.g.Flip over to the Create Options tab to tell Continua when to create the release.There are six Build Events available to choose from. Some allow you to select a Stage and some allow you to select a successful or failed Build StatusGenerally we want to create the release at the start of the build before the first stage starts.Now on to the crux of this whole process - deploying your application. We generally deploy to a Test environment first and then, once we are happy with the outcome, promote to a User Acceptance environment or directly to Production. Continua CI allows you to deploy a release previously created by a Create action in the same build event handler, the highest release version in the project or a specific release version. It's up to you to ensure that the release version exists before the deploy action is run. An environment can consist of multiple machines - you can specify which machines you want to deploy to. If no machines are specified then the release will be deployed to all machines in the environment.When selecting the Build Event for deployment, ensure that it is triggerred after the package has been built and pushed to the Octopus Deploy server. Here we have set this to be run once the Build stage has completed successfully.You can promote the latest release from one environment to another. Ideally this would be linked to the promotion of a stage e.g. a testing stage, so that the application can be promoted from a test environment to production environment.We have set our test stage to require manual promotion;and set our promote action to run when a build is continued after waiting for promotion.You can also pass variables from Continua CI to your deployment, these will be sent to the Octopus Deploy server before each action is run, updating the variables for the deployment project. We have used expressions is this example to send the build versions number and branch name. These variables can then be used to update project files with details for display or configure services differently depending on the source of the project.Once your build event handler dialog has been completed and saved, its time to start running the configuration. As the build processes Continua CI will display status information mirroring the process running on Octopus Deploy.You can also see full details of the deployment process in the build log.And all going well you will now see a successful deployment on your Octopus Deploy server!","tags":"ciandcd"},{"url":"http://www.ciandcd.com/using-skip-and-promote-conditions-in-continua-ci-15.html","title":"Using Skip and Promote Conditions in Continua CI 1.5","text":"From: https://www.finalbuilder.com/resources/blogs/postid/713/using-skip-and-promote-conditions-in-continua-ci-15 Skip Conditions Skip Conditions allow you to controll whether a Stage is skipped or run based on expressions. All the expressions must evaluate to true for the stage to run (if there are no expressions then the stage will not be skipped). In the above example, we have a Stage called Obfuscate, and we want it to be skipped if you turn off obfuscation (by setting a variable) or if we are not deploying a build (again, controlled by a variable). You can also disable a stage completely so it is always skipped. Promote Conditions In Continua CI 1.0, you can chose if the next Stage is automatically run, or the builds stops and requires a manual promotion to continue to the next Stage. In Continua CI 1.5, Promote Conditions allow you control whether to automatically promote or not, based on expressions. All the expressions must evaluate to true for the build to continue to the next stage (if there are no expressions then the build will stop with a status of waiting for promotion). In the above example, our build will continue on to the next Stage if the Deploy variable is set to true. Continua CI 1.5 is currently in Beta - you can get it here : http://www.finalbuilder.com/downloads/continuaci/continuaci-beta-version-history One of the most asked for features in Continua CI 1.0 was the ability to control which stages run, ie the ability to skip stages dynamically, based on what happened earlier in the build, and to be able to control whether the build should continue on to the next stage or wait for user intervention. In Continua CI 1.5, we made this possible with Skip and Promote Conditions.Skip Conditions allow you to controll whether a Stage is skipped or run based on expressions. All the expressions must evaluate to true for the stage to run (if there are no expressions then the stage will not be skipped).In the above example, we have a Stage called Obfuscate, and we want it to be skipped if you turn off obfuscation (by setting a variable) or if we are not deploying a build (again, controlled by a variable). You can also disable a stage completely so it is always skipped.In Continua CI 1.0, you can chose if the next Stage is automatically run, or the builds stops and requires a manual promotion to continue to the next Stage. In Continua CI 1.5, Promote Conditions allow you control whether to automatically promote or not, based on expressions.In the above example, our build will continue on to the next Stage if the Deploy variable is set to true.Continua CI 1.5 is currently in Beta - you can get it here :","tags":"ciandcd"},{"url":"http://www.ciandcd.com/mocking-multiple-interfaces.html","title":"Mocking Multiple Interfaces","text":"From: https://www.finalbuilder.com/resources/blogs/postid/716/mocking-multiple-interfaces-delphi-mocks Today we updated Delphi Mocks to enable the Mocking of multiple interfaces. This is useful when the interface you wish to Mock is cast to another interface during testing. For example you could have the following system you wish to test. type {$M+} IVisitor = interface; IElement = interface ['{A2F4744E-7ED3-4DE3-B1E4-5D6C256ACBF0}'] procedure Accept(const AVisitor : IVisitor); end; IVisitor = interface ['{0D150F9C-909A-413E-B29E-4B869C6BC309}'] procedure Visit(const AElement : IElement); end; IProject = interface ['{807AF964-E937-4A8A-A3D2-34074EF66EE8}'] procedure Save; function IsDirty : boolean; end; TProject = class(TInterfacedObject, IProject, IElement) protected function IsDirty : boolean; procedure Accept(const AVisitor : IVisitor); public procedure Save; end; TProjectSaveCheck = class(TInterfacedObject, IVisitor) public procedure Visit(const AElement : IElement); end; {$M-} implementation { TProjectSaveCheck } procedure TProjectSaveCheck.Visit(const AElement: IElement); var project : IProject; begin if not Supports(AElement, IProject, project) then raise Exception.Create('Element passed to Visit was not an IProject.'); if project.IsDirty then project.Save; end; The trouble previously was that when testing TProjectSaveCheck a TMock<IElement> would be required, as well as a TMock<IProject>. This is brought about by the Visit procedure requiring the IElement its passed to be an IProject for the work its going to perform. This is now very simple with the Implement<I> method available off TMock<T>. For example to test that Save is called when IsDirty returns true, the following test could be written; procedure TExample_InterfaceImplementTests.Implement_Multiple_Interfaces; var visitorSUT : IVisitor; mockElement : TMock<IElement>; begin //Test that when we visit a project, and its dirty, we save. //CREATE - The visitor system under test. visitorSUT := TProjectSaveCheck.Create; //CREATE - Element mock we require. mockElement := TMock<IElement>.Create; //SETUP - Add the IProject interface as an implementation for the mock mockElement.Implement<IProject>; //SETUP - Mock project will show as dirty and will expect to be saved. mockElement.Setup<IProject>.WillReturn(true).When.IsDirty; mockElement.Setup<IProject>.Expect.Once.When.Save; //TEST - Visit the mock element to see if our test works. visitorSUT.Visit(mockElement); //VERIFY - Make sure that save was indeed called. mockElement.VerifyAll; end; The Mock mockElement \"implements\" two interfaces IElement, and IProject. IElement is done via the constructor, and IProject is added through the Implement<I> call. The Implement<I> call adds another sub proxy to the mock object. This sub proxy then allows all the mocking functionality to be performed with the IProject interface. To access the Setup, and Expects behaviour there are overloaded generic calls on TMock. These return the correct proxy to interact with, and generic type ISetup<I> and IExpect<I>. This is seen in the call to mockElement.Setup<IProject>. This returns a ISetup<IProject> which allows definition of what should occur when IProject is used from the Mock. This feature is really useful when there is a great deal of casting of interfaces done in the system you wish to test. It can save having to mock base classes directly where multiple interfaces are implemented. The way this works under the hood is fairly straight forward. TVirtualInterfaces are used when an interface is required to be mocked. This allows the capturing of method calls, and the creation of the interface instance when its required. The Implement<I> functionality simply extends this so that when a TProxyVirtualInterface (inherited from TVirtualInterface) has QueryInterface called it also looks to its owning Proxy. If any other Proxies implement the requested interface its that TProxyVirtualInterface which is returned. In essence this allows us to fake the Mock implementing multiple interfaces, when in fact there are a list of TVirtualInterface's all implementing a single interface.","tags":"ciandcd"},{"url":"http://www.ciandcd.com/filtering-tests.html","title":"Filtering Tests","text":"From: https://www.finalbuilder.com/resources/blogs/postid/717/dunitx-updated-filtering-tests Still evolving DUnitX is still quite young, and still evolving. One of the features most often requested (other than the gui runner, which is still planned) is the ability to select which tests to run. I found myself wishing for that feature recently. I never missed it while the number of my tests were relatively small and fast, but as time went by, it was taking longer and longer to debug tests. So, time to add filtering of fixtures and tests. The command options support in DUnitX was to be honest, quite useless and poorly though out. So my first task was to tackle how options were set/used in DUnitX, and find an extensible way of handling command line options. The result turned out better than I exepected, so I have published a separate project for that. VSoft.CommandLine is a very simple library for defining and parsing command line options, which decouples the definition and parsing from where the parsed values are stored. I'll blog about this library separately. I did try to avoid breaking any existing test projects out there. To invoke the command line option parsing, you will need to add a call to TDUnitX.CheckCommandLine; at the start of you project code, eg: begin try TDUnitX.CheckCommandLine; //Create the runner runner := TDUnitX.CreateRunner; The call should be inside the try/except because it will throw exceptions if any errors are found with the command line options. I modified the IDE Expert to include the needed changes in any new projects it creates, I recommend running the expert to generate a project and then compare it to your existing dpr. Filtering The next thing to look at was how to apply filtering. After much experimentation, I eventually settled on pretty much copying how NUnit does it. I ported the filter and CategoryExpression classes from NUnit, with a few minor mods needed to adapt them to our needs. The cool thing here is I was able to port the associated unit tests over with ease! There are two types of filters, namespace/fixture/test filters, and category filters. Namespace/Fixture/Test filtering The new command line options are : --run - specify which Fixtures or Tests to run, separate values with a comma, or specify the option multiple times eg: --run:DUnitX.Tests.TestFixture,DUnitX.Tests.DUnitCompatibility.TMyDUnitTest eg: If you specify a namespace (ie unit name or part of a unit name) then all fixtures and tests matching the namespace will run. Category Filters A new CategoryAttribute allows you to a apply categories to fixtures and/or tests. Tests inherit their fixture's categories, except when they have their own CategoryAttribute. You can specify multiple categories, separated by commas, eg: [TestFixture] [Category('longrunning,suspect')] TMyFixture = class public [Test] procedure Test1; [Test] [Category('fast')] procedure Test2; In the above example, Test1 would have \"longrunning\" and \"suspect\" categories, whilst Test2 would have just \"fast\". You can filter tests using these categories, using the --include and/or --exclude command line options. When both options are specifies, all the tests with the included categories are run, except for those with the excluded categories. The following info is copied from the NUnit doco (on which these options are based) : Expression Action A|B|C Selects tests having any of the categories A, B or C. A,B,C Selects tests having any of the categories A, B or C. A+B+C Selects only tests having all three of the categories assigned A+B|C Selects tests with both A and B OR with category C. A+B-C Selects tests with both A and B but not C. -A Selects tests not having category A assigned A+(B|C) Selects tests having both category A and either of B or C A+B,C Selects tests having both category A and either of B or C As shown by the last two examples, the comma operator is equivalent to | but has a higher precendence. Order of evaluation is as follows: Unary exclusion operator (-) High-precendence union operator (,) Intersection and set subtraction operators (+ and binary -) Low-precedence union operator (|) Note : Because the operator characters have special meaning, you should avoid creating a category that uses any of them in it's name. For example, the category \"db-tests\" could not be used on the command line, since it appears to means \"run category db, except for category tests.\" The same limitation applies to characters that have special meaning for the shell you are using. I have also fixed some other minor issues with the naming of repeated tests and test cases to allow them to work with the filter. Other options Once you have added the command line check, run yourexe /? to see the other command line options available. None of the options are required so running the exe without any options will behave as it did before. Delphi 2010 Resolved - Thanks to Stefan Glienke for figuring this out - D2010 now support again . This fix was to remove any use of of STRONGLINKTYPES. One thing of note: at the moment these changes break our D2010 support. I get a linker error when I build : [DCC Fatal Error] F2084 Internal Error: L1737 Interestingly, the resulting executable is produced and does seem to run ok, however it makes debugging tests impossible, and of course it would fail in automated build. I did spend several hours trying to resolve this error but got nowhere. Since my usage of DUnitX is currently focused on XE2, I'm willing to live with this and just use an older version of DUnitX for D2010. I have tested with XE2, XE5 and XE6.","tags":"ciandcd"},{"url":"http://www.ciandcd.com/introducing-vsoftcommandlineparser-for-delphi.html","title":"Introducing VSoft.CommandLineParser for Delphi","text":"From: https://www.finalbuilder.com/resources/blogs/postid/719/introducing-vsoftcommandline-for-delphi Command line parsing Pretty much every delphi console application I have ever written or worked on had command line options, and every one of the projects tried different ways for defining and parsing the supplied options. Whilst working on DUnitX recently, I needed to add some command line options, and wanted to find a nice way to add them and make it easy to add more in the future. The result is VSoft.CommandLineParser (copies of which are included with the latest DUnitX). Defining Options One of the things I really wanted, was to have the parsing totally decoupled from definition and the storage of the options values. Options are defined by registering them with the TOptionsRegistry, via TOptionsRegistry.RegisterOption<T> - whilst it makes use of generics, only certain types can be used, the types are checked at runtime, as generic constraints are not flexible enough to specify which types we allow at compile time. Valid types are string, integer, boolean, enums & sets and floating point numbers. Calling RegisterOption will return a definition object which implements IOptionDefinition. This definition object allows you to set various settings (such as Required). When registering the option, you specify the long option name, the short option name, help text (will be used when showing the usage) and a TProc<T> anonymous method that will take the parsed value as a parameter. procedure ConfigureOptions; var option : IOptionDefintion; begin option := TOptionsRegistry.RegisterOption<string>('inputfile','i','The file to be processed', procedure(value : string) begin TSampleOptions.InputFile := value; end); option.Required := true; option := TOptionsRegistry.RegisterOption<string>('outputfile','o','The processed output file', procedure(value : string) begin TSampleOptions.OutputFile := value; end); option.Required := true; option := TOptionsRegistry.RegisterOption<boolean>('mangle','m','Mangle the file!', procedure(value : boolean) begin TSampleOptions.MangleFile := value; end); option.HasValue := False; option := TOptionsRegistry.RegisterOption<boolean>('options','','Options file',nil); option.IsOptionFile := true; end; For options that are boolean in nature, ie they have do not value part, the value passed to the anonymous method will be true if the option was specified, otherwise the anonymous method will not be called. The 'mangle' option in the above example shows this scenario. You can also specify that an option is a File, by setting the IsOptionFile property on the option definition. This tells the parser the value will be a file, which contains other options to be parsed (in the same format as the command line). This is useful for working around windows command line length limitations. Currently the parser will accept -option:value --option:value /option:value Note the : delimiter between the option and the value. Unnamed parameters are registered via the TOptionsRegistry.RegisterUnNamedOption<T> method. Unlike named options, unnamed options are positional, but only when more than one is registered, as they will be passed to the anonymous methods in the order they are registered. Parsing the options. Parsing the options is as simple as calling TOptionsRegistry.Parse, which returns a ICommandLineParseResult object. Check the HasErrors property to see if the options were valid, the ErrorText property has the parser error messages. Printing Usage If the parser reports errors, then typically you would show the user what the valid options are and exit the application, e.g: parseresult := TOptionsRegistry.Parse; if parseresult.HasErrors then begin Writeln(parseresult.ErrorText); Writeln('Usage :'); TOptionsRegistry.PrintUsage( procedure(value : string) begin Writeln(value); end); end else ..start normal execution here The TOptionsRegistry.PrintUsage makes it easy to print the usage to the command line. When I started working on this library, I found some really complex libraries (mostly .net) out there with a lot of options, but I decided to keep mine as simple as possible and only cover off the scenarios I need right now. So it's entirely possible this doesn't do everything people might need, but it's pretty easy to extend. The VSoft.CommandLineParser library (just three units) is open source and available on Github, with a sample application and unit tests (DUnitX) included.","tags":"ciandcd"},{"url":"http://www.ciandcd.com/continua-15-released.html","title":"Continua 1.5 released","text":"From: https://www.finalbuilder.com/resources/blogs/postid/721/continua-15-released Continua Version 1.5 is now available for download Today marks a milestone in Continua CI as we release version 1.5 of the product. Its been many months in the making, we hope you enjoy the update as much as we enjoyed making it. There are many features that we think you'll benefit from by updating to v1.5, some of these include: Reworked UI: Now using bootstrap framework for styling Redesigned dashboards that show more information including graphs. Added stages information to the Project tile and list views Disabled configurations are now displayed as faded Cloning: Can now clone whole projects and clone configurations between projects. Stage Conditions: Auto Promote conditions - stages can now use conditions to control whether to auto-promote to the next stage. Skip conditions - you can now provide conditions for skipping stages or disable a stage completely. New Actions: Update GitHub Status Action is now deprecated (use event handler instead - see below). NuGet restore action . Fake (F#) build runner. Repository Tags: (Git, Mercurial and Subversion repositories only) Continua CI can now detect and list repository tags. Tags are now displayed in changeset tabs on configuration and build views. Repository trigger can now be set to trigger on tag changes (new tags, edits and deletions) changes). You can now run a build on a tagged changeset . Octopus Deploy: Create/Deploy/Promote Octopus Deploy releases. Tag Repository Changesets: Apply tags to a repository changeset (Git, Mercurial and Subversion repositories only) Update GitHub Status: replaces the Update GitHub Status action . and many more changes including: Styling changes for improved handling on small screen sizes Report ordering: you can choose which one is displayed first on the build view. New expression functions: Utils.NewGuid() and Utils.RandomNumber() can be used for generation of temporary paths for example Additional LatestChangeset object within the repository object with Branch, BranchName, Comment, CommitterUserName, CommitterFullName, Created, FileCount, Id and RepositoryUsername properties to use in expressions Continua now supports DUnitX enhanced Command Line Options Updating Updating to this new release is the same regardless if you are using v1.0.X or a recent build from the beta or release candidate. Simply download the installer and run it, the setup will guide you through the install process. As usual we are available on support@finalbuilder.com if you run into any troubles. For this release you will need to update both the server and agents. A word of thanks The team wishes to thank everyone who has participated in the beta and release candidate stages for this release. Your positive feedback has been invaluable in shaping the features and functionality of the product. Thank you for your continued support.","tags":"ciandcd"},{"url":"http://www.ciandcd.com/how-to-fix-your-system-path-after-installing-delphi.html","title":"How to Fix your system path after installing Delphi","text":"From: https://www.finalbuilder.com/resources/blogs/postid/722/how-to-fix-your-system-path-after-installing-delphi Update : Still the same situation with XE8. The Windows Path environment variable has a limit of 1023 * 2,048 characters, a stupidly short limit in this day and age, and when this limit is exceeded the path is truncated. Why this limit still exists on windows I have no idea.. for that matter why it ever existed... anyway, we're stuck with it (along with it's best buddy, MAX_PATH). Each version of Delphi adds over 200 characters to your system path. Worst still, they add those 200+ characters to the front of the path, not the end. What happens, is that eventually, important entries get truncated off end of the path, and strange things happen . You will find programs will not run, the task bar displays the wrong icons for programs, even getting to the control panel can be problematic. If you look at the entries that XE7 added to the start of your path you will see something like this : C:\\Program Files (x86)\\Embarcadero\\Studio\\15.0\\bin; C:\\Program Files (x86)\\Embarcadero\\Studio\\15.0\\bin64; C:\\Users\\Public\\Documents\\Embarcadero\\Studio\\15.0\\Bpl; C:\\Users\\Public\\Documents\\Embarcadero\\Studio\\15.0\\Bpl\\Win64; Fortunately this can be shortened by the use of junction points. Sadly, this will polute your C: drive with new folders but it's better than the alternative. The trick is to create links for the most common paths, so on mine I created these For XE5 and earlier : mklink /j RS C:\\Program Files (x86)\\Embarcadero\\RAD Studio mklink /j rspub C:\\Users\\Public\\Documents\\RAD Studio XE6 and above mklink /j Studio C:\\Program Files (x86)\\Embarcadero\\Studio mklink /j spub C:\\Users\\Public\\Documents\\Embarcadero\\Studio Once you have those junction points, you can then edit your path and replace the long paths, for example (for XE7) : C:\\Program Files (x86)\\Embarcadero\\Studio\\15.0\\bin -> C:\\Studio\\15.0\\bin C:\\Program Files (x86)\\Embarcadero\\Studio\\15.0\\bin64 -> C:\\Studio\\15.0\\bin64 C:\\Users\\Public\\Documents\\Embarcadero\\Studio\\15.0\\Bpl -> C:\\spub\\15.0\\Bpl C:\\Users\\Public\\Documents\\Embarcadero\\Studio\\15.0\\Bpl\\Win64 - > c:\\spub\\15.0\\Bpl\\Win64 So for XE7, that cuts it down from 218 to 80 characters, if like me you also have multiple versions of Rad Studio installed, this can be a big saving. As for Rad Studio, it's extremely rude to add things to the start of the path, the truncation it causes can ruin a machine.. I wasted several hours again today after installing XE7. Embarcadero were told about this issue many times over several releases.. according to the PLEASE ADD IT TO THE END!! and save us all a bunch of time. * Correction, max path length is 2048 - very difficult to find a difinitive source of this information on the microsoft site - the max size of an env variable is 2048, however I have seen the path variable truncated at 1024 many times. Each version of Delphi adds over 200 characters to your system path. Worst still, they add those 200+ characters to the front of the path, not the end. What happens, is that eventually, important entries get truncated off end of the path, and. You will find programs will not run, the task bar displays the wrong icons for programs, even getting to the control panel can be problematic.If you look at the entries that XE7 added to the start of your path you will see something like this :C:\\Program Files (x86)\\Embarcadero\\Studio\\15.0\\bin;C:\\Users\\Public\\Documents\\Embarcadero\\Studio\\15.0\\Bpl;C:\\Users\\Public\\Documents\\Embarcadero\\Studio\\15.0\\Bpl\\Win64;Fortunately this can be shortened by the use of junction points. Sadly, this will polute your C: drive with new folders but it's better than the alternative.The trick is to create links for the most common paths, so on mine I created theseFor XE5 and earlier :mklink /j RS C:\\Program Files (x86)\\Embarcadero\\RAD Studiomklink /j rspub C:\\Users\\Public\\Documents\\RAD StudioXE6 and abovemklink /j Studio C:\\Program Files (x86)\\Embarcadero\\StudioOnce you have those junction points, you can then edit your path and replace the long paths, for example (for XE7) :So for XE7, that cuts it down from 218 to 80 characters, if like me you also have multiple versions of Rad Studio installed, this can be a big saving.As for Rad Studio, it's extremely rude to add things to the start of the path, the truncation it causes can ruin a machine.. I wasted several hours again today after installing XE7. Embarcadero were told about this issue many times over several releases.. according to the doco , XE7 will popup a message about this.. I didn't see it so not sure when that is supposed to appear, but in any event, if your installer or progam needs to add something to the path environment variable,and save us all a bunch of time.Correction, max path length is 2048 - very difficult to find a difinitive source of this information on the microsoft site - the max size of an env variable is 2048, however I have seen the path variable truncated at 1024 many times.","tags":"ciandcd"},{"url":"http://www.ciandcd.com/finalbuilder-8-beta.html","title":"FinalBuilder 8 Beta","text":"From: https://www.finalbuilder.com/resources/blogs/postid/729/finalbuilder-8-beta What's new in FinalBuilder 8 IDE Themes It's almost 5 years since FinalBuilder 7 was released. Since it's release we have shipped 44 official updates , nearly every update including new features or improvements. This program of continuous improvement has worked well, with customers not having to wait for major new versions to arrive to get support for new versions of Visual Studio or Delphi etc, but it has limited our ability to make major changes. So it's time for a new major version of FinalBuilder. The IDE has two new themes, Dark and Light (yes, imaginatively named!). The IDE defaults to Dark on first run, however you can change the theme in the options quite easily. The themes are still a work in progress, we are waiting on an update from a third party control vendor to resolve some issues. Debugger One of the most asked for features now available in FinalBuilder 8, stepping into included projects . In FinalBuilder 7 and earlier, you could only step over included projects, and wait for them to return. In FinalBuilder 8, you can step into the included project, if it is not already opened the IDE will open the project and switch to it automatically. To make this possible, there are now \"Step Into\" and \"Step Over\" functions. The Step into/over now also applies to targets (see below). Debugger breakpoints now have conditions : Actionlists renamed to Targets ActionLists have been renamed to Targets. Targets can now also define dependencies, so you can for example define Clean, Build, Test, and have Test depend on Build. If you execute the Test target, and Build has not already been executed, it will be executed first before Test. Targets can be specified on the command line. In FinalBuilder 7 and earlier, projects had a Main and an OnFailure (global error handler) actionlist. In FinalBuilder 8, projects just have a Default Target. Older projects will be imported such that the Main and OnFailure Targets are called from the Default Target inside a try/catch block. Run Target Action You can now return values from Targets (ie out parameters) . New Help System The help has moved online in the form of a wiki. This enables us to do inline help updates without needing to ship new builds. The new help is still being worked on, lots of screenshots are missing etc.. Non Visible Changes Stepping Engine The stepping engine was rewritten to enable stepping into included projects, and to enable target dependencies. This, work, together with the new variables architecture is where the bulk of effort/time was spent in the FinalBuilder 8 development cycle. Variables Architecture The variables architecture and the expression evaluator were rewritten to resolve several corner case issues that we were not able to resolve in FinalBuilder 7. The expression evaulator has a new parser that will allow us to more easily extend the syntax in the future. The User variable namespace was removed, it caused too many problems with projects not running under other users, not running on the build server etc. Use Project variables instead. Core Messaging Changes to the messaging has allowed us to improve the performance of the stepping engine and logging, with much less thread switching. This also improved the IDE performance. CLR Hosting The minimum CLR version is now .NET 4.0 (ie FinalBuilder requires .net 4.0 to be installed). Code Changes In addition to the architectural changes, we also spent a lot of time refactoring the code, running static analysis tools over the source, looking for memory leaks, potential bugs etc. One of the results of this is reduced memory usage during a build compared to FB7. The FB8 IDE does use slightly more memory than the FB7 IDE at startup (mostly due to the heavy use of delphi generics), however the runtime memory usage is much lower.A large part of the refactoring involved unit testing (we created a new unit test framework to suite our needs!) and creating a suite of integration tests. FBCmd The command line parameters have changed to be more consistent and easier to specify. You can also specify one or more targets to execute (when not specified, the default target is executed). New Project File Formats FinalBuilder has used an xml file format since version 1, however a common complaint over the years, has been that it is difficult to diff file versions. FinalBuilder 8 has tackled this in two ways. A new DSL style project file format (.fbp8) is now the default format, it is very easy to diff. project begin projectid = {04710B72-066E-46E7-84C7-C04A0D8BFE18} target begin name = Default targetid = {E6DE94D6-5484-45E9-965A-DB69885AA5E2} rootaction begin action.group begin id = {D860420B-DE46-4806-959F-8A92A0C86429} end end end end A new xml format (.fbx8), much less verbose than the old format. <?xml version=\"1.0\" encoding=\"UTF-8\"?> <finalbuilder> <project> <projectid>{6A717C24-D00F-4983-9FD0-148B2C609634}</projectid> <target> <name>Default</name> <targetid>{E6DE94D6-5484-45E9-965A-DB69885AA5E2}</targetid> <rootaction> <action.group> <id>{D860420B-DE46-4806-959F-8A92A0C86429}</id> </action.group> </rootaction> </target> </project> </finalbuilder> Compressed project files (.fbz8) use the dsl format internally (compressed projects are just a zip file with a project.fbp8 inside it). The default project file encoding is now UTF-8, which is more version control friendly (some version control systems treat utf-16 as binaries). New Actions There are no new actions at the moment, although several are in development, they will be added to the beta builds as they are completed. How do I get the Beta? Links to the beta downloads will be published to the What if I find a bug? We have created a We are particularly keen for people to load up their existing projects from older (ie 7 or earlier) versions of FinalBuilder, save them in FB8 format, and load them again and confirm that everything loaded ok. When will it be released? When it's ready ;) Links to the beta downloads will be published to the FinalBuilder Downloads page.We have created a Beta forum on our forums, or you can email support (please added Beta to the subject). When reporting an issue, be sure to include the beta build number and details about your environment. Please test with the latest beta build before reporting bugs.We are particularly keen for people to load up their existing projects from older (ie 7 or earlier) versions of FinalBuilder, save them in FB8 format, and load them again and confirm that everything loaded ok.When it's ready ;)","tags":"ciandcd"},{"url":"http://www.ciandcd.com/adding-ntlm-sso-to-nancyfx.html","title":"Adding NTLM SSO to Nancyfx","text":"From: https://www.finalbuilder.com/resources/blogs/postid/730/adding-ntlm-sso-to-nancyfx Nancyfx is a Lightweight, low-ceremony, framework for building HTTP based services on .Net and Mono. It's open source and available on github. Nancy supports Forms Authentication, Basic Authentication and Stateless Authentication \"out of the box\", and it's simple to configure. In my application, I wanted to be able to handle mixed Forms and NTLM Authentication, which is something nancyfx doesn't support. We have done this before with asp.net on IIS, and it was not a simple task, involving a child site with windows authentication enabled while the main site had forms (IIS doesn't allow both at the same time) and all sorts of redirection. It was painful to develop, and it's painful to install and maintain. Fortunately with Nancy and Owin , it's a lot simpler. Using Microsoft's implementation of the Owin spec, and Nancy's Owin support, it's actually quite easy, without the need for child websites and redirection etc. I'm not going to explain how to use Nancy or Owin here, just the part needed to hook up NTLM support. In my application, NTLM authentication is invoked by a button on the login page (\"Login using my windows account\") which causes a specific login url to be hit. We're using Owin for hosting rather than IIS and Owin enables us to get access to the HttpListener, so we can control the authentication scheme for each url. We do this by adding an AuthenticationSchemeSelectorDelegate. internal class Startup { public void Configuration(IAppBuilder app) { var listener = (HttpListener)app.Properties[\"System.Net.HttpListener\"]; //add a delegate to select the auth scheme based on the url listener.AuthenticationSchemeSelectorDelegate = request => { //the caller requests we try windows auth by hitting a specific url return request.RawUrl.Contains(\"loginwindows\") ? AuthenticationSchemes.IntegratedWindowsAuthentication : AuthenticationSchemes.Anonymous; } app.UseNancy(); } } What this achieves is to invoke the NTLM negotiation if the \"loginwindows\" url is hit on our nancy application. If the negotiation is successful (ie the browser supports NTLM and is able to identify the user), then the Owin environment will have the details of the user, and this is how we get those details out of Owin (in our bootstrapper class). protected override void ApplicationStartup(TinyIoCContainer container, IPipelines pipelines) { pipelines.BeforeRequest.AddItemToStartOfPipeline((ctx) => { if (ctx.Request.Path.Contains(\"loginwindows\")) { var env = ((IDictionary<string,>)ctx.Items[Nancy.Owin.NancyOwinHost.RequestEnvironmentKey]); var user = (IPrincipal)env[\"server.User\"]; if (user != null && user.Identity.IsAuthenticated) { //remove the cookie if someone tried sending one in a request! if (ctx.Request.Cookies.ContainsKey(\"IntegratedWindowsAuthentication\")) ctx.Request.Cookies.Remove(\"IntegratedWindowsAuthentication\"); //Add the user as a cooking on the request object, so that Nancy can see it. ctx.Request.Cookies.Add(\"IntegratedWindowsAuthentication\", user.Identity.Name); } } return null;//ensures normal processing continues. }); } Note we are adding the user in a cookie on the nancy Request object, which might seem a strange thing to do, but it was the only way I could find to add something to the request that can be accessed inside a nancy module, because everything else on the request object is read only. We don't send this cookie back to the user. So with that done, all that remains is the use that user in our login module Post[\"/loginwindows\"] = parameters => { string domainUser = \"\"; if (this.Request.Cookies.TryGetValue(\"IntegratedWindowsAuthentication\",out domainUser)) { //Now we can check if the user is allowed access to the application and if so, add //our forms auth cookie to the response. ... } } Of course, this will probably only work on Windows, not sure what the current status is for System.Net.HttpListener is on Mono. This code was tested with Nancyfx 1.2 from nuget. What this achieves is to invoke the NTLM negotiation if the \"loginwindows\" url is hit on our nancy application. If the negotiation is successful (ie the browser supports NTLM and is able to identify the user), then the Owin environment will have the details of the user, and this is how we get those details out of Owin (in our bootstrapper class).Note we are adding the user in a cookie on the nancy Request object, which might seem a strange thing to do, but it was the only way I could find to add something to the request that can be accessed inside a nancy module, because everything else on the request object is read only. We don't send this cookie back to the user. So with that done, all that remains is the use that user in our login moduleOf course, this will probably only work on Windows, not sure what the current status is for System.Net.HttpListener is on Mono. This code was tested with Nancyfx 1.2 from nuget.","tags":"ciandcd"},{"url":"http://www.ciandcd.com/jazz-team-blog-ibm-rational-publishing-engine-20-m5-beta-improved-document-styling.html","title":"Jazz Team Blog  IBM Rational Publishing Engine 2.0 M5 Beta – Improved document styling","text":"From: https://jazz.net/blog/index.php/2015/03/25/ibm-rational-publishing-engine-2-0-m5-beta-improved-document-styling/ We're glad to announce that the Rational Publishing Engine (RPE) 2.0 Beta on Bluemix has been updated to the M5 build ! There is no registration or special process required in order to access the beta. Aside from announcing this update, the intention of this post is to provide a little extra help to those looking for guidance on getting started using using the RPE 2.0 M5 Beta via some helpful resources. We'll also touch on what's new in this build of the beta. Goal of the beta This is meant to give RPE users an opportunity to provide feedback on the features and usability of our new web interface. The focus is on the report designer and end user scenarios. What we'd like to learn is: Is the design simple enough for our end users? Do you see all the capabilities that your report designers need? Is the overall interface intuitive enough? What is missing from this new component of RPE to make it attractive for your organization? We welcome all your feedback, so please send your thoughts to rpe20_beta_support@wwpdl.vnet.ibm.com ! Resources Help Guide: IBM Rational Publishing Engine 2 M5 Help Video Tutorials: What's new in this build NOTE : Due to the new features implemented in M5, we had to recreate the database which means that assets and documents created using the previous build have been lost. (i) Notifications The notifications widget at the top of the screen is now active and will show all new events that occurred since you last checked its contents. The only event supported for now is the completion (successful or not) of a document generation. (ii) Stylesheet support You can now upload stylesheets and use them in your reports. (iii) Advanced Configuration Mode for reports Report designers now have access to an advanced edit mode where the report configuration can be tailored in detail. It is now possible to: Set the default connection for the report data sources Set the default values for the report variables Rename variables and data sources to be more meaningful to the end user Hide variables and data sources from the end user In the image above, the report designer modifies the \"News Glimpse\" report by setting its data source connection to the BBC News Feed and at the same time hiding the data source. This will effectively set the report to this configuration and end users can run it without further configuration. (iv) Other changes The action list has been pruned with many of the redundant actions removed or redistributed. The default action in the Design page has been changed to \"Edit\". The user can view all documents generated for a report from the Generate page by clicking \"View Generated documents\" in the Actions menu. Users can create examples multiple times. The assets created in each run of \"Create Examples\" are independent of one another. Internet Explorer 10 and 11 are now supported. Error messages should be more informative now. Bug Fixes The following limitations of the previous build have been addressed: On the Generate page, if you select the Generate Later action, the scheduled run of the report is created but you cannot download the documents or the logs If there is an error while scheduling a report using the Generate Later action, check if the date is in the past. IE is supported Known Issues If after signing on you are redirected to an empty page by the IBM Single Sign-On you need to issue the original request in the browser, https://rpe.mybluemix.net/rpeng","tags":"ciandcd"},{"url":"http://www.ciandcd.com/jazz-team-blog-bluemix-devops-services-is-hiring.html","title":"Jazz Team Blog  Bluemix Devops Services is hiring!","text":"From: https://jazz.net/blog/index.php/2015/04/08/bluemix-devops-services-is-hiring/ We're hiring! IBM is serious about the Cloud. It's where business is headed and you should be there too. Developing and deploying web and mobile applications in a continuous delivery environment is complex and challenging. IBM Bluemix cloud technology is based on open standards such as Cloud Foundry and Docker, with industrial strength software-defined infrastructure provided by IBM SoftLayer. Bluemix DevOps Services combines a core devops tool chain and integrations with popular tools like GitHub, into a single highly productive environment built on a modern micro-services architecture. Work here to develop the future and change the computing landscape. If you have the drive, IBM has the grit and the clout to make it happen. Contact us at idsorg@us.ibm.com Dave Thomson Director and Distinguished Engineer, IBM Bluemix DevOps Services","tags":"ciandcd"},{"url":"http://www.ciandcd.com/jazz-team-blog-big-changes-coming-to-jazznet.html","title":"Jazz Team Blog  Big changes coming to jazz.net","text":"From: https://jazz.net/blog/index.php/2015/04/22/big-changes-coming-to-jazz-net/ You may have noticed some changes when you came to visit Jazz.net today. After 8 years, the jazz.net team thought it was time to give the site a more thorough overhaul then it has had in the past. Keeping Agile and Lean principles in mind, we're starting small and looking to get early feedback on the changes we make. To start, we are rolling out a new look for our Home page, a combined Product and Download landing page, as well as a new navigation bar and footer. For the new look and feel, we want to incorporate current IBM Design principles. This will help us to be more consistent with other IBM sites, and pare down some of the content that has made the pages get more crowded over the years. We also want to have our site be more mobile-friendly, to reflect the growing portion of our audience accessing our site from mobile devices. Over the coming months, we'll start rolling out updates to the rest of the site. In addition to updating the look for all the pages on the site, we'll be reviewing the content in the jazz.net Library and on the Deployment Wiki to make sure the content is up to date. We also want to make sure we have our development team engaged with the community, so look for more content about our Jazz based products over the coming months as well. As I mentioned above, we want to incorporate Agile and Lean principles in this rollout, and this is where you come in. We're looking for your feedback on the changes we're making. Hopefully you like it, and let us know if you do! If there are things we should do differently, or you have other thoughts about the overhaul, we want to hear that as well. The sooner we get the feedback, the better job we can do of incorporating it and making this site as useful to you as possible. So drop a comment on this blog, tell us on Facebook or Twitter ( @JazzDotNet ), or post in the forums using the tag \"jazz-dot-net-overhaul\"! We look forward to hearing from you!","tags":"ciandcd"},{"url":"http://www.ciandcd.com/jazz-team-blog-using-dashboards-for-status-reviews-an-interview-with-clm-product-delivery-lead-brian-lang.html","title":"Jazz Team Blog  Using dashboards for status reviews: An interview with CLM Product Delivery Lead Brian Lang","text":"From: https://jazz.net/blog/index.php/2015/05/12/using-dashboards-for-status-reviews-an-interview-with-clm-product-delivery-lead-brian-lang/ Having experience with different types of reporting and status updates, Brian Lang, the Collaborative Lifecycle Management (CLM) Product Delivery Leader, spent some time discussing with me some of the key productivity gains he's noticed in using Rational Team Concert (RTC). The focus here is on the benefits of having a collaborative and transparent workflow, especially when it comes to meeting preparations. Brian shares with us the benefits of using RTC Dashboards so that the data is available, current, and updated as part of the natural development workflow. Brian, what types of meetings have you needed to pull together presentations in the past and today? In the past, when I was using a chart-based status process, leading up to the Senior VP's Monthly Operations Review (MOR), there would be a review with my VP where we would review the status of everything in my portfolio. This included key dates of delivery, status (red, yellow, green), and any additional key points that we wanted to raise to an executive level. These topics might typically included key capabilities, beta feedback, or what kinds of achievements or accomplishments the team had this month. Also, we prepared risks and mitigations. Each product would have their own chart. Additionally, we covered other things like burn down charts, user stories, velocity, story points, and month to month trends. We also covered various forms of technical debt in the form of APARs, defect backlogs which rounded out the reviews. Now, in CLM, while we still have MORs, the process is different even though the types of data are the same. Rather than create a separate set of material, the materials are baked into our workflow with regular reviews with teams. The same sorts of materials that would have been prepared and reviewed are part of the workflow including scrum meetings, release team meetings, and daily communications and work activities. The schedule is laid out and published for the whole team. Risks and mitigations are captured and updated in plan items. If a plan item's risk changes from green to yellow or red, the viewlet in the dashboard will pull in that plan item, with the latest updates about actions. What kinds of preparation do you do for these meetings? When pulling together charts, we would work our way backwards from the review with the Senior VP. We ended up with four separate meetings to review material specific to the status update. With the CLM dashboards, we can be ready for an update at any time. It's transparent and anyone in the team can check on the overall status because transparency is built into the product and supports our process. How long did it take to create the dashboards? Viewlets are part of the RTC capabilities and some are out of the box. For example, risk and issues are available out of the box. It doesn't take long to set up the queries against project areas. Depending on how many and the complexity, it typically takes less than an hour to get them put up. Once they're there, they are automatically updated, so you don't have to repeat creating them. One example is that we have a tab on our dashboard for risks and issues. When we are tracking multiple releases, like we did with 5.0.2 and 6.0, we can separate them out, while keeping them on the same tab. Going back to collecting the data for the charts, how many people are typically involved in gathering and consolidating that information? First, the offering team pulls together the information. They usually have a release engineer, a project management, a development lead or chief programmer, an architect, a test lead, and a support lead, six people typically. Sometimes there is the first line manager too so about six to seven people gathering and generating content to be reviewed. The managers were the ones who presented the data to me, they are responsible for the content being presented. How many teams did you have for a typical monthly status review? When I was doing chart-based reporting, I had about three key teams. This meant that 18-21 people were preparing for the meeting with me. How long does it take to pull that information together for each team? It takes a couple of hours for each person, approximately two to three hours to verify and answer are we green, yellow, or red?; how many test cases did we run?; what kinds of things are blocking us, etc. That was spent running reports and following up with people for verification. If things are not looking good, we may need more activity, up to maybe four hours. That's about 24 hours per team. Add on a 60-90 minute review in the offering team meeting making up another nine hours of people time (33 hours), plus another three hours for turnaround time on any follow ups. We're looking at maybe 36 – 40 person hours to generate that first set of data for review. Then there is the review with me. The least number of people in the room are the three managers and maybe someone from their team if there was a key technical issue to discuss, so five to six people for another 90 minutes making up another nine to ten hours. Then another set of questions, and we're up to 45-50 hours. We have another review including me and another person or two, the support and development leads, for another hour. After all of that, it adds up to about 60-65 person hours to get ready for the Sr. VP MOR. Talk a little more about how the preparation differs. The preparation is baked into the process. I update the executive status on the dashboard weekly as part of my preparation for the ALM release team weekly meeting. That is where I update my overall take on the release. If we're yellow, I share why I think we're yellow and what we're doing to mitigate the risk. It takes about 15 minutes for me to update that and it's part of my workflow. The release team meeting is where we talk about the release and where we need to focus. It's less about status and more about what the team needs to work on accomplishing that week. The chart-based reporting teams were also working on those things, but they needed to document it outside their natural process. How they met and communicated was a key difference because these types of updates were not part of their daily or weekly workflow. It's hard to do an apples to apples comparison because they are such different ways of working. In the CLM world, everyone can see when risks and issues are updated. In the chart world, the risks and issues were only seen by about 10% of the team. If only a subset of the team understands the risk, it's hard to go to a newly hired developer and ask about the work they were doing on risk \"X\" because they might not know what you're talking about, or know about it in the same context. Also, the \"man hours\" involved may be similar with the dashboards because the whole team is involved in updating issues regularly, but it's a natural part of the workflow so it doesn't feel like anyone on the team is spending time gathering status. We use Jazz.net to build Jazz.net, using the dashboards to drive our work. Once that work is done, we all can see our prioritized backlog to pull off the next thing to do more work. The collaborative nature of the design makes transparency, priorities, and risk part of the process vs ppt where only a select number of people see it. Most meetings result in follow up actions, how are actions from the meetings captured? Actions in the chart world would have been tracked in an email. I'd get an email saying that the VP met with Sr. vP and here are the two to three things to work on based on the review. Then I'd assign the work, and follow up. If my VP doesn't remember, or loses the email, the actions fall through the cracks. We'd report on those actions through email and follows up in status meetings or 1×1s. In the RTC world, actions are captured as work items. We review the actions to pursue which are tracked and prioritized with other work items in RTC. So, if anyone has any questions, they can revert to the work item. There is a live audit trail, when it was opened, who opened it, who owns, what has happened, etc. It becomes part of the workflow. I can also add that in test we have major productivity gains from dashboards usage that are linked to test plans. They help to eliminate the manual chart compilation work and results are kept live and up to date, which is imperative as our releases and test cycles are getting shorter. I'd like to thank Brian for taking the time to discuss this topic. Using RTC for dashboards enables a natural development workflow supporting open communication and transparency while reducing overhead for the organization. Understanding the priority of the work, the status of the work, and the risks of the work all help keep teams focused. By making it part of the natural development workflow it increases productivity. I'm not sure who to attribute this quote to: \"If it hurts, do it more often\" ( I'd like to thank Brian for taking the time to discuss this topic. Using RTC for dashboards enables a natural development workflow supporting open communication and transparency while reducing overhead for the organization. Understanding the priority of the work, the status of the work, and the risks of the work all help keep teams focused. By making it part of the natural development workflow it increases productivity. I'm not sure who to attribute this quote to: \"If it hurts, do it more often\" ( maybe ?). But, it definitely has reduced stress and overhead in this context! Beth Zukowsky Program Director and Rational DevOps Protagonist","tags":"ciandcd"},{"url":"http://www.ciandcd.com/jazz-team-blog-success-with-system-verification-test-svt-automation.html","title":"Jazz Team Blog  Success with System Verification Test (SVT) Automation","text":"From: https://jazz.net/blog/index.php/2015/06/05/success-with-system-verification-test-svt-automation/ Back when the System Verification Test (SVT) team started thinking about Collaborative Lifecycle Management (CLM) 6.0, they put some automation goals in place. Having identified a key bottleneck of long periods of time being spent on setting up the configurations for scenario testing, the first goal was to build automated scripts to speed up the configurations in order for the manual testers to start testing faster and reduce the test cycle time. The second automation goal was aimed more conventionally at uncovering defects in code changes and increasing automated test coverage. This was focused on adding automation to cover PLE scenarios and also to enhancing scenarios to include new features like configuration management. Prior to 2014, System Test focus was on deploying configurations in an automated way. At the beginning of 2014, System Test had one and a half automation engineers who were automating the out of the box customer experience, the Money that Matters scenario. While that goal of ensuring that scenario was mostly automated was achieved by end of year, the process was painful and slow. In light of the DevOps transformation, at the and of 2014, SVT decided to invest more in automation. They added three more engineers to work on this. That wasn't an easy decision, but a necessary one, and it meant taking on a risk as less manual test coverage was provided as a result of this move. 2015 has been a great year and the five engineers working on automation have already commissioned seven automated scenarios this year. These have been focused on data generation, to help manual testers and shorten the test cycle in addition to the automated configurations. There is good communication with development teams. An automation team is mostly useless if you don't get the right amount of support from development when you raise concerns or defects if they don't follow up quickly. The close relationships with development allows the System Team to cover the cutting edge use cases and quickly respond to defects found. Now, turn around time is typically within a day. The teams have come from a time where there was little support for automated testing to today with a fully automated set of tests. They didn't have a BVT, or rather it was manual. Each team would blindly take the build and hope it installed. They moved to automation where the build is created, gets picked up, automation runs jUnits, then it gets deployed to our golden topologies, using different app servers, different tests, and they are all designed according to how customers use the products. The team is also using our tools, Rational Quality Manager (RQM) for test execution and reporting, and Rational Team Concert (RTC) for test development. They have gone from no automation to now where a large number of automated tests are executed daily against both maintenance and new release streams. This has been worth the investment, but it has not been easy. For one thing, there have been some plan changes along the way. Trying to keep up with developing the new automation while product plans are changing makes it difficult to make sure that the teams are covering the right user experience. Also, there are new members to the automation team who need to be trained not only on the development of the automation, but also the framework for the automated tests. There is a process in place to get the automation engineers up to speed quickly, but probably the most important part is working together as a team and starting simple before moving to the more complex areas, like the framework. There is a review process with the team so that everyone learns from one another and if there is a problem the reviewer will identify the area in need of improvement and also make a suggestion about how to solve the issue. Given that there is a good mix of manual testers and now test automation developers, it's important to work together for the least risk and the most coverage. This has paid off for the team, so that the team is building automation to help the manual testers find defects quickly, build automation to find regressions in existing code and to find defects in new features. There has been a significant number of defects found prior to GA so far, and the majority of them have been critical and higher severity. Just knowing that they won't be released is a huge confidence builder for the teams. The System Test team will continue to invest in automation, making use of the automation community that has been built up out of these efforts. The community is geographically distributed, under different leadership and has great collaboration with development. The openness and transparency allows for the development teams to execute newly created automated tests against their code changes prior to delivery. The increased confidence in quality has been and continues to be worth the investment. Beth Zukowsky Program Director and Rational DevOps Protagonist","tags":"ciandcd"},{"url":"http://www.ciandcd.com/jazz-team-blog-raising-your-game-with-configuration-management-in-and-across-your-tools.html","title":"Jazz Team Blog  Raising your game with configuration management in and across your tools","text":"From: https://jazz.net/blog/index.php/2015/06/09/raising-your-game-with-configuration-management-in-and-across-your-tools/ High-performing teams are never satisfied with their performance. They regularly ask, \"How can we do better?\" They have a certain restlessness. A sense of mission, stewardship, and empathy for the people who will use the things they design and build. It's our mission to provide tools and make possible development practices that help your high-performing teams improve their game. To that end, Collaborative Lifecycle Management (CLM) 6.0 brings significant new capabilities for configuration management within and across your Jazz tools — with the potential for tools from other vendors to participate in these innovations. In our days you'd be hard pressed to find a software development team that would undertake serious work without using a SCM system. In CLM 6.0, we are extending configuration management capabilities (including development streams, baselines, branching, merging, change sets, comparing across streams) to other tools, so practitioners in other disciplines can gain the same kinds of efficiencies. We are solving this in an open, federated way through (1) new implementations of configuration management in Rational DOORS Next Generation and Rational Quality Manager; and (2) support for Global Configurations as defined in in the OASIS OSLC Configuration Management specification. We expect these capabilities will help teams be more effective in using baselines, doing parallel development, working in large programs of projects, and doing product line engineering. Look for baby steps you can take. Walk now; run later. To learn more, check out the Continuous Engineering blog on developerWorks, or see the short videos on developerWorks . Daniel Moul Senior Product Manager P.S. Many thanks to those of you who downloaded beta milestones and provided feedback, or sat down with us to share your insights about what your engineers need to raise their game. We know you are on a journey; we are too. It's a privilege to run together.","tags":"ciandcd"},{"url":"http://www.ciandcd.com/jazz-team-blog-configuration-management-its-not-just-for-building-airplanes-or-cars.html","title":"Jazz Team Blog  Configuration management – it's not just for building airplanes or cars","text":"From: https://jazz.net/blog/index.php/2015/06/15/configuration-management-%e2%80%93-it%e2%80%99s-not-just-for-building-airplanes-or-cars/ Can you imagine a software team working without a configuration management system? Could your team do any collaboration without versions, streams and baselines? Do you ever need to reverse a change that introduced a defect? If so, then you need to keep track of versions with a history of who changed what, when, and why. Do you want some of your engineers to be able to work on the next release while the current release is in final stabilization and testing? If so, then you need multiple streams of development. Do you ever want to produce a report on the changes in a new release? If so, then you need to compare the versions in a previous baseline to the new baseline, or to the current state of the development stream. Do you ever need to create a patch to a previous release? If so, you would need a baseline of that previous release, and a way to branch a new stream from it. Do you ever need to try an experiment, or make a one-off change? If so, you need to create a new stream, make some changes there, and then possibly merge the contents of that experimental stream into the main line of development. Software Configuration Management provides these and other benefits, such as improved collaboration and reuse. Wouldn't you want these same capabilities and benefits in your other tools, including requirements management, modeling, and quality testing? DOORS Next Generation 6.0 and Rational Quality Manager 6.0 provide these features, just as Rational Rhapsody Design Manager has done since release 4.0. And now that you have configuration management in each tool, think about all these configuration management concepts but in the broader sense – the ability to do it across the lifecycle – for the overall combination of your requirements, designs, test plans, and more. Global Configurations, introduced in Collaborative Lifecycle Management 6.0, provides the ability to do just that – to keep track of versions, streams, and baselines of your entire system. Whether you are building airplanes, cars, financial systems, web sites, health care systems, or entertainment systems, configuration management is a key part of your path to success. Nick Crossley","tags":"ciandcd"},{"url":"http://www.ciandcd.com/jazz-team-blog-announcing-rational-publishing-engine-20-ga.html","title":"Jazz Team Blog  Announcing Rational Publishing Engine 2.0 GA","text":"From: https://jazz.net/blog/index.php/2015/06/17/announcing-rational-publishing-engine-2-0-ga/ The Rational Publishing Engine 2.0 is now available as a GA download. This major release includes a simplified web interface that will help clients focus on generating documents with minimal steps and also apply effective template reuse within their organization. To learn more about the capabilities of this release, read the \" What's new \" section in our Infocenter documentation for RPE 2.0 . You can try the document generation capabilities by accessing our cloud sandbox on https://rpe.mybluemix.net/rpeng/home . Login with your id and click on \"Create Examples\" for a quick start on using the new web interface. We look forward to your feedback on the design and usability of the system. If there are any use cases in your organization that is not addressed with the current system, please write to us at rpe20_beta_support@wwpdl.vnet.ibm.com In this release, we have adopted IBM Design Thinking methodology for the first time. We have received valuable inputs from our clients and our stakeholders through the design partner program. I would like to thank all our clients who participated in the design partner and in the Beta program. We look forward to your inputs on the GA version of Rational Publishing Engine 2.0.","tags":"ciandcd"},{"url":"http://www.ciandcd.com/jazz-team-blog-new-single-sign-on-options-in-clm-60.html","title":"Jazz Team Blog  New single sign-on options in CLM 6.0","text":"From: https://jazz.net/blog/index.php/2015/06/19/new-single-sign-on-options-in-clm-6-0/ Collaborative Lifecycle Management (CLM) has directly supported two types of single sign-on (SSO) authentication for some time. First, all applications installed in the same application server (whether IBM WebSphere Application Server or Tomcat) automatically share login sessions, such that if you log in to one application, you are also logged into all the other applications deployed in the same server. Second, when all applications are deployed in one or more WebSphere Application Servers, you can configure Lightweight Third Party Authentication (LTPA) SSO so that login sessions are shared across all the WebSphere servers. We are excited to announce that we have added two new SSO options in the CLM 6.0 release. You'll now have the option to use either Kerberos authentication, or what we call \"Jazz Security Architecture Single Sign-On\", which is based on the OpenID Connect standards. Kerberos is a well-established SSO protocol that is also the default authentication protocol used by Microsoft Windows, so if your organization uses Windows workstations, Microsoft Active Directory for user management, and deploy Jazz applications in WebSphere 8 or later, it will be possible to configure CLM so that your Windows login session is automatically used to log in to CLM. Kerberos can also be used with non-Windows workstations, as long as you use a Microsoft Active Directory server to manage your user accounts. While it has been possible to configure CLM servers to use Kerberos for some time, only web browser clients could take advantage of Kerberos login sessions – the RTC Eclipse client, the Microsoft Visual Studio RTC client, the other Windows .NET clients, the RTC build engine and clients, and the various command-line clients could not use Kerberos. Now, all those RTC clients will work with Kerberos. See Single sign-on authentication in CLM for the complete list of RTC clients that support Kerberos, and Configuring Kerberos for details on setting up Kerberos for CLM. The OpenID Connect (OIDC) authentication protocol was established in early 2014 as an extension of the OAuth 2.0 protocol, designed to be easier to adopt across a wide range of clients (native applications, browsers, browser-based applications, and mobile devices). It is extensible and configurable (with optional features). Jazz Security Architecture (JSA) is a particular profile of OIDC, specifying which optional features are included, and a few extensions. Authentication is handled by the Jazz Authorization Server (JAS); Jazz applications delegate to that server instead of relying on the application server to handle authentication. Single sign-on is supported across all applications that are configured to use the same JAS, independent of what sort of application servers they are deployed in, and what platform they are running on. To use Jazz Security Architecture SSO, you must install the Jazz Authorization Server , configure it and start it up , and either enable JSA SSO in CLM applications when installing (for a new installation), or enable JSA SSO after upgrading to the 6.0 release (for existing installations). The login form for the JAS looks very similar to the Jazz application login form, but you'll know that you're using the JAS for authentication if the login form says \"AUTHORIZATION SERVER\" instead of \"TEAM SERVER\": You can find more information on how authentication works in general, and the various options available, in the Jazz Server Authentication Explained article. We're hoping these new SSO options provide increased flexibility and ease-of-use for our users. John Vasta Senior Software Engineer","tags":"ciandcd"},{"url":"http://www.ciandcd.com/jazz-team-blog-introducing-safer-with-the-power-of-ibm-devops.html","title":"Jazz Team Blog  Introducing SAFe® with the Power of IBM DevOps","text":"From: https://jazz.net/blog/index.php/2015/06/24/introducing-safe%c2%ae-with-the-power-of-ibm-devops/ Do you need to orchestrate software development and delivery in your complex, heterogeneous environment but you're not sure where to start? Have you heard about SAFe but you don't know what it is or why you should care? New in Rational Team Concert V6.0 is a feature that supports the Scaled Agile Framework ® (SAFe) out of the box. This feature enables you to explore the framework and establish a SAFe Program of your own, complete with the infrastructure, artifacts, best practices, and guidance prescribed by SAFe built right into the tooling. SAFe is the market-leading process framework for scaling lean and agile across the enterprise, providing guidance and best practices to help organizations realize success. SAFe with the Power of IBM DevOps is the combination of SAFe plus IBM DevOps to provide a comprehensive process and tooling framework that helps your organization balance the efficiencies gained through adoption of lean principles with the effectiveness realized through agile adoption to deliver the right things right . The result is a framework which synchronizes development, testing and deployment across teams in heterogeneous environments, and enables collaboration of all roles in the organization in the planning and execution of software delivery. Get Up and Running Quickly The SAFe support in Rational Team Concert V6.0 provides an easy way for you to get up and running quickly to lead a SAFe-based transformation. Whether you are a SAFe novice who is just starting to explore, or an expert who is quite familiar with what it means to \"do SAFe\", our support combines the flexibility and scale you need to support true enterprise teams using multiple processes, technologies, and tooling. Let's take a look at a few details. Project Area Initialization Rational Team Concert 6.0 includes a new SAFe 3.0 Program process template that helps you to easily establish a SAFe Program-level tooling infrastructure. When you use the template to create a Rational Team Concert project area, you will get out-of-the-box support for Programs and Teams wanting to incorporate agile and lean practices into the development and delivery of software with SAFe best practices and guidance. This includes an Agile Release Train timeline, SAFe roles and permissions, and a SAFe Program/Team hierarchy with associated work item categories. While the project area is configured to support SAFe Programs with Teams in the same Rational Team Concert project area, it can easily be used to support Programs that are tracking the work of existing Teams in separate project areas as well. Furthermore, it can be used to establish project areas for SAFe Teams separate from the Program because the process template includes updated Scrum elements that are aligned with SAFe. Simple editing of the project area configuration provides you with the flexibility you need to support multiple different Program and Team topologies. SAFe Artifacts SAFe artifacts are manifested in Rational Team Concert through work item types, plan types, and plan views. These artifacts support the most critical SAFe best practices, including relative ranking based on Weighted Shortest Job First (WSJF), Kanban planning, and Program value-based delivery. Work Item Types The process template includes Program Epic and Feature work item types for the SAFe Program level, Story and Task work item types for the SAFe Team level, and PI Objectives for both Program and Team levels. Additional work item types for Defect, Risk, and Retrospective are also included. Program Epics and Features include the WSJF attribute to help ease adoption of lean thinking by providing an economic means of decision-making. This enables the prioritization of work that maximizes business benefit. The WSJF attribute is calculated using this formula: The WSJF component attributes (User/Business Value, Time Criticality, RR/OE, and Job Size) are provided as Fibonacci enumerations, which can be tailored to suit your organization's preferences. For the Epic, we also provide a Value Statement template to remind you of how best to articulate Program Epics. The PI Objective is a way to track value delivery, which is a critical aspect of Enterprise Scaled Agile and IBM DevOps principles. By capturing the notion of planned and actual value delivery, we can provide you with insights into your organization's ability to deliver value to your business and to customers. Plan Views Plan Views provided by the process template enable high-value SAFe processes. The SAFe Kanban System for Program Epics is a visual representation of rank and status. The ability to capture and enforce Work in Progress (WIP) limits enables your team to practice the lean principle of \"just enough\" investment, reminding you again to take an economic view in decision-making. The WSJF Ranked List view takes this concept a step further by helping your team learn how to quickly and relatively rank Features based on WSJF. Remember, it is not about high value alone, but the \"biggest bang for the buck\". This plan view helps your team learn how to consistently rank Features to maximize benefit—to your customers as well as to your business. Dashboards, Queries, Reports No tooling infrastructure provides value unless it can turn your planning and tracking data into actionable analysis, so the process template helps you get started by providing Program and Team dashboards, along with queries and reports that are consistent with the SAFe Program and Team metrics. Here are some examples: We also augment the SAFe guidance with our own, applying the knowledge of our experience working with enterprise customers to create incisive reports that you can use to drive decision-making. In particular, we provide insight into cross-team dependencies to help you address a critical pain point in managing software development and delivery across the Program. Learn by Doing So, the \"process\" and \"tooling\" to help your Program adopt lean and agile principles is provided—but what about the \"people\" aspect? The cultural transformation is perhaps the hardest part of an agile transformation initiative, no doubt about it. To help you with this, we incorporate process mentoring with the SAFe support in Rational Team Concert by providing you with work item templates that remind you of the typical tasks related to specific SAFe events and activities. The process template includes Work Item Templates for: Program Initiation Activities . Creates tasks that guide you on the activities required to initiate your SAFe Program Program Increment. Creates tasks that guide your team to lead a Program Increment, from the Release Planning Event through delivery Development Iteration. Creates tasks to help teams plan development iterations through delivery of functionality Innovation and Planning. Creates tasks for the Innovation and Planning sprints All of the work item tasks describe activities consistent with those prescribed by SAFe and include in-context mentoring via a direct link to the topic described by the task on the SAFe website . This enables you to learn as you go and helps to ease adoption of SAFe, step by step. Plan SAFely While no tool alone can plan for you or make your teams collaborate, IBM's SAFe solution can greatly simplify the transformation through tooling integrated with process guidance, coupled with the right data to help with your decision-making. I hope you like what you see. We would appreciate any and all feedback as we enhance our SAFe support going forward. As always, visit the Collaborative Lifecycle Management and Rational Team Concert What's Happening sections for the latest news. And to keep abreast of everything related to our SAFe support now and in the future, please visit our SAFe landing page on the IBM DevOps Community and check back often for updates! Thank you!","tags":"ciandcd"},{"url":"http://www.ciandcd.com/jazz-team-blog-unlocking-engineering-insight-for-an-iot-world.html","title":"Jazz Team Blog  Unlocking engineering insight for an IoT world","text":"From: https://jazz.net/blog/index.php/2015/06/24/unlocking-engineering-insight-for-an-iot-world/ Gary Cernosek Sr. Product Manager IBM IoT Continuous Engineering I'm writing this series of posts over the next few weeks to spur dialog on the topic of how organizations practicing in an IoT world need better ways to extract and derive value from the many sources that comprise their environment of engineering information. I plan to post this topic in four parts: Part 1: The State of engineering information . I open this series of blog entries with the context of why customers care about gaining insight from their engineering data now more than ever. Part 2: Aggregating engineering information from multiple sources . I'll next address the problems indicated in Part 1 by discussing old and new ways for bringing multiple sources of information together and unifying the way engineers gain access to and work with such information. Part 3: Integrating across disparate tools . Here I'll address the need to gain access to engineering information from tools not originally designed to be integrated with the outside world. Part 4: From information to insight . Imagine a world where we connect everything that needs to be connected. Then what? Here I'll discuss the vision for IBM's Continuous Engineering solution as it provides the basis for analytics and turning raw tool data into true engineering insight. Part 1: The State of engineering information Gary Cernosek Sr. Product Manager IBM IoT Continuous Engineering I know it's here, somewhere… Workers across engineering disciplines spend a lot of time (not) finding the information they need to do their jobs. Consider these statistics captured by KMWorld and The Ridge Group:1 Knowledge workers spend 15% to 35% of their time searching for information 40% of corporate users report they cannot find the information they need to do their jobs 50% of Internet searches are abandoned 90% of the time that knowledge workers spend in creating new reports is recreating information that already exists The problem is not creating good information, it's finding it! What's the root cause of these problems? Much of it has to do with how engineering environments tend to be highly fragmented across disparate tools. And the challenge to connect them is growing exponentially. Each engineering tool comes with its own user interface, and often multiple interfaces for use on the Web vs. desktop application. Behind the scenes, the tools offer various presentations of views and tasks, and often proprietary logic for workflow, process, search, query, scale, security, and collaboration. Storage methods vary from use of individual files on workstation or servers to databases with proprietary interfaces. This degree of variance makes it very difficult for organizations to ensure that engineering information is available to users and traceable across different tools—even when the tools come from the same vendor! The results are brittle/poor integrations, silos everywhere, high cost to maintain and administer the tools, and little reuse. What's so special about IoT? Maybe your organization has handled these challenges fine up to now. But many are finding that IoT presents new or amplified issues that challenge their status quo. Two trends tend to stand out for organizations delivering products and systems connected to the Internet: Market pressure to increase product delivery frequency and compress cycle time Sheer volume and complexity of software required in modern products and systems These trends are depicted in the graphics below and illustrate the need for organizations to be more ‘agile' and to build ‘smarter' products: Projects that used to take years are expected to deliver in months, and those previously completed in months now have make at least incremental progress in weeks, or even days. Environments that historically treated software as a ‘part' that was captured once per product release cycle and stored off in the product data management tool for simple compliance now require greater granularity of lifecycle assets and tighter coordination between software and hardware engineering processes. These factors are driving organizations to reevaluate the way they develop and deliver their products and systems. Do you identify with these issues and trends? If these issues and challenges resonate with your experience, comment in the blog. Let me and others know how it's affecting your day-to-day worklife and your organization's business results. And stay tuned for the next parts of my entries for \"Unlocking engineering insight for an IoT world.\" 1Sources: KMWorld, \"The high cost of not finding information,\" http://bit.ly/1AnNGZO Information Gathering in the Electronic Age: The Hidden Cost of the Hunt, The Ridge Group","tags":"ciandcd"},{"url":"http://www.ciandcd.com/jazz-team-blog-transforming-your-product-development-for-the-iot.html","title":"Jazz Team Blog  Transforming your product development for the IoT","text":"From: https://jazz.net/blog/index.php/2015/06/25/transforming-your-product-development-for-the-iot/ Development Practices for the IoT Era The Internet of Things (IoT) is not just about connecting things to the Internet and controlling them remotely. It is a major opportunity for developers of things (makers) and operators of things to unlock new value propositions from the lifecycle of things, and to improve innovation and the quality of things. In our context, things are products such as automobiles, medical devices, consumer goods, factory machines, etc. To leverage the value of connected and instrumented devices, there are several important aspects to consider, one of which is a proper digital product development process. Also, it is no longer a secret that in today's advanced products most of the innovation comes from the embedded software, so the effectiveness of the software development process is an important parameter of the overall product development process. Here are some key leverage points of connected products: It is possible to continuously collect operational data from devices when in operation. It is possible to remotely update the software that is embedded in the products. With the addition of social media, product makers can also get continuous information on how products are used in different market segments. However, to leverage such enablers, product makers need to change their traditional development processes. The development processes need to be much more dynamic and agile to leverage the connectivity and advanced engagement of connected products. Here are some key transformational aspects for the development process: Connected product complexity —Complexity is increasing with the additional functionalities provided by new interactions between products and product to cloud. Practices for handling this complexity rely on digital systems engineering processes that are based on digital representations of product requirements, product architecture, and product verification plans. Such digital systems engineering approaches enable continuous verification of product designs to eliminate risks early in the process as part of addressing this complexity. Continuous verification utilizes techniques such as simulation and rules-based checking to validate the requirements and the system architecture. Transforming data into engineering insight —The amount of operational data available from connected systems is overwhelming and typically engineering information is locked in isolated silos. Data coming from operations and manufacture may trace to product requirements, product design, and product test. Being able to properly analyze all those product engineering aspects requires complete digitalization, traceability, and analytics of all product development aspects. Increasing speed of development —The connected world increases the need to respond much more quickly to market findings and demands. The ability to effectively respond to change in multidisciplinary products depends on an effective change management process, where impact analysis of the change is conducted in a completely digital manner based on query across lifecycle data. It also relies on the ability to create change contexts across the lifecycle, without interfering with the overall system state before the change is actually approved. Creating such change contexts is enabled by configurations across the lifecycle. Specialization —With the advancement of social media around connected products there is going to be higher demand to create more specialized products to deal with competition and optimize product revenue. That requires capabilities to properly manage reuse and variation as part of the product development process. Ineffective ways to manage variation limit the ability of product makers to effectively leverage product variation. Streamlined process with continuous integration —IoT architectures require both proper support for embedded software that can be updated on devices, as well as software on cloud that analyzes and controls devices. In order to achieve this transformation of the engineering development process, customers should look to the IBM Internet of Things Continuous Engineering Solution . The IBM Continuous Engineering (CE) platform, which is based on the Rational solution for Collaborative Lifecycle Management (CLM), provides the infrastructure and capabilities to enable a digital engineering lifecycle, which is necessary to meet the challengse of rapid and effective multidisciplinary development. Any activity and artifact as part of the process are digital and cross-linked—whether those artifacts are requirements, product designs and architectures, test plans or change history. There is no need to rely on traditional documents in the process, which are typically the main blocker for digitalization of the lifecycle. Open, standards-based lifecycle data indexing, query, reporting, and analysis are also key to effectively supporting the stream of incoming changes as part of the connected lifecycle. Recent updates to the CE platform now provide the new capability to define cross-lifecycle configurations, enabling parallel work on new innovations as well as effectively handling product variations by efficient reuse. To summarize, the new generation of connected products that makes the Internet of Things is a major opportunity for product makers if they properly adapt their product development practices to leverage the opportunities and meet the challenges. As already identified by some key IoT-related initiatives, such as Industrie 4.0 in Germany, and Industrial Internet of Things (IIoT) initiatives in the United States, the required transformation is to shift product development to a digital platform.","tags":"ciandcd"},{"url":"http://www.ciandcd.com/jazz-team-blog-collaborative-lifecycle-management-60-in-a-nutshell.html","title":"Jazz Team Blog  Collaborative Lifecycle Management 6.0 in a nutshell","text":"From: https://jazz.net/blog/index.php/2015/06/26/collaborative-lifecycle-mangagement-6-0-in-a-nutshell/ Since our announcement on June 9th, we've spent the last couple of weeks here on Jazz.net sharing with all of you more information about the new features and their value. We're planning on spending some more time in the next couple of weeks continuing that sharing. But today is an exciting day for all of us on the Collaborative Lifecycle Management (CLM) team— Version 6.0 is generally available! This is a culmination of a lengthy development cycle for us. If you were following along here on Jazz.net, we developed new capabilities over the course of 11 sprints. Much of that, which you'll see as you look through the content in v6.0, was required for the effort—from the Jazz Foundation on up—to provide configuration management across the integrated solution. But what you don't see, or what maybe isn't immediately apparent, is all the work occurring behind the scenes. So, to highlight just a few of the changes: During v6.0, we shifted to building twice per day—10 times per week. This allowed us to slow the amount of feature change occurring in just daily builds to ensure a more stable experience. We expanded our development pipeline to 14 different tests, and those tests run against every build. If you look back as far as five years ago, we had no automated development pipeline. So from zero automated pipeline testing to running 140 pipeline tests (14 tests for 10 builds every week) in five years. We built a federated environment in which to test configuration management. This distributed environment was built to mimic some of our more complex customer shops and allow us a much greater depth of testing for v6.0 than we've had in the past. Feature development included Configuration Management as well as a number of DevOps enhancements. You'll see a lot of Configuration Management content on Jazz.net related to CLM v6.0. Rather than try to give it a full summary here, I'll point you to some key information: We've put Configuration Management behind an activation key. The intent here was to ensure you are walking through all the considerations related to CM before you turn it on. If you're worried that CM is really just for clients who are building complex hardware devices, it's not. You can use CM in simple scenarios as well. And finally, of course, once you've considered it and understand how you might use it, visit Getting Started with Configuration Management . But CM wasn't the only set of feature work in CLM v6.0. SAFe 3.0 brings a new template to Rational Team Concert (RTC) and extends a project to the business teams involved in development. You can get out of the box and up and running with the SAFe template, improving agility and predictability with role-based dashboards. Continued scalability improvements to Enterprise Edition, integration with UrbanCode Deploy, Single Sign-On (SSO) for Kerberos and OIDC for all RTC clients are numbered among other enhancements. Finally, our Reporting team improved setup and configuration, simplified the customer experience, integrated reports into QuickPlanner, and have given you \"near live\" operational reporting. I urge you to stop by our Deployment wiki , one of our most popular destinations. All in all, it's been a very active release for both Development and Operations, and one we're very proud of.","tags":"ciandcd"},{"url":"http://www.ciandcd.com/jazz-team-blog-whats-new-in-rational-quality-manager-60.html","title":"Jazz Team Blog  What's new in Rational Quality Manager 6.0?","text":"From: https://jazz.net/blog/index.php/2015/06/26/whats-new-in-rational-quality-manager-6-0/ IBM Rational Quality Manager (RQM) 6.0 brings both incremental improvements over v5 and a completely new dimension to managing test artifacts. On the incremental improvements front, the team has done a great job to continue to work closely with our customers to further improve the user experience on specific areas such as dashboard widgets or collaboration. At the same time, v6.0 introduces the support of configuration management for test artifacts, linked to other domains such as requirements or design, and contributing to the notion of global configurations across our Collaborative Lifecycle Management (CLM) solution. This version marks the beginning of a completely new set of capabilities that empower users to do parallel development and test, and to reuse artifacts in an effective way. Improved user experience Continuing on the work done in RQM 5.0.1 with the introduction of the Test Artifact dashboard widget, RQM 6.0 offers the new Test Statistics widget. Both of those widgets leverage the saved queries created in one of the test artifact views, such as the Test Case Execution Record view. Those live queries can now be reused with their results displayed directly on a dashboard. The Test Statistic widget offers the option to display the result in table format or in graphs such as bar charts, pie charts, or column charts. The team has also worked on improving collaboration support, specifically for concurrent modification of test artifacts. If two people modify the same test plan, suite, or case at the same time, the second person to attempt to save is now prompted with the option to merge the changes. Single sign-on authentication On top of the existing types of single sign-on (SSO) authentication already supported, this version introduces the Jazz Security Architecture SSO based on the OpenID Connect authentication protocol. The new Jazz Authorization Server simplifies the authentication administration. Read more in John Vasta's post about SSO options in CLM 6.0 . Reporting In v5, we introduced Jazz Reporting Service (JRS) as a new option to generate customizable analytics reports with a user-friendly web-based report builder. In v6.0, JRS is now included directly in the CLM package. This increases the availability for better reporting services for all of your applications without requiring the additional effort of organizing, installing, and configuring reporting capability through a separate download and install. There are also a number of new capabilities which make JRS the recommended analytics reporting solution for CLM, such as calculation and roll-up along with graphical report drill-through, new out-of-the-box reports, and interactive runtime filters in the dashboard widget. If you'd like to learn more, see Ernest Mah's post on reporting in CLM 6.0 . Configuration management RQM 6.0 is the first release that includes capabilities to aid in configuration management of test artifacts. On a basic level, configuration management enables users to better manage changes and to go back in time if needed through the creation of baselines and the ability to compare and merge. Users can create baselines to record a state in time of a Quality Management (QM) project area. Baselines are immutable and defined for an entire project area. Test artifacts of a baseline cannot be modified. Users can then compare the current state of a QM project area with any previous baseline. The comparison provides both a high-level view of the differences at the project area level as well as a detailed side-by-side comparison at the artifact level. Then, users have the opportunity to roll back some of the changes by merging a previous baseline into the current state and replacing some artifacts with the previous versions from that baseline. In a more advanced usage model, configuration management offers the ability for testers to work independently and in parallel on multiple versions or variants of test artifacts. Users can create parallel streams by branching from an existing baseline. Streams are versions of all the test artifacts of a project area that can be changed. Users can merge changes made in one stream into another by first looking at the differences and then replacing all or some of the current versions by the versions of the artifacts in the baseline. The local Quality Management configurations, both streams and baselines, can then be contributed to global configurations. Global configurations are cross-domain configurations that can link to multiple local configurations from the Requirements Management, Design Management, Software Configuration Management, and Quality Management applications. Global configurations are used to define a common context in which users can work and create all the deliverables for a given version or variant. Finally, by creating a hierarchy of global configurations, it is possible to manage composite product definitions and configurations that enable complex reuse scenarios at the subsystems and components levels. Learn more about RQM 6.0 For details of what's new in 6.0, check out New & Noteworthy in the Downloads section. See the Release Notes for a list of fixes. To see what's supported, see the System Requirements .","tags":"ciandcd"},{"url":"http://www.ciandcd.com/jazz-team-blog-whats-new-in-doors-next-generation-60.html","title":"Jazz Team Blog  What's new in DOORS Next Generation 6.0?","text":"From: https://jazz.net/blog/index.php/2015/06/26/what%e2%80%99s-new-in-doors-next-generation-6-0/ This release could well be introducing the most fundamental function into a requirements management (RM) tool since RM systems began. While we continue our drive with usability and productivity, we are also providing support for requirements configuration management (CM), built from the ground up as part of the native tool, rather than simply integrating an external CM system. Configuration Management DOORS Next Generation 6.0 is our first release providing native configuration management of requirements, enabling functionality for strategic reuse and Product Line Engineering. RM offers the ability to define the scope of a project, program, or deliverable—but unless you have control over change this scope could well consume more of your project costs than you expected. Placing requirements under CM allows for the scope to be controlled, while enabling your teams to work in parallel versions at the same time without the need to make project copies to handle variants. On a basic level, CM provides a way to manage groups of artifacts and their versions. Versions can be split to support different variants and merged in order to deliver changes of requirements back to accepted releases. With CM, users can efficiently create different streams to help manage parallel versions without the need to make copies of their requirements. Additionally, there is robust linking that is specific to the stream the user is working in. This means that following the link shows users the requirement, test, or design element on the other end of the link that is appropriate for the stream they are working in. The ability to compare across streams and baselines helps users to understand how their versions or variants are different and to correct any mistakes. The functionality has been designed expecting only a small number of people to engage directly with parallel versions and CM, while the rest of the engineers continue with their work, mostly as if nothing has changed. By default, these new capabilities are turned off and project administrators have to enable them for the server and for each project area where users want to take advantage of them. Global Configuration Management For some time we have been discussing the benefits of being able to support use cases such as \"link a version of a test case with a version of a requirement\". 6.0 extends CM with a federated approach to lifecycle information. Strategic reuse for complex systems and software is now possible for requirements (RDNG), design (RDM), test (RQM) and software development (RTC). Plan and manage the reuse of configurations in the many versions or variants of the product or software line. Define complex products and applications as hierarchies of components and reuse those components in multiple products and applications. Automatically handle links between artifacts when branching or delivering changes to another stream such as, links between tests and requirements. Create cross lifecycle baselines to support parallel development of multiple versions and variants, branching and merging, and change management. Diagram editor Users can create many types of diagrams to refine and communicate their ideas and to elaborate their requirements. The new diagram editor provides high-quality diagrams that are simple to create in all supported browsers without the need for a Java™ plug-in. The large selection of shapes and themed styles make it easy to create many types of diagrams with eye-catching color palettes. To increase productivity, users can use the keyboard to create all the diagrams. They can comment on and create links between individual diagram elements and other artifacts. Diagrams can be added to modules and embedded in rich-text artifacts. See the Release Notes for further information! This release could well be introducing the most fundamental function into a requirements management tool since RM systems began. While we continue our drive with usability and productivity we are also providing support for requirements configuration management, built from the ground up as part of the native tool, rather than simply integrating to an external CM system.","tags":"ciandcd"},{"url":"http://www.ciandcd.com/continuous-integration-and-deployment-solution.html","title":"Continuous integration and deployment solution!","text":"From: http://www.pmease.com/hotnews?id=1 QuickBuild 6.0 is now available Feature highlights in this release: Find repository/step/variable overrides and usages for configuration refactoring. Optionally trust authenticated user in specified http header to support single sign-on. Permission set definition to facilitate assigning same set of permissions repeatedly. Administrator can select to run as arbitrary user to facilitate checking user profile. Aggregate SCM changes to display change summary and statistics in high level configuration. Gerrit integration to verify open changes and score specified Gerrit label accordingly. JFrog Artifactory integration to publish and use artifacts during build. Persist unprocessed build requests after server shutdown and resume processing after startup. Accurev proof build to test active changes on QuickBuild before getting them promoted. Optionally run scripts after deletion of configuration and build. Able to view live log by step, and view log of finished steps before build finishes. Step to record SCM changes without checking out the repository. Able to display custom banner in QuickBuild page. For detailed explanation of all features added in this release, please visit We are proud to annouce QuickBuild 6.0.Feature highlights in this release:For detailed explanation of all features added in this release, please visit http://www.pmease.com/features/whats-new QuickBuild 5.1 is now available Feature highlights in this release: Verify GitHub pull requests and update pull request status based on build result. GitHub issue tracker integration to parse issues in commit messages. Leverage perforce shelve/unshelve feature to run pre-commit builds without using user agent. Retrieve changes of Subversion externals for source view and diff. Custom columns to display custom build and request info. Display reasons for waiting builds and steps. Define environment variables in composite steps for inheritance and overriding. Detect broken communication links to agents to fail build fast. Drag&drop to organize favorite dashboards. Dashboard list to display all dasbhoards in system. Resource access information to know about resource usage status. Coverity report rendering For detailed explanation of all features added in this release, please visit We are proud to annouce the formal release of QuickBuild 5.1Feature highlights in this release:For detailed explanation of all features added in this release, please visit http://www.pmease.com/features/whats-new QuickBuild 5.0 is now available Feature highlights in this release: Launch build agent on demand in cloud environment including Amazon EC2. Build pipeline to visualize commits life cycle across different build and deployment stages. Optionally store artifacts of configuration sub tree to specified build agents to reduce server load. Grid and server metrics collecting and trending. Alert definition and notification for key performance indicators. Enhanced tray monitor and refined message window. Toggle node and step information in build log. Share dashboards with specified users besides groups. Headless plugin build. New dashboard gadgets to display build pipeline, grid performance measurements and system alerts. For detailed explanation of all features added in this release, please visit We are proud to annouce the formal release of QuickBuild 5.Feature highlights in this release:For detailed explanation of all features added in this release, please visit http://www.pmease.com/features/whats-new QuickBuild 4.0 formal release is now available Feature highlights in this release: Customizable dashboard for users and groups to organize build information via gadgets. Report aggregation to provide build metrics summary of descendant configurations. Resource management for better control of build distribution and agent load. Grid partition to divide grid nodes between different configuration trees. User activity audit to track and review every modification to the system. CollabNet TeamForge integration for user authentication, file uploading, release creation, issue linking. and issue updating. Redmine integration to link QuickBuild builds with Redmine issues. Google Repo integration to detect changes, check out source, and create tags against Repo. Boost test integration to render test reports and display test trends. Redesigned report system for improved user experience and performance. RESTful API for changes, issues, and various reports. Plugin API for third party issue tracker and unit test framework integration. Searchable users and groups. For detailed explanation of all features added in this release, please visit We are proud to annouce the formal release of QuickBuild 4.Feature highlights in this release:For detailed explanation of all features added in this release, please visit http://www.pmease.com/features/whats-new QuickBuild 3.1 - distributed version control system integration and enhanced .NET support QuickBuild 3.1 is released to integrate with Git, Mercurial, Bazaar and Team Foundation Server. This integration makes possible below actions in a continuous integration or release management environment when dealing with these SCMs: Retrieve source code for build and test from tip or specified revision. Create tags for retrieved source code if necessary. Detect source changes between builds and notify committers under specified condition. Promote SCM revisions to higher stage, for example from qa to release. Git, Mercurial and Bazaar integration also includes the gated push feature, with which you can submit ready-for-push commits to QuickBuild for build/test, and have QuickBuild to push them to the official repository automatically after building/testing successfully. This release also supports to build .NET projects through MSBuild and Visual Studio solution builder. Refer to http://www.pmease.com/features/whats-new/ for details. QuickBuild 3.1 beta1 released to support Git, Mercurial, TFS and Bazaar You may visit Git, Mercurial, Team Foundation Server and Bazaar support is now in beta. In this beta, QuickBuild can checkout code, create tags, detect changes, view/diff source files from these version control systems. Proof build support is not yet included but will be delivered in future betas.You may visit this link to download the beta. Any feedbacks or suggestions are very welcomed! The formal release of QuickBuild 3 is now available This release works tightly with issue tracking systems to provide an integrated view of issues, builds and SCM changes. No longer worry about which issues are fixed in a particular build, or which build a particular issue is fixed in. QuickBuild tracks these information for you automatically! The release management functionality is improved considerably with the ability to use next unreleased version in issue tracker as next build version, and push built versions into issue tracker as released versions. Currently JIRA, Trac and Bugzilla are supported. Other feature highlights in this release: Step can be repeated for different set of parameters, either parallelly, or sequentially. For example, you may create a singe test step to have it execute for each combination of possible databases and OS platforms, or have it run on all applicable build agents. This can greatly reduce number of steps needed in a complex build workflow. QuickBuild can now terminate spawned build processes immediately and reliably when a build is canceled or timed out. You no longer need to manually kill relevant processes to release workspace mutexes. This works on Windows, Linux and Unix platforms. A non-admin account can now be authorized to administer a configuration subtree. Multiple promote actions can be defined with the ability to customize the condition of each action. For example, you may define a release action and have it appear only when build is recommended and current user belongs to release manager group. Inherited settings such as steps, repositories and variables will be displayed directly in descendant configurations. This makes examination and modification of inherited settings much easier. Build workflow can now be created/rearranged by dragging and dropping steps. Trends of duration and success rate of each executed steps are now available in statistics tab of a configuration. You can even compare these trends between different steps to find out which step fails the most and which step costs the most time. SCM changes screen is reworked to support text search in changes between two arbitrary builds. The same step can now be reused in different composition steps. Add the option of auto-detecting user time zone from browser to display local date/time. For detailed explanation of all features added in this release, please visit We are pround to annouce the formal release of QuickBuild 3.This release works tightly with issue tracking systems to provide an integrated view of issues, builds and SCM changes. No longer worry about which issues are fixed in a particular build, or which build a particular issue is fixed in. QuickBuild tracks these information for you automatically! The release management functionality is improved considerably with the ability to use next unreleased version in issue tracker as next build version, and push built versions into issue tracker as released versions. Currently JIRA, Trac and Bugzilla are supported.Other feature highlights in this release:For detailed explanation of all features added in this release, please visit http://www.pmease.com/features/whats-new QuickBuild 3.0 beta1 is now available The first beta of QuickBuild 3.0 is now available. This release integrates tightly with JIRA, Trac and Bugzilla to streamline the development process. Other improvements include reusable and repeatable steps, inheritance visibility, process tree killing, build engine optimization, UI polishments. Refer to release notes for details. QuickBuild 2.1 is available now QuickBuild 2.1 is just released with plugin and RESTful API, a cross-platform tray monitor, FxCop, NCover and CPD support, custom statistics, Oracle and SSL support, and much more. Refer to what's new for a complete list of new features added to this release. The brand new QuickBuild 2.0 is released Please refer to After years of development and test, QuickBuild 2.0 is finally released to embrace latest innovations in continuous integration and build management area. Most important features introduced in this version are pre-commit test, advanced build grid, versatile build reports, graphical build workflow design, visual build promotion, source code view/diff, and build comparison. QuickBuild 2.0 also includes enormous improvments such as intuitive user interface, fine-grained permission control, real time build progress and log monitoring, variable prompting.Please refer to the feature page for the complete list of achievements in this version.","tags":"ciandcd"},{"url":"http://www.ciandcd.com/live-webinar-reactive-stream-processing-with-akka-streams.html","title":"Live Webinar: Reactive Stream Processing with Akka Streams","text":"From: http://blog.jetbrains.com/blog/2015/01/14/live-webinar-reactive-stream-processing-with-akka-streams/ We are pleased to invite you to our upcoming webinar, Reactive Stream Processing with Akka Streams , featuring Konrad Malawski of Typesafe. Register now and join us Tuesday, January 27th, 15:00 – 16:00 GMT (10:00 AM – 11:00 AM EST). In this webinar, Konrad will give an overview of the Reactive Streams specification (with teams from Netflix, Pivotal, RedHat, Typesafe and the others), the issues it addresses and how all the implementations aim to consolidate on a shared streaming protocol. Also, you'll learn how to use Akka Streams for working with streaming data in an asynchronous type-safe and back-pressured manner. Hurry up and register now , space is limited! About the presenter Konrad Malawski is a late-night passionate dev living by the motto, \"Life is Study!\", hacking on the Akka toolkit at Typesafe. While working on Akka Streams he also implemented the Reactive Streams specifications Technology Compatibility Kit. You can follow him on Twitter – is a late-night passionate dev living by the motto, \"Life is Study!\", hacking on the Akka toolkit at Typesafe. While working on Akka Streams he also implemented the Reactive Streams specifications Technology Compatibility Kit. You can follow him on Twitter – @ktosopl Follow IntelliJ IDEA on our blog , Twitter @IntellIJIDEA , and our product pages .","tags":"ciandcd"},{"url":"http://www.ciandcd.com/jetbrains-2014-the-year-in-review.html","title":"JetBrains 2014: The Year in Review","text":"From: http://blog.jetbrains.com/blog/2015/01/21/jetbrains-2014-the-year-in-review/ Our team was hard at work in 2014 delivering new releases of our existing products and bringing new ones to market. In this post we are going to take a look at those and some of the other biggest moments of the year. If you remember back to JetBrains Day @ FooCafé in September 2013, we announced several new projects. 2014 saw those plans come to fruition. In February, ReSharper C++ Early Access Program went live, in May Nitra was made open source , September brought CLion EAP , and in December Upsource , our repository browsing and code review tool, reached a stable 1.0 build. We didn't stop there. Two new products were also added to our portfolio: 0xDBE , JetBrains' brand new IDE for DBAs and SQL developers, was first announced in June and in October PyCharm Education Edition , a free IDE for learning and teaching programming with Python, went public. You may be interested in checking out our Interactive Python Programming Course Contest . In 2014 we continued our commitment to the open source community through our various projects and by providing free open source licenses to non-commercial OS software projects. In July a new open source Kotlin website went live and steady releases of JetBrains Meta Programming System (MPS) continued to go out the door. Travel again back in time to May 2013 when Google announced their selection of IntelliJ IDEA as the base of Android Studio . Well, in December, the highly anticipated Android Studio 1.0 release hit the virtual shelves! This is a great example of open source collaboration working both ways with the work being done on Android Studio being incorporated back into IntelliJ IDEA Ultimate Edition and the free and open source Community Edition . One of the proudest moments of the year came in September when we announced JetBrains Student License Program . Through this program students and teachers have access to our entire product line of IDEs and .NET Tools. Within the first two weeks of the program, more than 34,000 students were approved and now there are nearly 100,000 students using JetBrains tools for free! Nearly a decade and a half on we haven't forgotten our startup roots. In February we announced JetBrains Startup Discount Plan . Software startup businesses that meet straightforward criteria get a 50% discount on all of our products . If your startup is just getting off the ground, this is a great place to begin. Lastly, here are some of the honors that our products picked up in 2014 . 2014 was a fantastic year and we expect more of the same excitement in 2015. We sincerely thank you; all of our friends and colleagues, for your continued support and wish you all the best in the coming year. Here's to another outstanding and productive year in 2015!","tags":"ciandcd"},{"url":"http://www.ciandcd.com/webinar-recording-reactive-stream-processing-with-akka-streams.html","title":"Webinar Recording: Reactive Stream Processing with Akka Streams","text":"From: http://blog.jetbrains.com/blog/2015/01/29/webinar-recording-reactive-stream-processing-with-akka-streams/ On Tuesday we had the pleasure to host a webinar together with Typesafe where Konrad Malawski, a Scala enthusiast who works on the Akka toolkit, gave a very comprehensive overview of the Reactive Streams specification and one of its implementations — Akka Streams . The slides from Konrad's presentation can be found at SlideShare . About the presenter Konrad Malawski is a late-night passionate dev living by the motto, \"Life is Study!\", hacking on the Akka toolkit at Typesafe. While working on Akka Streams he also implemented the Reactive Streams specifications Technology Compatibility Kit. You can follow him on Twitter – is a late-night passionate dev living by the motto, \"Life is Study!\", hacking on the Akka toolkit at Typesafe. While working on Akka Streams he also implemented the Reactive Streams specifications Technology Compatibility Kit. You can follow him on Twitter – @ktosopl Develop with Pleasure!","tags":"ciandcd"},{"url":"http://www.ciandcd.com/live-webinar-software-architecture-as-code-february-12th.html","title":"Live Webinar: Software Architecture as Code, February 12th","text":"From: http://blog.jetbrains.com/blog/2015/01/29/live-webinar-software-architecture-as-code-february-12th/ We are pleased to invite you to our upcoming webinar, Software Architecture as Code , featuring Simon Brown. Register now and join us Thursday, February 12th, 15:00 – 16:00 GMT (10:00 AM – 11:00 AM EST). It's 2015 and with so much technology at our disposal, we're still manually drawing software architecture diagrams in tools like Microsoft Visio. Furthermore, these diagrams often don't reflect the implementation in code, and vice versa. This session will look at why this happens and how to resolve the conflict between software architecture and code through the use of architecturally-evident coding styles and the representation of software architecture models as code. Space is limited; learn more and register now . Simon Brown is an independent consultant and helps organizations to build better software by adopting a lightweight, pragmatic approach to software architecture. He is the creator of the C4 software architecture model and the author of \"Software Architecture for Developers,\" a developer-friendly guide to software architecture, technical leadership and the balance with agility. is an independent consultant and helps organizations to build better software by adopting a lightweight, pragmatic approach to software architecture. He is the creator of the C4 software architecture model and the author of \"Software Architecture for Developers,\" a developer-friendly guide to software architecture, technical leadership and the balance with agility.","tags":"ciandcd"},{"url":"http://www.ciandcd.com/intellij-idea-and-webstorm-infoworlds-2015-technology-of-the-year-award-winners.html","title":"IntelliJ IDEA and WebStorm: InfoWorld's 2015 Technology of the Year Award Winners","text":"From: http://blog.jetbrains.com/blog/2015/01/30/intellij-idea-and-webstorm-infoworlds-2015-technology-of-the-year-award-winners/ On January 26th, 2015, InfoWorld announced their 2015 Technology of the Year award recipients . In total there were 32 winners representing the best of cloud, data, hardware and software applications. For the second year in a row WebStorm is a winner and IntelliJ IDEA returns to the list in 2015 ! WebStorm The WebStorm review by Martin Heller, InfoWorld Test Center Editor, highlights the core features that makes WebStorm \"more than an editor\" such as: built-in code inspections and code quality tools, Node.js and JavaScript debugger and tracer, Live edit, and integration with the testing tools. IntelliJ IDEA Just last month IntelliJ IDEA 14 picked up the 2015 Jolt Productivity Award for Coding Tools and now InfoWorld's 2015 Technology of the Year. What a great ending to 2014 and start to the new year! Here is part of what Rick Grehan had to say in his review. \"Granted, we wish the Community edition were equipped with the sorts of J2EE development tools found only in the Ultimate edition: database tools, support for frameworks such as JPA and Hibernate, deployment tools for application servers like JBoss AS, WildFly, and Tomcat. Nevertheless, the Community edition makes a fine Java application development platform that also gives you Android tools, as well as support for other JVM languages like Groovy, Clojure, and Scala (the last two via free plug-ins). Whichever version of IntelliJ IDEA you use, you'll find a rich array of tools designed to simplify otherwise tedious development chores.\" Read more about WebStorm (slide 15), IntelliJ IDEA (slide 16) and the other winners in InfoWorld's 2015 Technology of the Year Award slide show .","tags":"ciandcd"},{"url":"http://www.ciandcd.com/help-us-improve-jetbrainscom-and-win-a-license.html","title":"Help Us Improve JetBrains.com and Win a License","text":"From: http://blog.jetbrains.com/blog/2015/06/01/help-us-improve-jetbrains-com-and-win-a-license/ It has been a while since the last time that we updated our web site design, nearly three years ago when we switched to the current design from the one below. We're thinking about making another update some time soon, but as a team of very technical geeks we love numbers. We do have lots of data already from the different analytics systems we're using but we want to do a special survey right now dedicated specifically for JetBrains.com. It is people like you who are visiting the web site and using it to find the information that you need, so we are asking for your help. As the survey is about a web site, it might feel odd that we're asking some questions which might not seem relevant. However, often decisions we make are somewhat related to other aspects in our lives. We're catering the site to so many diverse individuals and some of the questions play an role in this. Having said that, some questions are optional. If you are willing to help us, please complete our survey . And yes, we have some prizes for those who complete the survey: a chance to win one of 10 personal licenses for a JetBrains product of your choice, or one of 20 Amazon vouchers worth $25 . Thank you!","tags":"ciandcd"},{"url":"http://www.ciandcd.com/stopping-support-for-java.html","title":"Stopping support for Java","text":"From: http://www.go.cd/2014/07/09/stopping-support-for-java-jdk-6.html There was a recommendation , in April 2013, that all users move their Go Server and Go Agent installations to Java 7. Oracle and OpenJDK no longer support Java 6. So it is time for Go to stop supporting it. 14.2.0 will be last Go release which will work with Java 6. Any new Go release beyond 14.2.0 might not work with Java 6. If you have not already moved to Java 7, we request you to do so. Should you face any issues please do write to the community mailing list .","tags":"ciandcd"},{"url":"http://www.ciandcd.com/continuous-delivery-with-go.html","title":"Continuous Delivery with Go","text":"From: http://www.go.cd/2014/08/07/go-webinar-recording.html Every couple weeks ThoughtWorks hosts learning sessions for people who want more information about continuous delivery with Go. This is a recording of the session from 7 August, 2014","tags":"ciandcd"},{"url":"http://www.ciandcd.com/sample-go-cd-virtualbox-based-environment.html","title":"Sample Go CD Virtualbox based environment","text":"From: http://www.go.cd/2014/09/09/Go-Sample-Virtualbox.html If you're interested in checking out Go but don't want to spend the time automating your own system, this might be a great option for you. Edit on 11 November, 2014 - This box has been updated to Go version 14.3. For information about what's new in this release please see http://www.go.cd/releases/ We've created an environment using Vagrant and Virtualbox. Once it's up, you'll have a full Go installation including several example pipleines. System Requirements In order to run this you'll need Virtualbox and Vagrant . Both of these are available for most operating systems. Using the box To get started, open a command prompt in an empty directory and type... vagrant init gocd/gocd-demo This will create a file called Vagrantfile in your current directory. Next, type... vagrant up Completion of this (especially the first time) will take quite a while, depending on your bandwidth. Vagrant will be downloading the full box image (almost 1.4GB) from Vagrantcloud while you wait. Note: If you have an existing Go installation on the same machine as this virtual machine you may get a port conflict. Vagrant will automatically map to a new port which will be shown in the startup messages. After a few minutes, you should be able to navigate to http://localhost:8153/ on your local machine and see the following... These pipelines are all related, as shown in the following value stream map screenshot... Feel free to play around with the installation and see how everything works. You can always reset the box to it's orginal state if you need to! What's on the machine? The box will be updated as new things come out, but as of this writing... Go 14.3 Server Go 14.3 Agent 3 very small PHP applications Basic Capistrano deployment scripts Local Git repo using Gitolite to manage permissions A couple simple phpunit tests A couple simple watir scripts All of the code is on the Virtualbox machine at /home/vagrant/projects. The easiest way to access this is to type 'vagrant ssh' at the command prompt in the same place you started the machine. The hope is that using this box you can see how real applications (even if they are small) are built, tested and deployed with Go. As always, Go questions can be asked at https://groups.google.com/forum/#!forum/go-cd","tags":"ciandcd"},{"url":"http://www.ciandcd.com/distributed-test-execution-with-go-tlb.html","title":"Distributed Test Execution with Go + TLB","text":"From: http://www.go.cd/2014/10/09/Distrubuted-Test-Execution.html Writing tests has finally become the norm. Consequently, running tests for every commit is central to & the most time consuming activity in any CI/CD setup. In a decent-sized production quality project you tend to have thousands of tests. That means the cycle time, i.e. the time it takes for a commit to reach deployable state (after running all unit, integration & functional tests), keeps growing. It gets harder when teams follow XP related practices like \"small commits, frequent commits\" since it causes parallel builds & resource starvation. One such example is Go's codebase. Just the \"Common\" & \"Server\" components of Go which comprises of unit & integration tests, together has ~6000 tests which will take about ~5 hours if run serially! The functional test suite is about 260+ tests with combined runtime of ~15 hours. That's close to a day & we haven't even run everything for a single commit! Note that the number of tests is so huge that just putting in a powerful box & running test in parallel will not bring it down to acceptable limits. Also, a large number of other problems surface if you start running tests in parallel on same box (without sandboxed environment) like concurrency issues etc. Solution [Go + TLB] Go improves the cycle time of its own build by making test execution faster, distributing it across many agents (machines). After this \"Common\" + \"Server\" takes 20 minutes. All functional tests run in 45 minutes. Thats close to an hour! Still not ideal (a few minutes - constrained by resource availability), but better. :) Test Load Balancer (TLB) TLB is an open-source library which provides the ability to break up a test suite into pieces and run a part. It guarantees 'Mutual Exclusion' & 'Collective Exhaustion' properties that are essential to reliably running tests in distributed fashion. TLB's strength lies in intelligent test distribution which is based on time, i.e. the tests will be distributed based on time they take to execute, making the jobs close to equal runs which leads to better resource utilization. It falls back on count based splitting if test times are not available. It also runs tests in 'Failed First' order, so if a test has failed in previous run it will be run before other tests which means faster feedback. Note: As of this writing, TLB integrates with JUnit (through Ant, Maven & Buildr), RSpec (through Rake), Cucumber (through Rake), Twist (through Ant & Buildr). Quick Setup Download TLB Unzip tlb-complete-0.3.2.tar.gz to tlb-complete-0.3.2 $ cd tlb-complete-0.3.2/server $ chmod +x server.sh $ ./server.sh start This should start server at http://host-ip-address:7019 Resources: Go Go is an open-source CI/CD tool. Its well known for its powerful modelling, tracing & visualization capabilities. While TLB is doing all the distribution, Go does what it does best - orchestrate the parallel execution. Run 'X' instances Starting release 14.3 you can spawn 'x' instances of a job. So if you want to distribute your tests across 10 machines you just need to set run instance count to 10 & Go will spawn 10 instances of the job when scheduling. Sample Configuration Setup a pipeline with material (SCM) that contains your tests. Setup Job to spawn required number of instances (run instance count). Setup TLB related environment variables at Environment / Pipeline / Stage / Job level. Setup the task to consume GO_PIPELINE_NAME , GO_STAGE_NAME , GO_PIPELINE_COUNTER , GO_STAGE_COUNTER , GO_JOB_RUN_INDEX & GO_JOB_RUN_COUNT environment variables that Go exposes. Upload junit xmls as test artifacts. Sample Pipeline Configuration <pipeline name= \"maven-project\" > <materials> <git url= \"https://github.com/test-load-balancer/sample_projects.git\" dest= \"sample_projects\" /> </materials> <stage name= \"unit-tests\" > <jobs> <job name= \"test-split\" runInstanceCount= \"3\" > <environmentvariables> <variable name= \"TLB_BASE_URL\" > <value> http://localhost:7019 </value> </variable> <variable name= \"TLB_TMP_DIR\" > <value> /tmp </value> </variable> <variable name= \"TLB_JOB_NAME\" > <value> ${GO_PIPELINE_NAME}-${GO_STAGE_NAME}-test-split </value> </variable> <variable name= \"TLB_JOB_VERSION\" > <value> ${GO_PIPELINE_COUNTER}-${GO_STAGE_COUNTER} </value> </variable> <variable name= \"TLB_PARTITION_NUMBER\" > <value> ${GO_JOB_RUN_INDEX} </value> </variable> <variable name= \"TLB_TOTAL_PARTITIONS\" > <value> ${GO_JOB_RUN_COUNT} </value> </variable> </environmentvariables> <tasks> <exec command= \"mvn\" workingdir= \"sample_projects/maven_junit\" > <arg> clean </arg> <arg> install </arg> <arg> -DskipTests </arg> <runif status= \"passed\" /> </exec> <exec command= \"mvn\" workingdir= \"sample_projects/maven_junit\" > <arg> clean </arg> <arg> test </arg> <arg> -DskipTests </arg> <arg> -Drun.tests.using.tlb=true </arg> <runif status= \"passed\" /> </exec> </tasks> <artifacts> <test src= \"sample_projects/maven_junit/target/reports/*.xml\" dest= \"test-reports\" /> </artifacts> </job> </jobs> </stage> </pipeline> Other features that helps with Test Parallelization Wait for all jobs to finish Go's modelling capability gives it the ability to run jobs in parallel but wait for all of them to finish before the next Stage / downstream Pipelines are triggered. Stop the downstream flow If any of the tests (and as a result the Job running the test) fails, the Stage is considered as failed. This causes the flow to stop as expected. Consolidated Test Report Once all the Jobs are done running, Go consolidates test reports & shows the result at stage level for easy consumption. Drill down You can drill down at job level to know more information like 'test count', 'console output' for the Job (test) etc. Partition re-run Go also provides ability to re-run a Job of a stage. This provides ability to run the partition that could have failed due to flaky test etc. The best part is, TLB runs the exact tests that it ran the last time making sure no test is missed out! TLB Correctness Check TLB provides an ability to check correctness, i.e. it will make sure all tests were run. You can configure to run this correctness check once all partitions are done executing, may be in next stage / pipeline. Power of dynamic splitting Go's one knob control to amount of parallelization means that when the number of tests increase/decrease all you will need to do is change the run instance count based on number of tests & resource availability & you are done! -- As always, Go questions can be asked at go-cd .","tags":"ciandcd"},{"url":"http://www.ciandcd.com/go-143-released.html","title":"Go 14.3 Released","text":"From: http://www.go.cd/2014/11/11/Go_14_3_announced.html Today we released Go 14.3 You can download it from here . Take a look at release notes to see details. This release saw lot of contributions from the community. A huge callout to the following contributors (not in any particular order) for their outstanding contributions : @lcs777 , @ciotlosm , @tusharm , @juniorz , @RikTyer , @mmb , @afoster , @sahilm , @gregoriomelo , @greenmoss , @dvarchev and Temmert (We have tried to be as accurate as possible. Sincere apologies if we missed mentioning anyone above) We would also like to thank people who reported issues/feature requests and participated in various discussions. That list is too big to be mentioned here, but please know that all the time and energy spent by everyone in improvising Go is very much appreciated. Thanks once again!","tags":"ciandcd"},{"url":"http://www.ciandcd.com/issue-with-uploading-compressed-artifacts-in-go-1430.html","title":"Issue with uploading compressed artifacts in Go 14.3.0","text":"From: http://www.go.cd/2014/11/14/Go_14_3_issue_with_uploading_compressed_artifacts.html There was an issue reported with respect to artifact uploads in the 14.3.0 release of Go. Issue On Go server running with OpenJDK7, uploading compressed artifacts fails. The operation is reported as success in console-log on job details page but actual artifact does not get uploaded. Please note this does not affect uploading console-log itself or even a simple file as an artifact. This is an issue only with Go v14.3.0. Read further to know if you are affected by this defect. Who does this affect? This defect would affect you only if your Go Server v14.3.0 is run using OpenJDK(jdk or jre) and the agent responsible for artifact upload is using a version of java other than the one used by Go server. You are unaffected by this defect if: you use Sun/Oracle java to run Go Server you have SunEC extension installed for your OpenJDK [EDIT: on your Go server] Both your server and agent processes run using OpenJDK (not necessarily the same version) [EDIT: without ECC cipher suites . As explained in \"What caused this?\" section, this issue occurs when java on agent supports ECC cipher suites but java on server doesnot. You should definitely run the litmus test to be sure.] Litmus test You should run a pipeline which uploads a compressed file as an artifact to ensure you are not affected by this defect. If the artifact shows up on the artifacts tab of the job and can be successfully downloaded, you are safe from the defect. Workaround We are aware that you have been waiting to try your hands on the new APIs and several bug fixes that came out in 14.3.0. Fortunately we have a few workarounds available to help you move ahead with the upgrade, meanwhile we would be working on finding the best possible fix for this issue. You could go for one of the workarounds listed below: Options: Install SunEC extension on your OpenJDK setup. Get sunec.jar and place it under jre/lib/ext/ folder. You must also place libsunec.so in jre/lib/amd64/ folder. OR Install Oracle JRE on your Go server and switch to that. For Go server running on Windows, this means updating the system level environment variable GO_SERVER_JAVA_HOME to oracle jre home For Linux based installations, update /etc/default/go-server to set JAVA_HOME to oracle jre home. For Others, set the value of system level environment variable JAVA_HOME to oracle jre home. OR Install the same version of java as you have on your agent. Update java used by Go server to appropriate value as suggested in the previous option. At this point restart Go Server. Trigger your affected pipeline to ensure the artifact got uploaded successfully. This should resolve the issue. If the issue persists even after applying the suggested workaround, do write to the go-cd mailing list reporting the same. What caused this? I cannot speak about this without getting into some technical details. As a part of upgrading to Rails v4, we also had to upgrade bouncycastle-bcprov library. Earlier versions of Go used bouncycastle-bcprov v1.40. With that the cipher suites accepted by Go server would always be one of SSL_RSA_WITH_RC4_128_SHA , SSL_RSA_EXPORT_WITH_RC4_40_MD5 or SSL_RSA_WITH_RC4_128_MD5 . Go 14.3.0 has moved on to use org.bouncycastle-bcprov v1.47. Bouncycastle v1.46 brought in support for ECC cipher suites . ECC cipher suites are made available by SunEC extension which is not packaged along with OpenJDK7 by default. Go server is run using Jetty which uses bouncycastle crypto package (i.e. bcprov) for the handshake. At this point, instead of agreeing on the expected SSL & RSA based ciphers, one of the ECC ciphers get picked during the agent/server handshake. Jetty6 (yes, we are still using this!) does not work well with the modern cipher suites and causes issues like the one mentioned above. To tackle such scenarios, all supported ciphers suites apart from the three mentioned above are excluded from Jetty. Since, ECC cipher suites were not available on the server, they did not get excluded. However if your JVM has ECC ciphers available (by default or after applying one of the workarounds), Go would ensure they get excluded and make Jetty happy. Jetty upgrade has been on the cards for a while, and that would possibly help resolve this. But that is a much involved task and hence works better as a long term solution. We need to evaluate other options as well which could fix the issue in the short term. Many thanks to Vladimir Bormotov for reporting this issue and allowing us to use his setup to gather the required debug information. As always, you could write to go-cd and go-cd-dev mailing lists if you have any ideas/feedback/questions.","tags":"ciandcd"},{"url":"http://www.ciandcd.com/go-144-released.html","title":"Go 14.4 Released","text":"From: http://www.go.cd/2014/12/17/Go_14_4_announced.html Today we released Go 14.4 You can download it from here . Take a look at release notes to see details. Sincere thanks to everyone who contributed to Go in form of features, ideas, issues / feature requests and much more! A special mention goes to @mythgarr , @hammerdr and to the Pivotal team: @mmb , @gajwani , @fkotsian , @bsnchan for their active contributions and support. Thanks once again!","tags":"ciandcd"},{"url":"http://www.ciandcd.com/go-plugin-competition.html","title":"Go Plugin Competition","text":"From: http://www.go.cd/2015/01/20/Go_plugin_competition.html Are you up for the challenge? Do you have what it takes to build an awesome Go plugin? Here's your chance to put your development skills to the test. ThoughtWorks invites you to the first ever Go plugin challenge. We want you to build a plugin that showcases the best of Go. Have you been playing with an idea on the side or has your organisation developed something really cool that others would love? This is your chance to showcase it and win a prize! Read more... Important Dates Submission Deadline: February 20, 2015 at 11:59 PM CST (February 21, 2015 at 5:59 AM GMT) Notification of Acceptance: February 27, 2015 Results: March 10, 2015","tags":"ciandcd"},{"url":"http://www.ciandcd.com/for-go-151-upgrade-your-java.html","title":"For Go 15.1 upgrade your Java","text":"From: http://www.go.cd/2015/04/23/Go_15_1_jdk7_announcement.html GoCD has stopped support JDK 6 for some time now. But we understand that some users were using Java 6, so we continued to support it as long as we could while helping users migrate their Go servers and agents to Java 7. Java 6 was declared end-of-life in February 2013, and Java 7 is scheduled to be declared end-of-life soon. Starting with the 15.1 release of GoCD, it will only run with Java 7. Users are encouraged to upgrade to the latest release of GoCD with Java 8. Starting with the next release, we plan on providing support for Java 8.","tags":"ciandcd"},{"url":"http://www.ciandcd.com/feature-branch-support.html","title":"Feature Branch Support","text":"From: http://www.go.cd/2015/04/27/Feature-Branch-Support.html Go 15.1 introduced support for writing material repository plugins, to extend the kind of source code material repositories that Go works with. This resulted in community-driven plugins developed for Go, to implement support for feature branches, with help from members of Go's core contributors. This blog posts has information specifically about GitHub Pull Request support. Note: In this post, the terms Branch and Pull Request are used interchangeably, since a Pull Request is essentially just a branch. As codebases grow and teams start writing more tests, they often hit upon a challenging problem. If they have setup their build, test and deploy pipelines as a normal team or teams working with trunk-based development would have, then increasing the number of tests they have results in a longer time to certify a build and deploy to production. Here is an example of a Value Stream Map from Go CD (Username: view, Password: password) itself, where running all the tests and generating installers can take hours: Figure 1: GoCD - Value Stream Map (Click to enlarge) Due to this, it becomes critical to keep the main Value Stream green all the time. A failed build would mean all other commits ready to go in have to wait until the failed build is fixed: Figure 2: Failed build stops everything (Click to enlarge) The root of this problem is a slow build, and sometimes that can be tackled directly. However, with the advent of short-lived feature branches (aka, Pull Requests in GitHub land), this problem can become worse. Since feature branches are not regularly verified before merging, merging them could itself be a little risky, and could cause the build to fail un-necessarily. In general, development workflows in organizations has moved to something which looks like: Pull Request (GitHub, Gerrit etc.) / Feature Branch => Code Review => Merge => Build Now, whether a feature branch based workflow is the best approach or not is hotly debated (see Martin Fowler's article on this). Organizations who follow a feature branch based workflow have been wanting support for it in Go. Historically, Go has advocated against feature branches and support for it has been limited. Go users have come up with some innovative work arounds, like this one from Vision Critical . Though the Go core contribution team continues to be wary of long-lived feature branches, short-lived feature branches create a window for validating changes before they are merged into the main branch. Since the majority of time spent in a CI/CD setup tends to be in running tests, and failed builds are typically due to test failures, you could run tests on a proposed change in a feature branch, get feedback about it and fix tests if needed, before merging it into the trunk. Though this does not always catch integration issues (that depends on what else was merged before this one was), it allows you to increase the chances of your main Value Stream staying green and in a deployable state for longer. A problem with this approach though, is that every change will be tested twice (once on the feature branch and once on the main branch after the merge) which means the effective time for a commit to reach production could be more, unless you have more hardware (agents) to run branch builds. The way forward Assuming you have chosen the approach mentioned above, you can now use Go 15.1, with its two new extension points - SCM end-point and the Notification end-point , to test feature branches before they are merged. To use this with GitHub requires the use of two community-driven and community-supported plugins: Git Branch Poller Plugin and the Build Status Notification Plugin . The first one is an SCM Material plugin, and is responsible for polling a configured repository for changes, while the second one is a Notification plugin, which is responsible for notifying GitHub about the suitability of a Pull Request for merging. Note : Even though this post specifically mentions GitHub only, plugins have been written to work with plain Git, Atlassian Stash, Gerrit and more! See the Go community plugins page for more information. Quick Setup Download the Git Branch Poller Plugin and the Build Status Notification Plugin . Place them under <go-server>/plugins/external . Restart the Go Server. Verify that the plugins are loaded correctly. Figure 3: Verify Plugins (Click to enlarge) Decide which parts of the value stream you want the Pull Requests to run till, and extract a template for those pipelines, so that you can have a parallel set of pipelines to run against Pull Requests. The need to create a separate set of pipelines is to make sure that the main build and the branch build never get interleaved, and a branch build never gets deployed into production, by mistake. Your decision should be based on how much of your tests can reasonably be run for every Pull Request, and how far down the Value Stream can a build containing those changes Go. For some, every test in the system needs to run before it is deemed merge-able and for some, only unit and integration tests might be enough. It depends. Suppose you have a setup of three pipelines like this: Figure 4: Example setup (Click to enlarge) and you decide that you want the first two pipelines to run for every Pull Request, you need to change your pipelines to look like this: Figure 5: Extract templates, create pipelines for PR (Click to enlarge) Based on your decision, extract templates and create the new pipelines: Figure 6: Extract template (Click to enlarge) In the new pipeline or pipelines that have been setup to run for every Pull Request, change the Git material to use the GitHub material (this material is provided by the GitHub poller plugin installed earlier): Figure 7: Add GitHub material (Click to enlarge) Figure 8: Add GitHub material - Details (Click to enlarge) Once you have setup the GitHub material for the pipeline, you can remove the Git material from that pipeline. That's it. Results Go will trigger builds for every new Pull Request and for new commits to existing Pull Requests: Figure 9: PR triggers build (Click to enlarge) Go will update Pull Request in GitHub with the build status: Figure 10: GitHub PR page gets updated (Click to enlarge) Figure 11: GitHub PR listing page gets updated (Click to enlarge) Fan-in and Value Stream Map work as expected: Figure 12: Fan-in and VSM work (Click to enlarge) Shortcomings and known issues: If multiple branches are updated at once, the plugin provides all of them as changes and Go will not run the pipeline for every change separately. Go currently combines multiple changes into a single pipeline run (to save time). A feature allowing \"force trigger pipeline for each change\" should be able to overcome this. This has not yet been accepted into the main GoCD codebase. If there are multiple commits in a branch, the plugin only returns the top commit as a change. Hence only one change shows up in the dashboard, value stream, etc. Also, since Go does not know about the other changes you will not be able to manually trigger a pipeline with the other commits. The UI is lacking in certain areas: For instance, it is not possible to add an SCM plugin material during pipeline creation, to associate an existing SCM to a pipeline you will need to edit Config XML etc. These will be fixed in upcoming releases. References Some discussions on the GoCD mailing lists and on GitHub about this: Sample Configuration Here is a part of the configuration used to create the images shown above: <scms> <scm id= \"b7386c23-71d5-4581-8129-bba5b67638e4\" name= \"sample-repo\" > <pluginConfiguration id= \"github.pr\" version= \"1\" /> <configuration> <property> <key> url </key> <value> https://github.com/srinivasupadhya/sample-repo.git </value> </property> </configuration> </scm> </scms> <pipelines group= \"sample-group-master\" > <pipeline name= \"sample-pipeline-master\" template= \"sample-pipeline\" > <materials> <git url= \"https://github.com/srinivasupadhya/sample-repo.git\" dest= \"sample-repo\" materialName= \"sample-repo\" /> </materials> </pipeline> <pipeline name= \"sample-downstream-pipeline-master\" template= \"sample-downstream-pipeline\" > <materials> <pipeline pipelineName= \"sample-pipeline-master\" stageName= \"sample-stage-2\" /> </materials> </pipeline> </pipelines> <pipelines group= \"sample-group-PR\" > <pipeline name= \"sample-pipeline-PR\" template= \"sample-pipeline\" > <materials> <scm ref= \"b7386c23-71d5-4581-8129-bba5b67638e4\" dest= \"sample-repo\" /> </materials> </pipeline> <pipeline name= \"sample-downstream-pipeline-PR\" template= \"sample-downstream-pipeline\" > <materials> <pipeline pipelineName= \"sample-pipeline-PR\" stageName= \"sample-stage-2\" /> </materials> </pipeline> </pipelines> <templates> <pipeline name= \"sample-pipeline\" > <stage name= \"sample-stage-1\" > <jobs> <job name= \"sample-job-1\" > <tasks> <exec command= \"ls\" /> </tasks> </job> </jobs> </stage> <stage name= \"sample-stage-2\" > <jobs> <job name= \"sample-job-2\" > <tasks> <exec command= \"ls\" /> </tasks> </job> </jobs> </stage> </pipeline> <pipeline name= \"sample-downstream-pipeline\" > <stage name= \"sample-stage-3\" > <jobs> <job name= \"sample-job-3\" > <tasks> <exec command= \"ls\" /> </tasks> </job> </jobs> </stage> </pipeline> </templates> As always, Go questions can be asked on the mailing list .","tags":"ciandcd"},{"url":"http://www.ciandcd.com/go-1510-released.html","title":"Go 15.1.0 Released","text":"From: http://www.go.cd/2015/04/29/Go_15_1_announced.html We would like to announce a new release of gocd. Head over to our downloads page to get your hands on the latest and greatest. Read more about what's new in this release from our release notes . Sincere thanks to everyone who contributed to Go in form of features, ideas, issues / feature requests and much more! A special mention goes to @ashwanthkumar , @alexschwartz , @sachinsudheendra , @pwen , @pamo , @bernardn , @danielsomerfield , @iliasbartolini for their active contributions and support. Thanks once again!","tags":"ciandcd"},{"url":"http://www.ciandcd.com/get-started-using-go.html","title":"Get Started Using Go","text":"From: http://www.go.cd/2015/05/06/Getting-Started-Resources.html Some resources to help get started using Go. Go User Documentation There are a couple sections of the user documentation that can be especially helpful to people new to Go. Concepts in Go - This covers some of the basic concepts used in Go. A good understanding of what Pipelines, Stages, Jobs and Tasks are will be very helpful. Managing Agents - The Go server produces the user interface for Go, but it doesn't actually run your jobs. Learn how to use Go Agents to \"do the work\". Setting up a new Pipeline See how to set up your first pipeline Of course there's a lot more information available as well. Other information online The Go mailing list - A great place to search for answers to questions you may have, or of course ask them if they aren't already covered. IRC - Connect to freenode with your own IRC client or use this web client. Don't forget to uncheck \"auth to services\" if you're not planning to login with your preset freenode information. Live demonstrations Webinars - ThoughtWorks presents live webinars every couple weeks so that you can see Go in action. There are also recordings of previous webinars on this blog. Professional Support ThoughtWorks - The first 30 days of professional support provided by ThoughtWorks is free. You'll get access to a global support team, and tough issues can be escalated directly to the Go development team. Alternative Trial Installation Docker Container - This is an easy way to see what Go does. As it says in the description, this is not a production container. You'll also need at least one instance of the agent container","tags":"ciandcd"},{"url":"http://www.ciandcd.com/using-windows-powershell-tasks.html","title":"Using Windows PowerShell tasks","text":"From: http://www.go.cd/2015/06/13/using-windows-powershell-tasks.html Some things to be aware of when using Windows PowerShell tasks. Go Agent default installation The default installation of a Go agent will use a 32-bit JRE unless you indicate otherwise. This JRE is embedded in the Go agent installer. If you want to use an alternative JRE (must satisfy Go's JRE requirements) after the initial installation, you can alter the \"wrapper.java.command\" key's value in the [InstallDirectory]\\config\\wrapper-agent.conf file to point to a different JRE. You will then need to restart the Go agent service to start using the alternative JRE. The [InstallDirectory] refers to the Go agents installation directory which by default is \"C:\\Program Files (x86)\\Go Agent\" . Pre-requisites for running PowerShell task commands You can only run on Windows based agents You should tag the agents if your are also using linux agents You probably want to ensure your agents all have the same version of PowerShell 32-bit Go agent If you are running a default Go agent installation then you will be running a 32-bit JRE. The 32-bit JRE will try to run PowerShell tasks in the 32-bit version of PowerShell, even if you give the full path to the 64-bit PowerShell executable in the task. If you need to execute a PowerShell script then you will need to alter the execution policy as follows: Open 32-bit version of PowerShell as an administrator: Start -> All Programs -> Accessories -> Windows Powershell -> Windows Powershell (x86) and type: # Alter execution policy set-executionpolicy remotesigned -force This will allow you to run local scripts on the Windows Go agent box. 64-bit Go agent If you are running a Go agent using a 64-bit JRE, it will run PowerShell tasks in the 64-bit version of PowerShell. If you need to execute a PowerShell script, then you will need to alter the execution policy as follows: Open 64-bit version of PowerShell as an administrator: Start -> All Programs -> Accessories -> Windows Powershell -> Windows Powershell and type: # Alter execution policy set-executionpolicy remotesigned -force This will allow you to run local scripts on the Windows Go agent box. PowerShell task commands You can configure the task as follows: command: powershell arg: .\\run.ps1 arg1value This assumes that the run.ps1 script is in the task's working directory. If you create the run.ps1 file with the following content you can see details of the execution context in the console log for the pipeline: param ( [string] $arg1 ) write-host \"Script: \" $MyInvocation . MyCommand . Path write-host \"Pid: \" $pid write-host \"Host.Version: \" $host . version write-host \"Is 64-bit process: \" $( [Environment] :: Is64BitProcess ) write-host \"Execution policy: \" $( get-executionpolicy ) write-host \"Arg1: \" $arg1 Propagating failures You need to ensure that PowerShell exits with an exit code that is not 0 in the event of a failure, this needs to cater to: Script errors External process calls that indicate failure You will need to decide how to handle these failures and if they should indicate the PowerShell task has been successful or not. This may mean that some script errors and external process calls failing is okay in your context. The following script demonstrates a strategy I use where I exit with a non zero code if any script error was encountered or an external process call fails: set-strictmode -version latest $ErrorActionPreference = 'Stop' function execute-externaltool ( [string] $context , [scriptblock] $actionBlock ) { # This function exists to check the exit code for the external tool called within the script block, so we don't have to do this for each call & $actionBlock if ( $LastExitCode -gt 0 ) { throw \"$context : External tool call failed\" } } try { write-host \"Script: \" $MyInvocation . MyCommand . Path write-host \"Pid: \" $pid write-host \"Host.Version: \" $host . version write-host \"Execution policy: \" $( get-executionpolicy ) # Query a service that does not exist, sc.exe will return with a non 0 exit code execute-externaltool \"Query a non existent service, will return with exit code != 0\" { c : \\ windows \\ system32 \\ sc . exe query service_does_not_exist } } catch { write-host \"$pid : Error caught - $_\" if ($? -and ( test-path variable : LastExitCode ) -and ( $LastExitCode -gt 0 )) { exit $LastExitCode } else { exit 1 } } This script uses a try catch block to handle all errors The $? and $LastExitCode caters to both script and external process calls We fall back on an exit code of 1 if we do not have an external process exit code This script uses an execute-externaltool function which takes a script block argument The script will invoke the script block It will then check for a non zero exit code (Assumes the script block just calls an external process), if so it will throw an exception. See also PowerShell execution policy Bypassing PowerShell execution policy Setting execution policy directly in the registry Go PowerShell runner plugin - I believe it can only be configured on Windows based Go servers About the author This is a guest post by Pat Mc Grath. You can find Pat on GitHub .","tags":"ciandcd"},{"url":"http://www.ciandcd.com/upcoming-api-changes.html","title":"Upcoming API Changes","text":"From: http://www.go.cd/2015/06/17/Upcoming-API-Changes.html With the upcoming release of Go 15.2, we'd like to begin unifying and improving some of the existing APIs that Go supports. Go's APIs are fairly old, have inconsistent and unpredictable content types (csv, xml, json, plain text). Going forward, we would like to announce an ongoing effort to improve these APIs to use something that is more modern, easy to discover, learn and build API clients for. We would be using the JSON HAL specification . Our API guidelines are published on our RFC . This will give us the opportunity to leverage Ruby and Rails to build these APIs, which should make it easier to incrementally iterate through and improve existing APIs to bring them to parity with our new guidelines. We welcome any feedback to improve our guidelines and contributions to improve existing APIs.","tags":"ciandcd"},{"url":"http://www.ciandcd.com/authentication-end-point.html","title":"Authentication end-point","text":"From: http://www.go.cd/2015/06/18/authentication-end-point.html Starting 15.2.0 Go Server will expose authentication end-point. What this means is Go users can add \"custom\" authentication schemes through plugins. With plugin settings & web request handling ability plugin developers get enough flexibility to write any authentication plugin they intend to write. Examples of integrations possible: OAuth Login - GitHub , Google , Hotmail, Yahoo! etc. Single Sign-on (SSO) - LDAP, Okta etc. 2-factor authentication - SMS verification etc. Custom username & password authentication How does it work? Below is an explanation of how GitHub OAuth Login plugin works. Generate OAuth token on GitHub. Figure 1: Generate oauth token (Click to enlarge) On plugin listing page users will see a gear icon (similar to one on the pipeline dashboard). Figure 1: Plugin listing with gear icon (Click to enlarge) Clicking on the gear icon opens a pop-up that renders \"Plugin Settings\". Figure 2: Configure plugin pop-up (Click to enlarge) Login Page Figure 3: Login Page with GitHub icon (Click to enlarge) Click on GitHub icon Figure 3: Authorize Go Server to access GitHub (Click to enlarge) Successful login Figure 3: On successful login (Click to enlarge) Ability to Search & Add users Figure 3: Search User (Click to enlarge) We hope plugin developers are able to use this feature to support their organizations authentication mechanism. References: As always, Go questions can be asked on the mailing list .","tags":"ciandcd"},{"url":"http://www.ciandcd.com/plugin-settings.html","title":"Plugin Settings","text":"From: http://www.go.cd/2015/06/18/plugin-settings.html Go is continously improving its plugin infrastructure. Starting 15.2.0 Go will support \"Plugin Settings\" that will allow plugins developers to accept global settings. Currently these configurations had to be supported via system properties or a file that is in specified format in a specified location, which makes it a little haphazard. With this feature \"all\" plugins will have one approach to accept plugins settings from user & access plugin settings from Go Server. How does it work? On plugin listing page users will see a gear icon (similar to one on the pipeline dashboard) for the plugins that accept plugin settings. Figure 1: Plugin listing with gear icon (Click to enlarge) Clicking on the gear icon opens a pop-up that renders \"Plugin Settings\" template that is provided by the plugin. Figure 2: Configure plugin pop-up (Click to enlarge) On \"Save\" the user inputs are validated by plugin. Figure 3: Configure plugin pop-up with errors (Click to enlarge) We hope plugin developers are able to use this feature to provide a better experience to their users. References: As always, Go questions can be asked on the mailing list .","tags":"ciandcd"},{"url":"http://www.ciandcd.com/hardly-anyone-knows-continuous-delivery.html","title":"Hardly Anyone Knows Continuous Delivery","text":"From: http://www.go.cd/2015/06/23/hardly-anyone-knows-cd.html Those of us who work in or around teams doing continuous delivery often think it's a mainstream thing. This couldn't be further from the truth. I work for ThoughtWorks, a company that implements processes and technologies we think are good long before most. We built the first CI server with Cruise Control, and Go was the first purpose built Continuous Delivery server. I go to a lot of conferences and events, read a lot of blogs, talk to a lot of peers, work with a lot of partners, etc. I thought most people involved with the creation of software had a pretty good idea what CD is. I was wrong. I just got back from a pretty big software conference that was a bit off my normal track. They had a few DevOps sessions this year, but historically this particular conference has been more about agile methodologies. As one of the sponsors, I spent a lot of time at the booth talking to people. The conversations in a trade show booth generally start with the visitor asking what we do (gotta work more on that so they don't have to) and me telling them that Go is a continuous delivery server. From there we go on to talk about what makes Go unique and why they should use it. At this show, when I told people Go is a continuous delivery server I was met with mostly blank stares. This was a conference attended by 100% people who create software for a living. The people attending care enough about their craft to spend (or get their company to spend) a couple thousand US dollars to come. But they had no idea what Continuous Delivery really is. I probably should note, this isn't meant as a knock on that conference at all. The lack of knowledge is a really bad thing. Not just for the Go CD project, but for software in general. The world runs on software. Too much of that software is bad. The practices around Continuous Delivery could make some of it better, or kill it before it gets out. So what can we do? Buy or borrow a copy of Continuous Delivery by Jez Humble and Dave Farley for your office. Make everyone read at least the chapters that apply to them. Yes, Jez and Dave both worked for ThoughtWorks when they were writing the book. Yes, Jez was the product owner of Go before the book came out. No, we won't make any money off the link if you buy it. I promise this isn't bias, it's the definitive work on the subject. Get The Phoenix Project by Gene Kim. It's a fictional novel and a bit corny at times, but people will learn a bit even if they don't mean to. Send people that don't know about Continuous Delivery to conferences that are specific to CD and DevOps. My favorite is DevOpsDays . You don't need huge, expensive conferences where you'll have to get finance approval to attend. The next one I'm going to is 200 bucks. If they don't have one in your area create one or find someone that will. (FYI, if anyone in Seattle is interested in doing that let me know) Take a friend who's CD impaired to a DevOps Meetup . As I'm writing this there are groups in 404 cities worldwide at that link alone. Trying to get your meetup going and struggling for content and/or speakers? Tell me, I know a few people and might be able to help. Stop assuming everyone knows what we're talking about when we talk about CD. Many of them are smiling and nodding the same way I do when my mother talks about her flowers. Feel free to comment with your own resource, this isn't even close to a definite list. One last thing... Stop telling people that the phrases DevOps and Continuous Delivery are overused. They aren't. Hardly anyone knows what Continuous Delivery is.","tags":"ciandcd"},{"url":"http://www.ciandcd.com/on-antifragility-in-systems-and-organizational-architecture.html","title":"On Antifragility in Systems and Organizational Architecture","text":"From: http://continuousdelivery.com/2013/01/on-antifragility-in-systems-and-organizational-architecture/ On Antifragility in Systems and Organizational Architecture In his new book, Antifragile , Nassim Taleb discusses the behaviour of complex systems and distinguishes three kinds: those that are fragile, those that are robust or resilient, and those that are antifragile. These types of systems differ in how they respond to volatility: \"The fragile wants tranquility, the antifragile grows from disorder, and the robust doesn't care too much.\" (p20) Taleb argues that we want to create systems that are antifragile – that are designed to take advantage of volatility. I think this concept is incredibly powerful when applied to systems and organizational architecture. Why Continuous Delivery Works Taleb shows why the traditional approach of operations – making change hard, since change is risky – is flawed: \"the problem with artificially suppressed volatility is not just that the system tends to become extremely fragile; it is that, at the same time, it exhibits no visible risks… These artificially constrained systems become prone to Black Swans. Such environments eventually experience massive blowups… catching everyone off guard and undoing years of stability or, in almost all cases, ending up far worse than they were in their initial volatile state\" (p105) 1 . This a great explanation of how many attempts to manage risk actually result in risk management theatre – giving the appearance of effective risk management while actually making the system (and the organization) extremely fragile to unexpected events. It also explains why continuous delivery works. The most important heuristic we describe in the book is \"if it hurts, do it more often, and bring the pain forward.\" The effect of following this principle is to exert a constant stress on your delivery and deployment process to reduce its fragility so that releasing becomes a boring, low-risk activity. Antifragile Systems Another of Taleb's key claims is that it is impossible to predict \"Black Swan\" events: \"you cannot say with any reliability that a certain remote event or shock is more likely than another… but you can state with a lot more confidence that an object or a structure is more fragile than another should a certain event happen.\" (p8). Thus we need \"to switch the blame from the inability to see an event coming… to the failure to understand (anti)fragility, namely, ‘why did we build something so fragile to these types of events?'\" (p136). Unlike risk, fragility is actually measurable. How do we measure the fragility of the systems we build? We try to break them, using techniques such as game days and systems like chaos monkey . The systematic application of stress to your systems is essential – not just to ensure your systems are antifragile, but to develop the muscles of the people who create and maintain them through constant practice. After all, it's the combination of the system and the people who build and run it that has the quality of antifragility. In this context, an important quality of legacy systems is their fragility. Legacy systems that aren't touched for a long time will turn into fragile \"works of art\": changing them is considered risky, the number of people who understand the system decreases with time, and their knowledge atrophies from lack of exercise. How do we create antifragile systems? Apply stress to them continuously so we are forced to simplify, homogenise, and automate. Antifragile Organizations We can measure the fragility of an organization by how long it takes before it liquidates its assets. Deloitte's Shift Index shows that the average life expectancy of a Fortune 500 company has declined from around 75 years half a century ago to less than 15 years today. Start-ups are notoriously fragile. But the ones that survive and grow turn into something potentially more dangerous – robust organizations. The problem with robust organizations is that they resist change. They aren't quickly killed by changes to their environment, but they don't adapt to them either – they die slowly. We see this effect all the time – changing the culture of an established organization is incredibly hard. Antifragile organizations are those that have a culture that enables them to learn fast from their environment and adapt to it so they can take advantage of volatility. Here are some characteristics of antifragile organizations: Systems thinking. Everybody in the organization knows the goals of the organization and makes sure their work is directly contributing towards these goals. Theory Y Management. Management needs to assume employees are self-motivated and will be able to learn how to solve problems themselves. Organizations need to make sure they hire antifragile people who will thrive in this environment. As Daniel Pink's Drive points out, giving your employees autonomy, purpose, and the opportunity to learn and master new skills is what stops them from quitting, thus increasing the antifragility of your organization. Continuous experimentation. As described in Toyota Kata , good management knows that the best solutions come from the workers. They create an environment in which practitioners are able to run experiments to learn as rapidly as possible. The feedback loops in command and control organizations are too slow for them to adapt effectively. Disruptive product development. Antifragile organizations aren't content with stress generated by their environment. Like humans exercising, they also try and disrupt themselves (the organizational equivalent of a game day). For example, Amazon cannibalized its own business , creating the Amazon Marketplace and the Kindle. Apple is cannibalizing its Mac business with the iPad. Fragile organizations resist disrupting their own product lines, as Toshiba did at first with flash memory . If you do a good job at this you never need to worry about the competition – you'll always beat them to it. Fragility and Agility As Taleb points out, \"antifragility is desirable in general, but not always, as there are cases in which antifragility will be costly, extremely so. Further, it is hard to consider robustness as always desirable—to quote Nietzsche, one can die from being immortal.\" (p22) Of course working out where on the spectrum you want your systems and your organization to lie is an art, and the great artists are those that know how to build systems, organizations, and products simply, quickly and cheaply so that they are antifragile with respect to our biggest enemy: time. How do they do that? Using the same heuristics described in \"antifragile organizations\", above, which closely mirror the Three Ways of Devops . As I read Antifragile , it reminded me of something I read a number of years ago: Kent Beck and Cynthia Andres' Extreme Programming Explained . The subtitle? Embrace Change. It strikes me that the concept of antifragile is what we were aiming for with agile the whole time: building systems (including human systems – organizations) that benefit from volatility. Endnotes Thanks to Badrinath Janakiraman for feedback on an earlier draft of this post. 1 He is talking about financial markets, which are rather less fragile than IT systems, hence his rather generous \"years of stability\"","tags":"ciandcd"},{"url":"http://www.ciandcd.com/book-review-the-phoenix-project.html","title":"Book Review: The Phoenix Project","text":"From: http://continuousdelivery.com/2013/01/book-review-the-phoenix-project/ Book Review: The Phoenix Project I am not going to do a ton of book reviews on this blog (I have one more planned for next month). I'll only bother posting reviews of books that I believe are both excellent and relevant to Continuous Delivery . This book easily satisfies both criteria. Full disclosure: Gene gave me a draft of this book for free for reviewing purposes. You've probably heard of Gene Kim, Kevin Behr and George Spafford before. They are the three amigos responsible for The Visible Ops Handbook , which can be found in the book pile of every good IT operator. Their new book, The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win , follows the format of Eliyahu Goldratt's classic, The Goal . Told from the perspective of newly-minted VP of IT Operations Bill Palmer, it describes the turnaround of failing auto parts company Parts Unlimited. This is to be achieved through the delivery of the eponymous Phoenix Project, a classic \"too big to fail\" software project designed to build a system which will revive the fortunes of the company. To quote (p51): The plot is simple: First, you take an urgent date-driven project, where the shipment date cannot be delayed because of external commitments made to Wall Street or customers. Then you add a bunch of developers who use up all the time in the schedule, leaving no time for testing or operations deployment. And because no one is willing to slip the deployment date, everyone after Development has to take outrageous and unacceptable shortcuts to hit the date. The results are never pretty. Usually, the software product is so unstable and unusable that even the people who were screaming for it end up saying that it's not worth shipping. And it's always IT Operations who still has to stay up all night, rebooting servers hourly to compensate for crappy code, doing whatever heroics are required to hide from the rest of the world just how bad things really are. Part One of the book describes in loving detail the enormous clusterfuck pie that is baked from these ingredients. The pie is spiced with an internal Sarbanes-Oxley audit which reveals 952 control deficiencies, an outage of the payroll processing system, and various other problems that conspire to deepen the woe of the operations group, all of which are clearly drawn from the deep well of the authors' real-life experiences. Apart from the main characters – our hero Bill, his boss Steve, and the evil villain Sarah – The Phoenix Project features a delightful rogues' gallery which anyone working in an enterprise will recognize: Brent Geller, the boy wonder whose encyclopedic knowledge of the company's Byzantine IT systems means that his involvement is necessary to get anything done. Patty McKee, the Director of Support who runs a change management process so bureaucratic that everybody bypasses it. John Pesche, the black binder wielding Chief Information Security Officer whose constant meddling under the guise of improving security has turned him into a pariah. The second part of the book details how the IT group is reborn from the ashes of the Phoenix Project into a high-performing organization that is a strategic partner to the business. This is achieved through the application of a heavy dose of lean thinking (including continuous delivery ) administered by Erik, a mercurial IT and manufacturing guru Steve is courting to join the board. The book does an excellent job of showing – as well as telling – how to apply the concepts (and the effect of doing so) in an enterprise with plenty of technical debt. Perhaps the most eyebrow-raising part of this section is the way in which John has his soul mercilessly crushed to the point where he goes on a multi-day drinking spree before he is rehabilitated towards the end of the book (he is a phoenix too). John's narrative arc is just one example of how the book also succeeds as a novel. It's gripping, with moments of drama and high emotion, as well as some great one-liners. There was even one point when I teared up (bear in mind that I also cried during Forrest Gump – unlike the book's central characters, I did not serve in the armed forces). Nobody who has read The Goal will miss The Phoenix Project's similarity in terms of style and plot. Perhaps my favourite thing about the book's pedagogical style is the way Erik (like Jonah in The Goal) uses the Socratic Method to give Bill the tools to solve his problems by himself. Of course this learning process is fictional, but it means you get to see Bill struggling with the questions and trying things out. It remains to be seen whether readers of the book will be able to apply these techniques as successfully as Bill without a real Erik to guide them. But of course, this is a limitation of any book. If I had one criticism it's that unlike real life, there aren't many experiments in the book that end up making things worse, and it's this process of failing fast, learning from your failures, and coming up with new experiments that is instrumental to a real learning culture. One important point worth noting if you are working in an organization like Parts Unlimited is this: the IT department's rebirth is only possible because of the Titanic proportions of the disaster that unfolds in Part One. For management to truly embrace change, a compelling event or a teachable moment (i.e. a Charlie Foxtrot) is required. Unless your organization faces the same existential threat that Parts Unlimited does, you'll have a much harder time convincing people they should adopt the tools described in the book. Overall, The Phoenix Project is a fantastic read. It's entertaining, cathartic, inspirational and informative. If, like me, you have an enormous backlog of books (and more work in process than you'd like) I suggest giving yourself a break and putting this one to the top of your list. It'll only take you a day or two, and despite its conceptual density it will leave you feeling refreshed and energized with a bunch of new ideas to try out. The Phoenix Project deserves to be read by everyone who works in – or with – IT.","tags":"ciandcd"},{"url":"http://www.ciandcd.com/continuous-delivery.html","title":"Continuous Delivery","text":"From: http://continuousdelivery.com/2013/05/announcing-flowcon/ Announcing FlowCon I spend quite a lot of time at conferences, and it consistently bothers me that they are so often focused on one particular function: development, testing, UX, systems administration. The point of continuous delivery is to accelerate the rate at which we can learn from each other – and from our customers. That requires everyone involved in the delivery process (including users, product owners and entrepreneurs) to collaborate throughout. So why isn't there a conference which focuses on flow – the emergent property of great teams? So I got together with a bunch of like-minded folks – Elisabeth Hendrickson , Gene Kim , John Esser and Lane Halley – and now there is a conference about creating flow: FlowCon . It's on Friday November 1 in San Francisco , and it's produced by ThoughtWorks and Trifork (creators of the GOTO conferences ). The conference is based around four values: Learning : Our goal is to provide the best possible conference forum for practitioners to learn from each other how to build great products and services. Open Information : We aim to uncover how great products and services are built in real life and make this information freely available to the widest audience possible. Diversity : We believe the technology community – and thus the conference speakers and participants – should reflect the demographics of our customers and the wider world. Spanning boundaries : We believe that the best products and services are created collaboratively by people with a range of skills and experiences. We have put together nearly half of the program , and we're delighted to announce that Adrian Cockcroft , Catherine Courage , Jeff Gothelf and Linda Rising will be giving keynotes. The program is still a work in process (a minimum viable product, if you will). In particular, the after lunch sessions are empty – for a good reason: we want you to speak in those slots . We're looking for people working to create flow in their organization – especially those who: Span multiple roles and work across organizational silos. Work in any of the following areas: a highly regulated environment; a large, traditional enterprise; in the pursuit of social and economic justice. Are willing to share obstacles encountered or mistakes made and how you overcame them – whether cultural or technological. Offer actionable advice \"the rest of us\" can apply today (even if we don't have the resources of Etsy / Amazon / Google). Your talk could be about culture, technology, design, process – the only really important criterion is that it draws on what you've learned about helping to create flow in your organization. If that sounds like you, please submit your proposal . If you know someone who would do a great job, please encourage them to submit. Our submission process is designed to be entirely merit-based, which means that the first round is anonymous. The deadline is midnight Pacific time, Sunday June 23, 2013. Tickets for the conference are now on sale – at $350 if you register before July 31, or $500 if you register afterwards. Whatever your role or domain, you're sure to find inspirational, disruptive thinking that will make you better at creating great products and services. I hope to see you there!","tags":"ciandcd"},{"url":"http://www.ciandcd.com/videos-from-the-continuous-delivery-track-at-qcon-sf-2012.html","title":"Videos from the Continuous Delivery track at QCon SF 2012","text":"From: http://continuousdelivery.com/2013/05/videos-from-the-continuous-delivery-track-at-qcon-sf-2012/ Videos from the Continuous Delivery track at QCon SF 2012 At last year's QCon San Francisco I got to curate a track on continuous delivery. One of the goals of the QCon conferences is \"information Robin Hood\" – finding ways to get out into public the secret sauce of high performing organizations. So I set out to find talks that would answer the questions I frequently get asked: can continuous integration, automated testing, and trunk-based development scale? How does continuous delivery affect the way we do product management? What's the business case for continuous delivery? How do you grow a culture that enables it? You'll find the all these questions answered in the talks below, from the leaders who have been at the forefront of continuous delivery at Amazon, Facebook, Google and Etsy. They also discuss the tools they built and the and practices they use to enable continuous delivery. Finally, you get me talking about how you can adopt continuous delivery at your organization. Thanks so much to Jesse Robbins, Frank Harris, Nell Thomas, John Penix and Chuck Rossi for these great talks, and to the folks behind QCon SF for an awesome conference. Jesse Robbins ran ops at Amazon before quitting to co-found Opscode (creators of Chef ). He is also co-founder of Velocity . In his copious spare time, he's a volunteer firefighter. Basically, Jesse is an enormous over-achiever. This is a fabulous – and hilarious – talk that discusses the hardest part of implementing continuous delivery: cultural change. This talk features my favourite devops aphorism: One of the main goals of continuous delivery is to get fast feedback on your hypotheses so you can build the right thing. In this talk Frank Harris and Nell Thomas of Etsy show off a bunch of their tools, including the A/B testing framework they built for running experiments (which uses feature toggles under the hood). They give an example of an experiment they're running right now, and discuss how the ability to gather and analyze data on customer behaviour in real time (see screenshot below) affects the way they do product development. In this talk, John Penix of Google shows off the awesome product he and his team built for continuous integration and cloud-based testing at Google. Teams at Google are free to choose their own development practices and toolchain, but this one has a pretty high uptake. When people ask me if trunk-based development and continuous integration can scale, I like to show them the following slide: In addition to discussing the process he uses to release twice a day, Facebook's lead release engineer Chuck Rossi shows off the extensive toolchain they built to deploy at scale. Highlights include Gatekeeper (screenshot below), which manages who gets to see which features as part of their dark launching process, and their deploy tool which categorizes all proposed patches based on the size of the patch, the amount of discussion around it, and the \"push karma\" of the committers. Amazon, Etsy, Google and Facebook are all primarily software development shops which command enormous amounts of resources. They are, to use Christopher Little's metaphor, unicorns. How can the rest of us adopt continuous delivery? That's the subject of my talk, which describes four case studies of organizations that adopted continuous delivery, with varying degrees of success. One of my favourites – partly because it's embedded software, not a website – is the story of HP's LaserJet Firmware team, who re-architected their software around the principles of continuous delivery. People always want to know the business case for continuous delivery: the FutureSmart team provide one in the book they wrote that discusses how they did it:","tags":"ciandcd"},{"url":"http://www.ciandcd.com/risk-management-theatre-on-show-at-an-organization-near-you.html","title":"Risk Management Theatre: On Show At An Organization Near You","text":"From: http://continuousdelivery.com/2013/08/risk-management-theatre/ How To Create A More Diverse Tech Conference Videos from the Continuous Delivery track at QCon SF 2012 » Risk Management Theatre: On Show At An Organization Near You Translations: 한국말 One of the concepts that will feature in the new book I am working on is \"risk management theatre\". This is the name I coined for the commonly-encountered control apparatus, imposed in a top-down way, which makes life painful for the innocent but can be circumvented by the guilty (the name comes by analogy with security theatre .) Risk management theatre is the outcome of optimizing processes for the case that somebody will do something stupid or bad, because (to quote Bjarte Bogsnes talking about management ), \"there might be someone who who cannot be trusted. The strategy seems to be preventative control on everybody instead of damage control on those few.\" Unfortunately risk management theatre is everywhere in large organizations, and reflects the continuing dominance of the Theory X management paradigm. The alternative to the top-down control approach is what I have called adaptive risk management, informed by human-centred management theories (for example the work of Ohno , Deming , Drucker, Denning and Dweck ) and the study of how complex systems behave, particularly when they drift into failure . Adaptive risk management is based on systems thinking, transparency, experimentation, and fast feedback loops. Here are some examples of the differences between the two approaches. Adaptive risk management (people work to detect problems through improving transparency and feedback, and solve them through improvisation and experimentation) Risk management theatre (management imposes controls and processes which make life painful for the innocent but can be circumvented by the guilty) Continuous code review in which engineers ask a colleague to look over their changes before check-in, technical leads review all check-ins made by their team, and code review tools allow people to comment on each others' work once it is in trunk. Mandatory code review enforced by check-in gates where a tool requires changes to be signed off by somebody else before they can be merged into trunk. This is inefficient and delays feedback on non-trivial regressions (including performance regressions). Fast, automated unit and acceptance tests which inform engineers within minutes (for unit tests) or tens of minutes (for acceptance tests) if they have introduced a known regression into trunk, and which can be run on workstations before commit. Manual testing as a precondition for integration, especially when performed by a different team or in a different location. Like mandatory code review, this delays feedback on the effect of the change on the system as a whole. A deployment pipeline which provides complete traceability of all changes from check-in to release, and which detects and rejects risky changes automatically through a combination of automated tests and manual validations. A comprehensive documentation trail so that in the event of a failure we can discover the human error that is the root cause of failures in the mechanistic, Cartesian paradigm that applies in the domain of systems that are not complex . Situational awareness created through tools which make it easy to monitor, analyze and correlate relevant data. This includes process, business and systems level metrics as well as the discussion threads around events. Segregation of duties which acts as a barrier to knowledge sharing, feedback and collaboration, and reduces the situational awareness which is essential to an effective response in the event of an incident. It's important to emphasize that there are circumstances in which the countermeasures on the right are appropriate. If your delivery and operational processes are chaotic and undisciplined, imposing controls can be an effective way to improve – so long as we understand they are a temporary countermeasure rather than an end in themselves, and provided they are applied with the consent of the people who must work within them. Here are some differences between the two approaches in the field of IT: Adaptive risk management (people work to detect problems through improving transparency and feedback, and solve them through improvisation and experimentation) Risk management theatre (management imposes controls and processes which make life painful for the innocent but can be circumvented by the guilty) Principle-based and dynamic: principles can be applied to situations that were not envisaged when the principles were created. Rule-based and static : when we encounter new technologies and processes (for example, cloud computing) we need to rewrite the rules. Uses transparency to prevent accidents and bad behaviour. When it's easy for anybody to see what anybody else is doing, people are more careful. As Louis Brandeis said, \"Publicity is justly commended as a remedy for social and industrial diseases. Sunlight is said to be the best of disinfectants; electric light the most efficient policeman.\" Uses controls to prevent accidents and bad behaviour. This approach is the default for legislators as a way to prove they have taken action in response to a disaster. But controls limit our ability to adapt quickly to unexpected problems. This introduces a new class of risks, for example over-reliance on emergency change processes because the standard change process is too slow and bureaucratic. Accepts that systems drift into failure. Our systems and the environment are constantly changing, and there will never be sufficient information to make globally rational decisions. Humans solve our problems and we must rely on them to make judgement calls. Assumes humans are the problem. If people always follow the processes correctly, nothing bad can happen. Controls are put in place to manage \"bad apples\". Ignores the fact that process specifications always require interpretation and adaptation in reality. Rewards people for collaboration, experimentation, and system-level improvements. People collaborate to improve system-level metrics such as lead time and time to restore service. No rewards for \"productivity\" on individual or function level. Accepts that locally rational decisions can lead to system level failures. Rewards people based on personal \"productivity\" and local optimization . For example operations people optimizing for stability at the expense of throughput, or developers optimizing for velocity at the expense of quality (even though these are false dichotomies.) Creates a culture of continuous learning and experimentation : People openly discuss mistakes to learn from them and conduct blameless post-mortems after outages or customer service problems with the goal of improving the system. People are encouraged to try things out and experiment (with the expectations that many hypotheses will be invalidated) in order to get better. Creates a culture of fear and mistrust . Encourages finger pointing and lack of ownership for errors, omissions and failure to get things done. As in: If I don't do anything unless someone tells me to, I won't be held responsible for any resulting failure. Failures are a learning opportunity . They occur in controlled circumstances, their effects are appropriately mitigated, and they are encouraged as an opportunity to learn how to improve. Failures are caused by human error (usually a failure to follow some process correctly), and the primary response is to find the person responsible and punish them, and then use further controls and processes as the main strategy to prevent future problems. Risk management theatre is not just painful and a barrier to the adoption of continuous delivery (and indeed to continuous improvement in general). It is actually dangerous, primarily because it creates a culture of fear and mistrust. As Bogsnes says, \"if the entire management model reeks of mistrust and control mechanisms against unwanted behavior, the result might actually be more, not less, of what we try to prevent. The more people are treated as criminals, the more we risk that they will behave as such.\" This kind of organizational culture is a major factor whenever we see people who are scared of losing their jobs, or engage in activities designed to protect themselves in the case that something goes wrong, or attempt to make themselves indispensable through hoarding information. I'm certainly not suggesting that controls, IT governance frameworks, and oversight are bad in and of themselves. Indeed, applied correctly, they are essential for effective risk management. ITIL for example allows for a lightweight change management process that is completely compatible with an adaptive approach to risk management. What's decisive is how these framework are implemented. The way such frameworks are used and applied is determined by—and perpetuates— organizational culture .","tags":"ciandcd"},{"url":"http://www.ciandcd.com/how-to-create-a-more-diverse-tech-conference.html","title":"How To Create A More Diverse Tech Conference","text":"From: http://continuousdelivery.com/2013/09/how-we-got-40-female-speakers-at-flowcon/ How To Create A More Diverse Tech Conference I have been advised by people I trust that it's not a good idea to talk about how you got serious female representation at your conference until after it's over. However the shameful RubyConf \"binders full of men\" debacle and the Neanderthal level of discussion around it has wound me up enough to write this account somewhat prematurely. So here is how we achieved >40% female representation on our speaker roster at FlowCon . Step 0. Care About The Outcome. When John Esser approached me to put together a conference about continuous delivery, devops and lean product development, I thought carefully about it. I've helped put together a conference program before ( QCon SF 2012 ), and that was pretty hard work, so I wanted to be sure I had the correct motivation. One of the things that I have always disliked about tech conferences is being surrounded by a bunch of other straight white guys (nothing personal, some of my best friends are straight white guys). It's a constant reminder of the fact that, due to a number of socioeconomic factors, straight white guys have it easier than others . I wanted to put together a conference which reflects my community as I would like it to look, not as it actually looks. So one of the four values the FlowCon program committee came up with was this: \"Diversity: We believe the technology community – and thus the conference speakers and participants – should reflect the demographics of our customers and the wider world.\" There are two reasons for this. Firstly, we can't effectively change the world through technology without diversity. To find out why, come and see Ashe Dryden talk about how \"diverse communities and workplaces create better products\" . Second, one of the main reasons I like working at ThoughtWorks is that one of the three pillars of our mission is to \"advocate passionately for social and economic justice.\" The fact there are so few women in IT reflects social and economic injustice inherent in our world. Making sure you actually have a mission for your conference is something I learned from helping out with QCon SF . It is a constant reminder of why you're doing it and what's important about it. If you don't have a mission, you're at the mercy of the implicit biases of the organizers. As RubyConf shows, you can't just throw in the \"one weird trick\" of anonymous submissions and expect that it will somehow solve the problem. Everybody on the program committee actually has to care about the outcome, or they won't put in the right amount of work to make it happen. Once you do that, the rest of the steps aren't that hard. Step 1. Make Sure Your Program Committee Is Aligned With Your Mission Once I had an idea about the mission of the conference, I reached out to some people whom I thought would share it. I was lucky enough that Elizabeth Hendrickson , Lane Halley and Gene Kim agreed to join John Esser and me on the program committee. One of the main reasons I asked those particular people, apart from being extremely competent and well-respected in their field, was another conference goal: \"Spanning boundaries: We believe that the best products are created collaboratively by people with a range of skills and experiences.\" The program committee has representation from the UX, testing, operations, product development and programming communities. Step 2. Make Sure Your Invited Speakers Are Aligned With Your Mission. We made the decision to have about half the program be invited speakers. Part of that was about ensuring that we had a solid core program. But it was also a chance for us to put our mission into practice, so that when we put out the call for proposals we had a bunch of confirmed speakers who demonstrated we were serious about our mission. Thus we made sure that the invited speakers were respected boundary spanners, and that 50% of them were women. This involved more work than we would have had to put in had we just invited our friends (a popular strategy for program committees). It was also telling that we got more refusals from women than we got from men due to schedule conflicts. The main factor here was that female speakers are actually in greater demand than men because there are relatively fewer of them. Step 3. The Anonymous Call For Proposals If you jump straight to step 3, it's likely you will suffer the fate of RubyConf and fail. If you use this as your only strategy for increasing representation it won't work. This strategy has been thoroughly discussed by others who have used this approach as part of increasing diversity at their conference. We created a form in Google Docs for people to propose talks. They had to enter their email address, but we mentioned in the form that they should use one that didn't identify them if they wanted their proposal to be more anonymous. Of the 82 people who submitted a talk proposal, 18 (21%) were women as far as we can work out (once the program was confirmed I used Rapportive to reverse-engineer email addresses based on publicly available information). Ultimately, three of the eight people who made it into the final program based on submitted proposals were women. The low female representation through the CFP is the reason our program isn't 50% female. Even getting the 21% of submissions that we did involved reaching out through mailing lists, Twitter, and our networks to encourage women to submit. This step, along with making it clear that you actually care, is essential if you in fact expect women to submit through the anonymous CFP. Observations These four steps resulted in 10 of our 24 speakers being women . I have three main observations coming out of this process: First, unlike increasing the number of women who take programming classes in school or enter the IT industry and don't immediately quit in horror, creating a conference with reasonable female representation is not actually a hard problem. Yes, we put in more work to achieve this goal than we would have had we not cared. But it wasn't significantly more. Conference organizers who claim to care but fail to achieve good representation should quit whining and take real steps to achieve this goal. The community should hold them to higher standards. If the conference speakers are a bunch of straight white guys, the only reason is that the organizers didn't care enough. Second, in the wake of RubyConf, I have been angered but unsurprised to observe the usual chorus about how increasing representation somehow means lowering standards. Not only is this incredibly insulting to the many extraordinary women working in our industry, but it is just false. I dare anyone to look at the kick-ass program we have put together for FlowCon and try and claim that we have somehow lowered standards to achieve great a barely acceptable level of representation. Another thing you will hear is that it is harder to find female speakers on \"hard\" topics such as programming than for \"soft\" ones. I find this claim baffling because in my experience changing organizational culture (considered a \"soft\" topic) is, in my experience, way way harder than knocking out lines of code (even well-factored unit-tested ones). But you'll see on our program that women are covering the whole gamut from organizational change to refactoring to configuration management . Third, it's not all good news. In particular, we have only one non-white speaker. I'll hold my hand up on this – we didn't explicitly set non-white representation as a goal within the program committee, and by the time it became obvious it was a problem (Step 3) it was too late to do anything. This demonstrates why steps 0-2 are important. If we run FlowCon again, we will do better. Meanwhile check out the program , and follow this link to register with a 10% discount. If you need more than a one day conference to come to San Francisco, Balanced Team are running their conference the following two days. End notes Another popular silencing tactic in this discussion is that bringing attention to the level of diversity in a conference is in itself a form of sexism or racism. There's a cartoon on the left which expresses nicely why this is in fact horribly misguided (or you could check out one of the many excellent articles on \"colourblindness\" and racism ). Check out the Geek Feminism blog and wiki for tons of useful information and advice on making things better for women in tech. Also check out the @CallbackWomen and @DevChix Twitter accounts to spread the word for your CFP. Ashe Dryden also wrote an excellent post on creating more diverse conferences. Another important factor when designing a woman-friendly conference is to create an anti-harassment policy. Check out this account of a woman who actually needed to use the anti-harassment policy (trigger alert). UPDATE Of course, this entry is now starting to receive the attention of anonymous trolls. I've left the first one as an example of the idiocy that passes for dialogue in this debate (and from supposedly smart people at that). But forthwith I'll be deleting anonymous or otherwise uncivil posts.","tags":"ciandcd"},{"url":"http://www.ciandcd.com/flowcon-2013-wrap-up-with-some-hard-data-on-gender-diversity-in-tech-conferences.html","title":"FlowCon 2013 Wrap-Up, With Some Hard Data on Gender Diversity in Tech Conferences.","text":"From: http://continuousdelivery.com/2013/12/flowcon-2013-wrap-up/ FlowCon 2013 Wrap-Up, With Some Hard Data on Gender Diversity in Tech Conferences. Thanks to all of you who came along to FlowCon! If you weren't able to make it, you can watch the videos for free thanks to BMC and ThoughtWorks Studios . The slides are also available for downloading. Let me first express my thanks to our producers: Geeta Schmidt and Niley Barros of Trifork and Rebecca Phillips of ThoughtWorks Studios . I also want to thank my fellow PC members Lane Halley , Elisabeth Hendrickson , Gene Kim and John Esser ; our fabulous speakers ; our generous sponsors ; and everyone who came along. The Program The goal of the program committee was to create a conference that represents our industry as we want it to look, not as it is right now. That's an ambitious goal that involves changing the way we think about everything from leadership and governance through product development and design , to IT operations . Not only did our speakers cover all these topics; they also provided real examples of how these changes, along with the cultural changes necessary to support them, have been achieved at enterprise scale. Thus we attacked one of the main objections we hear time and time again — \"that sounds great, but it couldn't work here\". Part of our vision was to provide a platform for people to speak about gnarly, real-life examples that demonstrate that, with sufficient hard work and ingenuity, ideas like continuous delivery, devops, and lean product development can provide significant competitive advantage through higher quality, cost savings, and happier customers, even in traditionally slow-moving and highly-regulated industries with large, complex, heterogeneous systems. Two talks that I am particularly happy to have on record are Gary Gruver's talk on doing continuous delivery for printer firmware at HP, and John Kordyback's talk on doing continuous delivery with mainframes in the financial services industry. Alternatively, if you want a vision of the state of the art of continuous delivery, it would be hard to beat Adrian Cockcroft's opening keynote (the most highly rated talk of the conference) on how Netflix approach building and running systems. Overall, both the individual quality of the talks and the vision they present in concert was incredibly inspiring. Gene Kim comments, \"The FlowCon program was amazing. In my mind, what was presented at FlowCon is what every IT practitioner will be required to know in 10 years time.\" Thank you again to all of our speakers. Data on Gender Diversity Part of representing the industry as we want it to look is changing its composition. Thus another personal goal for me was to gather data to support my hypothesis that taking steps to increase diversity at conferences doesn't mean reducing quality. FlowCon, like the excellent GOTO conferences that Trifork produces, records feedback from participants. Everybody leaving a session can give feedback on whether they thought the talk was good, mediocre or poor by tapping a red, amber or green rectangle on an iPhone on their way out. We then calculate overall satisfaction as follows: satisfaction = (green votes) / (total votes). When we got back all the data, the first thing I did is look at the average (mean) satisfaction for male speakers versus female speakers. It turns out that in both cases the average is between 71% and 72%. First of all, this demonstrates that there was no statistically significant difference in satisfaction between male and female speakers. This is important because it means our steps to increase diversity — including reaching out to a wide network to ensure that 50% of our invited speakers were women — didn't \"lower the bar\". There is also a deeper implication: any claim that the all-white-male conference programs that are so depressingly common in the tech industry are the result of some meritocratic process is BS. They are, rather, the result of not putting in enough effort to seek out high quality speakers from historically discriminated against groups. If our industry were truly meritocratic, the speaker line-up and attendees would resemble the wider population, because we know that there is no biological explanation for the overwhelming proportion of white dudes in our industry. So let's not fool ourselves any more with claims that taking steps to improve diversity is \"reverse discrimination\". Any time we don't take concrete, systematic steps forward we are silently complicit in perpetuating the status quo — which is why it's not good enough when leaders in the tech community ignore the problem. If you ignore the problem, you're part of the problem. Finally, I want to emphasize that what the program committee achieved was not very hard, once we spent some time thinking the problem through, and also that it was insufficient. We had a reasonable level of gender diversity, but the speakers were still overwhelmingly white. I don't have data for the diversity of our audience, but based on observation, there were more white guys than I would see if I walked out of the door onto the streets (and this is in San Francisco, which is far from being representative of the wider population). If you want to educate yourself further on these issues, I suggest watching Ashe Dryden's talk on programming diversity. And if you'd like to become more effective at creating change, check out Linda Rising's closing keynote . Here's to taking small steps every day to make 2014 a marginally, incrementally, better year than 2013 .","tags":"ciandcd"},{"url":"http://www.ciandcd.com/the-science-behind-the-2013-puppet-labs-devops-survey-of-practice.html","title":"The Science Behind the 2013 Puppet Labs DevOps Survey Of Practice","text":"From: http://continuousdelivery.com/2013/12/the-science-behind-the-2013-puppet-labs-devops-survey-of-practice/ The Science Behind the 2013 Puppet Labs DevOps Survey Of Practice By Gene Kim and Jez Humble Last year, we both had the privilege of working with Puppet Labs to develop the 2012 DevOps Survey Of Practice. It was especially exciting for Gene, because we were able to benchmark the performance of over 4000 IT organizations, and to gain an understanding what behaviors result in their incredible performance. This continues research that he has been doing of high performing IT organizations that started for him in 1999. In this blog post, Gene Kim and I will discuss the research hypotheses that we're setting out to test in the 2013 DevOps Survey Of Practice, explain the mechanics of how these types of cross-population studies actually work (so you help this research effort or even start your own), then describe the key findings that came out of the 2012 study. But first off, if you're even remotely interested in DevOps, go take the 2013 Puppet Labs DevOps Survey here ! The survey closes on January 15, 2014, so hurry! It only takes about ten minutes. 2013 DevOps Survey Research Goals Last year's study (which we'll describe in more detail below) found that high performing organizations that were employing DevOps practices were massively outperforming their peers: they were doing 30x more frequent code deploys, and had deployment lead times measured in minutes or hours (versus lower performers, who required weeks, months or quarters to complete their deployments). The high performers also had far better deployment outcomes: their changes and deployments had twice the change success rates, and when the changes failed, they could restore service 12x faster. The goal of the 2013 study is to gain a better understanding of exactly what practices are required to achieve this high performance. Our hypothesis is that the following are required, and we'll be looking to independently evaluate the effect of each of these practices on performance: small teams with high trust that span the entire value stream: Dev, QA, IT Operations and Infosec shared goals and shared pain that span the entire value stream small development batch sizes presence of continuous, automated integration and testing emphasis on creating a culture of learning, experimentation and innovation emphasis on creating resilient systems We are also testing two other hypotheses that one of us (Gene) is especially excited about, because it's something he's wanted to do ever since 1999! Lead time : In plant manufacturing, lead time is the time required to turn raw materials into finished goods. There is a deeply held belief in the Lean community that lead time is the single best predictor of quality, customer satisfaction and employee happiness. We are testing this hypothesis for the DevOps value stream in the 2013 survey instrument. Organizational performance : Last year, we confirmed that DevOps practices correlate with substantially improved IT performance (e.g., deploy frequencies, lead times, change success rates, MTTR). This year, we will be testing whether improved IT performance correlates with improved business performance. In this year's study, we've added inserted three questions that are known to correlate with organizational performance, which is known to correlate with business performance (e.g., competitiveness in the marketplace, return on assets, etc.). Our dream headline would be, \"high performing organizations not only do 30x more frequent code deployments than their peers, but they also outperform the S&P 500 by 3x as measured by shareholder return and return on assets.\" Obviously, there are many other variables that contribute to business performance besides Dev and Ops performance (e.g., profitability, market segment, market share, etc.). However, in our minds, the reliance upon IT performance is obvious: as Chris Little said, \"Every organization is an IT business, regardless of what business they think they're in.\" When IT does poorly, the business will do poorly. And when IT helps the organization win, those organizations will out-perform their competitors in the marketplace. (This hypothesis forms the basis of the hedge fund that Erik wants to create in the last chapter of \"The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win\" , where they would make long or short bets, based on the known operating characteristics of the IT organization.) The Theory Behind Cross-Population Studies and Survey Instruments Like last year, this year's DevOps survey is a cross-population study, designed to explore the link between organizational performance and organizational practices and cultural norms. What is a cross-population study? It's a statistical research technique designed to uncover what factors (e.g., practices, cultural norms, etc.) correlate with outcomes (e.g., IT performance). Cross-population studies are often used in medical research to answer questions like, \"is cigarette smoking a significant factor in early mortality?\" Properly designed cross-population studies are considered a much more rigorous approach of testing efficacy of what practices work than say, interviewing people about what they think worked, ROI stories from vendors, or collecting \"known, best practices.\" When doing survey design, we might state our hypotheses in the following form: \"we believe that IT organizations which have high trust have higher IT performance.\" In other words, the higher the trust levels in the IT organization, the higher the performance. We then put this question in the survey instrument, and then analyze the results. If we were to plot the results on a graph, we would put the dependent variable (i.e., performance) on the Y-axis, and the independent variable (i.e., presence of high trust) on the X-axis. We would then test to see if there is a correlation between the two. Shown below is an example of what it looks like when the two variables have low or no correlation, and one that has a significant positive correlation. If we were to find a significant correlation, such as displayed on the right, we could then assert that \"the higher your organization's trust levels, in general, the higher your IT performance.\" (Graph adapted from Wikipedia entry on Correlation and Dependence .) The 2012 DevOps Survey In this section, we will describe the the key findings that came out of the 2012 DevOps Survey, as well as a brief discussion of the research hypotheses that went into the survey design. In the DevOps community, we have long asserted that certain practices enables organizations simultaneously deliver fast flow of features to market, while providing world-class stability, reliability and security. We designed the survey to validate this, and tested a series of technical practices to determine which of them correlated with high performance. The survey ran for 30 days, and we had 4,039 completed respondents. (This is an astonishingly high number, by the way. When Kurt Milne and Gene Kim did similar studies in 2006, each study typically required $200K to do the survey design, gather responses from a couple hundred people, and then perform survey analysis.) You can find the slides that Gene Kim, Jez Humble and James Turnbull presented at the 2013 Velocity Conference here , and the full Puppet Labs infographics and results here . The first surprise was how much the high performing organizations were outperforming their non-high-performing peers: Agility metrics 30x more frequent code deployments 8,000x faster lead time than their peers Reliability metrics 2x the change success rate 12x faster MTTR In other words, they were more agile: they were deploying code 30x more frequently, and the lead time required to go from \"code committed\" to \"successfully running in production\" was completed 8,000x faster — high performers had lead times measured in minutes or hours, while lower performers had lead times measured in weeks, months or even quarters. Not only were the high performers doing more work, but they had far better outcomes: when the high performers deployed changes and code, they were twice as likely to be completed successfully (i.e., without causing a production outage or service impairment), and when the change failed and resulted in an incident, the time required to resolve the incident was 12x faster. We were astonished and delighted with this finding, as it showed not only that it was possible to break the core, chronic conflict, but that it seemed to confirm that just as in manufacturing, agility and reliability go hand in hand. In other words, lead time correlates with both both agility and reliability. (Gene will write more on his personal interpretations of the 2012 DevOps Survey Of Practice in a future post.) Conclusion We hope this gives you a good idea of why we've worked so hard on the 2012 and 2013 DevOps Survey, as well as how to conduct your own cross-population studies. Please let us know if you have any questions or if there's anything we can do for you. And of course, help us understand what in DevOps and Continuous Delivery work by taking 10 minutes to participate in the 2013 Puppet Labs DevOps Survey here by January 15, 2014 ! Thank you! –Gene Kim and Jez Humble","tags":"ciandcd"},{"url":"http://www.ciandcd.com/visualizations-of-continuous-delivery.html","title":"Visualizations of Continuous Delivery","text":"From: http://continuousdelivery.com/2014/02/visualizations-of-continuous-delivery/ Visualizations of Continuous Delivery Nhan Ngo , a QA engineer at Spotify , made four fabulous visualizations while reading Continuous Delivery . She has very kindly agreed to make them available under a Creative Commons license so feel free to share them, download them, and print them out (click to get a higher resolution version). Thank you Nhan!","tags":"ciandcd"},{"url":"http://www.ciandcd.com/the-2014-state-of-devops-report-is-here.html","title":"The 2014 State of DevOps Report Is Here!","text":"From: http://continuousdelivery.com/2014/06/the-2014-state-of-devops-report-is-here/ The 2014 State of DevOps Report Is Here! DevOps, a movement of people who care about developing and operating reliable, secure, high performance systems at scale, has always — intentionally — lacked a definition or manifesto. However (and this is fascinating in its own right) that doesn't mean that we can't measure the impact of DevOps, or how good people are at doing it. The proof of this, and also of the startling impact of the DevOps movement, is now available in the form of the 2014 State of DevOps report (which you can download for free ). The report, a collaboration between Nicole Forsgren Velasquez , Gene Kim , Puppet Labs , and yours truly , surveyed over 9,200 people worldwide, covering a wide range of industries and types of organization. Our goal for the report was ambitious. We set out to measure IT performance, business performance, the impact of particular practices (such as continuous integration, test automation, and version control), and also culture, and then to discover to what extent they influenced each other. How, you might ask, do you measure these things like culture and organizational performance? Following Douglas Hubbard's definition of measurement as \"A quantitatively expressed reduction of uncertainty based on one or more observations,\" it turns out that you can measure anything if you put your mind to it. The report describes both our methodology and the way we measured these apparent intangibles. Indeed we not only measured these things: we have sound, statistically significant data that shows that culture and DevOps practices impact both IT performance and organizational performance. In direct contradiction to a popular narrative of the last ten years, IT matters — indeed, the results show it is a competitive advantage — and DevOps culture and practices are instrumental in achieving both high IT performance and organizational performance. Readers of this blog will be especially interested to learn that: Trunk-based development, continuous integration, and automated testing measurably improve both IT performance and organizational performance. Having a high-trust culture has a strong impact on both IT performance and organizational performance. Using external change approval processes such as a change advisory board, as opposed to peer-based code review techniques, significantly impacts throughput while doing almost nothing to improve stability. Job satisfaction is the biggest predictor of organizational performance, and using DevOps practices are good predictors of job satisfaction. I'm very excited by the report. We improved on last year's method for measuring IT performance. We showed how you can measure culture and organizational performance. Most important, the analysis of our enormous data set demonstrates definitively that the strategies championed by the DevOps movement work, and that they provide a competitive advantage to your business. Many thanks to my collaborators, the fabulous team at PuppetLabs, and to all of you who took the survey. You can download the 2014 State of Devops Report for free.","tags":"ciandcd"},{"url":"http://www.ciandcd.com/jian-dan-de-proxyzhi-tinyhttpproxypy.html","title":"简单的proxy之TinyHTTPProxy.py","text":"From: http://www.cnblogs.com/itech/p/3800590.html 简单的proxy之TinyHTTPProxy.py 如果是在外企工作的话，可以访问美国的机器，这样就可以在美国的机器上为自己装个proxy，然后本地就可以很容易的使用proxy来上网了。 TinyHTTPProxy.py : 主页： http://www.voidtrance.net/2010/01/simple-python-http-proxy/ 下载： http://www.voidtrance.net/downloads/tiny-proxy-0.3.1.tar.gz 使用方法： 1）很好用，下载然后在后台运行。只依赖于基本的python modules，运行的时候不需要root权限。 proxy [-p port] [-l logfile] [-dh] [allowed_client_name ...]] -p - Port to bind to -l - Path to logfile. If not specified, STDOUT is used -d - Run in the background 2） Chrome中的switchsharper插件的配置： TinyHTTPProxy.py的源代码: #!/usr/bin/python __doc__ = \"\"\"Tiny HTTP Proxy. This module implements GET, HEAD, POST, PUT and DELETE methods on BaseHTTPServer, and behaves as an HTTP proxy. The CONNECT method is also implemented experimentally, but has not been tested yet. Any help will be greatly appreciated. SUZUKI Hisao 2009/11/23 - Modified by Mitko Haralanov * Added very simple FTP file retrieval * Added custom logging methods * Added code to make this a standalone application \"\"\" __version__ = \"0.3.1\" import BaseHTTPServer, select, socket, SocketServer, urlparse import logging import logging.handlers import getopt import sys import os import signal import threading from types import FrameType, CodeType from time import sleep import ftplib DEFAULT_LOG_FILENAME = \"proxy.log\" class ProxyHandler (BaseHTTPServer.BaseHTTPRequestHandler): __base = BaseHTTPServer.BaseHTTPRequestHandler __base_handle = __base.handle server_version = \"TinyHTTPProxy/\" + __version__ rbufsize = 0 # self.rfile Be unbuffered def handle(self): (ip, port) = self.client_address self.server.logger.log (logging.INFO, \"Request from '%s'\", ip) if hasattr(self, 'allowed_clients') and ip not in self.allowed_clients: self.raw_requestline = self.rfile.readline() if self.parse_request(): self.send_error(403) else: self.__base_handle() def _connect_to(self, netloc, soc): i = netloc.find(':') if i >= 0: host_port = netloc[:i], int(netloc[i+1:]) else: host_port = netloc, 80 self.server.logger.log (logging.INFO, \"connect to %s:%d\", host_port[0], host_port[1]) try: soc.connect(host_port) except socket.error, arg: try: msg = arg[1] except: msg = arg self.send_error(404, msg) return 0 return 1 def do_CONNECT(self): soc = socket.socket(socket.AF_INET, socket.SOCK_STREAM) try: if self._connect_to(self.path, soc): self.log_request(200) self.wfile.write(self.protocol_version + \" 200 Connection established\\r\\n\") self.wfile.write(\"Proxy-agent: %s\\r\\n\" % self.version_string()) self.wfile.write(\"\\r\\n\") self._read_write(soc, 300) finally: soc.close() self.connection.close() def do_GET(self): (scm, netloc, path, params, query, fragment) = urlparse.urlparse( self.path, 'http') if scm not in ('http', 'ftp') or fragment or not netloc: self.send_error(400, \"bad url %s\" % self.path) return soc = socket.socket(socket.AF_INET, socket.SOCK_STREAM) try: if scm == 'http': if self._connect_to(netloc, soc): self.log_request() soc.send(\"%s %s %s\\r\\n\" % (self.command, urlparse.urlunparse(('', '', path, params, query, '')), self.request_version)) self.headers['Connection'] = 'close' del self.headers['Proxy-Connection'] for key_val in self.headers.items(): soc.send(\"%s: %s\\r\\n\" % key_val) soc.send(\"\\r\\n\") self._read_write(soc) elif scm == 'ftp': # fish out user and password information i = netloc.find ('@') if i >= 0: login_info, netloc = netloc[:i], netloc[i+1:] try: user, passwd = login_info.split (':', 1) except ValueError: user, passwd = \"anonymous\", None else: user, passwd =\"anonymous\", None self.log_request () try: ftp = ftplib.FTP (netloc) ftp.login (user, passwd) if self.command == \"GET\": ftp.retrbinary (\"RETR %s\"%path, self.connection.send) ftp.quit () except Exception, e: self.server.logger.log (logging.WARNING, \"FTP Exception: %s\", e) finally: soc.close() self.connection.close() def _read_write(self, soc, max_idling=20, local=False): iw = [self.connection, soc] local_data = \"\" ow = [] count = 0 while 1: count += 1 (ins, _, exs) = select.select(iw, ow, iw, 1) if exs: break if ins: for i in ins: if i is soc: out = self.connection else: out = soc data = i.recv(8192) if data: if local: local_data += data else: out.send(data) count = 0 if count == max_idling: break if local: return local_data return None do_HEAD = do_GET do_POST = do_GET do_PUT = do_GET do_DELETE=do_GET def log_message (self, format, *args): self.server.logger.log (logging.INFO, \"%s %s\", self.address_string (), format % args) def log_error (self, format, *args): self.server.logger.log (logging.ERROR, \"%s %s\", self.address_string (), format % args) class ThreadingHTTPServer (SocketServer.ThreadingMixIn, BaseHTTPServer.HTTPServer): def __init__ (self, server_address, RequestHandlerClass, logger=None): BaseHTTPServer.HTTPServer.__init__ (self, server_address, RequestHandlerClass) self.logger = logger def logSetup (filename, log_size, daemon): logger = logging.getLogger (\"TinyHTTPProxy\") logger.setLevel (logging.INFO) if not filename: if not daemon: # display to the screen handler = logging.StreamHandler () else: handler = logging.handlers.RotatingFileHandler (DEFAULT_LOG_FILENAME, maxBytes=(log_size*(1<<20)), backupCount=5) else: handler = logging.handlers.RotatingFileHandler (filename, maxBytes=(log_size*(1<<20)), backupCount=5) fmt = logging.Formatter (\"[%(asctime)-12s.%(msecs)03d] \" \"%(levelname)-8s {%(name)s %(threadName)s}\" \" %(message)s\", \"%Y-%m-%d %H:%M:%S\") handler.setFormatter (fmt) logger.addHandler (handler) return logger def usage (msg=None): if msg: print msg print sys.argv[0], \"[-p port] [-l logfile] [-dh] [allowed_client_name ...]]\" print print \" -p - Port to bind to\" print \" -l - Path to logfile. If not specified, STDOUT is used\" print \" -d - Run in the background\" print def handler (signo, frame): while frame and isinstance (frame, FrameType): if frame.f_code and isinstance (frame.f_code, CodeType): if \"run_event\" in frame.f_code.co_varnames: frame.f_locals[\"run_event\"].set () return frame = frame.f_back def daemonize (logger): class DevNull (object): def __init__ (self): self.fd = os.open (\"/dev/null\", os.O_WRONLY) def write (self, *args, **kwargs): return 0 def read (self, *args, **kwargs): return 0 def fileno (self): return self.fd def close (self): os.close (self.fd) class ErrorLog: def __init__ (self, obj): self.obj = obj def write (self, string): self.obj.log (logging.ERROR, string) def read (self, *args, **kwargs): return 0 def close (self): pass if os.fork () != 0: ## allow the child pid to instanciate the server ## class sleep (1) sys.exit (0) os.setsid () fd = os.open ('/dev/null', os.O_RDONLY) if fd != 0: os.dup2 (fd, 0) os.close (fd) null = DevNull () log = ErrorLog (logger) sys.stdout = null sys.stderr = log sys.stdin = null fd = os.open ('/dev/null', os.O_WRONLY) #if fd != 1: os.dup2 (fd, 1) os.dup2 (sys.stdout.fileno (), 1) if fd != 2: os.dup2 (fd, 2) if fd not in (1, 2): os.close (fd) def main (): logfile = None daemon = False max_log_size = 20 port = 8000 allowed = [] run_event = threading.Event () local_hostname = socket.gethostname () try: opts, args = getopt.getopt (sys.argv[1:], \"l:dhp:\", []) except getopt.GetoptError, e: usage (str (e)) return 1 for opt, value in opts: if opt == \"-p\": port = int (value) if opt == \"-l\": logfile = value if opt == \"-d\": daemon = not daemon if opt == \"-h\": usage () return 0 # setup the log file logger = logSetup (logfile, max_log_size, daemon) if daemon: daemonize (logger) signal.signal (signal.SIGINT, handler) if args: allowed = [] for name in args: client = socket.gethostbyname(name) allowed.append(client) logger.log (logging.INFO, \"Accept: %s (%s)\" % (client, name)) ProxyHandler.allowed_clients = allowed else: logger.log (logging.INFO, \"Any clients will be served...\") server_address = (socket.gethostbyname (local_hostname), port) ProxyHandler.protocol = \"HTTP/1.0\" httpd = ThreadingHTTPServer (server_address, ProxyHandler, logger) sa = httpd.socket.getsockname () print \"Servering HTTP on\", sa[0], \"port\", sa[1] req_count = 0 while not run_event.isSet (): try: httpd.handle_request () req_count += 1 if req_count == 1000: logger.log (logging.INFO, \"Number of active threads: %s\", threading.activeCount ()) req_count = 0 except select.error, e: if e[0] == 4 and run_event.isSet (): pass else: logger.log (logging.CRITICAL, \"Errno: %d - %s\", e[0], e[1]) logger.log (logging.INFO, \"Server shutdown\") return 0 if __name__ == '__main__': sys.exit (main ()) 其他的python proxy： https://github.com/senko/tornado-proxy 完！","tags":"中文"},{"url":"http://www.ciandcd.com/markdown-yu-fa-he-gong-ju.html","title":"markdown 语法和工具","text":"From: http://www.cnblogs.com/itech/p/3800982.html 一 简明语法 来自： http://maxiang.info/ 二 markdown 语法 ： 三 markdown工具总结： 四 windows markdown 工具： cutemarked(推荐） 这个很不错，可以配置github的css样式，可以导出为html和pdf haroopad 下载不下来，但是看起来很不错 markdownsharpeditor 来自日本的工具，美工做的不是很好，但是可以支持MD extra的语法，可以使用 markdownpad 这个大家推荐的多，但是免费版的功能有限制，不喜欢。 五 在线的markdown工具： markdown-live-editor 如果没有安装本地的MD客户端，临时需要编辑MD的话，可以使用，不需要注册。在线编辑一个md文件，编辑后copy MD或HTML。 meeditor （推荐，但是不知道现在用的人多不，希望作者继续坚持发展） 需要注册，支持云存储。 简书 支持云存储，相互共享，还是不错的。上面文学方面的作者比较多。 stackedit 需要使用google或fackbook帐号，不是咱的菜 完！","tags":"中文"},{"url":"http://www.ciandcd.com/linuxhe-lei-unixxi-tong-shang-5ge-zui-jia-kai-yuan-bei-fen-gong-ju.html","title":"Linux和类Unix系统上5个最佳开源备份工具","text":"From: http://www.cnblogs.com/itech/p/4529663.html 一个好的备份最基本的目的就是为了能够从一些错误中恢复： 人为的失误 磁盘阵列或是硬盘故障 文件系统崩溃 数据中心被破坏等等。 所以，我为大家罗列了一些开源的软件备份工具。 当为一个企业选择备份工具的时候，你都考虑什么呢？ 确定你正在部署的软件具有下面的特性 开源软件 – 你务必要选择那些源码可以免费获得，并且可以修改的软件。确信可以恢复你的数据，即使是软件供应商/项目停止继续维护这个软件，或者是拒绝继续为这个软件提供补丁。 跨平台支持 – 确定备份软件可以很好的运行各种需要部署的桌面操作系统和服务器系统。 数据格式 – 一种开放的数据格式可以让你能够恢复数据，即使是供应商或是项目停止对软件的支持。 自动转换 – 自动转换本来是没什么，除了对于各种备份设备，包括图书馆，近线存储和自动加载，自动转换可以自动完成一些任务，包括加载，挂载和标签备份像磁带这些媒体设备。 备份介质 – 确定你可以备份到磁带，硬盘，DVD 和像 AWS 这样的云存储。 加密数据流 – 确定所有客户端到服务器的传输都被加密，保证在 LAN/WAN/Internet 中传输的安全性。 数据库支持 – 确定备份软件可以备份到数据库，像MySQL 或是 Oracle 。 备份可以跨越多个卷 – 备份软件(转储文件时)可以把每个备份文件分成几个部分，允许将每个部分存在于不同的卷。这样可以保证一些数据量很大的备份(像100TB的文件)可以被存储在一些单个容量较小的设备中，比如说像硬盘和磁盘卷。 VSS (卷影复制) – 这是 微软的卷影复制服务（VSS） ，通过创建数据的快照来备份。确定备份软件支持VSS的MS- Windows 客户端/服务器。 重复数据删除 – 这是一种数据压缩技术，用来消除重复数据的副本（比如，图片）。 许可证和成本 – 确定你对备份软件所用的 许可证了解和明白其使用方式 。 商业支持 – 开源软件可以提供社区支持（像邮件列表和论坛）和专业的支持（如发行版提供额外的付费支持）。你可以使用付费的专业支持为你提供培训和咨询。 报告和警告 – 最后，你必须能够看到备份的报告，当前的工作状态，也能够在备份出错的时候提供警告。 Bacula – 一个应用于多元化异构网络的客户端服务器备份工具 我个人应用这个软件来管理备份和通过网络来恢复系统，包括 Linux ， OSX， 和Windows。你可以通过CLI， GUI， 或者Web界面来配置Bacula。 操作系统：支持跨平台运行。 备份级别：完全，差异，增量，合并。 数据格式：支持自定义且完全开放。 自动转换：支持。 备份介质：支持磁带，磁盘和DVD。 加密数据流：支持。 数据库：支持MSSQL、PostgreSQL、Oracle 。 跨卷备份：支持 VSS（卷影复制）：支持。 许可：Affero General Public License v3.0。 下载链接： bacula.org Amanda – 又一个客户端服务器备份工具 AMANDA 是 Advanced Maryland Automatic Network Disk Archiver 的缩写。它允许系统管理员创建一个单独的备份服务器来将网络上的其他主机的数据备份到磁带驱动器、硬盘或者是自动换盘器。 操作系统：支持跨平台运行。 备份级别：完全，差异，增量，合并。 数据格式：开放（可以通过tar等工具恢复）。 自动转换：支持。 备份介质：支持磁带，磁盘和DVD。 加密数据流：支持。 数据库：支持MSSQL, Oracle。 跨卷备份：支持。 VSS（卷影复制）：支持。 许可：GPL, LGPL, Apache, Amanda License。 下载链接： amanda.org Backupninja – 轻量级备份系统 Backupninja 是一个简单易用的备份系统。你可以简单的拖放一个配置文件到 /etc/backup.d/ 目录来备份到多个主机。 操作系统：支持Linux，Unix。 备份级别：支持完全，差异备份（rsync + hard 链接） 数据格式：开放 自动转换：N/A。(注：N/A = Not Applicable)。 备份介质：磁盘，DVD，CD，ISO 镜像。 加密数据流：支持（ssh）和 通过duplicity远程加密备份 。 数据库：支持MySQL，PostgreSQL，OpenLDAP 和subversion 或trac。 跨卷备份：？？ VSS（卷影复制）:？？ 许可：GPL 下载链接： riseup.net Backuppc – 高效的客户端服务器备份工具 Backuppc 可以用来备份基于Linux 和Windows 系统的主服务器硬盘。它配备了一个巧妙的池计划来最大限度的减少磁盘储存、磁盘 I/O 和网络I/O。 操作系统：支持Linux，Unix 和Windows。 备份级别：支持完全和增量备份（rsync +hard 链接和pooling 计划） 数据格式：开放。 自动转换：N/A。 备份介质：磁盘和磁盘阵列。 加密数据流：支持。 数据库：支持（通过 Shell 脚本） 跨卷备份：？？ VSS（卷影复制）:？？ 许可：GPL。 下载链接： backuppc.sourceforge.net UrBackup – 最容易配置的客户端服务器系统 UrBackup 是一个非常容易配置的开源客户端服务器备份系统，通过镜像 方式和文件备份的组合完成了数据安全性和快速的恢复。磁盘卷备份可以使用可引导 CD 或U盘，通过Web界面或Windows资源管理器来恢复你的文件（硬恢复）。一个 Web 界面使得配置你自己的备份服务变得非常简单。 操作系统：支持Linux，FreeBSD，Unix，Windows 和少数基于NAS 的Linux操作系统，客户端只支持Linux 和Windows 操作系统。 备份级别：支持完全和增量备份。 数据格式：开放。 自动转换：N/A。 备份介质：磁盘，磁盘阵列和DVD。 加密数据流：支持。 数据库：？？ 跨卷备份：？？ VSS（卷影复制）：？？ 许可：GPL v3+ 下载链接： urbackup.org 其他供你考虑的一些极好用的开源备份软件 Amanda，Bacula 和上面所提到的这些软件功能都很丰富，但是对于一些小的网络或者是单独的服务器来说配置比较复杂。我建议你学习和使用一下的下面这些备份软件： 结论 我希望你会发现这篇有用的文章来备份你的数据。不要忘了验证你的备份和创建多个数据备份。注意，磁盘阵列并不是一个备份解决方案！使用任何一个上面 提到的程序来备份你的服务器、桌面和笔记本电脑和私人的移动设备。如果你知道其他任何开源的备份软件我没有提到的，请分享在评论里。 via: http://www.cyberciti.biz/open-source/awesome-backup-software-for-linux-unix-osx-windows-systems/ 作者： nixCraft 译者： barney-ro 校对： wxy 本文由 LCTT 原创翻译， Linux中国 荣誉推出","tags":"中文"},{"url":"http://www.ciandcd.com/4ge-ke-yi-fa-song-wan-zheng-dian-zi-you-jian-de-ming-ling-xing-gong-ju.html","title":"4个可以发送完整电子邮件的命令行工具","text":"From: http://www.cnblogs.com/itech/p/4530294.html 今天的文章里我们会讲到一些使用 Linux 命令行工具来发送带附件的电子邮件的方法。它有很多用处，比如在应用程序所在服务器上，使用电子邮件发送 一个文件过来，或者你可以在脚本中使用这些命令来做一些自动化操作。在本文的例子中，我们会使用foo.tar.gz文件作为附件。 有不同的命令行工具可以发送邮件，这里我分享几个多数用户会使用的工具，如 mailx 、 mutt 和 swaks 。 我们即将呈现的这些工具都是非常有名的，并且存在于多数Linux发行版默认的软件仓库中，你可以使用如下命令安装： 在 Debian / Ubuntu 系统 apt - get install mutt apt - get install swaks apt - get install mailx apt - get install sharutils 在基于Red Hat的系统，如 CentOS 或者 Fedora yum install mutt yum install swaks yum install mailx yum install sharutils 1) 使用 mail / mailx mailx 工具在多数Linux发行版中是默认的邮件程序，现在已经支持发送附件了。如果它不在你的系统中，你可以使用上边的命令安装。有一点需要注意，老版本的mailx可能不支持发送附件，运行如下命令查看是否支持。 $ man mail 第一行看起来是这样的： mailx [- BDdEFintv ~] [- s subject ] [- a attachment ] [- c cc - addr ] [- b bcc - addr ] [- r from - addr ] [- h hops ] [- A account ] [- S variable [= value ]] to - addr . . . 如果你看到它支持 -a 的选项（-a 文件名，将文件作为附件添加到邮件）和 -s 选项（-s 主题，指定邮件的主题），那就是支持的。可以使用如下的几个例子发送邮件。 a) 简单的邮件 运行 mail 命令，然后 mailx 会等待你输入邮件内容。你可以按回车来换行。当输入完成后，按Ctrl + D， mailx 会显示EOT表示结束。 然后 mailx 会自动将邮件发送给收件人。 $ mail user@example . com HI , Good Morning How are you EOT b) 发送有主题的邮件 $ echo \"Email text\" | mail - s \"Test Subject\" user@example . com -s 的用处是指定邮件的主题。 c) 从文件中读取邮件内容并发送 \" user@example . com < /path/ to / file \"message send from file $ mail d) 将从管道获取到的 echo 命令输出作为邮件内容发送 $ echo \"This is message body\" | mail - s \"This is Subject\" user@example . com e) 发送带附件的邮件 $ echo \" Body with attachment \"| mail -a foo.tar.gz -s \" attached file \" user@example.com -a 选项用于指定附件。 2) mutt Mutt是类Unix系统上的一个文本界面邮件客户端。它有20多年的历史，在Linux历史中也是一个很重要的部分，它是最早支持进程打分和多线程处理的客户端程序之一。按照如下的例子来发送邮件。 a) 带有主题，从文件中读取邮件的正文，并发送 $ mutt - s \"Testing from mutt\" user@example . com < /tmp/ message . txt b) 通过管道获取 echo 命令输出作为邮件内容发送 $ echo \"This is the body\" | mutt - s \"Testing mutt\" user@example . com c) 发送带附件的邮件 $ echo \"This is the body\" | mutt - s \"Testing mutt\" user@example . com - a / tmp / foo . tar . gz d) 发送带有多个附件的邮件 $ echo \"This is the body\" | mutt - s \"Testing\" user@example . com - a foo . tar . gz – a bar . tar . gz 3) swaks Swaks（Swiss Army Knife，瑞士军刀）是SMTP服务上的瑞士军刀，它是一个功能强大、灵活、可编程、面向事务的SMTP测试工具，由John Jetmore开发和维护。你可以使用如下语法发送带附件的邮件： $ swaks - t \" foo@bar.com \" -- header \"Subject: Subject\" -- body \"Email Text\" -- attach foo . tar . gz 关于Swaks一个重要的地方是，它会为你显示整个邮件发送过程，所以如果你想调试邮件发送过程，它是一个非常有用的工具。 它会给你提供了邮件发送过程的所有细节，包括邮件接收服务器的功能支持、两个服务器之间的每一步交互。 （LCTT 译注：原文此处少了 sharutils 的相关介绍，而多了 uuencode 的介绍。） 4) uuencode 邮件传输系统最初是被设计来传送7位编码（类似ASCII）的内容的。这就意味这它是用来发送文本内容，而不能发会使用8位的二进制内容（如程序文件或者图片）。 uuencode （\"UNIX to UNIX encoding\"，UNIX之间使用的编码方式）程序用来解决这个限制。使用 uuencode ，发送端将二进制格式的转换成文本格式来传输，接收端再转换回去。 我们可以简单地使用 uuencode 和 mailx 或者 mutt 配合，来发送二进制内容，类似这样： $ uuencode example . jpeg example . jpeg | mail user@example . com Shell脚本：解释如何发送邮件 #!/bin/bash FROM = \"\" SUBJECT = \"\" ATTACHMENTS = \"\" TO = \"\" BODY = \"\" # 检查文件名对应的文件是否存在 function check_files () { output_files = \"\" for file in $1 do if [ - s $file ] then output_files = \"${output_files}${file} \" fi done echo $output_files } echo \"*********************\" echo \"E-mail sending script.\" echo \"*********************\" echo # 读取用户输入的邮件地址 while [ 1 ] do if [ ! $FROM ] then echo - n - e \"Enter the e-mail address you wish to send mail from:\\n[Enter] \" else echo - n - e \"The address you provided is not valid:\\n[Enter] \" fi read FROM echo $FROM | grep - E '&#94;.+@.+$' > /dev/ null if [ $ ? - eq 0 ] then break fi done echo # 读取用户输入的收件人地址 while [ 1 ] do if [ ! $TO ] then echo - n - e \"Enter the e-mail address you wish to send mail to:\\n[Enter] \" else echo - n - e \"The address you provided is not valid:\\n[Enter] \" fi read TO echo $TO | grep - E '&#94;.+@.+$' > /dev/ null if [ $ ? - eq 0 ] then break fi done echo # 读取用户输入的邮件主题 echo - n - e \"Enter e-mail subject:\\n[Enter] \" read SUBJECT echo if [ \"$SUBJECT\" == \"\" ] then echo \"Proceeding without the subject...\" fi # 读取作为附件的文件名 echo - e \"Provide the list of attachments. Separate names by space. If there are spaces in file name, quote file name with \\\".\" read att echo # 确保文件名指向真实文件 attachments = $ ( check_files \"$att\" ) echo \"Attachments: $attachments\" for attachment in $attachments do ATTACHMENTS","tags":"中文"},{"url":"http://www.ciandcd.com/3-reasons-why-testing-software-security-should-start-early.html","title":"3 Reasons Why Testing Software Security Should Start Early","text":"from:http://java.dzone.com/articles/3-reasons-why-testing-software The software development life cycle is an extremely intensive process for developers and quality assurance professionals alike. If even one element is neglected, it can delay project schedules and affect user performance. Security is one aspect that must be built in from the inception of any app, and here are a few reasons why: Breaches can cost your business Let's say that an organization uses its application to order and manage inventory, payroll and other operational needs. If a malicious entity were to access this information, it could easily make fraudulent transactions, costing the company more than what was intended. Not to mention it will create a massive headache to set the record straight. TechTarget contributor Peter Gregory noted that this can happen when programs lack audit trails and processes required for secure purchasing. By building in this functionality early on, this type of situation can be avoided, allowing organizations to retain customer trust and money. \"Organizations that fail to involve information security in the life cycle will pay the price in the form of costly and disruptive events,\" Gregory wrote. \"Many bad things can happen to information systems that lack the required security interfaces and characteristics.\" Access to confidential data can be damaging If a business aims to use an app for information sharing and availability, protection must be at the forefront of this project throughout its life cycle. While some data may not be as costly to leak, the loss of confidential reports and documents can severely affect the organization's ability tofunction. QA teams must ensure that security practices are implemented and built upon constantly. TechTarget contributor Nick Lewis noted that firewalls and traditional methods will not be enough to keep targeted attacks at bay. Instead, testing the app for insufficient process validation, abuse of functionality, weak password recovery validation and information leakage will be critical toguarding the program. Analyze initial risk before jumping in One SDLC security practice to observe is a primary risk assessment before the start of a new project. Not all applications are equal, which means each program will be labeled with a different risk level. Some software will be publicly accessible, whereas others will be more business-critical and involve processing sensitive data. These uses will largely determine how much risk would be involved with a breach on such activities. This information will give QA teams a clear picture of the security roadmap needed, and can be implemented. \"Doing the preliminary risk assessment to establish the need for the system helps identify any security show stoppers before too much time and effort goes into the next SDLC phases,\" a SANS white paper stated. \"It also gets the design team thinking about security issues early in the design process.\" Cyberattacks and malware in the headlines have made security more prominent than ever before. By building in protections early in the SDLC, QA teams can ensure that they will be better able tohandle these threats without interruptions to regular business activities.","tags":"devops"},{"url":"http://www.ciandcd.com/organisation-pattern-trunk-based-development.html","title":"Organisation Pattern: Trunk Based Development","text":"from:http://java.dzone.com/articles/organisation-pattern-trunk-based-development Trunk Based Development is a version control strategy in which developers commit their changes to the shared trunk of a source code repository with minimal branching. Trunk Based Development became well known in the mid 2000s as Continuous Integration became a mainstream development practice, and today it is equally applicable to centralised Version Control Systems (VCS) and Distributed Version Control Systems (DVCS). In Trunk Based Development new features are developed concurrently on trunk as a series of small, incremental steps that preserve existing functionality and minimise merge complexity. Features are always released from trunk, and defect fixes are either released from trunk or a short-lived release branch. When development of a feature spans multiple releases its entry point is concealed to ensure the ongoing changes do not impede release cadence. The addition of a new feature can be concealed with a Feature Toggle , which means a configuration parameter or business rule is used to turn a feature on or off at runtime. As shown below a Feature Toggle is turned off while its feature is in development (v1), turned on when its feature is in production (v2), and removed after a period of time (v3). Updates to an existing feature can be concealed with a Branch By Abstraction , which means an abstraction layer is temporarily introduced to encapsulate both the old behaviour in use and the new behaviour in development. As shown below a Branch By Abstraction routes requests to the old behaviour while the new behaviour is in development (v1-v2), reroutes requests to the new behaviour when it is in production (v3), and is removed after a period of time (v4). Trunk Based Development is synonymous with Continuous Integration, which has been described by Jez Humble et al as \" the most important technical practice in the agile canon \". Continuous Integration is a development practice where all members of a team integrate and test their changes together on at least a daily basis, resulting in a shared mindset of collaboration and an always releasable codebase. This is verified by an automated build server continuously building the latest changes, and can include pre- and post-build actions such as code reviews and auto-revert on failure. Consider an organisation that provides an online Company Accounts Service, with its codebase maintained by a team practicing Trunk Based Development and Continuous Integration. In iteration 1 two features are requested – F1 Computations and F2 Write Offs – so the team discuss their concurrent development and decide on a Feature Toggle for F1 as it is a larger change. The developers commit their changes for F1 and F2 to trunk multiple times a day, with F1 tested in its on and off states to verify its progress alongside F2. In iteration 2 more features – F3 Bank Details and F4 Accounting Periods – begin development. F4 requires a different downstream submissions system, so the team design a Branch By Abstraction for submissions to ensure F1 and F3 can continue with the legacy submissions system until F4 is complete. F2 is signed off and released into production with F1 still toggled off at runtime. Some changes for F3 break the build, which triggers an automatic revert and a team discussion on a better design for F3. In iteration 3 a production defect is found in F2, and after the defect is fixed on trunk a release branch is agreed for risk mitigation. An F2.1 release branch is created from the last commit of the F2 release, the fix is merged to the branch, and F2.1 is released into production. F4 continues on trunk, with the submissions Branch By Abstraction tested in both modes. F3 is signed off and released into production using the legacy submissions system. In iteration 4 F1 is signed off and its Feature Toggle is turned on in production following a release. F4 is signed off and released into production, but when the Branch By Abstraction is switched to the new submissions system a defect is found. As a result the Branch By Abstraction is reverted at runtime to the legacy submissions system, and a F4.1 fix is released from trunk. In this example F1, F2, F3, and F4 clearly benefit from being developed by a team collaborating on a single shared code stream. For F1 the team agrees on the why and how of the Feature Toggle, with F1 tested in both its on and off states. For F2 the defect fix is made available from trunk and everyone is aware of the decision to use a release branch for risk mitigation. For F3 the prominence of a reverted build failure encourages people to contribute to a better design. For F4 there is a team decision to create a submissions Branch By Abstraction, with the new abstraction layer offering fresh insights into the legacy system and incremental commits enabling regular feedback on the new approach. Furthermore, when the new submissions system is switched on and a defect is found in F4 the ability to revert at runtime to the legacy submissions means the Company Accounts Service can remain online with zero downtime. This highlights the advantages of Trunk Based Development: Continuous Integration – incremental commits to trunk ensure an always integrated, always tested codebase with minimal integration costs and a predictable flow of features Adaptive scheduling – an always releasable codebase separates the release schedule from development efforts, meaning features can be released on demand according to customer needs Collaborative design – everyone working on the same code encourages constant communication, with team members sharing responsibility for design changes and a cohesive Evolutionary Architecture Operational and business empowerment – techniques such as Feature Toggle and Branch By Abstraction decouple release from launch, providing the operational benefit of graceful degradation on failure and the business benefit of Dark Launching features Breaking down features and re-architecting an existing system in incremental steps requires discipline, planning, and ingenuity from an entire team on a daily basis, and Trunk Based Development can incur a development overhead for some time if multiple technologies are in play and/or the codebase is poorly structured. However, those additional efforts will substantially reduce integration costs and gradually push the codebase in the right direction – as shown by Dave Farley and Jez Humble praising Trunk Based Development for \" the gentle, subtle pressure it applies to make the design of your software better \". A common misconception of Trunk Based Development is that it is slow, as features take longer to complete and team velocity is often lower than expected. However, an organisation should optimise globally for cycle time not locally for velocity, and by mandating a single code stream Trunk Based Development ensures developers work at the maximum rate of the team not the individual, with reduced integration costs resulting in lower lead times. Trunk Based Development is simple, but not easy. It has a steep learning curve but the continuous integration of small changesets into trunk will minimise integration costs, encourage collaborative design, empower runtime operational and business decisions, and ultimately drive the engine of Continuous Delivery. It is for this reason Dave Farley and Jez Humble declared \" we can't emphasise enough how important this practice is in enabling continuous delivery of valuable, working software \".","tags":"devops"},{"url":"http://www.ciandcd.com/p4zhong-ru-he-rollbackbackout-mergeintegration.html","title":"p4中如何rollback/backout merge/integration","text":"From: http://www.cnblogs.com/itech/p/3660299.html#FeedBack 原文： http://answers.perforce.com/articles/KB_Article/How-To-Rollback-An-Integration 当我们需要将一个branch上的代码修改集成到另一个branch的时候，我们需要执行命令p4 integ + p4 resolve + p4 submit来完成，通称我们称以上的操作为一次integration或merge。 一 rollback integration/merge 有的时候在我们做了p4 integ 来把代码从一个branch merge到了另一个branch，但是发现有错误，或者这个merge不需要了，这时我们将需要撤销我们刚才的merge操作。 1）如果你只是执行了p4 integ 还没有执行p4 submit的时候，这时候只需要简单地在你的client里执行p4 revert就可以撤销merge了。 2）如果的merge结果已经submit到了p4 server了，则考虑以下几种方法： 第一种方法： superuser执行p4 obliterate 将merge的CL从p4 server彻底删除； 第二种方法： 手动对merge中修改的文件恢复到前一个版本，即手动对merge修改的文件执行edit/resolve/submit； 第三种方法： 使用p4v提供的rollback功能，将某个CL rollback，或者把某些文件rollback到先前的CL或时间或label； 二 rollback已经提交的merge的3中方法比较 当rollback已经提交的merge，最重要的考虑是目标文件（merge修改过的文件）是否又有新的修改。如果在merge后没有新的CL修改与merge相同的文件，使用p4 obliterate是最快速且干净的方法，如果merge修改的文件被后来的其他的CL修改了，则需要考虑使用p4v中的rollback或手动edit/resolve/submit。 以下的三种方法都各有一些限制。 当使用p4 obliterate的时候，虽然merge的CL的历史记录被删除了，但是如果在merge后有新的CL修改相同的文件，则被修改的文件中merge相关的修改内容不能被删除。 当使用p4v或用以上的第二种方法手动rollback到先前的版本时，所有相关的CL的记录将不会删除。当下次从新merge的时候会遇到文件已经被integrated的问题，需要使用p4 integ -f来强制merge。 更多详细： http://answers.perforce.com/articles/KB_Article/How-To-Rollback-An-Integration http://answers.perforce.com/articles/KB_article/Backing-Out-Submitted-Changelists http://answers.perforce.com/articles/KB_Article/Backing-out-a-changelist-after-multiple-subsequent-changes 完！","tags":"中文"},{"url":"http://www.ciandcd.com/tong-guo-dockerrong-qi-yun-xing-chi-xu-ji-cheng-chi-xu-bu-shu.html","title":"通过Docker容器运行持续集成/持续部署","text":"From: http://dockone.io/article/468 【编者的话】 对于Docker主流的应用场景：持续集成和持续部署(CI/CD)大家也许并不陌生。这篇文章从独特的视角阐述了如何利用各种云平台构建属于自己的CI/CD容器，笔者还自己扩展了Gitlab CI引擎，对CI感兴趣的同学对这个文章应该很感兴趣。 我曾经使用Docker了一段时间，在过去的一年里伴随着众多的Docker容器涌入，帮助用户们更容易的部署Docker容器到生产环境中。一些工具是第三方公司提供，当然也包括Docker公司自己的容器工具(例如:Docker Swarm、Docker Machine和 Docker Compose)。尽管如此，最近我在测试一种新的Docker工作流，它可以允许我推送代码，并做测试、构建项目，还可以将代码部署到跑Docker的生产环境集群中。 接下来隆重介绍我的新工具 bar service—— 创建一个本地组件 推送代码到git仓库的feature/development分支 Shippable ，一个持续集成引擎，一旦检测到有新的提交(commit)，通过pull获取最新的代码并且运行相关的测试 如果测试全都通过了等着被部署到生产环境中，就会创建一个合并(merge)分支的请求并且执行该合并(merge) Shippable 一旦检测到有新的提交到远程master分支，同时会执行测试，并且推送源码到Heroku的git仓库中 Heroku 将会自动构建这个应用程序并且部署它。 这个工具真的可以方便、快捷地部署项目并且保证所有部署到生产环境的代码都经过了测试而且是可靠的。 尽管如此，还是有一个问题，那就是大规模部署Heroku需要花费高昂的费用。选择使用Heroku，你将会获得Heroku提供的免费使用优惠②，一个标准的512内存的由Heroku提供的Dyno容器只需要35$。公平的说，Heroku提供的虚拟主机服务已经够实惠了,但是一些应用的复杂程度要求Heroku提供更灵活的定制化需求，收取一定费用也无可厚非。 当然Docker可以替代Heroku，提供相应的代码部署服务。但是上面谈到的问题仍旧存在。当你的代码需要部署的时候，频繁手动启动和关闭服务是得不偿失的。让我们来比较一下不同持续集成/持续部署引擎和Docker相比有什么不同。 寻找解决方案 Docker并不是第一个也不是最后一个端到端的持续集成工作流解决方案。因此我们需要将众多不同的技术组合到一起用来完成我们想要的功能。下面将会介绍三个主流的提供构建引擎的服务:一个是CI/CD的测试运行引擎，一个是web容器，一个是容器适配器。 CI/CD 服务器,测试运行引擎 当我们选择一个CI/CD服务，你必须确定他们必须支持Docker容器的构建。下面的这些服务都包含了这一功能: 当然还有一些其他的服务，包括大名鼎鼎的Jenkins CI server。这里你可以自己去搜索这些服务。一些服务在容器上运行构建，但是他们相互之间是完全独立的。稍后你就会看到，你可以在Docker-in-Docker上运行服务，同时可以使用在Docker容器中运行的其他服务来构建你的Docker容器。 在我的试验中，我选择了和Gitlab 版本管理服务器端同时运行GitLab CI 系统。通过Gitlab版本管理服务器，我可以很清楚的看到每次commit。选择Gitlab不仅是因为他是一个免费的开源源码 如果你选择使用Gitlab CI System搭建你的持续集成框架，你必须提供属于自己的测试运行服务。宿主机的持续集成软件只能运行特定的测试服务，实际上它自己并不能自己执行测试任务，通常都是启动由本机提供相应的测试服务来执行的，你不光可以通过服务提供商启动引擎或者你直接运行你本机上的服务(虚拟机VM或Docker容器)。 服务器托管提供商 当然你同时也需要一个服务器托管提供商让Docker守护进程可以在它之上运行。不幸的是，使用Docker意味着经常需要运行和管理自己的服务器，也就是你将要负责这个主机的运维。但是，我想通过下面的内容说明，你可以在Docker运行一个高可用、多数据中心架构，这意味着即便是停机一个小时也不会像以前那样对业务影响那么巨大。 常用的服务器托管提供商包括下面的几个③： 业务流程 即是你有了一套构建好了的Docker容器和能够运行Docker守护进程的服务器。你还是需要一个能够被轻易启动的容器并且能够在构建一个新镜像的时候能重复部署他们。正如我最近正在使用的名叫 目前，Tutum是一个能帮助你管理你的容器部署工作流的服务形式。与此同时，你还能在各种云平台上快速动态增加节点，创建新服务，通过一个私有的register来部署你的应用。 此外，Tutum还为你的容器创建了一个私有网络，意味着可以通过你的Tutum账户你的Docker容器拥有自己的私有固定IP地址并且通过路由来访问其他容器。无论你的物理机是否在同一个数据中心，还是通过世界各地的不同的服务器托管提供商。它都将允许你创建一个基于多服务器、多云平台的弹性的解决方案。如果你之前见过 我一直在寻找类似上面谈到的这种服务。就在前不久，我刚做了一个实验：在多容器Docker之间通过vpn创建一个P2P网络。这是很久之前的Docker网络配置达不到的需求，但是现在Tutum做到了。 Tutum 同时也整合了CI/CD组件，并且支持git推送(push)。当Tutum 完成功能的时候，它可能成为唯一的你需要包含的到源码仓库中的其他服务。 Tutum 拥有我们需要进行CI/CD的几个关键组件： 一个私有的为容器镜像准备的注册中心 当新的镜像推送到注册中心的时候重新部署容器 简便的容器扩容(在界面视图上,可以通过滑动 N 或 M 的方式调整容器大小) 在Tutum的界面上添加节点 其他我们值得关注的组件： 在对一个web应用容器缩放后,基于自省的方式的DNS动态解析。举个例子,你的haproxy路由会自动发现新增容器并且添加到路由列表中。 私有的网络覆盖 不再受不同云供应商的限制,创建自己的节点 把所有东西组装到一起 下面就挑干货说下我的实验过程： 宿主机使用Gitlab搭建的源码远程仓库 宿主机使用Gitlab CI搭建的CI/CD引擎 RunAbove作为服务器提供商对所有CI/CD运行引擎的容器托管④ Tutum 作为业务流程和服务的管理服务提供 当所有的事情结束，也组装完毕，git的提交记录活动图，如下图所示: 部署Tutum 代理 正因为如此，我们事实上是使用Tutum去部署GitLab CI runners。我给你的建议是，先安装Tutum agents。启动所有你希望使用的服务，这样在Tutum的仪表盘上就能看到 Bring your own node 字样的按钮。点击它，你将会收到一个如下面所示的命令行： curl -Ls https://get.tutum.co/ | sudo -H sh -s XXXXXXXXXXXXXXXXXXX 通常在某一个节点运行上述命令自动添加到你的Tutum账户，与此同时进程也会自动添加到其他节点(每次当你点击 Bring your own node 按钮的时候)。 一旦Tutum agent安装到你的所有节点上，你将会在仪表盘上看到它。在这一点上，你也许想给这个节点贴上适当的标签。允许对应的服务器在哪个节点上运行。举个例子，你可能有一组节点标记为 tag 或 production 目的是为了创建一套环境，亦或者是用来做持续集成的一个节点标记，这些只能在宿主机上运行。 你可以通过点击节点名称来标记一个节点，添加的标记将会显示在左侧边栏。 部署一个Gitlab CI 运行引擎 现在你可以通过Tutum来部署一个Gitlab CI运行引擎。然而，我们需要一种特殊类型的持续运行引擎——需要能够在这个容器中运行Docker并且能够构建我们自己的Docker镜像。 你可能想，这怎么可能呢？假如我们使用Tutum运行Gitlab CI 引擎，它只能运行在容器里。那么问题来了，如何在Docker容器里运行Docker呢？ 其实这完全可以做到。事实上，我们可以运行在你能想象到的更深层Docker中。我们的Docker容器架构如下图所示： 正如你所看到的，在我们的节点的Docker容器里Tutum运行GitLab CI引擎。此外，该GitLab CI引擎实际上使用Docker构建镜像并运行测试,这意味着我们有两个嵌套层次。 通过fork GitLab CI Runner，我已经建立了相应功能的分支。这样就可以有效的跟踪在 在建立你的GiLab CI runner前，确保你已经存在一个GitLab的实例仓库，并且能够同时运行GitLab CI引擎。正如前面提到的，你可以建立自己的实例，或者直接使用gitlab官方提供的免费托管仓库和持续部署服务。 一旦你注册了Gitlab仓库。你所做的只需要通过点击链接你的Gitlab CI 账号。在你关联了Gitlab CI账号之后，你将会通过Gitlab的仪表盘看到你的Gitlab仓库列表，并且通过点击\"Add project to CI\" 按钮来添加你的项目到Gitlab CI 引擎中。点击完毕后，你就会Gitlab的仪表盘上看到你新添加的项目信息了。如下图所示： 如果你轻戳Gitlab的用户界面，你将会注意到一个菜单标题为\"Runners\"。在这个页面有一个注册标记，同时还有如何创建一个新的\"Runner\"的功能说明。在下面的例子中，我将会使用Tutum部署我们的Gitlab CI 镜像。确定你已经复制了注册标记和Gitlab CI的url地址——一会你将会用到他们。 在你的Tutum界面上，创建一个新的服务。在Tutum中服务的概念是一个Docker容器组，他们使用相同的软件并运行着相同配置。 每个服务可以有零个或者多个容器在同一时间运行，并且Tutum将会管理你所有节点的协调和缩放工作。 在启动向导的第一个界面，你将会看到几个标签，让你选择不同来源的Docker镜像源。Tutum拥有一个内置的、私有的注册中心，通过搜索其他注册中心同时支持一些具有特定功能的镜像，当然这里也包括Docker的官方镜像。切换到\"公共镜像\"选项卡，搜索\"wizardapps/dind-gitlab-ci-runner\"镜像，这就是我前面描述的在github上fork的GitLab CI 引擎源码仓库中我自己建立的分支。 你一旦确定选择了镜像，你将会看到一个界面上面你对两个容器之间调度的功能选项和一些基本配置，对于部署策略，虽然默认的设置也可以运行持续集成，但是我建议还是通读下Tutum官方的说明文档.除非你想做平行构建，否则将容器数量设置为1。如果你先前已经对部署在Tutum的节点打了对应的标签，请确保在\"Delpoy Tags\"输入框输入了正确的标签名称。从本质上讲，Tutum会努力寻找所有满足你要求\"Delpoy Tags\"中声明的标签，并且部署他们到一个节点。 下图说明当你第一次必须要改动的一个重要的配置信息，这是在\"Advanced Options\"部分的\"Privileged Mode\"选项，Docker需要选中这个选项，这样保证Tutum可以很容易的获得该信息。 在配置好私有模式后，你将会看到下一个屏幕-有关环境变量配置信息。 像Docker命令行一样，Tutum允许你声明一个环境变量使之可以在你的Docker容器里使用。 不仅如此，在Tutum中，每一个部署的容器都可以使用这个环境变量。尽管我们不会再Gitlab CI 引擎上使用link功能，我们使用的是Tutum提供的动态link,容器依然允许使用其他容器中的环境变量。 下面是三种需要设置的重要的环境变量： REGISTRATION_TOKEN： 注册信息就是之前我们在Gilab CI 中拷贝的\"Runner\"。 CI_SERVER_URL：在Gitlab CI页面上之前提供给我们的url地址，如果你使用的是官方提供的gitlab CI服务，直接填写\" https://ci.gitlab.com/ \"。 GITLAB_SERVER_FQDN：这里声明的是gitlab 的域名，这是用来执行一个 ssh-keyscan 。如果你用的是官方的托管服务，请填写 \"ci.gitlab.com\"。 在配置完这些变量之后，终于可以\"创建并部署\"我们的服务了。 一旦你的容器启动完毕, 你可以返回GitLab CI的\"Runners\"页面并且你看到一个新的入口。现在你可以着手准备建立 GitLab CI 任务吧。 创建一个GitLab CI 任务 终于到最后一步了，我们在Gitlab CI 上添加一个实际的脚本目的是建立一个 持续集成/持续部署的工作流。现在，这将取决于你的实际项目的种类，但大体方向应该是相同的，你会使用Docker镜像来构建Docker容器，然后上传到你的镜像库。在这种情况下，将会上传到Tutum提供的私有镜像库。 在\"工作\"选项卡中，Gitlab CI工作可以进行修改。在这个部分，有两个不同的子选项-\"test\"和\"deploy\"。正如他们名字所暗示的，测试脚本通常用来运行单元和集成测试。部署脚本只运行一旦测试完成后的特定分支。这样就可以允许你在每次提交(commit)后运行你的测试脚本，与此同时部署脚本当且仅当所有测试全部通过的时候部署远程master分支。 下面是脚本实例： docker login-u[USERNAME]-e[EMAIL]-p=\"[PASSWORD]\"tutum.co Build the Docker image and tag it for Tutum docker build-twizardapps/app. docker tag-fwizardapps/app tutum.co/wizardapps/app:latest 上面的测试脚本并没有实际运行任何测试，但是他确能为我们的应用程序构建Docker镜像并打上对应的标签。如果你在初始化阶段使用自己的脚本，请确保你的用户、邮箱、密码和Tutum上的注册信息报纸一致。因为Tutum提供的是一个私有Docker注册中心，确保你的测试脚本能通过相应的校验⑤。 然后，我们还可以创建一个部署脚本，实际上是推送我们的镜像到Tutum的注册中心，并开始构建。 docker push tutum.co/wizardapps/app 自动部署 到了这一步，你的系统应该已经构建成功并运行，新代码应该可以持续集成并上传到Tutum的注册中心。剩下的工作就是在Tutum上创建你自己的服务，确保重启后还可以继续使用。 这和我们创建Gitlab CI 服务类似，所不同的是我们让服务\"私有化\"，我们只需要打开\"Auto Redeploy\"开关即可。配置所有的服务端口，环境变量，链接和卷标，然后点击一下部署即可。 恭喜你，你现在已经拥有了只有在经过测试后，可以快速进行持续部署，自动化部署的Docker应用程序。 其他资源 GitLab CI Multi-Runner 地址： 我们通过在Tutum里建立每个项目上都需要的持续集成 \"服务\"，可以很快上手。作为一种替代方案，你可以尝试使用GitLab CI Multi-Runner，这个项目通过一个配置文件允许多个项目同时进行构建。 ①. 我有一些很酷的东西和大家分享，这就是ThreeBar——一个远远超出你想象的工具。一旦你做好准备加入，你将会发现Docker持续-部署-应用的强大功能。 ②. Heroku 运营模式，您只需要按小时支付使用费。每一个\"虚拟机\"运行你的代码被称之为\"Dyno\"，你可以仅仅通过运行一个命令就可以做到一键部署，例如web服务器或者其他请求队列服务。你将会每个月获得750小时的\"Dyno\"免费使用权。这意味着如果你愿意的话可以在Heroku上运行一个免费的web服务器。 ③. 我个人使用了以下所有的服务器提供商，我也通过博客链接点击获得了一些收入。但是，很可惜，任何一个服务器提供商都不会允许你在创建你自己Docker服务并运行在Linux实例上。 ④. RunAbove的沙箱是一个伟大的实验，因为在它上面有大量的RAM，SSD存储，并且花费极低——每月只有3美金在一个2G内存的服务器上。尽管如此，在那个时候他们不被SAL所接受，因此你只能选择其他的服务提供商。 ⑤. 此时此刻，你必须把你的Tutum的账号和密码直接扔到脚本中。很不幸的是，utum不提供这样的单独密码注册表API，所以这个方案也因此留下一个潜在的安全漏洞。 原文链接： CI/CD with Docker Containers （翻译：隋鑫 校对：魏小红） 【编者的话】 对于Docker主流的应用场景：持续集成和持续部署(CI/CD)大家也许并不陌生。这篇文章从独特的视角阐述了如何利用各种云平台构建属于自己的CI/CD容器，笔者还自己扩展了Gitlab CI引擎，对CI感兴趣的同学对这个文章应该很感兴趣。我曾经使用Docker了一段时间，在过去的一年里伴随着众多的Docker容器涌入，帮助用户们更容易的部署Docker容器到生产环境中。一些工具是第三方公司提供，当然也包括Docker公司自己的容器工具(例如:Docker Swarm、Docker Machine和 Docker Compose)。尽管如此，最近我在测试一种新的Docker工作流，它可以允许我推送代码，并做测试、构建项目，还可以将代码部署到跑Docker的生产环境集群中。接下来隆重介绍我的新工具 bar service—— ThreeBar ①，一个可以在Heroku上运行持续集成/持续部署(CI/CD)系统。通常情况下，新开发的代码部署到服务器遵循下面的流程:这个工具真的可以方便、快捷地部署项目并且保证所有部署到生产环境的代码都经过了测试而且是可靠的。尽管如此，还是有一个问题，那就是大规模部署Heroku需要花费高昂的费用。选择使用Heroku，你将会获得Heroku提供的免费使用优惠②，一个标准的512内存的由Heroku提供的Dyno容器只需要35$。公平的说，Heroku提供的虚拟主机服务已经够实惠了,但是一些应用的复杂程度要求Heroku提供更灵活的定制化需求，收取一定费用也无可厚非。当然Docker可以替代Heroku，提供相应的代码部署服务。但是上面谈到的问题仍旧存在。当你的代码需要部署的时候，频繁手动启动和关闭服务是得不偿失的。让我们来比较一下不同持续集成/持续部署引擎和Docker相比有什么不同。Docker并不是第一个也不是最后一个端到端的持续集成工作流解决方案。因此我们需要将众多不同的技术组合到一起用来完成我们想要的功能。下面将会介绍三个主流的提供构建引擎的服务:一个是CI/CD的测试运行引擎，一个是web容器，一个是容器适配器。当我们选择一个CI/CD服务，你必须确定他们必须支持Docker容器的构建。下面的这些服务都包含了这一功能:当然还有一些其他的服务，包括大名鼎鼎的Jenkins CI server。这里你可以自己去搜索这些服务。一些服务在容器上运行构建，但是他们相互之间是完全独立的。稍后你就会看到，你可以在Docker-in-Docker上运行服务，同时可以使用在Docker容器中运行的其他服务来构建你的Docker容器。在我的试验中，我选择了和Gitlab 版本管理服务器端同时运行GitLab CI 系统。通过Gitlab版本管理服务器，我可以很清楚的看到每次commit。选择Gitlab不仅是因为他是一个免费的开源源码 托管仓库 和对托管在上面的项目提供的 持续部署引擎 ，同时也是一个通过安装gitlab运行你自己的服务的开源软件。如果你选择使用Gitlab CI System搭建你的持续集成框架，你必须提供属于自己的测试运行服务。宿主机的持续集成软件只能运行特定的测试服务，实际上它自己并不能自己执行测试任务，通常都是启动由本机提供相应的测试服务来执行的，你不光可以通过服务提供商启动引擎或者你直接运行你本机上的服务(虚拟机VM或Docker容器)。当然你同时也需要一个服务器托管提供商让Docker守护进程可以在它之上运行。不幸的是，使用Docker意味着经常需要运行和管理自己的服务器，也就是你将要负责这个主机的运维。但是，我想通过下面的内容说明，你可以在Docker运行一个高可用、多数据中心架构，这意味着即便是停机一个小时也不会像以前那样对业务影响那么巨大。常用的服务器托管提供商包括下面的几个③：即是你有了一套构建好了的Docker容器和能够运行Docker守护进程的服务器。你还是需要一个能够被轻易启动的容器并且能够在构建一个新镜像的时候能重复部署他们。正如我最近正在使用的名叫 Tutum 的业务流程服务。目前，Tutum是一个能帮助你管理你的容器部署工作流的服务形式。与此同时，你还能在各种云平台上快速动态增加节点，创建新服务，通过一个私有的register来部署你的应用。此外，Tutum还为你的容器创建了一个私有网络，意味着可以通过你的Tutum账户你的Docker容器拥有自己的私有固定IP地址并且通过路由来访问其他容器。无论你的物理机是否在同一个数据中心，还是通过世界各地的不同的服务器托管提供商。它都将允许你创建一个基于多服务器、多云平台的弹性的解决方案。如果你之前见过 Flannel by CoreOS ，Tutum的私有网络和它差不多。我一直在寻找类似上面谈到的这种服务。就在前不久，我刚做了一个实验：在多容器Docker之间通过vpn创建一个P2P网络。这是很久之前的Docker网络配置达不到的需求，但是现在Tutum做到了。Tutum 同时也整合了CI/CD组件，并且支持git推送(push)。当Tutum 完成功能的时候，它可能成为唯一的你需要包含的到源码仓库中的其他服务。Tutum 拥有我们需要进行CI/CD的几个关键组件：其他我们值得关注的组件：下面就挑干货说下我的实验过程：当所有的事情结束，也组装完毕，git的提交记录活动图，如下图所示:正因为如此，我们事实上是使用Tutum去部署GitLab CI runners。我给你的建议是，先安装Tutum agents。启动所有你希望使用的服务，这样在Tutum的仪表盘上就能看到字样的按钮。点击它，你将会收到一个如下面所示的命令行：通常在某一个节点运行上述命令自动添加到你的Tutum账户，与此同时进程也会自动添加到其他节点(每次当你点击按钮的时候)。一旦Tutum agent安装到你的所有节点上，你将会在仪表盘上看到它。在这一点上，你也许想给这个节点贴上适当的标签。允许对应的服务器在哪个节点上运行。举个例子，你可能有一组节点标记为目的是为了创建一套环境，亦或者是用来做持续集成的一个节点标记，这些只能在宿主机上运行。你可以通过点击节点名称来标记一个节点，添加的标记将会显示在左侧边栏。现在你可以通过Tutum来部署一个Gitlab CI运行引擎。然而，我们需要一种特殊类型的持续运行引擎——需要能够在这个容器中运行Docker并且能够构建我们自己的Docker镜像。你可能想，这怎么可能呢？假如我们使用Tutum运行Gitlab CI 引擎，它只能运行在容器里。那么问题来了，如何在Docker容器里运行Docker呢？其实这完全可以做到。事实上，我们可以运行在你能想象到的更深层Docker中。我们的Docker容器架构如下图所示：正如你所看到的，在我们的节点的Docker容器里Tutum运行GitLab CI引擎。此外，该GitLab CI引擎实际上使用Docker构建镜像并运行测试,这意味着我们有两个嵌套层次。通过fork GitLab CI Runner，我已经建立了相应功能的分支。这样就可以有效的跟踪在 github 和官方Docker注册中心的变更。在建立你的GiLab CI runner前，确保你已经存在一个GitLab的实例仓库，并且能够同时运行GitLab CI引擎。正如前面提到的，你可以建立自己的实例，或者直接使用gitlab官方提供的免费托管仓库和持续部署服务。一旦你注册了Gitlab仓库。你所做的只需要通过点击链接你的Gitlab CI 账号。在你关联了Gitlab CI账号之后，你将会通过Gitlab的仪表盘看到你的Gitlab仓库列表，并且通过点击\"Add project to CI\" 按钮来添加你的项目到Gitlab CI 引擎中。点击完毕后，你就会Gitlab的仪表盘上看到你新添加的项目信息了。如下图所示：如果你轻戳Gitlab的用户界面，你将会注意到一个菜单标题为\"Runners\"。在这个页面有一个注册标记，同时还有如何创建一个新的\"Runner\"的功能说明。在下面的例子中，我将会使用Tutum部署我们的Gitlab CI 镜像。确定你已经复制了注册标记和Gitlab CI的url地址——一会你将会用到他们。在你的Tutum界面上，创建一个新的服务。在Tutum中服务的概念是一个Docker容器组，他们使用相同的软件并运行着相同配置。每个服务可以有零个或者多个容器在同一时间运行，并且Tutum将会管理你所有节点的协调和缩放工作。在启动向导的第一个界面，你将会看到几个标签，让你选择不同来源的Docker镜像源。Tutum拥有一个内置的、私有的注册中心，通过搜索其他注册中心同时支持一些具有特定功能的镜像，当然这里也包括Docker的官方镜像。切换到\"公共镜像\"选项卡，搜索\"wizardapps/dind-gitlab-ci-runner\"镜像，这就是我前面描述的在github上fork的GitLab CI 引擎源码仓库中我自己建立的分支。你一旦确定选择了镜像，你将会看到一个界面上面你对两个容器之间调度的功能选项和一些基本配置，对于部署策略，虽然默认的设置也可以运行持续集成，但是我建议还是通读下Tutum官方的说明文档.除非你想做平行构建，否则将容器数量设置为1。如果你先前已经对部署在Tutum的节点打了对应的标签，请确保在\"Delpoy Tags\"输入框输入了正确的标签名称。从本质上讲，Tutum会努力寻找所有满足你要求\"Delpoy Tags\"中声明的标签，并且部署他们到一个节点。下图说明当你第一次必须要改动的一个重要的配置信息，这是在\"Advanced Options\"部分的\"Privileged Mode\"选项，Docker需要选中这个选项，这样保证Tutum可以很容易的获得该信息。在配置好私有模式后，你将会看到下一个屏幕-有关环境变量配置信息。像Docker命令行一样，Tutum允许你声明一个环境变量使之可以在你的Docker容器里使用。不仅如此，在Tutum中，每一个部署的容器都可以使用这个环境变量。尽管我们不会再Gitlab CI 引擎上使用link功能，我们使用的是Tutum提供的动态link,容器依然允许使用其他容器中的环境变量。下面是三种需要设置的重要的环境变量：在配置完这些变量之后，终于可以\"创建并部署\"我们的服务了。一旦你的容器启动完毕, 你可以返回GitLab CI的\"Runners\"页面并且你看到一个新的入口。现在你可以着手准备建立 GitLab CI 任务吧。终于到最后一步了，我们在Gitlab CI 上添加一个实际的脚本目的是建立一个 持续集成/持续部署的工作流。现在，这将取决于你的实际项目的种类，但大体方向应该是相同的，你会使用Docker镜像来构建Docker容器，然后上传到你的镜像库。在这种情况下，将会上传到Tutum提供的私有镜像库。在\"工作\"选项卡中，Gitlab CI工作可以进行修改。在这个部分，有两个不同的子选项-\"test\"和\"deploy\"。正如他们名字所暗示的，测试脚本通常用来运行单元和集成测试。部署脚本只运行一旦测试完成后的特定分支。这样就可以允许你在每次提交(commit)后运行你的测试脚本，与此同时部署脚本当且仅当所有测试全部通过的时候部署远程master分支。下面是脚本实例：上面的测试脚本并没有实际运行任何测试，但是他确能为我们的应用程序构建Docker镜像并打上对应的标签。如果你在初始化阶段使用自己的脚本，请确保你的用户、邮箱、密码和Tutum上的注册信息报纸一致。因为Tutum提供的是一个私有Docker注册中心，确保你的测试脚本能通过相应的校验⑤。然后，我们还可以创建一个部署脚本，实际上是推送我们的镜像到Tutum的注册中心，并开始构建。到了这一步，你的系统应该已经构建成功并运行，新代码应该可以持续集成并上传到Tutum的注册中心。剩下的工作就是在Tutum上创建你自己的服务，确保重启后还可以继续使用。这和我们创建Gitlab CI 服务类似，所不同的是我们让服务\"私有化\"，我们只需要打开\"Auto Redeploy\"开关即可。配置所有的服务端口，环境变量，链接和卷标，然后点击一下部署即可。恭喜你，你现在已经拥有了只有在经过测试后，可以快速进行持续部署，自动化部署的Docker应用程序。GitLab CI Multi-Runner 地址： https://github.com/ayufan/gitlab-ci-multi-runner 我们通过在Tutum里建立每个项目上都需要的持续集成 \"服务\"，可以很快上手。作为一种替代方案，你可以尝试使用GitLab CI Multi-Runner，这个项目通过一个配置文件允许多个项目同时进行构建。①. 我有一些很酷的东西和大家分享，这就是ThreeBar——一个远远超出你想象的工具。一旦你做好准备加入，你将会发现Docker持续-部署-应用的强大功能。②. Heroku 运营模式，您只需要按小时支付使用费。每一个\"虚拟机\"运行你的代码被称之为\"Dyno\"，你可以仅仅通过运行一个命令就可以做到一键部署，例如web服务器或者其他请求队列服务。你将会每个月获得750小时的\"Dyno\"免费使用权。这意味着如果你愿意的话可以在Heroku上运行一个免费的web服务器。③. 我个人使用了以下所有的服务器提供商，我也通过博客链接点击获得了一些收入。但是，很可惜，任何一个服务器提供商都不会允许你在创建你自己Docker服务并运行在Linux实例上。④. RunAbove的沙箱是一个伟大的实验，因为在它上面有大量的RAM，SSD存储，并且花费极低——每月只有3美金在一个2G内存的服务器上。尽管如此，在那个时候他们不被SAL所接受，因此你只能选择其他的服务提供商。⑤. 此时此刻，你必须把你的Tutum的账号和密码直接扔到脚本中。很不幸的是，utum不提供这样的单独密码注册表API，所以这个方案也因此留下一个潜在的安全漏洞。","tags":"中文"},{"url":"http://www.ciandcd.com/zai-centoszhong-an-zhuang-jenkins-masterce-shi-huan-jing.html","title":"在centos中安装jenkins master测试环境","text":"From: http://www.cnblogs.com/itech/p/3504722.html#FeedBack 在centos中安装jenkins 1）安装目录 pwd (/home/AAA) 2）检查java是否安装 [AAA@Centos_AAA jenkins]$ java -version java version \"1.6.0_22\" OpenJDK Runtime Environment (IcedTea6 1.10.4) (rhel-1.41.1.10.4.el6-x86_64) OpenJDK 64-Bit Server VM (build 20.0-b11, mixed mode) 3）下载jenkins mkdir -p jenkins/jenkins_home mkdir -p jenkins/jenkins_node cd jenkins http://mirrors.jenkins-ci.org/war/latest/jenkins.war wget 4）启动jenkins start_jenkins.sh #!/bin/bash JENKINS_ROOT=/home/AAA/jenkins export JENKINS_HOME=$JENKINS_ROOT/jenkins_home java -jar $JENKINS_ROOT/jenkins.war --httpPort=8000 chmod a+x start_jenkins.sh nohup ./start_jenkins.sh > jenkins.log 2>&1 & 5）访问jenkins： 在浏览器中输入http://192.168.0.120:8000 6）配置jenkins： configure golbal security->enable security jenkins own user database + enable users to sign up + anyone can do anythings then signup one user(jenkins_aaa：0) and login 7）安装jenkins plugins： 一开始在可选插件下面没有任何的插件可以安装，后来查了好多资料才发现在安装插件的页面的高级tab的右下角有个坑爹的按钮（立即获取）要点下。 完！","tags":"中文"},{"url":"http://www.ciandcd.com/zai-centoszhong-an-zhuang-jenkins-masterwei-service.html","title":"在centos中安装jenkins master为service","text":"From: http://www.cnblogs.com/itech/p/3504906.html#FeedBack 需要sudo或root权限。 1）确保centos中的java为openjdk。 如果java -version输出为 java -version java version \"1.5.0\" gij (GNU libgcj) version 4.4.6 20110731 (Red Hat 4.4.6-3) 则需要卸载java，安装openjdk版本 yum remove java 然后安装openjdk如下： yum install java-1.6.0-openjdk运行java -version 如下：java -version java version \"1.6.0_22\" OpenJDK Runtime Environment (IcedTea6 1.10.6) (rhel-1.43.1.10.6.el6_2-i386) OpenJDK Client VM (build 20.0-b11, mixed mode) OpenJDK Client VM (build 20.0-b11, mixed mode) 2)安装稳定版本的jenkins •sudo wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat-stable/jenkins.repo •sudo rpm --import http://pkg.jenkins-ci.org/redhat-stable/jenkins-ci.org.key •sudo yum install jenkins 3）配置： [root@Centos_AAA ~]# mkdir -p /home/AAA/jenkins/jenkins_home2 [root@Centos_AAA ~]# chown jenkins:jenkins /home/AAA/jenkins/jenkins_home2 [root@Centos_AAA ~]# ll -d /home/AAA/jenkins/jenkins_home2 drwxrwxr-x. 2 jenkins jenkins 4096 Jan 4 02:38 /home/AAA/jenkins/jenkins_home2 在配置文件/etc/sysconfig/jenkins中修改jenkins_home为/home/AAA/jenkins/jenkins_home2。 3） 启动jenkins •sudo service jenkins start/stop/restart •sudo chkconfig jenkins on 以上的安装和配置的解释： 1）jenkins通过daemon启动，查看 /etc/init.d/jenkins； 2）jenkins user被创建，且用来运行jenkins service。如果你需要修改为不同的user，需要修改文件的owner：/var/log/jenkins, /var/lib/jenkins, and /var/cache/jenkins； 3）log在文件/var/log/jenkins/jenkins.log； 4）/etc/sysconfig/jenkins 为jenkins的启动参数； 5） 默认地，jenkins运行于8080端口，确保防火墙允许端口， http://www.cyberciti.biz/faq/disable-linux-firewall-under-centos-rhel-fedora/。 6）jenkins RPM repository 被设置，查看/etc/yum.repos.d/jenkins.repo； 完！","tags":"中文"},{"url":"http://www.ciandcd.com/gitlab-7104-released.html","title":"GitLab 7.10.4 released","text":"From: https://www.gitlab.com/2015/05/11/gitlab-7-dot-10-dot-4-released/ GitLab 7.10.4 released Last week we had to pull our 7.10.2 release as in a small number of installations the migrations would fail because of a uniqueness constraint on tags. We did not release GitLab 7.10.3, as we improved a migration after creating the 7.10.3 version tag and wanted to include that in our patch release. Today we release GitLab 7.10.4 which solves the issues with the migrations and contains all fixes also present in 7.10.2. If you've already successfully upgraded to 7.10.2, you do not need to update at this time. The fixes in this patch: Fix migrations broken in 7.10.2 Add missing indices to tags for some installations Make tags for GitLab installations running on MySQL case sensitive And the following were fixed with 7.10.2, also included here: A bug when using the Gitorious importer A bug that prevented adding group members through the admin screen Broken links on the merge request page leading to CI services A 500 error when trying to search in the wiki A 500 error when trying to add new tags to a project A bug where commit data would not appear in some subdirectories due to escaped slashes A bug where branches with escaped characters in their names would not always work in the compare view Upgrade barometer There is a migration that loops through all tags. This can take a while for larger installations. The upgrade can be performed online. Theoretically, there is a small chance that if a tag is created during the migration of that specific tag, the tag counter gets a value that is slightly higher or lower than its actual value. We do not believe this is reason to schedule downtime and recommend performing the upgrade online. Updating To update, check out our update page . Enterprise Edition Omnibus packages for GitLab Enterprise Edition 7.10.4 are available for subscribers here . For installations from source, use this guide . Interested in GitLab Enterprise Edition? For an overview of feature exclusive to GitLab Enterprise Edition please have a look at the features exclusive to GitLab EE . Access to GitLab Enterprise Edition is included with a subscription . No time to upgrade GitLab yourself? A subscription also entitles to our upgrade and installation services. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","tags":"scm"},{"url":"http://www.ciandcd.com/a-new-gitlab-logo.html","title":"A new GitLab Logo","text":"From: https://www.gitlab.com/2015/05/18/a-new-gitlab-logo/ A new GitLab Logo We hear you: Gitlab seems like a cool service, but my god that logo is scary — Matt Bachmann (@MattBachmann) March 11, 2015 We have a scary, angry looking raccoon dog logo. this creepy human/racoon hybrid that is the @gitlab logo is starting to really freak me out pic.twitter.com/HJarlbRNOo — hatewell (@hatwell) January 16, 2015 We figured we could use a better representation of GitLab. Update May 20th: After careful consideration we have decided that this is our new logo: We like the way it looks in GitLab: And compared to the old logo: The options we didn't pick: If you have a better suggestion than one of the ones above, a certain preference or opinion, we'd love to hear it. The final choice of our new logo rests with Dmitriy . Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","tags":"scm"},{"url":"http://www.ciandcd.com/simple-words-for-a-gitlab-newbie.html","title":"Simple words for a GitLab Newbie","text":"From: https://www.gitlab.com/2015/05/18/simple-words-for-a-gitlab-newbie/ For most of us, when we work with a new tool, there's a process of learning the right vocabulary and the best steps to make things happen; this while we try to keep the best attitude. Not very long ago, I learned how to use Git and GitLab and it was a little bit painful. I read a lot about it, but it was mostly vocabulary that didn't make any sense to me. If you've been there or if you are there now, you'll know what I'm talking about (some people may have it naturally). So, to make this learning process easier for others, I took many of the basic Git vocabulary and wrote easy definitions for each word. I hope they are useful for you and please share them with your Git and Gitlab newbie friends! Cloud Based Services What is a cloud based service? It's a service or resource that is opposed to services that are hosted on the servers inside a company, which is the traditional way of doing it. It helps people and companies lower their costs and be more efficient while helping with different functions such as trannings, storage, etc. GitLab.com is a cloud based service because it can be hosted both in house and in the cloud. Source control or revision control software What is source control? It's a system that records and manages changes to projects, files and documents. It helps you recall specific versions later. It also makes it easier to collaborate, because it shows who has changed what and helps you combine contributions. Continuous Integration What is continuous integration? It's the system of continuously incorporating the work advances with a shared mainline in a project. Git and GitLab together make continuous integration happen. Continuous deployment What is continuous deployment? It means that whenever there is a change to the code, it is deployed or made live immediately. This is in contrast to continous integration, where code is continuously being merged in the mainline and is always ready to be deployed, rather than actually deployed. When people talk about CI and CD what they usually mean to say is that they are constantly and automatically testing their code against their tests using a tool such as GitLab CI and upon passing to a certain action. That action could be merging the code into a branch (master, production, etc), deploying it to a server or building a package / piece of software out of it. Non-continuous integration would be everyone working on something and only integrating all the work as the very last step. Obviously, that results in many conflicts and issues, which is why CI is adopted widely nowadays. Git What is Git? Git is a system where you can create projects of different sizes with speed and efficiency. It helps you manage code, communicate and collaborate on different software projects. Git will allow you to go back to a previous status on a project or to see its entire evolution since the project was created. You could think of it as a time machine which will allow you to go back in time to whenever you'd like in your project. With Git, 3 basic issues were solved when working on projects: 1. It became easier to manage large projects. 2. It helps you avoid overwriting the team's advances and work. 3. With git, you just pull the entire code and history to your machine, so you can calmly work in your own little space without interference or boundaries. It's much simpler and much more light-weight. Repository What is a repository? The place where the history of your work is stored. Remote repository What is a remote repository? It's a repository that is not-on-your-machine, so it's anything that is not your computer. Usually, it is online, GitLab.com for instance. The main remote repository is usually called \"Origin\". Commit What is a commit? It's the way you call the latest changes of source code that you made on a repository. When changes are tracked, commits mark the changes on a document. Master What is a master? It's how you call the main and definitive branch (the independent line of development of a project). Branch What is a branch? It's an independent line of development. They are a brand new working directory, staging area, and project history. New commits are recorded in the history for the current branch, which results in taking the source from someone's repository (the place where the history of your work is stored) at certain point in time, and apply your own changes to it in the history of the project. Fork What is a fork? It's a copy of an original repository (the place where the history of your work is stored) that you can put somewhere else or where you can experiment and apply changes that you can later decide if publishing or not, without affecting your original project. Git Clone What is a clone? It's to get a copy of a git project to look at or to use the code. Git Merge What is to merge? It's integrating separate changes that you made to a project, on different branches. md: markdown What is markdown? It's a plain text format that will make any document easy-to-write and easy-to-read. Push a repository What is to push a repository? It's to incorporate a local branch (the independent line of development of a project) to a remote repository (online version of your project). README.md What is a README.md? I't a file in a simple format which summarizes a repository. If there's also a README (without the .md), the README.md will have priority. SSH (secure shell protocol) What is SSH? It's how you call the commands that help communicate through a network and that are encrypted and secure. It's used for remote logins and it helps users connect to a server in a secure way. Stage Files What is to stage a file? It's how you call the act of preparing a file for a commit (the latest changes of source code in a repository). GitLab What is GitLab? GitLab is an online Git repository manager with a wiki, issue tracking, CI and CD. It is a great way to manage git repositories on a centralized server. GitLab gives you complete control over your repositories or projects and allows you to decide whether they are public or private for free. GitLab.com GitLab.com hosts your (private) software projects for free. It offers free public and private repositories, issue-tracking and wikis. It runs GitLab Enterprise Edition and GitLab CI. No installation required, you can just sign up for a free account. Support Package: Free subscribers can use the GitLab.com Support Forum if they have questions. GitLab.com Bronze Support will let you email support directly for timely, personal and private answers. This costs $9.99 per user per year for next-business-day response time and is available in packs of 20 users. GitLab Community Edition (CE) Free, self hosted application where you can get support from the Community Feature rich: Git repository management, code reviews, issue tracking, activity feeds and wikis. It comes with GitLab CI for continuous integration and delivery. Open Source: MIT licensed, community driven, 700+ contributors, inspect and modify the source, easy to integrate into your infrastructure. Scalable: support 25,000 users on one server or a highly available active/active cluster. Merge requests with line-by-line comments, CI and issue tracker integrations. GitLab Enterprise Edition (EE) Self hosted application that comes with additional support. Builds on top of the Community Edition and includes extra features mainly aimed at organizations with more than 100 users. It has LDAP group sync, audit logs and multiple roles. It includes deeper authentication and authorization integration, has fine-grained workflow management, has extra server management options and it integrates with your tool stack. GitLab EE runs on your servers. GitLab Continuous Integration (CI) Free, self hosted application that integrates with GitLab CE/EE. Also availble as SaaS at ci.gitlab.com. Easy to set up since it is included in Omnibus packages of GitLab or use it for free on ci.gitlab.com. Beautiful interface with a clear menu structure. Performant and stable, as tests run distributed on separate machines. Will help you receive test results faster with each commit running in parallel on multiple jobs. Free to use and completely open source. All CI code is MIT licensed. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","tags":"scm"},{"url":"http://www.ciandcd.com/gitlab-gitorious-and-free-software.html","title":"GitLab, Gitorious, and Free Software","text":"From: https://www.gitlab.com/2015/05/20/gitlab-gitorious-free-software/ GitLab, Gitorious, and Free Software This is a guest post by Mike Gerwitz , a free software hacker and activist, and author of GNU ease.js . In early March of this year, it was announced that GitLab would acquire Gitorious and shut down gitorious.org by 1 June, 2015. Reactions from the community were mixed, and understandably so: while GitLab itself is a formidable alternative to wholly proprietary services, its acquisition of Gitorious strikes a chord with the free software community that gathered around Gitorious in the name of software freedom . After hearing that announcement, as a free software hacker and activist myself , I was naturally uneasy. Discussions of alternatives to Gitorious and GitLab ensued on the libreplanet-discuss mailing list. Sytse Sijbrandij (GitLab B.V. CEO) happened to be present on that list; I approached him very sternly with a number of concerns, just as I would with anyone that I feel does not understand certain aspects of the free software philosophy . To my surprise, this was not the case at all. Sytse has spent a lot of time accepting and considering community input for both the Gitorious acquisition and GitLab itself. He has also worked with me to address some of the issues that I had raised. And while these issues won't address everyone's concerns, they do strengthen GitLab's commitment to software freedom , and are commendable. I wish to share some of these details here; but to do so, I first have to provide some background to explain what the issues are, and why they are important. Free Software Ideology Gitorious was (and still is) one of the most popular Git repository hosts, and largely dominated until the introduction of GitHub. But even as users flocked to GitHub's proprietary services , users who value freedom continued to support Gitorious, both on gitorious.org and by installing their own instances on their own servers. Since Gitorious is free software , users are free to study, modify, and share it with others. But software freedom does not apply to Services as a Software Substitute (SaaSS) or remote services—you cannot apply the four freedoms to something that you do not yourself possess—so why do users still insist on using gitorious.org despite this? The matter boils down to supporting a philosophy: The GNU General Public License (GPL) is a license that turns copyright on its head: rather than using copyright to restrict what users can do with a program, the GPL instead ensures users' freedoms to study, modify, and share it. But that isn't itself enough: to ensure that the software always remains free (as in freedom), the GPL ensures that all derivatives are also licensed under similar terms. This is known as copyleft , and it is vital to the free software movement. Gitorious is licensed under the GNU Affero General Public License Version 3 (AGPLv3) —this takes the GPL and adds an additional requirement: if a modified version of the program is run on a sever, users communicating with the program on that server must have access to the modified program's source code. This ensures that modifications to the program are available to all users ; they would otherwise be hidden in private behind the server, with others unable to incorporate, study, or share them. The AGPLv3 is an ideal license for Gitorious, since most of its users will only ever interact with it over a network. GitLab is also free software: its Expat license (commonly referred to ambiguously as the \"MIT license\") permits all of the same freedoms that are granted under the the GNU GPL. But it does so in a way that is highly permissive: it permits relicensing under any terms, free or not. In other words, one can fork GitLab and derive a proprietary version from it, making changes that deny users their freedoms and cannot be incorporated back into the original work. This is the issue that the free software community surrounding Gitorious has a problem with: any changes contributed to GitLab could in turn benefit a proprietary derivative. This situation isn't unique to GitLab: it applies to all non-copyleft (\"permissive\") free software licenses . And this issue is realized by GitLab itself in the form of its GitLab Enterprise Edition (GitLab EE): a proprietary derivative that adds additional features atop of GitLab's free Community Edition (CE). For this reason, many free software advocates are uncomfortable contributing to GitLab, and feel that they should instead support other projects; this, in turn, means not supporting GitLab by using and drawing attention to their hosting services. The copyleft vs. permissive licensing debate is one of the free software movement's most heated. I do not wish to get into such a debate here. One thing is clear: GitLab Community Edition (GitLab CE) is free software. Richard Stallman (RMS) responded directly to the thread on libreplanet-discuss , stating plainly: We have a simple way of looking at these two versions. The free version is free software, so it is ethical. The nonfree version is nonfree software, so it is not ethical. Does GitLab CE deserve attention from the free software community? I believe so. Importantly, there is another strong consideration: displacing proprietary services like GitHub and Bitbucket, which host a large number of projects and users. GitLab has a strong foothold, which is an excellent place for a free software project to be in. If we are to work together as a community, we need to respect GitLab's free licensing choices just as we expect GitLab to respect ours. Providing respect does not mean that you are conceding: I will never personally use a non-copyleft license for my software; I'm firmly rooted in my dedication to the free software philosophy , and I'm sure that many other readers are too. But using a non-copyleft license, although many of us consider it to be a weaker alternative, is not wrong . Free JavaScript As I mentioned above, software freedom and network services are separate issues —the four freedoms do not apply to interacting with gitlab.com purely over a network connection, for example, because you are not running its software on your computer. However, there is an overlap: JavaScript code downloaded to be executed in your web browser. Non-free JavaScript is a particularly nasty concern: it is software that is downloaded automatically from a server—often without prompting you—and then immediately executed. Software is now being executed on your machine, and your four freedoms are once again at risk. This, then, is the primary concern for any users visiting gitlab.com : not only would this affect users that use gitlab.com as a host, but it would also affect any user that visits the website. That would be a problem, since hosting your project there would be inviting users to run proprietary JavaScript. As I was considering migrating my projects to GitLab, this was the first concern I brought up to Sytse . This problem arises because gitlab.com uses a GitLab EE instance: if it had used only its Community Edition (GitLab CE)—which is free software—then all served JavaScript would have been free. But any scripts served by GitLab EE that are not identical to those served by GitLab CE are proprietary, and therefore unethical. This same concern applies to GitHub, Bitbucket, and other proprietary hosts that serve JavaScript. Sytse surprised me by stating that he would be willing to freely license all JavaScript in GitLab EE , and by offering to give anyone access to the GitLab EE source code who wants to help out. I took him up on that offer. Initially, I had submitted a patch to merge all GitLab EE JavaScript into GitLab CE, but Sytse came up with another, superior suggestion, that ultimately provided even greater reach. I'm pleased to announce that Sytse and I were able to agree on a license change (with absolutely no friction or hesitation on his part) that liberates all JavaScript served to the client from GitLab EE instances. There are two concerns that I had wanted to address: JavaScript code directly written for the client, and any code that produced JavaScript as output. In the former case, this includes JavaScript derived from other sources: for example, GitLab uses CoffeeScript, which compiles into JavaScript. The latter case is important: if there is any code that generates fragments of JavaScript—e.g. dynamically at runtime—then that code must also be free, or users would not be able to modify and share the resulting JavaScript that is actually being run on the client. Sytse accepted my change verbatim, while adding his own sentence after mine to disambiguate. At the time of writing this post, GitLab EE's source code isn't yet publicly visible, so here is the relevant snippet from its LICENSE file: The above copyright notices applies only to the part of this Software that is not distributed as part of GitLab Community Edition (CE), and that is not a file that produces client-side JavaScript, in whole or in part. Any part of this Software distributed as part of GitLab CE or that is a file that produces client-side JavaScript, in whole or in part, is copyrighted under the MIT Expat license. Further Discussion My discussions with Sytse did not end there: there are other topics that have not been able to be addressed before my writing of this post that would do well to demonstrate commitment toward software freedom . The license change liberating client-side JavaScript was an excellent move. To expand upon it, I wish to submit a patch that would make GitLab LibreJS compliant ; this provides even greater guarantees, since it would allow for users to continue to block other non-free JavaScript that may be served by the GitLab instance, but not produced by it. For example: a website/host that uses GitLab may embed proprietary JavaScript, or modify it without releasing the source code. Another common issue is the user of analytics software; gitlab.com uses Google Analytics. If you would like to help with LibreJS compliance, please contact me . I was brought into another discussion between Sytse and RMS that is unrelated to the GitLab software itself, but still a positive demonstration of a commitment to software freedom —the replacement of Disqus on the gitlab.com blog with a free alternative. Sytse ended up making a suggestion, saying he'd be \"happy to switch to\" Juvia if I'd help with the migration. I'm looking forward to this, as it is an important discussion area (that I honestly didn't know existed until Sytse told me about it, because I don't permit proprietary JavaScript!). He was even kind enough to compile a PDF of comments for one of our discussions, since he was cognizant ahead of time that I would not want to use Disqus. (Indeed, I will be unable to read and participate in the comments to this guest post unless I take the time to freely read and reply without running Disqus' proprietary JavaScript.) Considering the genuine interest and concern expressed by Sytse in working with myself and the free software community, I can only expect that GitLab will continue to accept and apply community input. Actions Speak Louder Than Words It is not possible to address the copyleft issue without a change in license, which GitLab is not interested in doing. So the best way to re-assure the community is through action. To quote Sytse : I think the only way to prove we're serious about open source is in our actions, licenses or statements don't help. There are fundamental disagreements that will not be able to be resolved between GitLab and the free software community—like their \"open core\" business model . But after working with Sytse and seeing his interactions with myself, RMS, and many others in the free software community, I find his actions to be very encouraging. Are you interested in helping other websites liberate their JavaScript? Consider joining the FSF's campaign , and please liberate your own ! This post is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License . Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","tags":"scm"},{"url":"http://www.ciandcd.com/security-advisory-for-logjam-vulnerability.html","title":"Security advisory for Logjam vulnerability","text":"From: https://www.gitlab.com/2015/05/21/security-advisory-for-logjam-vulnerability/ Security advisory for Logjam vulnerability A recently announced Logjam vulnerability allows an attacker to do a man-in-the-middle attack, allowing them to downgrade a TLS connection to 512-bit DH parameters. More details on what that is and means can be found on openssl blog . Impact on GitLab GitLab is using, by default, up-to-date SSL ciphers: Export Cipher Suites are not used. Elliptic-Curve Diffie-Hellman ciphers are used By default, 1024-bit DH groups are used This means that GitLab is safe in principle. When using 1028-bit DH groups there is a small chance that an attacker with nation-state resources could be eavesdropping. If you find this insufficient for your GitLab installation, you can generate 2048-bit DH groups and enable the ssl_dhparam option in NGINX config. Params can be generated with: 1 openssl dhparam -out dhparams.pem 2048 After the dhparams.pem file has been generated you will need to tell Nginx where the file is located: GitLab installations using omnibus-gitlab packages For packages version 7.11.0 and up. Place the dhparams.pem file in /etc/gitlab/ssl/ directory. In /etc/gitlab/gitlab.rb , enable the following setting: 1 nginx [ 'ssl_dhparam' ] = \"/etc/gitlab/ssl/dhparams.pem\" and do sudo gitlab-ctl reconfigure . More information can be found in the omnibus-gitlab nginx documentation . Workaround for packages prior to version 7.11.0 Place the dhparams.pem file in /etc/gitlab/ssl/ directory. In /etc/gitlab/gitlab.rb , enable the following setting: 1 nginx [ 'custom_gitlab_server_config' ] = \"ssl_dhparam /etc/gitlab/ssl/dhparams.pem; \\n \" and run sudo gitlab-ctl reconfigure . GitLab installations from source Place the generated dhparams.pem in a suitable location, for example /etc/nginx/ssl/dhparams.pem . In GitLab nginx config find ssl_dhparam config and set it to ssl_dhparam /etc/nginx/ssl/dhparams.pem; . Reload your nginx config. Impact on GitLab.com GitLab.com is using 1028-bit DH groups. Due to incompatibilities with older Java-based clients we haven't enabled 2048-bit DH params yet as this would prevent some people from using GitLab.com. We are looking into ways to keep a good SSLlabs score and allowing users with older Java-base clients to use GitLab.com. We are examining the impact of this and we will update this blog post once we have more information. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","tags":"scm"},{"url":"http://www.ciandcd.com/gitlab-711-released-with-two-factor-authentication-and-a-publicly-viewable-enterprise-edition.html","title":"GitLab 7.11 released with Two-factor Authentication and a publicly viewable Enterprise Edition","text":"From: https://www.gitlab.com/2015/05/22/gitlab-7-11-released/ It's the 22nd of the month, so we have a new GitLab release ready! GitLab 7.11 brings more improvements to the look and feel of GitLab, two-factor authentication, a version check and more! Of course we're also releasing GitLab CI 7.11, with a new backup and restore utility, improvements in the UI and other new features. This month's MVP is James Newton (newton on IRC)! James is very active on our #gitlab IRC channel, often helping people out with issues or helping people getting started with GitLab. We're very happy to have James supporting the community and believe that is deserving of a MVP award! Thanks James! Better looking sidebar We changed the look of the sidebar to reflect its function better and make it look more pretty: Clean project dashboard The project dashboard was a good example of design by committee, one GitLab contributor noted. We broomed through it and cleaned it up: Two-factor authentication Keep your code more secure and start using two-factor authentication (2FA)! GitLab has built-in 2FA in both CE and EE now and makes use of the convenient Google Authenticator. All you have to do is go to your Profile > Account and scan the QR code using Google's app. From now on, on login you'll be required to provide the code the app gives you for GitLab. Two-factor authentication only works with the web-UI for now. User roles in comments Now you know who's who in your favorite project. On comments you will see the role of the person in that project: Task lists everywhere Want a task list in the comments? Now you can! Version Check GitLab releases a new version every single month on the 22nd, so we understand that people are not always up to date. We wanted to give you some help with this, so from now on you can quickly see which version of GitLab you have running by visiting the Help or Admin page. It will show if you are up to date and if there is a security release you should have installed. Read more about the version check in our blog post about it. You can turn off the version check under Admin > Settings. License keys for Enterprise Edition GitLab Enterprise Edition used to live in a private repository, which was fine up until now. However, with the addition of our package server, we want to make it easier to start using GitLab Enterprise Edition. Rather than locking up the package repository of GitLab EE, we decided to open up all the code and packages and start using license keys. The code is still proprietary, but now is publicly viewable . This has several advantages. The installation of GitLab EE becomes as easy as installing GitLab CE. You no longer needs access to specific repositories, rather you can download it using the same methods as CE (including AWS/Azure templates, Docker images, etc). In addition, the code for Enterprise Edition is now becoming open to inspect for everyone. This will make it easier to send enhancements and makes it easier to do a trial of Enterprise Edition. Getting organizations to purchase a subscription after their trial expires or at renewal time sometimes took a substantial effort from us. We don't want to raise prices for customers that renew without prompting because we need to invest more time in unresponsive customers. Therefore we decided to introduce license keys that prompt customers automatically. We regret the inconvenience that license keys introduce but we think it is the best solution to keep prices low. True-up model for subscriptions The worst thing about license keys is that they can be very inflexible. Most GitLab installations quickly grow in popularity within the organization. Having to purchase a new license key every time this happens is very inefficient. Also, we noticed that the majority of our customers didn't have a compliant subscription, for us this indicates that having to renew the subscription multiple times a year is very inconvenient. Therefore we will switch to a true-up model that allows you to grow now and pay later. When you get a new license you should get it for your current number of active users. For users that are added during the year you pay half price when you renew. So if you have 100 active users today you get a 100 user subscription. Suppose that when you renew a year from now you have 300 active users. You pay for a 300 user subscription and pay half a year for the 200 users that you added during the year. Getting the license key If you are currently a GitLab customer, you should have received your license key already at the email you registered with your payment. You can also email sales at gitlab dot com to request it at any time. New subscribers will receive their license key automatically. Installing the license key To install the license, vist /admin/license in your GitLab instance as an admin. Here you can upload your .gitlab-license file, which will instantly unlock GitLab Enterprise Edition. You can also download and review your current license here. Please note that we will release GitLab 7.10.5 soon, that will allow you to upload the license key to your GitLab instance before upgrading, to avoid unnecessary downtime. Two-Factor Authentication for LDAP / Active Directory (EE-only) Want to use two-factor authentication together with your LDAP or Active Directory integration? With GitLab Enterprise Edition you can. New GitLab CI Features With the release of GitLab 7.11, we also updated GitLab CI to 7.11. Some changes worth mentioning are an improved runners page, public accessible build and commit pages for public projects , a new backup/restore utility that will backup your CI database and HipChat notifications! Other awesome changes in GitLab CE We can never cover all the new stuff in each GitLab release, but these are worth to have a quick look at as well: Quick quote-reply You can now reply with a quotation by simply selecting text in an issue or merge request and pressing r . It will set the focus to the editing window and have the quoted text already in it! Atom feeds for all! There is now an atom feed for each project! Settings in admin UI We moved default project and snippet visibility settings to the admin web interface. Improved UI for mobile GitLab is now better viewable on mobile! WIP your MRs! If you add WIP or [WIP] (work in progress) to the start of the title of a merge request, it will be protected from merging now. This release has more improvements, including security fixes, please check out the Changelog to see the all named changes. Upgrade barometer Coming from 7.10, the migrations in 7.11 are pretty fast (under 1 minute), but one of them is tricky: we rename any existing users with names ending in a period (‘.'). This migration updates both the database and the filesystem and previous versions of this migration have proven to be fragile. If you have no user namespaces with paths ending in ‘.' in your database and if you trust your users not to create any until after you upgrade to GitLab 7.11 you can perform this upgrade online. If not, we recommend to take downtime (this is what we did for gitlab.com). You can find the current number of affected database records with the following command: 1 sudo gitlab-rails runner \"puts Namespace.where(type: nil).where(%q{path LIKE '%.'}).count\" Installation If you are setting up a new GitLab installation please see the installing GitLab page . Updating Check out our update page . Please note that cookbook-omnibus-gitlab, our Chef cookbook that installs/manages GitLab omnibus packages, does not yet support packages.gitlab.com. See this issue . Enterprise Edition The mentioned EE-only features and things like LDAP group support can be found in GitLab Enterprise Edition. For a complete overview please have a look at the feature list of GitLab EE . Access to GitLab Enterprise Edition is included with a subscription . No time to upgrade GitLab yourself? A subscription also entitles you to our upgrade and installation services. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","tags":"scm"},{"url":"http://www.ciandcd.com/gitlab-7105-released.html","title":"GitLab 7.10.5 released","text":"From: https://www.gitlab.com/2015/05/27/gitlab-7-dot-10-dot-5-released/ GitLab 7.10.5 released In GitLab 7.11 we have introduced the requirement of a license key for users of GitLab Enterprise Edition. This can cause a moment of downtime when upgrading, as you will need to upload the license key before being able to push to the GitLab instance. With this patch release we're adding a license upload functionality that allows you to upload your license in GitLab 7.10.5, preventing downtime when upgrading to GitLab 7.11 Enterprise Edition. This patch release also includes a fix for GitLab Annex and patches a MySQL vulnerability in GitLab CI. If you are not using GitLab Enterprise Edition, you can skip this patch and go straight to GitLab 7.11 . Upgrade barometer This is a minor update, without any migrations. No downtime is necessary. Updating To update, check out our update page . As Enterprise Edition user, if you want to update to 7.10.5 rather than straight to 7.11, download and install the Omnibus package at the old download location, here. . For installations from source, use this guide . Enterprise Edition Interested in GitLab Enterprise Edition? For an overview of feature exclusive to GitLab Enterprise Edition please have a look at the features exclusive to GitLab EE . Access to GitLab Enterprise Edition is included with a subscription . No time to upgrade GitLab yourself? A subscription also entitles to our upgrade and installation services. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","tags":"scm"},{"url":"http://www.ciandcd.com/gitlab-7114-released.html","title":"GitLab 7.11.4 released","text":"From: https://www.gitlab.com/2015/05/28/gitlab-7-dot-11-dot-4-released/ GitLab 7.11.4 released We've released GitLab 7.11.4 for GitLab CE, EE and CI. It includes the following fixes for CE and EE: Fix rendering of list bullets Force a rel=\"nofollow\" attribute on all external links in markdown For GitLab Enterprise Edition this patch release also fixes a bug in git-annex. This fix was also included in the (unannounced) 7.11.3 patch. Upgrade barometer This is a minor update, without any migrations. No downtime is necessary. Updating To update, check out our update page . Enterprise Edition Interested in GitLab Enterprise Edition? For an overview of feature exclusive to GitLab Enterprise Edition please have a look at the features exclusive to GitLab EE . Access to GitLab Enterprise Edition is included with a subscription . No time to upgrade GitLab yourself? A subscription also entitles to our upgrade and installation services. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","tags":"scm"},{"url":"http://www.ciandcd.com/gitlabcom-outage-on-2015-05-29.html","title":"GitLab.com outage on 2015-05-29","text":"From: https://www.gitlab.com/2015/06/04/gitlab-dot-com-outage-on-2015-05-29/ GitLab.com outage on 2015-05-29 GitLab.com suffered an outage from 2015-05-29 01:00 to 2015-05-29 02:34 (times in UTC). In this blog post we will discuss what happened, why it took so long to recover the service, and what we are doing to reduce the likelihood and impact of such incidents. Background GitLab.com is provided and maintained by the team of GitLab B.V., the company behind GitLab. On 2015-05-02 we performed a major infrastructure upgrade, moving GitLab.com from a single server to a small cluster of servers, consisting of a load balancer (running HAproxy), three workers (NGINX/Unicorn/Sidekiq/gitlab-shell) and a backend server (PostgreSQL/Redis/NFS). This new infrastructure configuration improved the responsiveness of GitLab.com, at the expense of having more moving parts. GitLab.com is backed up using Amazon EBS snapshots. To protect against inconsistent snapshots our backup script ‘freezes' the filesystem on the backend server with fsfreeze prior to making EBS snapshots, and ‘unfreezes' the filesystem immediately after. Timeline Italic comments below are written with the knowledge of hindsight 1:00 The GitLab.com backup script is activated by Cron on the backend server. For unknown reasons, the backup script hangs/crashes before or during the ‘unfreeze' of the filesystem holding all user data. 1:07 Our on-call engineer is paged by Pingdom . The on-call engineer tries to diagnose the issue on the worker servers but is unable to diagnose the problem. The issue was on the backend server, not on the workers. 1:30 The on-call engineer decides to call in more help. The other team members with access and knowledge to resolve the issue are all in Europe at this time, where it is 3:30/4:30am. 1:45 A second engineer in Europe has been woken up and takes the lead on the investigation of the outage. More workers are rebooted because they appear to be stuck. It becomes apparent that the workers cannot mount the NFS share which holds all Git repository data. 1:51 One of the engineers notices that the load on the backend server is more than 150. A normal value would be less than 5. 2:10 The engineers give up on running commands on the workers to bring the NFS share back, and start investigating the backend server. The engineers discuss whether they should reboot the backend server but they are unsure if it is safe given that this setup is fairly new. 2:21 The engineers reboot the backend server. The reboot is taking a long time. The AWS ‘reboot' command first tries a soft reboot, and only does a hard reboot after a 4-minute timeout. The soft reboot probably hung when it tried to shut down services that were trying to write to the ‘frozen' disk. 2:30 The backend server has rebooted and the engineers regain SSH access to it. The worker servers are able to mount the NFS share now but GitLab.com is still not functioning because the Postgres database server is not responding. One of the engineers restarts Postgres on the backend server. It may have been that Postgres was still busy performing crash recovery. 2:34 Gitlab.com is available again. Root causes Although we cannot explain what went wrong with the backup script it is hard to come to another conclusion that something did go wrong with it. The length of the outage was caused by insufficient training and documentation for our on-call engineers following the infrastructure upgrade rolled out on May 2nd. Next steps We have removed the freeze/unfreeze steps from our backup script. Because this (theoretically) increases the risk of occasional corrupt backups we have added a second backup strategy for our SQL data. In the future we would like to have automatical validation of our GitLab.com backups. The day before this incident we decided the training was our most important priority. We have started to do regular operations drills in one-on-one sessions with all of our on-call engineers. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","tags":"scm"},{"url":"http://www.ciandcd.com/note-on-license-expiration-in-gitlab-7105-ee.html","title":"Note on license expiration in GitLab 7.10.5 EE","text":"From: https://www.gitlab.com/2015/06/04/note-on-license-expiration-in-gitlab-7-dot-10-dot-5-ee/ Note on license expiration in GitLab 7.10.5 EE If you're upgrading to GitLab Enterprise Edition 7.11, which introduces licenses keys, you're probably planning to upgrade to 7.10.5 first. This way you are able to upload your license key in advance . One of our customers notified us of a faulty description in the license uploader in GitLab 7.10.5. Upon uploading, the license is checked properly, however the text in the license view in the admin page in GitLab will show: While it should look like this: This only occurs in GitLab 7.10.5 and does not affect functionality. The license information is correctly shown in GitLab 7.11 and up. If you have any questions or comments do not hesitate to comment below or contact support. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","tags":"scm"},{"url":"http://www.ciandcd.com/how-gitlab-uses-unicorn-and-unicorn-worker-killer.html","title":"How GitLab uses Unicorn and unicorn-worker-killer","text":"From: https://www.gitlab.com/2015/06/05/how-gitlab-uses-unicorn-and-unicorn-worker-killer/ How GitLab uses Unicorn and unicorn-worker-killer We just wrote some new documentation on how Gitlab uses Unicorn and unicorn-worker-killer, available on doc.gitlab.com but also included below. We would love to hear from the community if you have other questions so we can improve this documentation resource! Update 19:29 CEST: made link to doc.gitlab.com more specific. Understanding Unicorn and unicorn-worker-killer Unicorn GitLab uses Unicorn , a pre-forking Ruby web server, to handle web requests (web browsers and Git HTTP clients). Unicorn is a daemon written in Ruby and C that can load and run a Ruby on Rails application; in our case the Rails application is GitLab Community Edition or GitLab Enterprise Edition. Unicorn has a multi-process architecture to make better use of available CPU cores (processes can run on different cores) and to have stronger fault tolerance (most failures stay isolated in only one process and cannot take down GitLab entirely). On startup, the Unicorn ‘master' process loads a clean Ruby environment with the GitLab application code, and then spawns ‘workers' which inherit this clean initial environment. The ‘master' never handles any requests, that is left to the workers. The operating system network stack queues incoming requests and distributes them among the workers. In a perfect world, the master would spawn its pool of workers once, and then the workers handle incoming web requests one after another until the end of time. In reality, worker processes can crash or time out: if the master notices that a worker takes too long to handle a request it will terminate the worker process with SIGKILL (‘kill -9'). No matter how the worker process ended, the master process will replace it with a new ‘clean' process again. Unicorn is designed to be able to replace ‘crashed' workers without dropping user requests. This is what a Unicorn worker timeout looks like in unicorn_stderr.log . The master process has PID 56227 below. 1 2 3 4 [2015-06-05T10:58:08.660325 #56227] ERROR -- : worker=10 PID:53009 timeout (61s > 60s), killing [2015-06-05T10:58:08.699360 #56227] ERROR -- : reaped #<Process::Status: pid 53009 SIGKILL (signal 9)> worker=10 [2015-06-05T10:58:08.708141 #62538] INFO -- : worker=10 spawned pid=62538 [2015-06-05T10:58:08.708824 #62538] INFO -- : worker=10 ready Tunables The main tunables for Unicorn are the number of worker processes and the request timeout after which the Unicorn master terminates a worker process. See the omnibus-gitlab Unicorn settings documentation if you want to adjust these settings. unicorn-worker-killer GitLab has memory leaks. These memory leaks manifest themselves in long-running processes, such as Unicorn workers. (The Unicorn master process is not known to leak memory, probably because it does not handle user requests.) To make these memory leaks manageable, GitLab comes with the unicorn-worker-killer gem . This gem monkey-patches the Unicorn workers to do a memory self-check after every 16 requests. If the memory of the Unicorn worker exceeds a pre-set limit then the worker process exits. The Unicorn master then automatically replaces the worker process. This is a robust way to handle memory leaks: Unicorn is designed to handle workers that ‘crash' so no user requests will be dropped. The unicorn-worker-killer gem is designed to only terminate a worker process in between requests, so no user requests are affected. This is what a Unicorn worker memory restart looks like in unicorn_stderr.log. You see that worker 4 (PID 125918) is inspecting itself and decides to exit. The threshold memory value was 254802235 bytes, about 250MB. With GitLab this threshold is a random value between 200 and 250 MB. The master process (PID 117565) then reaps the worker process and spawns a new ‘worker 4' with PID 127549. 1 2 3 4 5 [2015-06-05T12:07:41.828374 #125918] WARN -- : #<Unicorn::HttpServer:0x00000002734770>: worker (pid: 125918) exceeds memory limit (256413696 bytes > 254802235 bytes) [2015-06-05T12:07:41.828472 #125918] WARN -- : Unicorn::WorkerKiller send SIGQUIT (pid: 125918) alive: 23 sec (trial 1) [2015-06-05T12:07:42.025916 #117565] INFO -- : reaped #<Process::Status: pid 125918 exit 0> worker=4 [2015-06-05T12:07:42.034527 #127549] INFO -- : worker=4 spawned pid=127549 [2015-06-05T12:07:42.035217 #127549] INFO -- : worker=4 ready One other thing that stands out in the log snippet above, taken from Gitlab.com, is that ‘worker 4' was serving requests for only 23 seconds. This is a normal value for our current GitLab.com setup and traffic. The high frequency of Unicorn memory restarts on some GitLab sites can be a source of confusion for administrators. Usually they are a red herring . Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","tags":"scm"},{"url":"http://www.ciandcd.com/implementing-gitlab-ciyml.html","title":"Implementing .gitlab-ci.yml","text":"From: https://www.gitlab.com/2015/06/08/implementing-gitlab-ci-dot-yml/ Implementing .gitlab-ci.yml We wrote about why we're replacing GitLab CI jobs with a .gitlab-ci.yml file. As we've started on implementing this large change, we wanted to share the details of that process with you and would love to hear what you think. To recap the previous article : currently you are required to write out your CI jobs in GitLab CI's interface. We're replacing this with a single file .gitlab-ci.yml , that you place in the root of your repository. Schema change Currently, on a push to GitLab, GitLab sends a web-hook to the CI Coordinator. The coordinator creates a build based on the jobs that are defined in its UI, which can then be executed by the connected Runners. In the new schema, GitLab sends the web-hook and the .gitlab-ci.yml contents to the CI Coordinator, which creates builds based on the yml file. In turn, these builds are executed by the Runners as before. Migrating to new style Keeping two different ways of doing things would be a strain on development and support, not to mention confusing. So we're not just deprecating the old style of defining jobs, we're removing it entirely and will migrate existing jobs. Upon upgrading your existing jobs defined in the GitLab CI Coordinator will be converted into a YAML file with the new syntax. You can download this file at any time from the project settings. When the GitLab webhook triggers and doesn't transmit the content from .gitlab-ci.yml , the coordinator will use the converted YAML file instead. This makes migrating to the new style very easy. You can start by simply copy-pasting the contents of the converted YAML file to the root of your repository. Existing projects will continue to build successfully, yet new projects do not have the option to use anything else. An example .gitlab-ci.yml To get an idea of how the .gitlab-ci.yml will look, we've prepared an example for a Ruby on Rails project (such as GitLab itself). Of course, this is due to change as we're still working on this. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # Refs to skip skip_refs: \"deploy*\" # Run before each script # Refs to skip skip_refs: \"deploy*\" # Run before each script before_script: - export PATH=$HOME/bin:/usr/local/bin:/usr/bin:/bin - gem install bundler - cp config/database.yml.mysql config/database.yml - cp config/gitlab.yml.example config/gitlab.yml - touch log/application.log - touch log/test.log - bundle install --without postgres production --jobs $(nproc) - \"bundle exec rake db:create RAILS_ENV=test\" # Parallel jobs, each line is a parallel build jobs: - script: \"rake spec\" runner: \"ruby,postgres\" name: \"Rspec\" - script: \"rake spinach\" runner: \"ruby,mysql\" name: \"Spinach\" tags: true branches: false # Parallel deploy jobs on_success: - \"cap deploy production\" - \"cap deploy staging\" UPDATE Dmitriy and Sytse spend some time thinking about file syntax. Scripting should be simple and memorable. Thats why we come with better proposal: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 before_script: - gem install bundler - bundle install - bundle exec rake db:create rspec: test: \"rake spec\" tags: - ruby - postgres only: - branches spinach: test: \"rake spinach\" tags: - ruby - mysql except: - tags staging: deploy: \"cap deploy stating\" tags: - capistrano - debian except: - stable production: deploy: - cap deploy production - cap notify tags: - capistrano - debian only: - master - /&#94;deploy-.*$/ Contribute GitLab is nothing without its community. Contribute or follow the development in the GitLab CI repository . Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","tags":"scm"},{"url":"http://www.ciandcd.com/did-you-install-gitlab-from-source-check-your-git-version.html","title":"Did you install GitLab from source? Check your Git version","text":"From: https://www.gitlab.com/2015/06/12/did-you-install-gitlab-from-source-recently-check-your-git-version/ Did you install GitLab from source? Check your Git version Although the preferred way to install GitLab is to use our omnibus packages , you can also install GitLab Community Edition or Enterprise Edition ‘from source'. If you used this installation method, and if you compiled Git from source in the process then please check whether your Git version defends against Git vulnerability CVE-2014-9390. This issue does not apply to our Omnibus packages (DEB or RPM). Although GitLab itself is not affected by CVE-2014-9390, a GitLab server may be used to deliver ‘poisoned' Git repositories to users on vulnerable systems. Upgrading Git on your GitLab server stops users from pushing poisoned repositories to your GitLab server. Due to an oversight, the guide for installing GitLab from source still contained instructions telling administrators to install Git 2.1.2 if the version of Git provided by their Linux distribution was too old. Git 2.1.2 does not defend against CVE-2014-9390. If your GitLab server uses /usr/local/bin/git please check your Git version using the instructions in this upgrade guide . Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","tags":"scm"},{"url":"http://www.ciandcd.com/changes-to-enterprise-edition-subscription-pricing.html","title":"Changes to Enterprise Edition subscription pricing","text":"From: https://www.gitlab.com/2015/06/12/price_changes/ Changes to Enterprise Edition subscription pricing Today we are announcing two changes to GitLab Enterprise Edition subscription pricing. The changes are intended to better reflect the value of each offering and ensure our subscription options cater to the needs of different organizations. In short, our basic subscription is now $19,10 more expensive, but in 10-user packs. Our Plus subscription is now $100 more affordable. Standard and terms remain unchanged. As of today (June 12, 2015) the following will take affect: Basic Subscriptions will cost $390 per year for a 10-user pack ($39 per user / per year). Current Basic Subscribers will be offered a 25% discount on this new pricing at their next renewal. However, new pricing will apply to subsequent renewals and any additional user packs. Basic subscriptions are now available in 10-user packs, making it slightly more affordable for small teams. Plus Subscriptions will cost $14,900 for a 100-user pack ($149 per user / per year). Current Plus subscribers will receive a prorated refund on the price differece. There are no changes in the software features or service level of Basic or Plus subscriptions, which you can view on our website here . Standard Subscription pricing will also remain unchanged at $4,900 per year for each 100-user pack ($49 per user / per year). All current quotes will be honored until their expiration (60 days from issue date) but the new pricing will apply to any subsequent orders, including renewals. Our goal is to keep GitLab the most affordable enterprise grade development platform available. These changes should not have any significant effect on our ability to achieve that. We felt our Basic plan was underpriced and Plus plan was overpriced. These changes reduce the price difference between them. If you have questions about the changes or about pricing in general, please contact our sales team at sales@gitlab.com. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","tags":"scm"},{"url":"http://www.ciandcd.com/feature-highlight-approve-merge-request.html","title":"Feature Highlight: Approve Merge Request","text":"From: https://www.gitlab.com/2015/06/16/feature-highlight-approve-merge-request/ Feature Highlight: Approve Merge Request With less than a week until GitLab 7.12, we've got a nice preview for you today: Merge Request Approvals in GitLab EE. Usually you accept a merge request the moment it is ready and reviewed. But in some cases you want to make sure that every merge request is reviewed and signed off by several people before merging it. With GitLab Enterprise Edition 7.12, you can enforce such a workflow that requires multiple reviewers with the new Merge Request Approval feature. To enable approvals, go to project settings page and set the \"Approvals required\" field to a numeric value. For example, if you set it to 3 each merge request has to receive 3 approvals from different people before it can be merged through the user interface. After setting the approval, you will see an Approve button on merge requests, rather than an Accept button. Once the merge request has enough approvals, you will be able to merge it as usual. We'd love to hear what you think of this new feature in the comments below. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","tags":"scm"},{"url":"http://www.ciandcd.com/gitlabcom-and-logjam.html","title":"GitLab.com and Logjam","text":"From: https://www.gitlab.com/2015/06/17/gitlab-com-and-logjam/ GitLab.com and Logjam We've previously announced security advisory for Logjam vulnerability . In that announcement we've mentioned that GitLab.com is using 1024-bit DH groups to retain compatibility with older Java-based clients. We've updated the default/recommended SSL ciphers for all GitLab installations and implemented new ciphers on GitLab.com. After some reasearch and testing we've decided to change the SSL cipher suite served by the web server/load balancer. This decision was made after weighing on the trade-offs between having the stronger DH params and denying access to Java 6 based clients. Using 2048-bit DHE params Generating the 2048-bit DHE params was advised to help against the Logjam vulnerability. While this is a way to go for most servers, with GitLab.com we have to keep in mind that we have users using older Java-based clients. Adopting the stronger params suites would prevent those users using GitLab.com completely. Although the number of these users is not high, denying them access does not seem like an option. Removing DHE suites DHE suites have a couple of issues: DHE is slow Not all browsers support all the necessary suites One advantage of having DHE together with ECDHE suites is that this allows forward secrecy to all clients. We then turned to investigating how others are handling this issue and we found out that, for example, Google sites mostly do not have DHE suites in their configuration . With this in mind we've tried removing the DHE suites and the result was as follows: All major browsers and clients retain forward secrecy using ECDHE SSL labs score went from B to A There is no forward secrecy for Android 2.3.7, Java 6 and OpenSSL 0.9.8 After considering the trade-offs, we've decided to remove the DHE suites from our cipher suite on GitLab.com. Forward secrecy is now denied for Android 2.3.7, Java 6 and OpenSSL 0.9.8 but we suspect that number of users affected will be extremely low. We have also updated the recommended configurations for omnibus-gitlab packages and GitLab installation from source. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","tags":"scm"},{"url":"http://www.ciandcd.com/release-manager-the-invisible-hero.html","title":"Release Manager - The invisible hero","text":"From: https://www.gitlab.com/2015/06/25/release-manager-the-invisible-hero/ Real heroes are sometimes unknown and we can only see their accomplishments. In GitLab we have one invisible hero every month, when we have our monthly release. As you may know, we've never failed to release a new GitLab version on the 22nd of every month. As GitLab grows, the release process becomes more complex and becoming a release manager is a more difficult, but a necessary job. Eight working days before the next release, and we start the countdown. A new volunteer \"hero\" is elected by the team. But, why is it such a challenging job? A release manager is the person who makes sure that everything is ready for the monthly release. They follow up on every single detail and make sure that the new version is working perfectly, including all the improvements and features. They also need to delegate some tasks and make sure that the procedure is being followed. Consider that right now, GitLab is huge. Our community dishes out around 900 commits a month on GitLab alone. Add Enterprise Edition, GitLab CI and runners, Omnibus-GitLab packages and you get several thousand changes done by hundreds of developers across projects which need to come together (and work) in one day. This is a lot of responsibility for one person. So, how do we manage to make it all into a single release every month? In GitLab we have a release directory for the release documents. The most powerful document for the release is called monthly.md . Release manager tasks can be broken down into: Make sure that GitLab CE, EE and GitLab CI repositories have an updated installation and upgraded guides Make sure that the Omnibus-GitLab package will be ready for the release Release the RC version, do QA, deploy on GitLab.com and ci.GitLab.com Follow reported regressions and make sure that developers are aware/working on a fix Decide which fixes can go into the release Coordinate the package building Make sure that the blog post contains all the necessary information Do the final release Decide if there needs to be a patch release Coordinate patch release A release manager volunteers to work late (or early) to get the packages out or deploy the new version to one of our services. No one is forcing you to do so, but if you don't, it will complicate the following day. This is a weakness in our process, so we need to work on improving this situation. History I don't know the exact date when the release manager duty was thought off but it was around version 6.4 . At that time, we had a couple of other things that were the release manager tasks: Notify everyone of the code-freeze (nothing was merged to master during this time), enforce it and build the packages manually. Yes, manually. This meant connecting to all machines separately and doing few commands to initiate package building. GitLab.com had a separate repository with some custom code, so the deploy needed to be done manually too. I still have nightmares as a result of these 2 things. As you can imagine, this made the release manager tasks very undesirable and limited to a few people. Even with all the improvements that followed, this job is still not popular. Improvements Since the painful beginings of the release manager tasks, we've done number of improvements. We did a massive change to the process and made it even more continuous integration oriented than it was before. There are risks to it, but also massive gains: Code freeze was removed so there is no need to watch over anyone's shoulders Keeping X git repos in sync. Syncing repositories is now a one-line script where the argument is the version that is being released Automatizing our release process. Omnibus-gitlab packages infrastructure got built, so only supplying the shas of the release version is enough to kick off the automatic builds on all platforms and machines Infrastructure for deploying GitLab.com and ci.gitlab.com got created and they are being updated by using a few lines of commands and packages The release documentation has been updated so many times that room for error is minimal (if you follow the steps closely) You would expect that all these improvements would make the Release manager job more appealing since you get to: Boss around over all of your colleagues. This includes the project lead and the CEO. It is especially sweet when you can say NO to an unreasonable request. After all, all requests are unreasonable but your own and now you get to push that through You decide at your leasure when something will be included and pushed You are the boss of everything (for a period of time) because everyone says: \"Hey, you are the release manager, your call\" With all the hard work, how do we choose a volunteer release manager? Choosing the release manager is probably one of the hardest tasks. During our team call, the release manager for the previous release mentions the subject of selecting a new release manager. At that exact moment, there's silence, cameras and mics start breaking down, people forget the whole English language, there is always someone at the door so you need to open it and lots of faces are just looking around the room. After a few minutes of silence, decision is made, but mostly because we are all friends and we don't want to see a colleague suffer for another month. We've tried improving the desirablity of this task by making procedures easier, but that is still a challenge. At some point I've asked what kind of reward we could put forward to make people happy to volunteer, but there are no good ideas yet. My ideas where limited to: Material reward: a gift might be OK for some people, but others have no need for things. In this case we could publicly thank them and acknowledge their work. \"Spiritual\" reward: We do say \"thank you\" to the RM a lot, but this gets spent. Tweeting the name of the release manager might work as a recongnition for some, but I am afraid that it won't work for introverts in our team. Being more public might also yield more work for them. Buying a beer or cocktail: This feels like something that would be appreciated, but it would only work for a few employees, since we are a very remote company. Maybe a beer voucher could be sent. With that I was out of ideas. This blog post is an attempt to say a thank you to all the release managers. You know who you are and you are a true invisible hero for accomplishing the tasks to make everything go out on schedule. Do you have any ideas? Release manager - my hero. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","tags":"scm"},{"url":"http://www.ciandcd.com/p4python-goes-pip.html","title":"P4Python goes pip","text":"From: http://www.perforce.com/blog/150514/p4python-goes-pip-0 In recent years Python has changed its package manager strategy, and the result is pip . Pip is a powerful package manager that simplifies the creation and consumption of Python packages, turning the Python Package Index into a hub of an ever-growing number of useful packages. P4Python always had to stay away from the package index because it requires binary builds for some platforms. With the advent of the wheel format, this has changed. Wheels are Python packages that can contain binary builds on Windows and OSX, allowing package creators to precompile their packages. P4Python 2015.1 has been uploaded to the Python Package Index. For you this means installing the latest release of P4Python becomes a simple: pip install p4python Easier, isn't it? However, there are a few preparations you need to make before you can run this command the first time successfully: You need to have the right version of Python installed: 2.7, 3.3 or 3.4. Python 2.6 is supported but we have not uploaded binaries for it. You need to install pip. It comes with Python 3.4 and 2.7.9 automatically; everyone else will need to install it, for example from here , and make sure the pip executable is in your PATH. There is currently no binary wheel format available for Linux, so pip will download the source code of P4Python and attempt to build it. This requires ‘python-dev' and ‘build-essential' installed on Debian-based distributions (using apt) and the equivalent on RPM-based (using yum). In order to build P4Python automatically from Pip, setup.py has also gained some new tricks. First of all, it now uses setuptools and not distutils, so you need to have setuptools installed if you want to build it on, say, Python 2.6. If you run setup.py without the –apidir option, setup will now go off and attempt to download the correct P4API binaries from the Perforce FTP site for your platform. The API will be downloaded and unpacked into the temp directory and used automatically by the build process. You can still download the source or the packages, of course, and install P4Python the traditional way if you prefer. I found pip amazingly simple to use, and I hope it will make your life a lot easier when installing P4Python. As usual, if you have any issues or requests, let us know in Perforce Support or ping me on my Twitter handle @p4sven . Happy hacking.","tags":"scm"},{"url":"http://www.ciandcd.com/helix-swarm-20151-released.html","title":"Helix Swarm 2015.1 Released","text":"From: http://www.perforce.com/blog/150608/helix-swarm-20151-released Swarm is two years old this month! It's rewarding to think that just two years ago, our collaboration engine was only just getting into the hands of our customers. Fast-forward to today, where Swarm plays a big part in the daily workflows of so many innovative companies. With much of the functionality now matured, we wanted to expand Swarm beyond just our English-speaking customers. With the latest release of Helix Swarm, we've translated the product into Japanese. It's available through our exclusive partner in Japan, the TOYO Corporation . TOYO provides expert consulting and support to our Japanese customers, and Swarm joins the Helix Versioning Engine and our popular visual client, P4V , in the suite of Perforce products available in Japanese. Localization Support for Swarm The Swarm team spent the last couple of months creating a localization framework and translating the product and documentation into Japanese. The next languages on our list are Korean and Simplified Chinese. If there's a language you'd like us to add to our list of localizations, please send us your request via the Perforce Swarm Forums or by emailing support and we'll put it on our radar. Aside from the localization support , other new functionalities include: Files and folders are downloadable as ZIP archives Swarm now limits the number of files to display in a committed change to a configurable default of 1000 Configurable timeout sets thresholds for large commits More details can be found in the What's new in 2015.1 section of our user guide.","tags":"scm"},{"url":"http://www.ciandcd.com/living-la-vida-helix-submitting-without-fear.html","title":"Living la Vida Helix: Submitting Without Fear","text":"From: http://www.perforce.com/blog/150609/living-la-vida-helix-submitting-without-fear One of the common complaints I hear about centralized version control systems is that they are scary. With every commit being immediately visible there is a feeling that you may screw up everything for your co-workers. What's worse is that you generally don't have the power to clean up after yourself. How many of us have had to sheepishly go ask the admin to obliterate something? With P4D (which we now call Helix Versioning Engine) becoming a proper DVCS, you now can manipulate history that has not yet been shared with other people. Than means you can commit to your heart's content, and then sweep through later to keep just the interesting commits. It also means that if you accidentally submit something you can deal with it. Just recently while doing some cleanup work in the Workshop I had just one of these cases. I'd like to walk you through what happened so that you can see how unsubmit and resubmit will help you. Setting the scene A user had reported that a number of files that I had added the day before had all of their line endings mangled. The files were already in the shared server, so I didn't want to run p4 unsubmit there, and anyway I feel it is important for my failures to remain on display for all to see. So I got to work updating the files. p4 fetch Everyting was up-to-date. Next to find the files with the bad line endings. grep -lIUr --color \"&#94;R\" I was lucky and it was just a handful of files. Thankfully turning Windows line endings into Unixones is a piece of cake with P4D. p4 client -o | sed s/LineEnd: local/LineEnd: share | p4 client -i Now to get the files synced with the correct line endings and submitted: p4 sync -f p4 submit -d \"Fixing up some busted line endings that snuck in\" All was well and good until I realized that in my excitement I'd mangled some solution files which probably wanted those '\\R's. Thankfully I hadn't pushed, so I could quickly clean up my mess. p4 changes -m1 p4 unsubmit @ 12345 I identified my last change number, and then unsubmitted it. At this point I had all of my changed files in a shelf. In this case I had only one changelist, but I still decided to use p4 resubmit to apply the change. p4 resubmit makes it easy to reapply the changes in order. p4 resubmit This kicks me into interactive mode. Because there is a lot you can do with resubmit and I always forget the options, I hit '?' to see the list. Specify next action ( l/m/e/c/r/R/s/d/b/v/V/a/q ) or ? for help: ? The following actions are available: c Modify the change description for this change m Merge this change, then submit if no conflicts e Merge this change, then exit for further editing r Interactively resolve this change, then submit if no conflicts a Add (squash) this change into the next unsubmitted change s Skip this change and move on to the next d Delete this change without submitting it b Begin again from the earliest remaining change l List the changes remaining to be processed v View the current change in short form V View the current change with full diffs R Display the status of resolved and unresolved merges q Quit the resubmit operation< ? Display this help. In this case I wanted to resubmit all of the files except the solution files, so I selected e That merged my change back in, but then dropped me back to the command prompt so I could further mangle the files. A quick revert got rid of the changed solution files, and then I used p4 resubmit -Re to resume the resubmit process. p4 revert ....sln p4 resubmit -Re P4D submitted the change again, and cleaned up the shelf for me since I no longer needed it. With that tidied up I was ready to push and share my changes with the community. p4 push Sharing that broken change wouldn't have been the end of the world, but I felt so much more in control being able to clean up those .sln files before pushing out my change. Ever wish you could undo a merge between branches? With p4 unsubmit you can. Helix Versioning Engine gives you a way to safely experiment, modifying history as need be to make sure the changes your coworkers see are the ones you want them to see. Interested in trying it yourself? You're just a download of our Helix Versioning Engine and p4 init away from being able to try this all yourself. If you'd like to push to a shared server the Workshop has been running 2015.1 since beta, and Helix Cloud is also using it. As always we're here to help, so if you have questions, just shout!","tags":"scm"},{"url":"http://www.ciandcd.com/news-from-infosecurity-2015.html","title":"News from InfoSecurity 2015","text":"From: http://www.perforce.com/blog/150610/news-infosecurity-2015-0 I've just returned from my first visit to InfoSecurity 2015 in London . With the launch earlier this year of Perforce Helix Threat Detection this was a great opportunity to review the state of the cyber-security world, hear about the key challenges facing governments, businesses and individuals and review some of the solutions being offered. This couldn't be more timely as, right when I'm writing this, it has been announced that the U.S. government suffered a serious attack earlier this year which compromised the personal details of thousands of federal employees. My particular area of interest is the emerging role of security in DevOps. There are a few key aspects to consider: As a developer what do you need to do and how does that fit with agile and development processes? As a Release Manager/Operations Specialist/DevOps Engineer what do you need to know to roll out and manage secure applications? As a Chief Information Security Officer or Risk Manager what is going on in the development and operations areas that I ought to be concerned about? I haven't got space here to cover all of these topics, but here are a few highlights from the conversations I had at the conference. Development Managers and DevOps specialists are increasingly aware of the need for secure applications. They are concerned that as release cycle times reduce with the adoption of Continuous Delivery they don't reduce security nor slow down deliveries. Some companies are working out how to do this by involving security experts in the earliest stages of sprint planning and ensuring security stories are \"groomed\" to ensure they are properly positioned for priority in their backlogs. They're also adopting tools for automated code and application validation. It was interesting to see an increasing number of tools addressing the need for dynamic security testing. Although the term seems to have been around a few years already, there were a number of people talking about \" Rugged DevOps \" and I think this is an area that will continue to grow. Security experts, especially those involved in IT audits or risk assessments are busier than ever. Some are aware of the potential risks that may exist in their development organizations but I suspect the majority are not. This is the result of two issues. Firstly, they may not fully appreciate the value of the software being developed. They know that they need to protect customer and staff personal data, but they don't necessarily realize that the software is actually their company's competitive differentiator and could be critical if leaked to a competitor. Secondly there is a lot of technology involved that they don't understand. They may be familiar with firewalls, VPNs, email, etc., but developers often bring tools into the business without their knowledge and these tools, such as Subversion or Git are inherently vulnerable. It's increasingly hard to keep track of business documents in a world full of email, cloud file sharing services and BYOD mobiles, but this technical software content is even harder to grasp. I saw a number of tools that try to address some of these problems by monitoring network traffic rather than trying to lock down each application. This generates another problem though – if you're monitoring hundreds or thousands of different file types and communications, it quickly becomes an impossible management challenge. A few tools are trying to address that problem by using analytics to analyze the basic data and infer what looks like suspicious behavior. This helps with the management issue but they still don't understand the context of the data being moved around the organization which makes them inefficient for DevOps. I didn't see anything that was close to Perforce Helix Threat Detection , which focuses on protecting this valuable IP being created by design and development teams. Because it uses the rich data available from the Helix Versioning Engine it understands the context of the files being accessed. It can not only track that a user may be accessing more files than usual (and most tools can't work out what \"normal\" means), but it also understands whether those files are in projects they \"normally\" use or whether they're using the files in ways that are unusual for the user. I'm really looking forward to the webinar Perforce are hosting on June 16th where the Forrester DevOps Analysts, Kirt Bittner, and Security Analyst, Rick Holland will talk about the issues raised above and the solutions to them.","tags":"scm"},{"url":"http://www.ciandcd.com/next-round-of-online-training-helix-dvcs.html","title":"Next Round of Online Training: Helix DVCS","text":"From: http://www.perforce.com/blog/150612/next-round-online-training-helix-dvcs Back in March, we announced our new Helix platform which includes highly anticipated distributed version control (DVCS) capabilities. For some of you, hearing that we now offer DVCS may have been music to your ears. For others, it may have invoked curiosity and added another acronym to your lexicon. Given the recent proliferation of Git-style workflows, we are seeing a lot of developers finding themselves working with their own private, local versioning repositories while collaborating with teammates via the new init/clone/pull/push command set. But did you know that you can now rewrite the history of changes in your personal repository before sharing them? To help you come up to speed with Helix DVCS, we are pleased to announce the availability of a new instructor-led training course. The Helix DVCS course will take place online via Webex and will include hands-on lab exercises within our lab environment. The class is taught by our expert Professional Services consultants who have a lot of experience advising customers. Topics on this new half-day course will include: Why do you need DVCS? Overview of DVCS architecture and workflows Basic DVCS operations How to perform initial setup Working with multiple streams Rewriting history The first class is now scheduled for European customers on June 29, 9am – 1pm, British Summer Time (GMT+1). We will have a DVCS training for North American customers in the weeks that follow. So sign up here and bring along your questions about DVCS. The class does assume that you are already familiar with Perforce Helix, so if you're new to Helix, we also offer introductory courses. Check out our course schedule for details. Any questions, just email training@perforce.com.","tags":"scm"},{"url":"http://www.ciandcd.com/helix-dvcs-how-to-initialize-like-a-pro.html","title":"Helix DVCS - How to Initialize Like a Pro","text":"From: http://www.perforce.com/blog/150610/helix-dvcs-initialization-tips-tricks We are all very excited about the new distributed version control system (DVCS) capabilities of Perforce Helix. Here are a few tips for getting started. Keep in mind that in order to use Helix DVCS, you need to have the 2015.1 version of both Helix client P4 and Helix server P4D installed. Some of the commands (e.g., init and clone ) are implemented in P4, so you need the latest version of both executables. The first thing you need to do when you want to use a local Helix server (called a personal server) is to run \"p4 init\". This command will create the personal server for you (in a subdirectory called .p4root) and set up the P4CONFIG and P4IGNORE files, as well. \"p4 init\" also turns your current directory into the client workspace root for your new personal server , which is useful if you already have some files and realize it might be a good thing to version them: p4 init p4 rec p4 submit -n \"Initial checkin\" In the above, \"rec\" is a handy alias for \"reconcile\" to save you typing . If you start a new project from scratch and want to place it in another directory instead, use the \"-d\" option like such: p4 –d path-to-new-project init Case and Unicode Let's take a closer look at the output of the \"p4 init\" command: Matching server configuration from ‘wayfarer-p4d:1666': case-sensitive (-C0), non-unicode (-n) Server sknop-dvcs-1429629213 saved. One might ask: what is case-sensitive and Unicode about? Because the Helix versioning engine supports many platforms, both case sensitive and insensitive, you can choose how your personal server handles case. By default, the Helix versioning engine adopts the case policy of the platform you run it on: insensitive on Mac and Windows, sensitive on Linux and other Unix platforms. Also by default, the Helix versioning engine does no Unicode translation and simply accepts any encoding for file content and metadata. For cross-platform development it is better to put a shared server into Unicode mode. For a personal server you may not care at first what these settings are, but what if you want to push your changes to another server at a later stage? The settings of your personal server have to match the settings on the destination server or there could be chaos, as the destination server will refuse the push if the settings do not match. It is cumbersome to change case sensitivity and Unicode settings after the Helix versioning engine is populated, so it is important to get this right up front. \"p4 init\" will \"guess\" what the standard settings within your enterprise are by connecting to and inquiring with the Helix versioning engine specified by the P4PORT environment variable (or \"perforce:1666\" if that is not set). If you'd rather inquire with a particular server when initializing a personal server, use the \"-p\" option: p4 init –p myserver:1666 Alternately, you can also explicitly set case and Unicode support with the following options: Option Meaning -C1 Case insensitive -C0 Case sensitive -n No Unicode support -xi Unicode support Server and User NameNote well: if you have P4CHARSET defined in your environment and not set to \"none\", a new personal server will automatically be initialized as a Unicode-enabled server. So what is the story with the server and user name? The name of your personal server and client workspace coincide. Although in principle you can have more than one workspace against your personal server, in practice there is rarely any need for it. Locally the name does not matter, but when you push your changes into another server, the changes are linked to your local workspace name. An automatically generated name like \"sknop-dvcs-1429629213\" is highly likely do be unique, but you are free to choose a different name if you so wish by using the \"-c\" option. The same is true for your user name: locally it does not matter and will typically coincide with either your OS user name and/or whatever P4USER is set to, but when pushing to another server the user name becomes important. Take the Perforce workshop for example: my local user name is always \"sknop\", but for the workshop I use \"sven_erik_knop\". If I create a local DVCS server under the user name \"sknop\", submit my changes, set up a remote to the workshop, and push, I'll receive only an error message. Fortunately, the solution is very simple. I add another user to my local server and update my local protection table: p4 user –f sven_erik_knop p4 protect Now I can push my changes under the new user name (I might have to log into the target server first): p4 –u sven_erik_knop push Conclusion A simple \"p4 init\" will create you a new personal server to which you can submit changes, but if you want to push these changes to another server, it makes sense to pay attention to case sensitivity, Unicode support, and workspace and user name. Let me know if you are using our new DVCS features and how you are getting on. My Twitter handle is @p4sven. For a live technical overview of DVCS features in the Helix Versioning engine sign up for our DevTalk Webinar on June 26th.","tags":"scm"},{"url":"http://www.ciandcd.com/finding-the-needle-in-a-haystack-with-helix-threat-detection.html","title":"Finding the ‘Needle in a Haystack' with Helix Threat Detection","text":"From: http://www.perforce.com/blog/150617/finding-%E2%80%98needle-haystack%E2%80%99-helix-threat-detection Software development projects in bigger companies typically involve large teams collaborating across multiple locations. A large corporation may employ tens of thousands of developers working on thousands of projects over a span of many years. For many companies, developer access to older software projects and files may continue long after the project has been completed, sometimes because of lax processes and stagnant access control policies. Yet, these projects can represent valuable IP worth tens of millions of dollars. In light of the ramifications of a competitor getting ahold of these files, what can companies do to better protect their crown jewels from theft? The answer might be found in the source code management (SCM) or version control tools companies use to drive their development workflows. SCM tools typically track access to key projects and files via audit logs. However, the sheer volume of these logs can overwhelm security teams. A month of log data might yield millions of different interactions with files and projects, making it virtually impossibe to find important clues. Done the right way, however, this approach can bring the real threats to the surface. A recent Fortune article entitled Using Log Data and Machine Learning to Weed out the Bad Guys shares how a large company applied our Helix Threat Detection capabilities to quickly identify data theft. Likening this approach to ‘finding a needle in a haystack,' the article describes how effective it can be to apply behavioral analytics to the audit logs in our Helix Versioning Engine. Leveraging Machine Learning to Establish a Baseline Conventional security tools (e.g., SIEMs) are often rule-based and require time-consuming manual setting of thresholds and iterative tuning of multiple parameters in order to identify anomalous behavior. Yet manually setting alerts to trigger when developers access an arbitrary number of files may be problematic for large projects and can inundate security teams with too many false positives. A better approach is to use machine-learning algorithms and risk-based-behavior-analytics models to audit logs to first establish a baseline understanding of normal behavior. It's possible to create cluster models that group similar users based on their past activities. Continuous self-learning more accurately identifies high-risk events, like someone accessing a project he or she doesn't normally work on, putting a spotlight on threats to an organization's most sensitive assets. Identifying High-Risk Behaviors Once you've establised what's normal behavior, the next step is to apply advanced mathematical models that generate a behavioral risk score. This score represents multiple factors, including the importance of an asset or file, the method of access, the activity (e.g., volume or type), and the user. These behavioral analytics models can then be used to find anomalies by: Comparing access patterns, data usage patterns and data movement patterns against historic behavior Determining similar user patterns across the environment and comparing behavioral patterns between users and groups of users Detecting dissimilar patterns among members of the same project group or job role Comparing individuals against the entire user group To learn more about the behavioral analytics models used in Helix Threat Detection, download the white paper Helix Threat Detection: IP Security and Risk Analytics. To learn more download our white paper: A Unified Approach to Securing and Protecting IP. READ NOW","tags":"scm"},{"url":"http://www.ciandcd.com/perforce-takes-to-the-road-in-2015.html","title":"Perforce Takes to the Road in 2015","text":"From: http://www.perforce.com/blog/150622/perforce-takes-road-2015 We're hitting the road again this year with a series of one-day events focused on better ways to build and secure complex products. These events will prove interesting to professionals at every phase of the product lifecyle and from companies of any size. Register today for in-depth discussions of the challenges companies face in getting complex products to market quickly without sacrificing quality or security. These events are also great opportunities to network with your peers and to hear about the latest innovations from Perforce. No matter if you're a Perforce user or not, you'll find it a valuable use of your time. Hear from the Experts Hear from thought leaders who are well practiced in modern development practices like Continuous Delivery and DevOps. You'll get practical advice that you can put to good use immediately. Keep Good Company Spend a day with like-minded individuals and innovative product developers. The majority of sessions will be by Perforce customers, who will share practical advice from their real-world experiences. Of course, our trainers and consultants will also be on hand to answer any questions you have about new products and capabilities from Perforce. Stay for the Party! These events are free of charge and full of informational sessions. They also promise to be very fun. No Perforce event is complete without an elegant setting, great food and lavish drinks to end the day in style. You won't be disappointed! So far, we've scheduled tour stops in the following cities (with more to come): Milan – June 25 Sydney – July 21 Berlin – Sept 16 London – Sept 22 Space is limited, so register today and secure your place!","tags":"scm"},{"url":"http://www.ciandcd.com/merging-without-a-base-in-perforce-helix.html","title":"Merging Without a Base in Perforce Helix","text":"From: http://www.perforce.com/blog/150623/merging-without-base-perforce-helix I recently had a question about merging files in Perforce Helix that have no direct lineage. For example, let's say you want to merge file //depot/main/foo.c (source) to //depot/dev/foo.c (target), which is fine, but you realize the target file was not branched from the source and there is no base (common ancestor). To determine the base, Helix uses integration history created by previous integration commands to know which file revisions to integrate. However, since dev/foo.c was not branched from main/foo.c there is no integration history between these two paths: a baseless merge. To handle this scenario Helix does a two-way merge for the best results. All the diffs are considered conflicts. Why? Because in the absence of a base, we do not have any way to determine what differences are \"changes\" relative to the base, which is how we normally determine whether diffs are conflicting or not. We found the best and most accurate way to handle this scenario is scheduling a two-way merge. In the past, Helix would not allow baseless merges without the use of p4 integ command flags; -i/-I uses the first revision of the source as the base for baseless merges, rather than an empty file. With the old \"-i\" behavior, an arbitrarily chosen base can lead to lost changes as seen in the figure below. This is why we changed the behavior. If you are stuck in the past, these flags are preserved for backward compatibility but are deprecated. To recap, with the current integration behavior, no flags are needed to handle baseless merges. The Helix integration engine does the right thing for a more realistic and accurate result! Happy merging!","tags":"scm"},{"url":"http://www.ciandcd.com/how-to-archive-a-review-in-helix-swarm.html","title":"How to Archive a Review in Helix Swarm","text":"From: http://www.perforce.com/blog/150624/how-archive-review-helix-swarm We get a lot of positive feedback about Helix Swarm, but some customers are not sure about all of the workflow steps, so here's a quick review. The code review in Helix Swarm can be in one of the following states: Needs Review - The review has started and the changes need to be reviewed. Needs Revisions - The changes have been reviewed and the reviewer has indicated that further revisions are required. Approved - The review has completed. The changes need to be committed. Rejected - The review has completed. The changes are undesirable and should not be committed. Archived - The review has completed for now. However, it is neither rejected nor approved; it is simply put aside in case it is needed in the future. Most of these states are self-explanatory with the exception of \"Archived\" state, which is where my fellow customers feel a bit lost. They wonder how they can archive and restore reviews for future consideration. Which can also mean that they inadvertently created a review for something they didn't intend to. That said, the review in question can be archived, restored, updated with new set of files and then routed through the workflow. Let's look at how to \"Archive\" the review: When you open a review in Swarm, there is a drop-down at the top right corner of the review. Select the \"Archive\" option if you think the review needs to be deffered for future consideration. Once you set the state of the review to \"Archive\", the review disappears from the list of open reviews. Now, at a later stage, the project team decides that the review should be restored for immediate consideration. Let's look at how to restore a previously archived review: The previously archived review can be found under, \"Closed\" review tab. To narrow down your search further, the \"Archived\" review icon can be clicked to list only archived reviews. Select the review you wish to schedule for consideration by clicking on the hyperlinked review ID. Again, go to the drop-down at the top right corner of the review. Simply click \"Needs Review\" and the review will reappear in the list of Opened reviews. Are you using Helix Swarm? We invite you to try it for free–it is included in Perforce Helix's free 20–user edition. Download now and tell us what you think!","tags":"scm"},{"url":"http://www.ciandcd.com/support-lgbtq-tech-organizations-with-the-pridetocat-shirt-github.html","title":"Support LGBTQ tech organizations with the Pridetocat Shirt · GitHub","text":"From: https://github.com/blog/2016-support-lgbtq-tech-organizations-with-the-pridetocat-shirt With the purchase of the Pridetocat Shirt you will be assisting Lesbians Who Tech , Maven , and Trans*H4CK to further their work. All proceeds from sales will be donated to these organizations that are helping educate, connect and empower LGBTQ people in tech. This limited edition shirt is available in the GitHub Shop until August 31st. More info about the LGBTQ tech organizations that benefit from the purchase of this shirt: Lesbians Who Tech Lesbians Who Tech is a global community of 9,000 queer women in tech. It exists to provide value to queer women in tech, a demographic that is rarely represented in both the tech community and the LGBTQ community. Trans*H4CK Trans*H4CK is a hackathon and speaker series that tackles social problems by developing new and useful open source tech products that benefit the trans and gender non-conforming communities, while bringing visibility to transgender tech innovators and entrepreneurs. Maven Maven partner with local LGBTQA youth serving organizations and LGBTQA tech professionals to provide free tech camps, workshops, Game Jams/hackathons for the queer youth community.","tags":"scm"},{"url":"http://www.ciandcd.com/focus-on-your-changes-in-github-for-windows-github.html","title":"Focus on your changes in GitHub for Windows · GitHub","text":"From: https://github.com/blog/2015-focus-on-your-changes-in-github-for-windows GitHub for Windows now makes it even easier to see everything local to your machine, whether it's uncommitted changes or commits you haven't synced yet. One of the things you'll notice when creating commits is the new, compact list of changed files in your working directory. GitHub for Windows shows the number of files that a commit changed and lets you drill down to see what changed in a given file. The updated branch selector now groups your recently used branches so that you can jump straight back in to what you were doing before that pesky hotfix distracted you. We've given branch creation a dedicated place in the toolbar. As a bonus, you can pick which branch to base the new one off. Finally, you can collapse the repository list to reclaim some screen space. If you have GitHub for Windows installed it will automatically update to the latest version. If you don't have it installed, download GitHub for Windows from windows.github.com .","tags":"scm"},{"url":"http://www.ciandcd.com/filter-pull-requests-by-status-github.html","title":"Filter Pull Requests by Status · GitHub","text":"From: https://github.com/blog/2014-filter-pull-requests-by-status When we shipped the new GitHub Issues , we made it easy to scope lists of Issues and Pull Requests with filters like author, date, mentions, and team mentions. With the new status: filter you can now filter the Pull Requests in your repositories by combined status . If you're taking advantage of the Status API , or using an integration that does, try out the new filters: status:success Only pull requests with all successful statuses status:failure Only pull requests that have statuses in the failure or error state status:pending Only pull requests with no statuses or at least one status in the pending state","tags":"scm"},{"url":"http://www.ciandcd.com/atom-at-codeconf-2015-github.html","title":"Atom at CodeConf 2015 · GitHub","text":"From: https://github.com/blog/2018-atom-at-codeconf-2015 CodeConf is coming June 25 & 26 to Nashville and will feature the best that the open source community has to offer. We're excited to share that there will be several talks about the Atom ecosystem presented for your enjoyment and edification, kicked off by GitHub CEO Chris Wanstrath . Speakers will include: @bolinfest talking about Nuclide @lee-dohm talking about the Atom community @paulcbetts talking about Slack and Electron We will also be hosting an Atom workshop led by Nathan Sobo , and a lounge where you will be able to meet with the core team and hack on Atom together. Grab your CodeConf and workshop tickets now and we'll see you there in Nashville!","tags":"scm"},{"url":"http://www.ciandcd.com/announcing-github-japan-github.html","title":"Announcing GitHub Japan · GitHub","text":"From: https://github.com/blog/2017-announcing-github-japan GitHub <3s Japan, and today we're excited to announce the formation of GitHub Japan G.K., a subsidiary of GitHub, Inc. Our new office in Tokyo is our first official office outside of the United States. The Japanese developer community GitHub couldn't exist without the Japanese open source community — after all, our site is built on Rails , which is built on Ruby , an open source project started in Japan . Japan has historically been one of the most active countries on GitHub, ranking in the top 10 countries visiting github.com since GitHub was founded in 2008. The thriving software community in Japan keeps growing; in 2014, activity on github.com from Japan increased more than 60 percent from the previous year. GitHub Enterprise in Japan In addition to an active local open source community, Japanese businesses including Hitachi Systems , CyberAgent and GREE are collaborating and building the best software with GitHub Enterprise. To that end, we're also announcing that we'll be partnering locally to provide Japanese language technical support for GitHub Enterprise users, as well as the ability to pay in Japanese Yen in Japan. Stay up to date Keep tabs on everything happening in our Tokyo office by following @GitHubJapan on Twitter and checking out github.co.jp . We'd also love to see you at our meetup in Osaka on June 6 . Yoroshiku-Onegaiitashimasu! 初めましてGitHub Japanです！ GitHub <3s Japan, 本日、私達はGitHub, Inc.の子会社である、GitHub Japan合同会社の設立の発表ができる事をとても光栄に思っております 。東京にオープンした新しいオフィスは、米国外でオープンする初のオフィスになります。 〜日本のデベロッパー・コミュニティにむけて〜 GitHubは、日本で生まれたオープンソース・プロジェクトのRubyで作られたRailsというフレームワークによって開発されており、日本のオープンソース・コミュニティーなしではGitHubは存在しえないと言っては過言ではない程、日本とGitHubは深いつながりがあります。 また、2008年のGitHub設立当初から、日本からgithub.comへのアクセス数は上位10ヶ国に入り続けてきました。そして、日本のユーザーは現在も増加し続けており、2014年の日本ユーザーのGitHub上でのアクティビティは、前年比60％も増加しました。 ～「GitHub Enterprise」の日本展開～ GitHubは広く開かれた開発を支援するオープンソース・プラットフォーム以外にも、全世界で企業向けに「GitHub Enterprise」を提供して参りました。これまで「GitHub Enterprise」は、英語でのサポートのみだったにもかかわらず、日本国内では、 株式会社日立システムズ 、 ヤフー株式会社 、 株式会社サイバーエージェント や グリー株式会社 などの大手企業をはじめとして、多くの先進的な企業にご活用頂いて参りました。そして今回、さらに迅速できめ細かいサービスやサポートを提供するため、GitHubは大手代理店と業務提携を行い、日本語による「GitHub Enterprise」の法人向け導入サポートも開始しました。この販売パートナー提携により、円建て決済や日本語のテクニカルサポートも可能になります。 GitHub の最新の情報を得よう 東京オフィスで何が起こっているか知る為にはTwitterで @GitHubJapan をフォローするか、 github.co.jp にアクセスしてくださいね。そして 大阪で開催されるuser meetup にも是非お越しください！ お待ちしております！. よろしくお願い致します！","tags":"scm"},{"url":"http://www.ciandcd.com/how-to-undo-almost-anything-with-git-github.html","title":"How to undo (almost) anything with Git · GitHub","text":"From: https://github.com/blog/2019-how-to-undo-almost-anything-with-git One of the most useful features of any version control system is the ability to \"undo\" your mistakes. In Git, \"undo\" can mean many slightly different things. When you make a new commit, Git stores a snapshot of your repository at that specific moment in time; later, you can use Git to go back to an earlier version of your project. In this post, I'm going to take a look at some common scenarios where you might want to \"undo\" a change you've made and the best way to do it using Git. Undo a \"public\" change Scenario: You just ran git push , sending your changes to GitHub, now you realize there's a problem with one of those commits. You'd like to undo that commit. Undo with: git revert <SHA> What's happening: git revert will create a new commit that's the opposite (or inverse) of the given SHA. If the old commit is \"matter\", the new commit is \"anti-matter\"—anything removed in the old commit will be added in the new commit and anything added in the old commit will be removed in the new commit. This is Git's safest, most basic \"undo\" scenario, because it doesn't alter history—so you can now git push the new \"inverse\" commit to undo your mistaken commit. Fix the last commit message Scenario: You just typo'd the last commit message, you did git commit -m \"Fxies bug #42\" but before git push you realized that really should say \"Fixes bug #42\". Undo with: git commit --amend or git commit --amend -m \"Fixes bug #42\" What's happening: git commit --amend will update and replace the most recent commit with a new commit that combines any staged changes with the contents of the previous commit. With nothing currently staged, this just rewrites the previous commit message. Undo \"local\" changes Scenario: The cat walked across the keyboard and somehow saved the changes, then crashed the editor. You haven't committed those changes, though. You want to undo everything in that file—just go back to the way it looked in the last commit. Undo with: git checkout -- <bad filename> What's happening: git checkout alters files in the working directory to a state previously known to Git. You could provide a branch name or specific SHA you want to go back to or, by default, Git will assume you want to checkout HEAD , the last commit on the currently-checked-out branch. Keep in mind: any changes you \"undo\" this way are really gone. They were never committed, so Git can't help us recover them later. Be sure you know what you're throwing away here! (Maybe use git diff to confirm.) Reset \"local\" changes Scenario: You've made some commits locally (not yet pushed), but everything is terrible, you want to undo the last three commits—like they never happened. Undo with: git reset <last good SHA> or git reset --hard <last good SHA> What's happening: git reset rewinds your repository's history all the way back to the specified SHA. It's as if those commits never happened. By default, git reset preserves the working directory. The commits are gone, but the contents are still on disk. This is the safest option, but often, you'll want to \"undo\" the commits and the changes in one move—that's what --hard does. Redo after undo \"local\" Scenario: You made some commits, did a git reset --hard to \"undo\" those changes (see above), and then realized: you want those changes back! Undo with: git reflog and git reset or git checkout What's happening: git reflog is an amazing resource for recovering project history. You can recover almost anything—anything you've committed—via the reflog. You're probably familiar with the git log command, which shows a list of commits. git reflog is similar, but instead shows a list of times when HEAD changed. Some caveats: HEAD changes only. HEAD changes when you switch branches, make commits with git commit and un-make commits with git reset , but HEAD does not change when you git checkout -- <bad filename> (from an earlier scenario—as mentioned before, those changes were never committed, so the reflog can't help us recover those. git reflog doesn't last forever. Git will periodically clean up objects which are \"unreachable.\" Don't expect to find months-old commits lying around in the reflog forever. Your reflog is yours and yours alone. You can't use git reflog to restore another developer's un-pushed commits. So... how do you use the reflog to \"redo\" a previously \"undone\" commit or commits? It depends on what exactly you want to accomplish: If you want to restore the project's history as it was at that moment in time use git reset --hard <SHA> If you want to recreate one or more files in your working directory as they were at that moment in time, without altering history use git checkout <SHA> -- <filename> If you want to replay exactly one of those commits into your repository use git cherry-pick <SHA> Once more, with branching Scenario: You made some commits, then realized you were checked out on master . You wish you could make those commits on a feature branch instead. Undo with: git branch feature , git reset --hard origin/master , and git checkout feature What's happening: You may be used to creating new branches with git checkout -b <name> —it's a popular short-cut for creating a new branch and checking it out right away—but you don't want to switch branches just yet. Here, git branch feature creates a new branch called feature pointing at your most recent commit, but leaves you checked out to master . Next, git reset --hard rewinds master back to origin/master , before any of your new commits. Don't worry, though, they are still available on feature . Finally, git checkout switches to the new feature branch, with all of your recent work intact. Branch in time saves nine Scenario: You started a new branch feature based on master , but master was pretty far behind origin/master . Now that master branch is in sync with origin/master , you wish commits on feature were starting now, instead of being so far behind. Undo with: git checkout feature and git rebase master What's happening: You could have done this with git reset (no --hard , intentionally preserving changes on disk) then git checkout -b <new branch name> and then re-commit the changes, but that way, you'd lose the commit history. There's a better way. git rebase master does a couple of things: First it locates the common ancestor between your currently-checked-out branch and master . Then it resets the currently-checked-out branch to that ancestor, holding all later commits in a temporary holding area. Then it advances the currently-checked-out-branch to the end of master and replays the commits from the holding area after master 's last commit. Mass undo/redo Scenario: You started this feature in one direction, but mid-way through, you realized another solution was better. You've got a dozen or so commits, but you only want some of them. You'd like the others to just disappear. Undo with: git rebase -i <earlier SHA> What's happening: -i puts rebase in \"interactive mode\". It starts off like the rebase discussed above, but before replaying any commits, it pauses and allows you to gently modify each commit as it's replayed. rebase -i will open in your default text editor, with a list of commits being applied, like this: The first two columns are key: the first is the selected command for the commit identified by the SHA in the second column. By default, rebase -i assumes each commit is being applied, via the pick command. To drop a commit, just delete that line in your editor. If you no longer want the bad commits in your project, you can delete lines 1 and 3-4 above. If you want to preserve the contents of the commit but edit the commit message, you use the reword command. Just replace the word pick in the first column with the word reword (or just r ). It can be tempting to rewrite the commit message right now, but that won't work— rebase -i ignores everything after the SHA column. The text after that is really just to help us remember what 0835fe2 is all about. When you've finished with rebase -i , you'll be prompted for any new commit messages you need to write. If you want to combine two commits together, you can use the squash or fixup commands, like this: squash and fixup combine \"up\"—the commit with the \"combine\" command will be merged into the commit immediately before it. In this scenario, 0835fe2 and 6943e85 will be combined into one commit, then 38f5e4e and af67f82 will be combined together into another. When you select squash , Git will prompt us to give the new, combined commit a new commit message; fixup will give the new commit the message from the first commit in the list. Here, you know that af67f82 is an \"ooops\" commit, so you'll just use the commit message from 38f5e4e as is, but you'll write a new message for the new commit you get from combining 0835fe2 and 6943e85 . When you save and exit your editor, Git will apply your commits in order from top to bottom. You can alter the order commits apply by changing the order of commits before saving. If you'd wanted, you could have combined af67f82 with 0835fe2 by arranging things like this: Fix an earlier commit Scenario: You failed to include a file in an earlier commit, it'd be great if that earlier commit could somehow include the stuff you left out. You haven't pushed, yet, but it wasn't the most recent commit, so you can't use commit --amend . Undo with: git commit --squash <SHA of the earlier commit> and git rebase --autosquash -i <even earlier SHA> What's happening: git commit --squash will create a new commit with a commit message like squash! Earlier commit . (You could manually create a commit with a message like that, but commit --squash saves you some typing.) You can also use git commit --fixup if you don't want to be prompted to write a new commit message for the combined commit. In this scenario, you'd probably use commit --fixup , since you just want to use the earlier commit's commit message during rebase . rebase --autosquash -i will launch an interactive rebase editor, but the editor will open with any squash! and fixup! commits already paired to the commit target in the list of commits, like so: When using --squash and --fixup , you might not remember the SHA of the commit you want to fix—only that it was one or five commits ago. You might find using Git's &#94; and ~ operators especially handy. HEAD&#94; is one commit before HEAD . HEAD~4 is four commits before HEAD - or, altogether, five commits back. Stop tracking a tracked file Scenario: You accidentally added application.log to the repository, now every time you run the application, Git reports there are unstaged changes in application.log . You put *.log in the .gitignore file, but it's still there—how do you tell git to to \"undo\" tracking changes in this file? Undo with: git rm --cached application.log What's happening: While .gitignore prevents Git from tracking changes to files or even noticing the existence of files it's never tracked before, once a file has been added and committed, Git will continue noticing changes in that file. Similarly, if you've used git add -f to \"force\", or override, .gitignore , Git will keep tracking changes. You won't have to use -f to add it in the future. If you want to remove that should-be-ignored file from Git's tracking, git rm --cached will remove it from tracking but leave the file untouched on disk. Since it's now being ignored, you won't see that file in git status or accidentally commit changes from that file again. That's how to undo anything with Git. To learn more about any of the Git commands used here, check out the relevant documentation:","tags":"scm"},{"url":"http://www.ciandcd.com/improved-organization-permissions-github.html","title":"Improved organization permissions · GitHub","text":"From: https://github.com/blog/2020-improved-organization-permissions Organizations have always been the best way for teams to work together and collaborate on code. We're happy to announce major improvements to GitHub organization permissions . These improvements include new customizable member privileges, fine-grained team permissions, and more open communication. The improved permissions system gives your organization the flexibility to work the way you want. Here are just a few highlights: (Opt-in) Members can view and mention all teams, even when they're not on those teams. (Opt-in) Members can create repositories without help from an owner. Members can create new teams to self-organize with the people they work with. Owners can give just the right amount of access to contractors and interns by adding them to repositories without giving them the privileges of organization members. And many more! Learn about GitHub's improved organization permissions . All of these new features give your organization the ability to work together seamlessly without everyone needing to be an owner. Once these features launch, organization owners will be able to turn on new permissions as needed. Simply opt-in when you're ready. Early access We're rolling out our improved permissions system to a select group of users who will be asked to provide feedback over a short survey as part of the program. If you're interested in being one of the first to try it out on GitHub.com, sign your organization up for early access . In the next few months, every organization on GitHub.com will have the improved permissions system.","tags":"scm"},{"url":"http://www.ciandcd.com/an-updated-header-just-for-you-github.html","title":"An updated header, just for you · GitHub","text":"From: https://github.com/blog/2022-an-updated-header-just-for-you Navigating what's most important to you on GitHub.com just got a little easier with our updated site header. The new header gives you faster access to your pull requests and issues dashboards from anywhere on the site. If you're unfamiliar with them, these dashboards list all of your open pull requests and issues—as well as those you've been mentioned in or are assigned to—in one place. Use them to stay up to date on what needs to be done across your projects. Lastly, clicking your avatar now opens a new dropdown menu with links to your profile, account settings, and more. As a small bonus, we've also included a new Your stars link for easy access to your starred repositories. Enjoy!","tags":"scm"},{"url":"http://www.ciandcd.com/read-only-deploy-keys-github.html","title":"Read-only deploy keys · GitHub","text":"From: https://github.com/blog/2024-read-only-deploy-keys You can now create deploy keys with read-only access. A deploy key is an SSH key that is stored on your server and grants access to a single GitHub repository. They are often used to clone repositories during deploys or continuous integration runs. Deploys sometimes involve merging branches and pushing code, so deploy keys have always allowed both read and write access. Because write access is undesirable in many cases, you now have the ability to create deploy keys with read-only access. New deploy keys created through GitHub.com will be read-only by default and can be given write access by selecting \"Allow write access\" during creation. Access level can be specified when creating deploy keys from the API as well.","tags":"scm"},{"url":"http://www.ciandcd.com/a-closer-look-at-europe-github.html","title":"A closer look at Europe · GitHub","text":"From: https://github.com/blog/2023-a-closer-look-at-europe Last week we opened our first international office in Japan . This week we thought we'd take a closer look at Europe, which happens to be the largest demographic of GitHub users around the world, representing 36% of site traffic. Around 32 million people visit GitHub each month, and most of this traffic comes from outside of the United States (74% in fact!). The most active countries in Europe are Germany, the United Kingdom, and France, but if we look at users per capita we see a different story -- Sweden, Finland, and the Netherlands lead the way. London, Paris and Stockholm top the list of European cities most active on GitHub. The goals of building better software are universal, and several European organizations are setting the example. Companies like SAP and XS4ALL are driving innovation with software, while The UK Government Digital Services and dozens of other European government agencies and services are developing new ways to serve citizens. Today, around 10% of GitHub employees are based in Europe, with a dozen new faces in the last year alone -- many of whom are focused solely on helping our European customers build great software. A few of us are here in the UK for London Tech Week and EnterConf in Belfast. There will be plenty more meetups ahead if we don't see you there.","tags":"scm"},{"url":"http://www.ciandcd.com/meet-greet-and-workshop-tickets-github.html","title":"Meet & Greet and Workshop Tickets · GitHub","text":"From: https://github.com/blog/2027-codeconf-updates-meet-greet-and-workshop-tickets CodeConf is next week, and I couldn't be more excited to bring the open source community together to exchange ideas and have some fun in Nashville. There are a few updates I'd like to share: On June 24, the day before the conference, we'll be hosting a meet & greet for attendees who would like to register early. This event is free and open to the public, so if you aren't attending CodeConf but live in the Nashville area and would like to stop by, grab a ticket here . We'll be congregating on the second floor of Acme Feed & Seed downtown beginning at 5:30pm The workshop schedule has been updated, and I have opened up more space in each session for those interested. If you'd like to snag one of the newly available tickets, go for it! There's still time to grab a CodeConf ticket. Take a look at the website for the full schedule of sessions, workshops, and sponsors. I hope to see you in Nashville.","tags":"scm"},{"url":"http://www.ciandcd.com/student-hackathon-organizers-join-us-for-hackcon-at-github-hq-github.html","title":"Student hackathon organizers, join us for Hackcon at GitHub HQ · GitHub","text":"From: https://github.com/blog/2026-student-hackathon-organizers-join-us-for-hackcon-at-github-hq We're hosting Hackcon III at our San Francisco office on July 18th and 19th. Hackcon is the place to be for student hackathon organizers. The event is run by our friends at Major League Hacking and will bring together 150 student leaders for two days of talks and workshops. Participants will share experience and best practices in everything from starting a campus group to producing large scale campus events. If you lead a student hacker community at your university, we'd love to see you at Hackcon. You can find more information about the event and pre-register at hackcon.io . You can also check out the videos from Hackcon I and Hackcon II on YouTube.","tags":"scm"},{"url":"http://www.ciandcd.com/octicon-buttons-are-here-github.html","title":"Octicon Buttons Are Here! · GitHub","text":"From: https://github.com/blog/2030-octicon-buttons-are-here Graphs, and pencils, and locks...Oh my! Now you can collect themed Octicon buttons with the four new button packs offered in the GitHub Shop .","tags":"scm"},{"url":"http://www.ciandcd.com/announcing-atom-10-github.html","title":"Announcing Atom 1.0 · GitHub","text":"From: https://github.com/blog/2031-announcing-atom-1-0 GitHub is pleased to announce that version 1.0 of the Atom text editor is now available from atom.io . Read the full behind the scenes story over on the Atom blog . The entire Atom team is attending CodeConf this week and will be presenting a session all about Atom 1.0 featuring Chris Wanstrath , Ben Ogle , and Daniel Hengeveld . Watch along tomorrow, June 26th, at 11AM EDT: https://live-stream.github.com","tags":"scm"},{"url":"http://www.ciandcd.com/mobile-file-finder-github.html","title":"Mobile File Finder · GitHub","text":"From: https://github.com/blog/2032-mobile-file-finder The GitHub File Finder is now available on your mobile device. Just click the \"Jump to file\" link on any repository.","tags":"scm"},{"url":"http://www.ciandcd.com/cruisecontrolzai-linuxshang-de-pei-zhi-shi-li.html","title":"[CruiseControl]在Linux上的配置实例","text":"From: http://www.cnblogs.com/itech/archive/2010/07/18/1780363.html 转自： http://www.blogjava.net/lihao336/archive/2010/06/29/324822.html CruiseControl配置说明 CruiseControl是一个持续集成工具，持续集成是一个很好的敏捷实践。在项目中搭建CruiseControl环境，通过执行频繁的项目构建，及时暴露出隐藏的Bug，从而减少定位以及修改Bug的时间和代价。本文介绍了配置CruiseControl的步骤。配置环境为Ubuntu 9.10，CruiseControl 2.8.3 Binary，版本管理工具为SVN。 一．准备工作 1.配置Java环境 calvin@calvin-desktop:~$ sudo gedit /etc/profile 在文件末尾添加Java环境变量： #set java environment export JAVA_HOME=/home/calvin/development/jdk export JRE_HOME=/home/calvin/development/jdk/jre export CLASSPATH=$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH 登出再登入生效。 2.解压CruiseControl 解压到CruiseControl-2.8.3-bin.zip 到/home/calvin/development/cruisecontrol/cruisecontrol-bin-2.8.3中，其中三个值得注意的目录是： 目录 内容 projects 存放CruiseControl从源代码仓库中检出的代码。 logs 存放构建报告和日志。 artifacts 存放构建成功后产生的输出文件（如jar包，apk包等）。 3.从SVN仓库中checkout出源码 CruiseControl第一次不支持自动检出代码，所以在构建项目之前要先把项目代码手工checkout到本地。本文假设项目名为sampleproject，SVN源代码仓库位于/home/calvin/development/cruisecontrol/DemoSystem/repositoryserver/svnrepository中。 安装subversion客户端 calvin@calvin-desktop:~$ sudo apt-get install subversion 在project目录下新建sampleproject文件夹，用来存放准备build的工程。 calvin@calvin-desktop:~/development/cruisecontrol/cruisecontrol-bin-2.8.3/projects$ mkdir sampleproject calvin@calvin-desktop:~/development/cruisecontrol/cruisecontrol-bin-2.8.3/projects$ svn checkout file:////home/calvin/development/cruisecontrol/DemoSystem/repositoryserver/svnrepository sampleproject 二．配置config.xml cruisecontrol根据config.xml中配置的项目信息执行构建。 < cruisecontrol > <!-- 这里的name应该和projects目录下的项目名一致 --> < project name =\"sampleproject\" > <!-- 监听项目状态的变化 --> < listeners > < currentbuildstatuslistener file =\"logs/${project.name}/status.txt\" /> </ listeners > <!-- build之前从svn服务器上更新本地代码 --> < bootstrappers > < svnbootstrapper localWorkingCopy =\"projects/${project.name}/trunk\" /> </ bootstrappers > < modificationset quietperiod =\"30\" > < svn localWorkingCopy =\"projects/${project.name}/trunk\" /> </ modificationset > <!-- 设定每隔interval秒，CruiseControl去检查并执行一次构建 --> < schedule interval =\"60\" > < ant anthome =\"apache-ant-1.7.0\" buildfile =\"projects/${project.name}/trunk/build.xml\" target =\"all\" /> </ schedule > <!-- 发布构建结果 --> < publishers > < onsuccess > <!-- 将构建的结果（如jar包，apk包等）发布到指定的位置 --> < artifactspublisher dir =\"projects/${project.name}/trunk/dist\" dest =\"../logs/SampleCCProject\" /> </ onsuccess > <!-- htmlemail buildresultsurl=\"http://211.94.155.199:8080/cruisecontrol/buildresults/mobiletv mailhost=\"localhost\" returnname=\"CruiseControl result\" returnaddress=\"localhost@localhost\" --> <!-- failure address=\"caojie@cienet.com.cn\" /> <success address=\"${mail.address.mobiletv\" /> </htmlemail> <htmlemail> <success address=\"${mail.address.mobiletv}\"/> <failure address=\"caojie@cienet.com.cn\"/> </htmlemail> --> < email mailhost =\"smtp.263xmail.com\" returnaddress =\"calvinlee@cienet.com.cn\" skipusers =\"true\" reportsuccess =\"fixes\" subjectprefix =\"[CruiseControl]\" buildresultsurl =\"http://buildserver:8080/cruisecontrol/buildresults\" > < failure address =\"calvinlee@cienet.com.cn\" /> < success address =\"calvinlee@cienet.com.cn\" /> </ email > </ publishers > </ project > </ cruisecontrol > 三.配置cruisecontrol.sh cruisecontrol.sh是cruisecontrol的启动脚本，通过这个脚本中初始化一些环境变量，开始执行build loop，并开启jetty服务器。 打开cruisecontrol.sh，在文件末尾找到 $JAVA_HOME/bin/java $CC_OPTS -Djavax.management.builder.initial=mx4j.server.MX4JMBeanServerBuilder \"-Dcc.library.dir=$LIBDIR\" \"-Djetty.logs=$JETTY_LOGS\" -jar \"$LAUNCHER\" $@ -jmxport 8000 -webport 8080 -rmiport 1099 & 去掉 & 符号，这样启动cruisecontrol时，阻止将其放到后台执行，要关闭cruisecontrol时，按下 Ctrl+C 即可。 修改-jmxport、-webport、-rmiport等参数可以自定义项目构建结果的发布端口。 PS: 如果项目的log文件比较大，或者构建时使用内存较多，可以在cruisecontrol.sh中增加JAVA命令行参数，扩大JVM的内存，以免出现OutOfMemory错误: CC_OPTS=\"-Xms128m -Xmx256m\" 四.配置dashboard-config.xml dashboard用来显示cruisecontrol管理的所有项目的构建状态信息。 dashboard-config.xml用来配置dashboard的显示，其中，配置<features allowforcebuild=\"true\"/>，那么在每个项目的右侧有一个按钮，单击该按钮将强迫该项目进行构建，而不必等到其下一次检查，也不必等到它有版本变化。 < dashboard > < buildloop logsdir =\"\" artifactsdir =\"\" /> < features allowforcebuild =\"true\" /> < trackingtool projectname =\"\" baseurl =\"\" keywords =\"\" /> < subtabs > < subtab class =\"net.sourceforge.cruisecontrol.dashboard.widgets.ErrorsAndWarningsMessagesWidget\" /> </ subtabs > </ dashboard > 五.运行cruisecontrol 1.给cruisecontrol.sh 添加执行权限 calvin@calvin-desktop:~/development/cruisecontrol/cruisecontrol-bin-2.8.3$ sudo chmod +x cruisecontrol.sh 2.运行 cruisecontrol.sh 脚本 calvin@calvin-desktop:~/development/cruisecontrol/cruisecontrol-bin-2.8.3$ ./cruisecontrol.sh 六.查看项目构建结果 1.通过http://localhost:8080/dashboard/ 访问项目构建结果。 其中，项目有三种状态： Discontinued：表示CruiseControl可以找到该项目的日志文件，但在config.xml中并没有配置该项目，这种情况下CruiseControl不会去构建它。 Inactive：表示在config.xml中配置了该项目，但是没有发现关于这个项目的Log信息，即在CruiseControl的日志目录中还没有该项目的日志文件，或日志文件被人为删除了。CruiseControl会根据配置信息对这个项目进行检查新版本并进行构建。 Active：表示CruiseControl即可以找到该项目的日志文件，又在config.xml中可以发现它。此时，这个项目可能是构建成功，也可能是构建失败，还可能是构建中。 2.通过http://localhost:8080/cruisecontrol/ 查看项目构建计划","tags":"中文"},{"url":"http://www.ciandcd.com/jenkinszhong-zhi-xing-batchhe-python.html","title":"Jenkins中执行batch和Python","text":"From: http://www.cnblogs.com/itech/archive/2011/11/14/2248507.html#FeedBack Jenkins的job->build 支持Ant，maven，windows batch和Shell， 但是我们知道python，perl，ruby等脚本其实也是shell脚本，所以这里的Shell可以扩展为python，perl，ruby等。 例如： 下面执行windows batch 和python 执行后的输入如下： 可以看到windows batch和shell脚本被保存到slave上的临时目录下，然后再执行。 完！","tags":"中文"},{"url":"http://www.ciandcd.com/perforceyu-fen-bu-shi-tuan-dui-de-kai-fa.html","title":"perforce与分布式团队的开发","text":"From: http://www.cnblogs.com/itech/archive/2011/08/17/2143348.html perforce与分布式团队的开发 一 perforce proxy和 perforce replicated server perforce通过perforce proxy和perforce replicated servers来很好地支持分布式团队的开发。proxy和replicated servers可以根据情况单独地使用或混合使用。可以通过简单地增加新的proxy或replicated server来适应全球的新的用户的增加。 perforce proxy perforce proxy的执行文件为p4p，是一个自维护的proxy server，他缓存远程的p4server的versioned files到本地网络，从而大大地减少了本地用户传输文件的时间。如果一个文件已经被其他的用户请求到本地的proxy，本地的其他用户对此文件的访问都将直接操作本地的proxy，所以用户操作体验将会很流畅。同时p4p也减轻了main server的负荷。因为p4p自动地从main server获取跟新，所以我们也不需要对proxy进行备份。 perforce replication server replicated perforce server维护了整个repository的一个只读的拷贝，可以只replicate p4 的datebase或者replicate所有的p4 database文件和versioned files。对main server的只读的操作可以被转到replicated server， 此方法也减轻了main server的负荷。同时replication还有高可用的作用，当mainserver停机时，开发人员还可以访问replication server来执行读操作。 或者可以快速地将replicated server转化为main server。 broker作为traffic director，将只读的命令发送到本地的replicated server，将需要写权限的命令转发到main server或 proxy。使用broker和replicated server后用户的只读操作将会很流畅。 perforce broker proxy与replication的区别： proxy只是自动地将versioned files缓存到了proxy所在的server，来减少远程用户文件的传输时间。p4server的database还是在main server的。所以proxy的主要作用是减少文件传输的时间，提高用户体验。 replication是将所有的database或者versioned files+database文件都拷贝到replication所在的server，但是replication server只接受只读的操作。所以replication的主要作用是减少main server的负荷。 二 perforce proxy proxy的在windows上可以通过perforce.exe来安装proxy为service，或者在命令行运行p4p来启动proxy。 当作为service运行时，需要设置service的参数，例如命令p4 set -S \"Perforce Proxy\" P4POPTIONS=\"-p 1999 -t mainserver:1666\"为perforce proxy service设置的启动参数为\"-p 1999 -t mainserver:1666\"，\"-p 1999\"表示此proxy的监听端口为1999，\"-t mainserver:1666\"表示此proxy将是mianserver:1666的proxy。用户可以使用\"ipofproxyserver:1999\"来访问此proxy。 proxy也可以直接从命令行启动，例如p4p -p 1999 -t central:1666 -r /var/proxyroot启动的proxy监听端口1999，且mian server是central:1666,proxy的缓存目录为/var/proxyroot。 注意： *不需要对proxy的缓存目录备份； *在sync文件时可以使用-Zproxyverbose来查看文件是来自mainserver还是proxyserver，例如p4 -Zproxyverbose sync noncached.txt； *使用-c参数表示在proxy和mianserver间文件传输压缩，压缩后减少了网络的负担，但是需要在增加了server的压缩和解压负担； *p4p只有当用户提交文件修改或请求文件时，才将文件缓存到proxy的存储中，也就是说proxy不会提前自动地缓存所有的文件。但是一但文件被缓存过则直接从proxy的缓存获取； *proxy的建立者可以使用-Zproxyload来将versionedfiles跟新到proxy的缓存，来减少以后的文件传输时间，例如p4 -Zproxyload sync //depot/main/...； *可以使用文件夹连接来避免缓存目录的空间大小的限制； 三 replicated server 作用： *可以减少main server的负担和停机时间。 长时间的查询操作，报表，构建，和checkpoint都可以在replicated server上来完成。在replicated server上checkpoint可以减少对main server的lock时间。 且报表和checkpoint，只需要p4 server的database被replicated。对于构建任务，需要构建进程访问main server的versioned files。 *提供热备用的server。如果main server失败了，可以使用p4 replicate命令来维护replicated server作为热备用的server。此时replicated server需要database和versioned files均被replicated。 p4 replicated命令只能replicated database文件。对于versioned files的replicate需要使用p4 pull命令来完成。 可以使用p4 replicate 或 p4 pull 和 p4d来建立replicated server。 1）p4 replicate ： 表示replicate p4的database从一个server到另一个server； 2）p4 pull : 表示replicate p4的database和versioned files从一个server到另一个server； 3）大部分情况下我们使用p4 pull，因为不仅replicate p4的database，而且还自动replicate p4的versioned files； 4）对于replicated server的启动参数需要为p4d -M -D， -M表示对replicated server的database文件为只读的，-D表示对replicated server的versionedfiles为只读的。总之replicated server只处理只读的操作； 5）也可以使用p4 configure db.replication=readonly 和 lbr.replication=readonly来设置replicated server的只读行为； 6）使用p4d -In name 或 p4 configure name来给p4 server命名； 7）可以指定replicated server与mainserver交互的用户，如p4d -u svcuser，此类型的用户需要为service类型，只能用做replicated或proxy server，不占用license； 8）设置环境变量P4TARGET，或者在p4d -t host:port，或者在p4d运行后使用p4 configure P4TARGET 来指定replicated server所指向的main server； 9）startup.n 用来指定p4 pull的进程数量，可以多个p4 pull同时运行； replicated server的一些限制： *p4 replicate和p4 pull命令所读取的main server的journals都必须是非压缩的。 *replicate server必须有license文件，可以联系p4公司创建重复的license文件。 *replicate server与main server必须有相同的time zone设置。 五 broker p4broker的启动很简单，例如p4broker -c config_file -d。也可以使用p4 set -s P4BROKEROPTIONS \"-c c:\\p4broker\\broker.conf\"来设置p4broker的命令行参数。使用p4broker -C > p4broker.conf来创建conf的模板。 一般conf包含3部分： * general信息，例如target表示默认的命令处理的p4server；listen表示broker要监听的端口；admin的信息等。 * alternate server definitions：表示replicated servers地址. command handler specifications： 表示命令的分发规则。 完！","tags":"中文"},{"url":"http://www.ciandcd.com/perforceji-ben-cao-zuo.html","title":"perforce基本操作","text":"From: http://www.cnblogs.com/itech/archive/2011/08/09/2131743.html 以下的p4的基本操作是基于P4V，p4V是跨平台的p4 visual client。 1） 登录和环境变量。server，等于环境变量P4PORT的值，表示p4 server的地址； user，等于环境变量P4USER的值，为p4的用户名，唯一标识一个用户；workspace，等于P4CLINT的值，表示server与本地的目录的对应。 1） 登录和环境变量。server，等于环境变量P4PORT的值，表示p4 server的地址； user，等于环境变量P4USER的值，为p4的用户名，唯一标识一个用户；workspace，等于P4CLINT的值，表示server与本地的目录的对应。 2） workspace, 也称为client spec，表示了server与本地的源码的目录对应关系。 3） 增加新的文件，操作为workspace->在文件或文件夹上右键 -> mark for add 4）删除文件，操作为depot-> 在文件或文件夹上右键 -> mark for delete 5) 修改某些文件，操作为depot-> 在文件或文件夹上右键 -> checkout 6）changelist, 每次submit提交都会对应唯一的一个changelist。 7） 一次提交，操作为pending -> default ( pending change list) -> submit submit 如下图： 8） 比较文件或文件夹的不同，操作为在文件或文件夹上右键->diffagainst 9）文件的修改历史，操作为depot-> 选择文件或文件件-> file history（folder history） 10) 同步代码，操作为depot -> 在文件或文件夹右键 -> get latest revision(get revision) 完！","tags":"中文"},{"url":"http://www.ciandcd.com/antshi-wu-da-zui-jia-shi-jian-zhuan.html","title":"ANT十五大最佳实践[转]","text":"From: http://www.cnblogs.com/itech/archive/2010/10/28/1863699.html#FeedBack ANT 十五大最佳实践 转自： http://oreilly.com.cn/news/ant15toppractices.php?c=java 作者： Eric M. Burke , coauthor of Java Extreme Programming Cookbook 原文： http://www.onjava.com/pub/a/onjava/2003/12/17/ant_bestpractices.html 译者：徐彤 http://www.cnblogs.com/itech/admin/msn:xt121@hotmail.com 在Ant出现之前，构建和部署Java应用需要使用包括特定平台的脚本、Make文件、各种版本的IDE甚至手工操作的\"大杂烩\"。现在，几乎所有的开源Java项目都在使用Ant，大多数公司的内部项目也在使用Ant。Ant在这些项目中的广泛使用自然导致了读者对一整套Ant最佳实践的迫切需求。 本文总结了我喜爱的Ant技巧或最佳实践，多数是从我亲身经历的项目错误或我听说的其他人经历的 \"恐怖\"故事中得到灵感的。比如，有人告诉我有个项目把XDoclet 生成的代码放入带有锁定文件功能的版本控制工具中。当开发者修改源代码时，他必须记住手工检出（Check out）并锁定所有将要重新生成的文件。然后，手工运行代码生成器，只到这时他才能够让Ant编译代码，这一方法还存在如下一些问题： 生成的代码无法存储在版本控制系统中。 Ant（本案例中是Xdoclet）应该自动确定下一次构建涉及的源文件，而不应由程序员手工确定。 Ant的构建文件应该定义好正确的任务依赖关系，这样程序员就不必为了完成构建而不得不按照特定顺序调用任务。 当我开始一个新项目时，我首先编写Ant构建文件。Ant文件明确地定义构建的过程，并被团队中的每个程序员使用。本文所列的技巧基于这样的假定：Ant构建文件是一个必须仔细编写的重要文件，它应在版本控制系统中得到维护，并被定期进行重构。下面是我的十五大Ant最佳实践。 1. 采用一致的编码规范 Ant用户有的喜欢有的痛恨其构建文件的XML语法。与其跳进这一令人迷惑的争论中，不如让我们先看一些能保持XML构建文件简洁的方法。 首先也是最重要的，花费时间格式化你的XML让它看上去很清晰。不论XML是否美观，Ant都可以工作。但是丑陋的XML很难令人读懂。倘若你在任务之间留出空行，有规则的缩进，每行文字不超过90列左右，那么XML令人惊讶地易读。再加上使用能够高亮XML语法的优秀编辑器或IDE工具，你就不会有阅读的麻烦。 同样，精选含意明确、容易读懂的词汇来命名任务和属性。比如，dir.reports就比rpts好。特定的编码规范并不重要，只要拿出一套规范并坚持使用就行。 2. 将build.xml放在项目根目录中 Ant构建文件build.xml可以放在任何位置，但是放在项目顶级目录中可以保持项目简洁。这是最常用的规范，开发者能够在顶级目录中找到预期的build.xml。把构建文件放在根目录中，也能够使人容易了解项目目录树中不同目录之间的逻辑关系。以下是一个典型的项目目录层次： [root dir] | build.xml +--src +--lib (包含第三方 JAR包) +--build (由 build任务生成) +--dist (由 build任务生成) 当build.xml在顶级目录时，假设你处于项目某个子目录中，只要输入：ant -find compile 命令，不需要改变工作目录就能够以命令行方式编译代码。参数-find告诉Ant寻找存在于上级目录中的build.xml并执行。 3. 使用单一的构建文件 有人喜欢将一个大项目分解成几个小的构建文件，每个构建文件分担整个构建过程的一小部分工作。这确实是看法不同的问题，但是应该认识到，将构建文件分割会增加对整体构建过程的理解难度。要注意在单一构建文件能够清楚表现构建层次的情况下不要过工程化(over-engineer)。 即使你把项目划分为多个构建文件，也应使程序员能够在项目根目录下找到核心build.xml。尽管该文件只是将实际构建工作委派给下级构建文件，也应保证该文件可用。 4. 提供良好的帮助说明 应尽量使构建文件自文档化。增加任务描述是最简单的方法。当你输入ant -projecthelp时，你就可以看到带有描述的任务清单。比如，你可以这样定义任务： <target name=\"compile\" description=\"Compiles code, output goes to the build dir.\"> 最简单的规则是把所有你想让程序员通过命令行就可以调用的任务都加上描述。对于一般用来执行中间处理过程的内部任务，比如生成代码或建立输出目录等，就无法使用描述属性。 这时，可以通过在构建文件中加入XML注释来处理。或者专门定义一个help任务，当程序员输入ant help时来显示详细的使用说明。 <target name=\"help\" description=\"Display detailed usage information\"> <echo>Detailed help...</echo></target> 5. 提供清除任务 Detailed help... 每个构建文件都应包含一个清除任务，用来删除所有生成的文件和目录，使系统回到构建文件执行前的初始状态。执行清空任务后还存在的文件都应处在版本控制系统的管理之下。比如： <target name=\"clean\" description=\"Destroys all generated files and dirs.\"> <delete dir=\"${dir.build}\"/> <delete dir=\"${dir.dist}\"/> </target> 除非是在产生整个系统版本的特殊任务中，否则不要自动调用clean任务。当程序员仅仅执行编译任务或其他任务时，他们不需要构建文件事先执行既令人讨厌又没有必要的清空任务。要相信程序员能够确定何时需要清空所有文件。 6. 使用ANT管理任务从属关系 假设你的应用由Swing GUI组件、Web界面、EJB层和公共应用代码组成。在大型系统中，你需要清晰地定义每个Java包属于系统的哪一层。否则任何一点修改都要被迫重新编译成百上千个文件。糟糕的任务从属关系管理会导致过度复杂而脆弱的系统。改变GUI面板的设计不应造成Servlet和EJB的重编译。 当系统变得庞大后，稍不注意就可能将依赖于客户端的代码引入到服务端。这是因为典型的IDE项目文件编译任何文件都使用单一的classpath。而Ant能让你更有效地控制构建活动。 设计你的Ant构建文件编译大型项目的步骤：首先，编译公共应用代码，将编译结果打成JAR包文件。然后，编译上一层的项目代码，编译时依靠第一步产生的JAR文件。不断重复这一过程，直到最高层的代码编译完成。 分步构建强化了任务从属关系管理。如果你工作在底层Java框架上，偶然引用到高层的GUI模板组件，这时代码不需要编译。这是由于构建文件在编译底层框架时在源路径中没有包含高层GUI面板组件的代码。 7. 定义并重用文件路径 如果文件路径在一个地方一次性集中定义，并在整个构建文件中得到重用，那么构建文件更易于理解。以下是这样做的一个例子： <project name=\"sample\" default=\"compile\" basedir=\".\"> <path id=\"classpath.common\"> <pathelement location=\"${jdom.jar.withpath}\"/> ...etc </path> <path id=\"classpath.client\"> <pathelement location=\"${guistuff.jar.withpath}\"/> <pathelement location=\"${another.jar.withpath}\"/> <!-- reuse the common classpath --> <path refid=\"classpath.common\"/> </path> <target name=\"compile.common\" depends=\"prepare\"> <javac destdir=\"${dir.build}\" srcdir=\"${dir.src}\"> <classpath refid=\"classpath.common\"/> <include name=\"com/oreilly/common/**\"/> </javac> </target> </project> ...etc 当项目不断增长构建日益复杂时，这一技术越发体现出其价值。你可能需要为编译不同层次的应用定义各自的文件路径，比如运行单元测试的、运行应用程序的、运行Xdoclet的、生成JavaDocs的等等不同路径。这种组件化路径定义的方法比为每个任务单独定义路径要优越得多。否则，很容易丢失任务从属关系的轨迹。 8. 定义恰当的任务从属关系 假设dist任务从属于jar任务，那么哪个任务从属于compile任务哪个任务从属于prepare任务呢？Ant构建文件最终定义了任务的从属关系图，它必须被仔细地定义和维护。 应该定期检查任务的从属关系以保证构建工作得到正确执行。大的构建文件随着时间推移趋向于增加更多的任务，所以到最后可能由于不必要的从属关系导致构建工作非常困难。比如，你可能发现在程序员只需编译一些没有使用EJB的GUI代码时又重新生成了EJB代码。 以\"优化\"的名义忽略任务的从属关系是另一种常见的错误。这种错误迫使程序员为了得到恰当的结果必须记住并按照特定的顺序调用一串任务。更好的做法是：提供描述清晰的公共任务，这些任务包含正确的任务从属关系；另外提供一套\"专家\"任务让你能够手工执行个别的构建步骤，这些任务不提供完整的构建过程，但是让那些专家用户在快速而恼人的编码期间能够跳过某些步骤。 9.使用属性 任何需要配置或可能发生变化的信息都应作为Ant属性定义下来。对于在构建文件中多次出现的值也同样处理。属性既可以在构建文件头部定义，也可以为了更好的灵活性而在单独的属性文件中定义。以下是在构建文件中定义属性的样式： <project name=\"sample\" default=\"compile\" basedir=\".\"> <property name=\"dir.build\" value=\"build\"/> <property name=\"dir.src\" value=\"src\"/> <property name=\"jdom.home\" value=\"../java-tools/jdom-b8\"/> <property name=\"jdom.jar\" value=\"jdom.jar\"/> <property name=\"jdom.jar.withpath\" value=\"${jdom.home}/build/${jdom.jar}\"/> etc... </project> etc... 或者你可以使用属性文件： <project name=\"sample\" default=\"compile\" basedir=\".\"> <property file=\"sample.properties\"/> etc... </project> etc... 在属性文件 sample.properties中: dir.build=build dir.src=src jdom.home=../java-tools/jdom-b8 jdom.jar=jdom.jarjdom.jar.withpath=${jdom.home}/build/${jdom.jar} dir.build=builddir.src=srcjdom.home=../java-tools/jdom-b8jdom.jar=jdom.jarjdom.jar.withpath=${jdom.home}/build/${jdom.jar} 用一个独立的文件定义属性是有好处的，它可以清晰地定义构建中的可配置部分。另外，在开发者工作在不同操作系统的情况下，你可以在不同的平台上提供该文件的不同版本。 10. 保持构建过程独立 为了最大限度的扩展性，不要应用外部路径和库文件。最重要的是不要依赖于程序员的CLASSPATH设置。取而代之的是，在构建文件中使用相对路径并定义自己的路径。如果你引用了绝对路径如C:\\java\\tools，其他开发者未必使用与你相同的目录结构，所以就无法使用你的构建文件。 如果你部署开放源码项目，应该提供包含编译代码所需的所有JAR文件的发行版本。当然，这是在遵守许可协议的基础上。对于内部项目，相关的JAR文件都应在版本控制系统的管理中，并捡出（check out）到大家都知道的位置。 当你必须引用外部路径时，应将路径定义为属性。使程序员能够用适合他们自己的机器环境的参数重载这些属性。你也可以使用以下语法引用环境变量： <property environment=\"env\"/> <property name=\"dir.jboss\" value=\"${env.JBOSS_HOME}\"/> 11. 使用版本控制系统 构建文件是一个重要的制品，应该像代码一样进行版本控制。当你标记你的代码时，也应用同样的标签标记构建文件。这样当你需要回溯到旧版本并进行构建时，能够使用相应版本的构建文件。 除构建文件之外，你还应在版本控制中维护第三方JAR文件。同样，这使你能够重新构建旧版本的软件。这也能够更容易保证所有开发者拥有一致的JAR文件，因为他们都是同构建文件一起从版本控制系统中捡出的。 通常应避免在版本控制系统中存放构建成果。倘若你的源代码很好地得到了版本控制，那么通过构建过程你能够重新生成任何版本的产品。 12. 把Ant作为\"最小公分母\" 假设你的开发团队使用IDE工具，当程序员通过点击图标就能够构建整个应用时为什么还要为Ant而烦恼呢？ IDE的问题是一个关于团队一致性和重现性的问题。几乎所有的IDE设计初衷都是为了提高程序员的个人生产率，而不是开发团队的持续构建。典型的IDE要求每个程序员定义自己的项目文件。程序员可能拥有不同的目录结构，可能使用不同版本的库文件，还可能工作在不同的平台上。这将导致出现这种情况：在Bob那里运行良好的代码，到Sally那里就无法运行。 不管你的开发团队使用何种IDE，一定要建立所有程序员都能够使用的Ant构建文件。要建立一个程序员在将新代码提交版本控制系统前必须执行Ant构建文件的规则。这将确保代码是经过同一个Ant构建文件构建的。当出现问题时，要使用项目标准的Ant构建文件，而不是通过某个IDE来执行一个干净的构建。 程序员可以自由选择任何他们习惯使用的IDE工具或编辑器。但是Ant应作为公共基线以保证代码永远是可构建的。 13. 使用zipfileset属性 人们经常使用Ant产生WAR、JAR、ZIP和 EAR文件。这些文件通常都要求有一个特定的内部目录结构，但其往往与你的源代码和编译环境的目录结构不匹配。 一个最常用的方法是写一个Ant任务，按照期望的目录结构把一大堆文件拷贝到临时目录中，然后生成压缩文件。这不是最有效的方法。使用zipfileset属性是更好的解决方案。它让你从任何位置选择文件，然后把它们按照不同目录结构放进压缩文件中。以下是一个例子： <ear earfile=\"${dir.dist.server}/payroll.ear\" appxml=\"${dir.resources}/application.xml\"> <fileset dir=\"${dir.build}\" includes=\"commonServer.jar\"/> <fileset dir=\"${dir.build}\"> <include name=\"payroll-ejb.jar\"/> </fileset> <zipfileset dir=\"${dir.build}\" prefix=\"lib\"> <include name=\"hr.jar\"/> <include name=\"billing.jar\"/> </zipfileset> <fileset dir=\".\"> <include name=\"lib/jdom.jar\"/> <include name=\"lib/log4j.jar\"/> <include name=\"lib/ojdbc14.jar\"/> </fileset> <zipfileset dir=\"${dir.generated.src}\" prefix=\"META-INF\"> <include name=\"jboss-app.xml\"/> </zipfileset> </ear> 在这个例子中，所有JAR文件都放在EAR文件包的lib目录中。hr.jar和billing.jar是从构建目录拷贝过来的。因此我们使用zipfileset属性把它们移动到EAR文件包内部的lib目录。prefix属性指定了其在EAR文件中的目标路径。 14. 测试Clean任务 假设你的构建文件中有clean和compile的任务，执行以下的测试。第一步，执行ant clean；第二步，执行ant compile；第三步，再执行ant compile。第三步应该不作任何事情。如果文件再次被编译，说明你的构建文件有问题。 构建文件应该只在与输出文件相关联的输入文件发生变化时执行任务。一个构建文件在不必执行诸如编译、拷贝或其他工作任务的时候执行这些任务是低效的。当项目规模增长时，即使是小的低效工作也会成为大的问题。 15. 避免特定平台的Ant封装 不管什么原因，有人喜欢用简单的、名称叫做compile之类的批文件或脚本装载他们的产品。当你去看脚本的内容你会发现以下内容： ant compile 其实开发人员都很熟悉Ant，并且完全能够自己键入ant compile。请不要仅仅为了调用Ant而使用特定平台的脚本。这只会使其他人在首次使用你的脚本时增加学习和理解的烦扰。除此之外，你不可能提供适用于每个操作系统的脚本，这是真正烦扰其他用户的地方。 总结 太多的公司依靠手工方法和特别程序来编译代码和生成软件发布版本。那些不使用Ant或类似工具定义构建过程的开发团队，花费了太多的时间来捕捉代码编译过程中出现的问题：在某些开发者那里编译成功的代码，到另一些开发者那里却失败了。 生成并维护构建脚本不是一项富有魅力的工作，但却是一项必需的工作。一个好的Ant构建文件将使你能够集中到更喜欢的工作——写代码中去！ 参考","tags":"中文"},{"url":"http://www.ciandcd.com/chi-xu-ji-cheng-li-lun-he-shi-jian-de-xin-jin-zhan.html","title":"持续集成理论和实践的新进展","text":"From: http://www.cnblogs.com/itech/archive/2011/07/26/2116999.html#FeedBack 转自： http://www.infoq.com/cn/articles/ci-theory-practice 最近雷镇同学将Martin Fowler先生的著名论文《持续集成》第二版翻译成中文并发布出来，掀起了国内对于持续集成理论和实践讨论的新的高潮。笔者在本文中将全面对比持续集成 论文前后两版的异同，分析并展示ThoughtWorks在持续集成领域的理论和实践方面的研究成果，以图对国内企业实施持续集成起到参考和借鉴作用。需 要说明的是，本文所介绍的内容毕竟限于笔者的水平，并且主要是ThoughtWorks内部开发和对外咨询实践的总结，所以未必对读者所遇到的情况是适用 的，请自行甄别。 《持续集成》第二版虽然是最近才翻译出来，但是实际上Martin Fowler先生完成此文是在5年前的事情。这五年恰好是ThoughtWorks中国公司快速成长的五年。在这五年内ThoughtWorks中国在持 续集成领域也有很多的发展，这包括：著名的持续集成工具Cruise主要是由中国公司负责开发 1 ; 中国公司帮助国内很多大中型企业完成持续集成实施和相关的流程改进;2009年中国公司的很多同事对于持续集成的度量进行了深入的讨论并且最终由胡凯将其 实现为一款软件iAnalysis;2010年至2011年成功的交付了从需求提供方到多个技术服务提供商的持续集成方案，以及企业级自动化中心方案。所 以，本文主要包括两部分内容，一部分是通过对比第一版与第二版的异同介绍2000年到2006年之间持续集成领域的主要发展，另一部分则是介绍第二版发表 之后持续集成领域的新进展。读者如果之前没有阅读过《持续集成》论文的第二版，建议将本文第一部分一同阅读，因为本文并非对论文的重述，所以很多地方还需 要参考原文中的内容。 第一部分 《持续集成》第一版与第二版 《持续集成》第一版由ThoughtWorks首席科学家Martin Fowler先生和Matthew Foemmel共同完成 2 ，第二版由Martin Fowler先生更新。 《持续集成》的第一版中并没有给出比较正式的定义，虽然作者在文中说是借鉴了XP实践中的术语，但是目前能看到的XP实践中对持续集成的定义实际上大多数都是指向了Martin的文章。那么我们还是来看看第二版中给出的定义。 持续集成是一种软件开发实践。在持续集成中，团队成员频繁集成他们的工作成果，一般每人每天至少集成一次，也可以多次。每次集成会经过自动构建（包 括自动测试）的验证，以尽快发现集成错误。许多团队发现这种方法可以显著减少集成引起的问题，并可以加快团队合作软件开发的速度。 3 第二版相对于第一版增加了不少内容，其中最重要的几点包括： 详细介绍了使用持续集成进行软件开发的工作流程。 突出了配置管理在持续集成实践中的作用。 提出分阶段构建的概念。 增加了持续集成报告的内容。 增加了持续部署的内容。 给出了引入持续集成的建议。 1) 持续集成的流程 在持续集成领域，我们经常会用到的一个术语就是\"构建（Build）\"。很多人认为\"构建=编译+链接（Build=Compile+Link）\"，Martin在第一版中指出一次成功构建包括： 所有最新代码从配置管理工具中取出（check out或者update）。 所有的代码从干净的状态开始编译。 将编译结果链接并部署，以备执行。 执行部署的应用并运行测试套。 如果上述所有操作没有任何错误，没有人工干预，并通过了所有测试，我们认为这才是一次成功的构建。 实际上，目前很多团队对成功持续集成构建的定义基本上是符合上述定义的。这个定义的特点在于它是相对独立的，它是一个从干净状态的源代码最终获得可运行的通过验证的软件的过程。 Martin在第二版中则在成功构建的基础上给出了成功集成的定义。成功集成关注的不是一次\"编译+链接+部署+验证\"的过程，而是从开发流程的角度介绍一次完整的在持续集成约束下的代码提交过程 4 ： 将已集成的源代码复制一份到本地计算机。 修改产品代码和添加修改自动化测试。 在自己的计算机上启动一个自动化构建。 构建成功后，把别人的修改更新到我的工作拷贝中。 再重新做构建。 把修改提交到源码仓库。 在集成计算机上并基于主线的代码再做一次构建。 只有这次构建成功了，才说明改动被成功的集成了。 下图展示了Martin对成功集成的定义： 当然在第一版的\"代码提交\"这一节，Martin也提到了本地构建的概念，只是不如第二版这么明确。 2) 配置管理 Martin在第一版中有两处提及配置管理，分别是：单一代码源（Single Source Point）和代码提交（Checking In）。第二版中则包括：通过持续集成构建特性（Building a Feature with Continuous Integration）、只维护一个代码仓库（Maintain a Single Source Repository）、每人每天都要向主线提交代码（Everyone Commits To The Mainline Every day）、每次提交都应在集成计算机上重新构建主线（Every Commit Should Build the Mainline on an Integration Machine）。不仅条目数量上增加明显，作者提出的很多实践都是基于配置管理来讲的。 工具 配置管理是持续集成的输入。在第一版中作者所推荐的配置管理工具是CVS，到第二版中作者推荐的配置管理工具已经换成了SVN 5 （参见第二部分中的配置管理工具部分）。 分支策略 实现进度与质量的平衡是配置管理的重要目的。Martin在第二版中对滥用分支给出了警告： 尽量减少分支数量。典型的情况是保持一条主线，......，每个人都从这条主线开始自己的工作。（对之前发布版本进行Bug修正或者临时性的实验都是创建分支的正当理由。） 但是这里给出的建议对于大型团队来说并不十分合适。我们将在第二部分对于配置管理的分支策略进行详细描述。 内容 Martin在第一版中给出的原则是： 任何人都可以找到一台干净的机器，连上网，通过一个命令就可以取得要构建所开发的系统需要的所有源文件。 第二版中的原则增加了对构建的支持 6 ： 任何人都可以找到一台干净的机器，做一次取出（checkout）动作，然后对系统执行一次完整的构建。 3) 分阶段构建（Staged Build） 分阶段构建是Cruise（已经更名为Go）引入的重要概念。其主要的意义在于： 分离关注度不同的验证阶段，比如Commit Build和Regression Tests，团队会对不同的验证阶段采取不同的策略 构建流程可视化 通过分阶段并发构建来缩短反馈周期 当构建的时间过长时，我们通常会要求开发人员只运行速度较快的价值较高的构建阶段就可以继续自己的开发任务，而不必等待漫长的次级构建完成。这里作者提到ThoughtWorks不同的团队有很多有趣的实践，我们将在第二部分向读者介绍其中的一部分。 报告 作者在第二版中专门拿出一节\"每个人都能看到进度（Everyone Can See What's Happening）\"来介绍有关持续集成报告的内容。因为： 持续集成的目的是为了沟通。 这是第二版相对于第一版来说一个非常明显的变化。在第一版中通知的手段还主要是电子邮件，实际上在作者撰写第二版的时候，ThoughtWorks已经不赞成将电子邮件作为主要的持续集成通知工具了。更好的沟通工具包括音乐、熔岩灯、显示器等。 对于沟通的重视从工具的角度也可以体现出来。Cruise Control最主要做的事情是任务调度，在报告部分做的相对来说非常粗糙，比较有价值的报告大部分是从Cruise移植过去的。Cruise在从一开始 就非常重视这一点，通过Cruise你可以非常清晰地知道，代码发生了什么变化、正在进行的构建的状态和历史构建的状态。网页的形式对于分布式团队来说具 有不可替代的优势。 正如我们前面所说的，音乐、熔岩灯等物理手段，具有更强的信息辐射能力。站起来往周围看一看就知道哪个团队的构建成功了，哪个失败了。 4) 持续部署 持续集成实践有一个基本的思想就是：越是痛苦的事情，越要经常做。集成之后更令人心惊胆颤的事情就是——部署。部署到生产环境的流程通常要严格得多，然而所有的工作必须经历了生产环境的验证才算是成功的，所以——持续部署才是王道。Martin在第二版中建议： 你应该有一个脚本帮助你很容易地将系统部署到生产环境中去。......同时要特别考虑的是要能够自动回滚。 引入持续集成的建议 作者在第二版中特别给出了逐步引入持续集成的建议。包括： 引入版本控制。 实现自动化构建。 添加自动化测试。 加快提交构建。 寻找帮助。（比如ThoughtWorks） 第二部分 持续集成领域的新进展 正如前文所说，ThoughtWorks中国公司在过去的几年里面对于持续集成实践和帮助客户实施持续集成都积累了很多的经验，同时在理论体系方面也更加丰富完整。这也使ThoughtWorks在这个领域继续保持了行业领先的位置。 正如我们在第一部分讲到的，持续集成不应该只作为一个孤立的实践来应用。我们的经验表明如果只把持续集成作为一个孤立的实践应用很难从持续集成长期 受益。持续集成往往进入\"长红\"或者\"长绿\"的不正常的状态。长红意味着系统长期无法集成；长绿则往往意味着缺少足够的验证。为了术语上的澄清，我们明确 地将持续集成的定义区分为狭义的持续集成和广义的持续集成。 狭义的持续集成：基于某种或者某些变化对系统进行的经常性的构建活动。 广义的持续集成：软件开发团队在上述活动的约束下所采用的开发流程。 1) 狭义的持续集成 一般来说，狭义的持续集成包括如下几个方面：持续检查、持续编译（链接）、持续验证、持续部署、持续基础设施、持续报告等6个方面。 持续检查 持续检查的目的是保证代码风格一致，主要关注于代码的静态质量和内部质量，比如变量命名方式、大括号位置等等。大部分的现代集成开发环境（IDE） 都具备实时检查代码质量的功能。为了保证主线上的代码质量能够达到一致的标准，应当在持续集成脚本中加入静态检查阶段。比如，Java的PMD、 FindBugs等等。 持续编译 持续编译是一个很朴素的想法，就是保证主线上的代码始终处于可编译的状态。但是这对于很多大中型团队来说却并非想当然的简单。这是因为很多团队并未 采用集体代码所有权策略，导致存在依赖的团队的代码无法编译。针对这样的问题，一方面我们建议采用集体代码所有权；另一方面，对于确实因为安全原因需要隔 离的代码应该边界、明确接口，很少存在大部分代码需要对大部分人保密的情况。 持续检查和持续编译是持续集成中最基本的验证手段。 持续验证 持续验证的目的是检查主线上的代码是否能够实现所要求的功能，或者已有的功能是否被破坏。在大部分的构建中，验证部分是耗时最长、成本最高的部分， 但同时也是收益最大的部分。在这个阶段，我们看到的主要问题是验证不充分和验证时间过长。针对这样的问题，我们通常采用分层分级的持续集成策略。后面有详 细的描述。 持续部署 对于大型软件应用来说，部署往往是一个费时费力又容易出错的步骤，因为这里面涉及到数据迁移、版本兼容等各种棘手的问题。持续部署的思想是将这些工作标准化自动化，使其能够可靠地、自动地、快速地运行。持续部署是当前DevOps运动中的热门话题之一。 持续基础设施集成 现代大型软件开发，尤其是互联网应用开发中经常依赖于一些常见的基础设施——比如Spring、Tomcat、Database等等。这些基础设施发生变化的时候，我们应当及时地触发持续集成，以确保我们的系统是能够与新的基础设施一起工作的。 持续报告 报告是将持续集成的状态以适当的形式展现给干系人的基本手段。报告是持续集成的晴雨表，所以它必须直观、易懂，而且对关注点不同的角色展现不同的内 容和粒度。比如，开发人员可能更关心错误的日志；项目经理可能更关心测试覆盖率；产品经理可能更关心持续集成通过率的趋势等等。 2) 广义的持续集成及持续集成策略 当要把持续集成实践应用到团队的时候，有很多额外的技术或者非技术因素需要考虑。 组织结构 持续集成是一个重要的沟通工具，而开发过程中两个最需要紧密沟通的角色就是开发和测试。在我们常见的组织结构中开发和测试往往隶属于不同的部门，甚 至这些部门隶属于不同的高级经理。这往往会给持续集成的推广带来很大的阻力。这是因为持续集成从环境搭建到运行维护都需要两种角色的通力合作。我们的经验 是这类涉及到人力资源的事情除非某一级\"共同的大老板\"出面，否则是很难协调的。\"借调\"这样的方式往往不能保证效果。 流程 放到团队的角度看待流程应当更加关注于各个成员之间的配合。每个开发人员提交代码之前应当确保是经过本地构建的；开发人员在提交之前应该确认主线上的代码是通过了持续集成的；测试人员测试的版本应该是通过了某次持续集成的，并且有相应的具体版本信息。 为了保障流程的顺利执行，我们还经常采用持续集成看板、提交令牌等辅助手段。 环境 环境是指持续集成运行时所依赖的软件和硬件的集合。我们经常遇到的一个问题是，软件在一台机器上能够通过持续集成的验证，而在另一台机器上则不能通 过。这通常是因为我们对持续集成环境的定义不明确造成的。所以在搭建持续集成和在组织内推广持续集成的时候，我们需要特别注意持续集成环境的标准化，明确 指出持续集成运行时依赖哪些第三方库，机器配置如何，端口和网络状况如何等等。 我们经常采用将持续集成环境加入配置管理的方式来解决环境标准化的问题。 分层 在大型团队（超过100人）中，扁平的开发组织结构是运行起来是比较困难的。常见的做法是按照特性，将团队划分为10人左右的小团队。一般来说，如 果团队数量超过10个，还会再增加一层架构。这时候，配置管理的策略也应当做出调整。常见的做法是为每个团队拉出一个分支，设置一个集成分支用于将各个特 性分支的内容整合在一起。需要注意的是，这里每个分支都应该具备所有代码的访问权限，也就是所有分支是同根的、等价的。 测试分级 在实施持续集成的时对于测试的类型应该有比较明确的定义。一般来说，我们经常把测试分为三级——单元测试、集成测试和系统测试。这是一个很大的话 题，这里只是说明此处的单元测试并不是指针对函数的测试。虽然单元测试主要是函数基本的测试，但是每个单元测试应该针对的是特性或者对应代码在实现该特性 上所发挥的作用。 单元测试的运行速度通常非常快，应该在数秒到数分钟。单元测试应该覆盖绝大部分的特性需求。集成测试单个测试的运行速度通常会比单元测试慢一个数量 级，比如存在文件读写或者其他的IO和网络操作。集成测试主要用于保证系统的各个组件之间的调用是工作的。系统测试的运行速度一般会比集成测试慢，通常需 要将整个系统运行起来，比如Web开发中的selenium测试。系统测试主要用于测试系统的正确路径（Happy Path）可以工作。有的团队会开发很多基于整个系统的回归测试，也属于系统测试。就场景的覆盖而言，单元测试>集成测试>系统测试，从运行 时间来看则相反。 3) 持续集成的成熟度评估 在帮助客户实施和推广持续集成的过程中我们逐渐总结出一些原则，帮助客户评估现状，分析和设计未来的目标。该评估方法借鉴了ThoughtWorks敏捷成熟度模型中有关配置管理、测试、构建等内容，并做了适当的简化。 构建： 级别 描述 3+：对外防御的 团队能够根据自己的需要，协调其他团队的持续集成，当依赖的其他团队的代码和组件或者第三方库和基础设施发生改变时自动进行构建。团队对于外部依赖的可靠性有信心。 3：对内防御的 构建是自动的。由测试和检查来保证团队内部的开发质量。持续集成的修复具有最高的优先级。团队对持续集成的结果有信心。 2：频繁的 构建是自动的，而且构建的速度较快。构建的触发条件是明确的，通常每次代码提交都会触发构建。团队中的每个人都会触发构建，并且了解构建的状态。 1：重复执行 构建是自动的，但是执行的不够频繁。构建的触发是随机的或者频率是非常低的（低于每天一次）。构建的速度通常非常慢，比如一次构建超过半个小时。 0：可重复的 主要依赖于手动的方式构建软件，但是每次构建的方式都是相同的或者相似的。通常有相关的文档的指导。经常团队指定某个人负责构建软件，虽然大部分人都能够做这件事情。 -1：手动的 主要依赖于手动的方式集成软件。每次集成的方式可能不一样。经常团队中只有个别人能够将软件集成起来。 测试： 级别 描述 3+：全面集成的 全团队对测试负责。测试驱动整个开发过程。测试与构建完全集成。 3：测试驱动的 业务分析人员和开发人员均参与测试。测试在构建过程中自动执行。开发人员实践测试驱动开发。 2：集成的 开发人员参与测试。部分测试集成在构建过程中执行。大部分测试在软件开发过程中执行。 1：共享的 开发人员参与测试。测试并未集成在构建过程中。部分测试在软件开发过程中执行，大部分测试在软件开发结束后执行。 0：审查的 测试由专门的测试人员负责。有部分测试是在软件开发过程中执行。但大部分测试在软件开发结束后执行。 -1：独立的 测试由专门的测试人员负责。仅在软件开发结束后执行。 配置管理： 级别 描述 3+：企业级的 企业有统一的配置管理策略。 3：跨项目的 配置管理在多个项目之间协调。 2：自动的 配置管理策略与持续集成策略紧密结合，团队成员有频繁提交的意识。一般采用乐观锁策略，原子提交。 1：集成的 版本管理下的内容是受控的。通常在版本管理中的代码是可以编译的。开发人员能够访问到自己工作所需要的代码。开发人员按照一定的规则访问配置管理工具和提交代码。一般采用悲观锁策略。版本管理工具和构建过程集成在一起的。 0：基本的 有基本的版本管理。但版本管理下的内容不全面，或者不足以支撑团队的开发。 -1：无配置管理 没有配置管理。或者使用方式完全错误。 4) 常用实践和工具 持续集成看板 7 问题： 我们需要让整个团队能够方便快捷的了解持续集成的状态。 上下文： 在完成了基本的构建基础设施的搭建之后，我们需要让团队成员及时获得持续集成的状态信息。传统的邮件方式可能会使人厌烦，或者错过重要的构建信息。 解决方案和实现： 安装一个显示器，将构建的状态信息以简明的方式展示在显示器上。将显示器放置在团队所有成员都能够很容易看到的地方。 个人构建中心 问题： 在某些情况下，构建环境的成本很高，而我们需要每一个开发人员提交之前完成一次个人构建。 上下文： 有些测试是依赖于设备的，而这些设备非常昂贵，并且配置复杂，所以无法给每个开发/测试人员一套构建环境。我们发现根据本地构建的理论模型，每个人的提交应该是串行的，这样我们实际上可以做到让这些人共享一套环境，但是从逻辑上就像是每个人有一套自己的环境一样。 解决方案： 在运行个人构建的时候将本地的代码同步到一台共享的机器上执行构建，构建完成后结果反馈给提交这次个人构建的人。 实现： 个人构建中心的实现有很多种方案。这些方案的区别主要在于如何将代码同步到个人构建中心服务器上。两种常见的方式：一个是使用rsync或者类似方式同步；另一个是使用分布式配置管理工具如git/hg同步。其拓扑结构如下： 第一种方式相对独立灵活；第二种方式稳定、高效，但是对于配置管理工具有依赖。 后果： 个人构建中心节省了大量的计算资源，同时也容易使得中心服务器成为单点失败的源头。一旦中心服务器出现问题，可能会导致团队的流程受到较大影响。 提交令牌 问题： 在实施本地构建的时候，向目标分支的提交应该是串行的，以避免构建被破坏后难以定位问题来源。但是团队往往缺乏一种有效的机制来保证这种串行。 上下文： 有些团队试图通过技术的手段来解决这个问题，比如通过配置管理上的锁机制，这种方式和乐观锁模式有较大冲突。有些团队通过团队内部沟通的方式解决， 比如谁提 交之前都会通知别人，或者通过持续集成监视器来了解当前的构建状态，以决定自己是否可以提交。这些方式各自有各自的适用情形，较容易理解。 解决方案和实现： 使用一个实物作为令牌，准备提交的代码的人首先取得令牌，当代码提交完成（包括相应的提交构建）之后，将令牌交还。令牌要醒目，并且移动方便。小型奖杯、毛绒玩具、较大的头饰（如下图）都是不错的令牌。 分阶段构建 问题： 在某些团队中完整构建所花费的时间可能很长，如果每次提交都运行完整的构建会浪费很多时间。 上下文： 随着持续集成的日益完善，我们往往会发现验证所花费的时间越来越长，而大部分验证趋于稳定，失败的情况很少见。通过技术手段缩短构建时间是解决问题的根本办法，但是缩短构建时间是一个耗时耗力的工作，很难短期内见效。 解决方案和实现： 将构建分为几个阶段执行，在本地构建中仅执行速度比较快、可信度比较高、出错概率比较大的验证。利用晚上或者其他合适的时间执行全面的验证——我们这次构建称为全量构建。需要注意的是，这种情况下仍然要保证提交构建和本地构建的一致性。 iAnalysis iAnalysis是一款轻量级的持续集成报告工具。该工具的核心思想是将持续集成构建过程中产生的数据以趋势和对比的方式展示出来。正如前文所 说，我们在2009年的ThoughtWorks Away-Day上讨论了敏捷度量的话题，大家最后一致认为，数据有两种最基本的用法——横向对比和纵向对比。横向对比就是不同的人、不同的团队之间对 比；纵向对比就是现在和过去对比。iAnalysis正是这种思想的体现。 关于作者 肖鹏，ThoughtWorks资深咨询师，程序员，敏捷过程教练。拥有7年以上软件开发实践经验，多次担任大中型企业敏捷流程改进咨询和培训，服 务对象涉及通信设备制造、通信运营、互联网行业等。他关注于设计模式、架构模式、敏捷软件开发等领域，并致力于软件开发最佳实践的推广和应用。他曾参与翻 译《Visual Studio 2005技术大全》，主持翻译《面向模式的软件架构》第四卷和第五卷等图书。 1 目前Cruise的开发任务已经不在中国公司了。 2 实际上这篇文章介绍的是Matthew所在团队在2000年早些时候已经在使用的实践，ThoughtWorks中国公司的总经理郭晓先生当时也在这个团 队。 3 为了与ThoughtWorks常用的术语保持一致，部分术语与雷镇和熊节同学所译略有差别，下同。 4 笔者对其格式略作处理，与原文稍有出入。 5 Martin最近在自己的一篇博客上对几种流行的配置管理工具做了对比。 6 注意，本文并非为指出第一版的缺陷，只是通过对比来说明作者论文中重点的变化。 7 这里只是借用\"看板\"这个词的字面含义，与精益中的看板有区别。 完！","tags":"中文"},{"url":"http://www.ciandcd.com/perforce-fen-zhi-de-suo-ding.html","title":"perforce 分支的锁定","text":"From: http://www.cnblogs.com/itech/archive/2011/08/16/2140493.html#FeedBack perforce分支的锁定（Perforce branch locking） 通常地在某些milestone的build的时候或者某些branch的生命周期结束了，我们都需要锁定branch来禁止对branch的修改。 可以通过如下的两种方法来锁定perforce branch： 1）权限表（protection table） 通常地我们使用一个group来管理一个branch的权限。例如默认地如下： write group envision * //dev/envision/esi/... 当 我们需要锁定branch //dev/envision/esi/... 时，可以在p4admin的ＧＵＩ管理工具中修改protection table的权限行或在最后增加新的一行来lock branch，例如read group envision * //dev/envision/esi/... 也可以在脚本中使用p4 protect来修改权限表， 从而锁定或解锁某个ｂｒａｎｃｈ。 2）triggers 也 可以使用一个trigger来达到锁定branch的目的。此trigger的执行过程为：当有changelist来的时候，检查所包含的文件是否属于 指定的branch，如果属于要锁定的branch则拒绝提交，从而达到lock branch的目的。所有的要lock的branches可以放到某个txt文件，然后trigger的脚本通过检查此txt来获得那些branches 要lock，当需要lock新的branch，只需要加到此txt中就可以了。 使用trigger方法的缺点的增加了所有的submit的时间。","tags":"中文"},{"url":"http://www.ciandcd.com/antgao-ji-task.html","title":"Ant高级-task","text":"From: http://www.cnblogs.com/itech/archive/2011/10/31/2230414.html 一 Task的命令行参数 有些task可接受参数，并将其传递给另一个进程。为了能在变量中包含空格字符，可使用嵌套的arg元素。 Attribute Description Required value 一个命令行变量；可包含空格字符。只能用一个 line 空格分隔的命令行变量列表。 file 作为命令行变量的文件名；会被文件的绝对名替代。 path 一个作为单个命令行变量的path-like的字符串；或作为分隔符，Ant会将其转变为特定平台的分隔符。 例子 <arg value=\"-l -a\"/> 是一个含有空格的单个的命令行变量。 <arg line=\"-l -a\"/> 是两个空格分隔的命令行变量。 <arg path=\"/dir;/dir2:\\dir3\"/> 是一个命令行变量，其值在DOS系统上为\\dir;\\dir2;\\dir3；在Unix系统上为/dir:/dir2:/dir3 。 二 时间戳<tstamp/> 在生成环境中使用当前时间和日期，以某种方式标记某个生成任务的输出，以便记录它是何时生成的，这经常是可取的。这可能涉及编辑一个文件，以便插入一个字符串来指定日期和时间，或将这个信息合并到 JAR 或 zip 文件的文件名中。 这种需要是通过简单但是非常有用的 tstamp 任务来解决的。这个任务通常在某次生成过程开始时调用，比如在一个 init 目标中。这个任务不需要属性，许多情况下只需 <tstamp/> 就足够了。 tstamp 不产生任何输出；相反，它根据当前系统时间和日期设置 Ant 属性。下面是 tstamp 设置的一些属性、对每个属性的说明，以及这些属性可被设置到的值的例子： 属性说明例子 DSTAMP 设置为当前日期，默认格式为yyyymmdd 20031217 TSTAMP 设置为当前时间，默认格式为 hhmm 1603 TODAY 设置为当前日期，带完整的月份2003 年 12 月 17 日 例如，在前一小节中，我们按如下方式创建了一个 JAR 文件： <jar destfile=\"package.jar\" basedir=\"classes\"/> 在调用 tstamp 任务之后，我们能够根据日期命名该 JAR 文件，如下所示： <jar destfile=\"package-{DSTAMP}.jar\" basedir=\"classes\"/> 因此，如果这个任务在 2003 年 12 月 17 日调用，该 JAR 文件将被命名为 package-20031217.jar。 还可以配置 tstamp 任务来设置不同的属性，应用一个当前时间之前或之后的时间偏移，或以不同的方式格式化该字符串。所有这些都是使用一个嵌套的 format 元素来完成的，如下所示： <tstamp> <format property=\"OFFSET_TIME\" pattern=\"HH:mm:ss\" offset=\"10\" unit=\"minute\"/> </tstamp> 上面的清单将 OFFSET_TIME 属性设置为距离当前时间 10 分钟之后的小时数、分钟数和秒数。 用于定义格式字符串的字符与 java.text.SimpleDateFormat 类所定义的那些格式字符相同 。 三 发送email的task 使用SMTP服务器发送邮件 例子： <mail mailhost=\"smtp.myisp.com\" mailport=\"1025\" subject=\"Test build\"> <from address=\"me@myisp.com\"/> <to address=\"all@xyz.com\"/> <message>The {buildname} nightly build has completed</message> <fileset dir=\"dist\"> <includes name=\"**/*.zip\"/> </fileset> </mail> 四 ssh和scp 在ant中，使用ssh命令远程启动和停止另外一台机器上的tomcat 下面是远程停止192.168.0.2这台机器上的ｔｏｍｃａｔ： <target name=\"sshexecstop\" > < sshexec host=\"192.168.0.2\" username=\"root\" password=\"123456\" trust=\"true\" command=\"cd /usr/local/tomcat/bin;./shutdown.sh\" /> < /target> 下面是远程启动192.168.0.2这台机器上的ｔｏｍｃａｔ： <target name=\"sshexecstart\"> <sshexec host=\"192.168.0.2\" username=\"root\" password=\"123456\" trust=\"true\" command=\"cd /usr/local/tomcat/bin;./startup.sh\" /> </target> 在ant中，使用scp将本地的文件（appwar目录下的文件）远程拷贝到另外一台机器的tomcat下： <target name=\"copy\" > < scp todir=\"root:123456@192.168.0.2:/usr/local/tomcat/webapps/\" trust=\"true\"> < fileset dir=\"/home/cruisecontrol/projects/eholderWeb/build/appwar\" /> < /scp> < /target> 五 input 与用户交互输入的task < input message=\"请选择一个Target \" validargs=\"compile,jar,test\" addproperty=\"my.input\"/> 六 exec 执行其他进程的task <exec executable=\"cmd.exe\"> <arg line=\"/c start http://localhost:8080 > </exec> 七其他的task if task ant原来可以在target级进行if判断(unless,if 属性)，但实在太不方便了。 Conditions 但Ant预先封装的一堆condition很是很方便的。这些condition完全从实际出发，包括文件是否存在， http://localhost:8080 是否连通都可以作为条件，见Ant的参考手册。 For task 支持\"a,b,c,d\" 字符串数组的循环与文件目录，Fileset的循环。 Parallel task Parallel非常有用，比如我想一边开tomcat，一边做别的，就需要使用它，否则就只有用spawn=true属性把tomcat放在后台运行。spawn有很多不好的地方，比如不能即时在console看到信息，停止ant运行不能把tomcat关掉等。 Parallel相当于一个容器，放在里面的每个task都会被并行执行。如果想把某几个task顺序执行，用相当于()的Sequential task 包起来。 Waitfor task 暂停ant执行直到条件符合，比如<waitfor><http url=http://localhost:8080/></waitfor>就会等待tomcat启动后才会继续往下执行。 八 antcall 与ant depends：depends中的targets在本target执行前按照从左到右的定义顺序调用。 antcall ： 用来调用同一个build.xml中的其他的target，相当于高级语言中的函数调用。 ant： 调用其他的build.xml中的target。 九 自定义TASK的使用 ANT已经内置了不少task，像copy、replace、javac等等，但是有时候还需要一些特定的任务来完成操作，比如在生成JAD文件 时，需要一个Midlet-Jar-Size的参数，得到JAR文件的大小，但是通过内部的task无法实现，因此可以自己编写类来实现此功能，但必须保 证该类是从Task类继承过来的。 例： <taskdef name=\"filesize\" classname=\"ant.FileSizeTask\" classpath=\"${LIB_PATH}/FileSizeTask.jar\" /> <filesize file=\"${Base}/Demo_Build/${jarName}\" property=\"size\" /> <replace dir=\"store\" includes=\"${jadName}\" encoding=\"UTF-8\"> <replacefilter token=\"@FILESIZE@\" value=\"${size}\" /> </replace> 完！","tags":"中文"},{"url":"http://www.ciandcd.com/scmyuan-ma-guan-li-perforce-labelshi-li.html","title":"[SCM]源码管理 -perforce label实例","text":"From: http://www.cnblogs.com/itech/archive/2012/10/20/2732041.html 一 静态label 静态label使用labelsync或tag来生成，实际上包含了具体的文件和版本信息。 文件必须同时包含在clientspec和label的view中，clientspec和label的view可以不相同。 1）server上两个depots（depot和depot2），且本地sync到最新的changelist@2 C:\\>p4 -p localhost:1666 -u aaa -c aaa_test depots Depot depot 2012/10/20 local depot/... 'Default depot' Depot depot2 2012/10/20 local depot2/... 'Created by AAA. ' C:\\>p4 -p localhost:1666 -u aaa -c aaa_test changes Change 2 on 2012/10/20 by AAA@AAA_Test 'second ' Change 1 on 2012/10/20 by AAA@AAA_Test 'first ' 2）但是clientspec只包含了一个depot C:\\>p4 -p localhost:1666 -u aaa -c aaa_test client -o Client: AAA_Test Update: 2012/10/20 11:12:11 Access: 2012/10/20 11:21:04 Owner: AAA Host: AAA-PC Description: Created by AAA. Root: D:/p4/AAA_Test Options: noallwrite noclobber nocompress unlocked nomodtime normdir SubmitOptions: submitunchanged LineEnd: local View: //depot/... //AAA_Test/depot/... 3) 创建label1 C:\\>p4 -p localhost:1666 -u aaa -c aaa_test label label1 Label label1 saved. C:\\>p4 -p localhost:1666 -u aaa -c aaa_test label -o label1 # A Perforce Label Specification. # # Label: The label name. # Update: The date this specification was last modified. # Access: The date of the last 'labelsync' on this label. # Owner: The user who created this label. # Description: A short description of the label (optional). # Options: Label update options: [un]locked, [no]autoreload. # Revision: Optional revision specification to make an automatic label. # View: Lines to select depot files for the label. # # Use 'p4 help label' to see more about label views. Label: label1 Update: 2012/10/20 11:27:16 Access: 2012/10/20 11:27:16 Owner: AAA Description: Created by AAA. Options: unlocked noautoreload View: //depot2/... //depot/... 4）将本地的depot下的文件sync到label1中 （可以看到虽然label中view包含了depot2，但是由于我们本地的client中只包含depot下的文件，所以最终label1中也只有depot下的文件） C:\\>p4 -p localhost:1666 -u aaa -c aaa_test labelsync -l label1 //...#have //depot/File1.txt#1 - added //depot/file2.txt#1 - added C:\\>p4 -p localhost:1666 -u aaa -c aaa_test files //...@label1 //depot/File1.txt#1 - add change 1 (text) //depot/file2.txt#1 - add change 1 (text) 5）用tag命令创建label2 （可以看到虽然label中view包含了depot2，但是由于我们本地的client中只包含depot下的文件，所以最终label2中也只有depot下的文件） C:\\>p4 -p localhost:1666 -u aaa -c aaa_test tag -l label2 //...#have //depot/File1.txt#1 - added //depot/file2.txt#1 - added C:\\>p4 -p localhost:1666 -u aaa -c aaa_test label -o label2 # A Perforce Label Specification. # # Label: The label name. # Update: The date this specification was last modified. # Access: The date of the last 'labelsync' on this label. # Owner: The user who created this label. # Description: A short description of the label (optional). # Options: Label update options: [un]locked, [no]autoreload. # Revision: Optional revision specification to make an automatic label. # View: Lines to select depot files for the label. # # Use 'p4 help label' to see more about label views. Label: label2 Update: 2012/10/20 11:30:43 Access: 2012/10/20 11:30:43 Owner: AAA Description: Created by AAA. Options: unlocked noautoreload View: //depot2/... //depot/... C:\\>p4 -p localhost:1666 -u aaa -c aaa_test files //...@label2 //depot/File1.txt#1 - add change 1 (text) //depot/file2.txt#1 - add change 1 (text) 6）修改clientspec包含depot2 C:\\>p4 -p localhost:1666 -u aaa -c aaa_test client Client AAA_Test not changed. C:\\>p4 -p localhost:1666 -u aaa -c aaa_test client -o Client: AAA_Test Update: 2012/10/20 11:31:47 Access: 2012/10/20 11:31:04 Owner: AAA Host: AAA-PC Description: Created by AAA. Root: D:/p4/AAA_Test Options: noallwrite noclobber nocompress unlocked nomodtime normdir SubmitOptions: submitunchanged LineEnd: local View: //depot/... //AAA_Test/depot/... //depot2/... //AAA_Test/depot2/... 7) 使用label1来sync代码 C:\\>p4 -p localhost:1666 -u aaa -c aaa_test sync -f //...@label1 //depot/File1.txt#1 - refreshing D:/p4/AAA_Test\\depot\\File1.txt //depot/file2.txt#1 - refreshing D:/p4/AAA_Test\\depot\\file2.txt 8）不使用label来sync代码 C:\\>p4 -p localhost:1666 -u aaa -c aaa_test sync -f //... //depot/File1.txt#1 - refreshing D:/p4/AAA_Test\\depot\\File1.txt //depot/file2.txt#1 - refreshing D:/p4/AAA_Test\\depot\\file2.txt //depot2/test1.txt#1 - added as D:/p4/AAA_Test\\depot2\\test1.txt //depot2/test2.txt#1 - added as D:/p4/AAA_Test\\depot2\\test2.txt 二 动态的label 动态的label需要创建的时候指定revision来指定版本，其实内部并没有记录具体的文件，只指定了版本(手动编辑Revision: @2字段）。不需要使用labelsync或tag来sync文件。 1）创建label3 （注意 Revision：@2 表示此label与changlist 2 关联） C:\\>p4 -p localhost:1666 -u aaa -c aaa_test label label3 Label label3 saved. C:\\>p4 -p localhost:1666 -u aaa -c aaa_test label -o label3 # A Perforce Label Specification. # # Label: The label name. # Update: The date this specification was last modified. # Access: The date of the last 'labelsync' on this label. # Owner: The user who created this label. # Description: A short description of the label (optional). # Options: Label update options: [un]locked, [no]autoreload. # Revision: Optional revision specification to make an automatic label. # View: Lines to select depot files for the label. # # Use 'p4 help label' to see more about label views. Label: label3 Update: 2012/10/20 11:46:23 Access: 2012/10/20 11:46:23 Owner: AAA Description: Created by AAA. Options: unlocked noautoreload Revision: @2 View: //depot2/... //depot/... 2）当前所有的chagnelist C:\\>p4 -p localhost:1666 -u aaa -c aaa_test changes Change 3 on 2012/10/20 by AAA@AAA_Test 'c ' Change 2 on 2012/10/20 by AAA@AAA_Test 'second ' Change 1 on 2012/10/20 by AAA@AAA_Test 'first ' 3）查看label3的使用 C:\\>p4 -p localhost:1666 -u aaa -c aaa_test files //...@label3 //depot/File1.txt#1 - add change 1 (text) //depot/file2.txt#1 - add change 1 (text) //depot2/test1.txt#1 - add change 2 (text) //depot2/test2.txt#1 - add change 2 (text) C:\\>p4 -p localhost:1666 -u aaa -c aaa_test sync -f //...@label3 //depot/File1.txt#1 - refreshing D:/p4/AAA_Test\\depot\\File1.txt //depot/file2.txt#1 - refreshing D:/p4/AAA_Test\\depot\\file2.txt //depot2/test1.txt#1 - updating D:/p4/AAA_Test\\depot2\\test1.txt //depot2/test2.txt#1 - refreshing D:/p4/AAA_Test\\depot2\\test2.txt 查看某个label中文件的最新版本： p4 changes -m 3 //...@your_label 查看某个label中所有的文件版本：p4 files //...@your_label 使用tag来跟新label ： p4 tag -l your_label //your_depot_path/...@changelist 使用label和labelsync来将本地client的所有文件的版本打进label： p4 label yourlable && p4 labelsync -l yourlabel //...#have 完！","tags":"中文"},{"url":"http://www.ciandcd.com/shu-zi-qian-ming-de-yan-zheng.html","title":"数字签名的验证","text":"From: http://www.cnblogs.com/itech/archive/2011/07/20/2111247.html#FeedBack 通常的我们在软件发布前要对binary（dll，exe）进行数字签名，数字签名可以标识软件的发布商，也可以通过数字签名来检查此软件是否被修改或受病毒影响。 在需要检查某个文件的数字签名时，通常情况下总是需要打开文件的属性对话框，切换到\"数字签名\"对话框，然后才能在这里查看到相关的数字签名信息，如下图： sigcheck是来自sysinternals的数字签名验证工具，可以查看指定的文件或目录下的哪些文件没有数字签名。 此工具是命令行工具，可以用来批量检查某个文件夹下相关文件的数字签名。 帮助如下： 使用如下： 对目录C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\Common7\\IDE及子目录下的所有的binaries检查数字签名，将没有签名的文件输出到sigcheck.txt中。 sigcheck -s -q -e -u \"C:\\Program Files (x86)\\Microsoft Visual Studio 10.0\\Common7\\IDE\" > sigcheck.txt 运行后的sigcheck.txt类似于如下： c:\\program files (x86)\\microsoft visual studio 10.0\\common7\\ide\\coloader80.tlb: Verified: Unsigned File date: 12:17 AM 3/18/2010 Publisher: Microsoft Corporation Description: VS Loader Type Library Product: Microsoft?Visual Studio?2010 Version: 10.0.30319.1 File version: 10.0.30319.1 built by: RTMRel c:\\program files (x86)\\microsoft visual studio 10.0\\common7\\ide\\devenv.com: Verified: Unsigned File date: 3:04 AM 3/18/2010 Publisher: Microsoft Corporation Description: Microsoft Visual Studio Command Line Product: Microsoft?Visual Studio?2010 Version: 10.0.30319.1 File version: 10.0.30319.1 built by: RTMRel c:\\program files (x86)\\microsoft visual studio 10.0\\common7\\ide\\PrivateAssemblies\\Microsoft.TeamFoundation.OfficeIntegration.Common.tlb: Verified: Unsigned File date: 4:28 AM 3/18/2010 Publisher: Microsoft Corporation Description: Microsoft.TeamFoundation.OfficeIntegration.Common.dll Product: Microsoft?Visual Studio?2010 Version: 10.0.30319.1 File version: 10.0.30319.1 built by: RTMRel c:\\program files (x86)\\microsoft visual studio 10.0\\common7\\ide\\PrivateAssemblies\\Microsoft.VisualStudio.OLE.Interop.dll: Verified: Unsigned File date: 6:22 PM 11/20/2009 Publisher: Microsoft Corporation Description: Product: Microsoft?Visual Studio .NET Version: 7.10.6070 File version: 7.10.6070 完！","tags":"中文"},{"url":"http://www.ciandcd.com/buildrelease-managementteam-city.html","title":"[BuildRelease Management]Team City","text":"From: http://www.cnblogs.com/itech/archive/2010/07/28/1786245.html#FeedBack 一 Team City 1）架构 server - agent 2）支持 3）主页 http://www.jetbrains.com/teamcity/index.html 二 运行 1）安装 下载免费专业版,支持3个agents，20个build配置和20个注册用户。 2）运行 在安装目录下运行C:\\TeamCity\\bin\\runall.bat start, 会启动server和agent。在IE中输入http://localhost，首次进入需要接收license协议和创建一个user。 3）首页上的入门视频： http://www.jetbrains.com/teamcity/documentation/demos/installation/TC40-install.mov http://confluence.jetbrains.net/display/TCD7/TeamCity+Documentation 三 总体感觉TeamCity与SCM和Builder集成度高，不灵活！ 完！","tags":"中文"},{"url":"http://www.ciandcd.com/buildrelease-managementccnetgai-gua.html","title":"[BuildRelease Management]CC.NET概括","text":"From: http://www.cnblogs.com/itech/archive/2010/08/03/1784184.html 一 CruiseControl.net CruiseControl.NET 是.NET平台上的自动化持续集成Server。它是Java版本的CruiseControl的CSharp版本。 1）运行过程如图 二 CruiseControl.net资源 1）主页 http://confluence.public.thoughtworks.org/display/CCNET/Welcome+to+CruiseControl.NET 2）下载 http://sourceforge.net/projects/ccnet/ 3）Build CC.NET的CC.NET http://ccnetlive.thoughtworks.com/ 3）可视化配置工具 http://ccnetconfig.codeplex.com/ 4）Server配置 http://confluence.public.thoughtworks.org/display/CCNET/Configuring+the+Server 完！","tags":"中文"},{"url":"http://www.ciandcd.com/buildreleasebuild-number-id.html","title":"[BuildRelease]build number / id","text":"From: http://www.cnblogs.com/itech/archive/2011/08/30/2159348.html#FeedBack build number， 也称为build id， 在build release的流程中唯一标示一个build，也是正式的产品的product version 和file version后两位（Major.minor.xxx.xxx）的来源，可以使用合适的方法将build number转化到product version和file version中。 build number可以为如下类型之一： 1）自增型，最简单的build number表示方式，例如1,2,3,4 。。。 2）week.day, 从软件的生命周期开始的周和天来表示，例如第五周星期二为5.2 3）date.time,通过日期加时间来表示，例如2010年5月5日5点5分5秒时05052010.050505 4) date.changelist, 使用日期加源代码版本的changelist信息，例如20121010.10000 5) interation.number， 使用interation加当前interation中的编号，例如第3个interation中的第5个build为3.5 一般地对于continous的build，只需要使用1）； 对于敏捷软件开发的，建议使用5），与敏捷开发的周期对应； 对于传统的软件开发的，如果产品是每周发布给QA，可以使用2）； 或者使用4）更好地显示跟源代码的关系； 完！","tags":"中文"},{"url":"http://www.ciandcd.com/jenkins-masterslavejia-gou.html","title":"Jenkins Master/Slave架构","text":"From: http://www.cnblogs.com/itech/archive/2011/11/11/2245849.html 一 Jenkins Master/Slave架构 Master/Slave相当于Server和agent的概念。Master提供web接口让用户来管理job和slave，job可以运行在master本机或者被分配到slave上运行。一个master可以关联多个slave用来为不同的job或相同的job的不同配置来服务。 当job被分配到slave上运行的时候，此时master和slave其实是建立的双向字节流的连接，其中连接方法主要有如下几种： 1）master通过ssh来启动slave Jenkins内置有ssh客户端实现，可以用来与远程的sshd通信，从而启动slave agent。这是对*unix系统的slave最方便的方法，因为*unix系统一般默认安装有sshd。在创建ssh连接的slave的时候，你需要提供slave的host名字，用户名和ssh证书。创建public/private keys，然后将public key拷贝到slave的~/.ssh/authorized_keys中，将private key 保存到master上某ppk文件中。jenkins将会自动地完成其他的配置工作，例如copy slave agent的binary，启动和停止slave。但是你的job运行所依赖其他的项目需要你自己设置。 2）master通过WMI+DCOM来启动windows slave 对于Windows的Slave，Jenkins可以使用Windows2000及以后内置的远程管理功能（WMI+DCOM），你只需要提供对slave有管理员访问权限的用户名和密码，jenkins将远程地创建windows service然后远程地启动和停止他们。 对于windows的系统，这是最方便的方法，但是此方法不允许运行有显示交互的GUI程序。 注意：不想其他类型的链接方式，此种方式slave（note）的名字非常重要，将被用来当做slave的地址访问slave。 3）实现自己的脚本来启动slave 如果上面成套的方法不够灵活，你可以实现自己的脚本来启动slave。你需要将启动脚本放到master，然后告诉jenkins master在需要的时候调用此脚本来启动slave。 典型地，你的脚本使用远程程序执行机制，例如SSH，RSH，或类似的方法（在windows，可以通过cygwin或psexec来完成）， 在脚本的最后需要执行类似java -jar slave.jar来启动slave。slave.jar可以从 http://yourjenkinsserver:port/jnlpjars/slave.jar 下载，也可以在脚本的开始先下载此slave.jar从而保证slave.jar正确的版本。 但是如果使用ssh slave plugin的话，此plugin将自动地更新slave.jar。 4）通过Java web start来启动slave jave web start（jnlp）是另一种启动slave的方法。用这种方法你需要登录到slave，打开浏览器，打开slave的配置页面来连接。还可以安装为windows service来使得slave在后台运行。 如果你需要运行的程序需要UI的交互，使用下面的方法：在slave系统上创建jenkins用户，设置自动登录，在系统的startup items增加slave JNLP文件的快捷方式，使得slave在系统登录的时候自动启动。 5）直接启动slave 此方式类似于java web start，可以方便地在*unix系统上将slave运行为daemon。需要配置slave为JNLP类型连接，然后在slave机器上执行 java -jar slave.jar -jnlpUrl http://yourserver:port/computer/slave-name/slave-agent.jnlp 二 Slave配置的好的建议 * 每个slave都有用户jenkins，所有的机器使用相同的UID和GID，使得slave的管理更加简单； * 每个机器上jenkins用户的home目录都相同/home/jenkins, 拥有相同的目录结构使得维护简单； * 所有的slave运行sshd，windows运行cygwin sshd； * 所有的slave安装ntp client，用来与相同的ntp server同步； * 使用脚本sh来自动地配置slave的环境，例如创建jenkins用户，安装sshd，安装java，ant，maven等； * 使用脚本来启动slave，保证slave总是运行在相同的参数下： #!/bin/bash JAVA_HOME=/opt/SUN/jdk1.6.0_04 PATH=$PATH:$JAVA_HOME/bin export PATH java -jar /var/jenkins/bin/slave.jar 完！","tags":"中文"},{"url":"http://www.ciandcd.com/cruisecontrolbinaryan-zhuang-he-qi-dong.html","title":"[CruiseControl]binary安装和启动","text":"From: http://www.cnblogs.com/itech/archive/2010/05/21/1740144.html#FeedBack 一CruiseControl安装 0）安装Java SDK，然后设置JAVA_HOME环境变量。 1）下载解压，例如2.8.3版本： http://sourceforge.net/projects/cruisecontrol/files/CruiseControl/2.8.3/ 。 2）使用cruisecontrol.bat 或 cruisecontrol.sh 来启动CruiseControl。 3）在 http://localhost:8080/cruisecontrol/ 查看project的log结果。 4）在 http://localhost:8080/dashboard 查看所有的project的report结果。 5）在 http://localhost:8080/documentation 可以查看本地版本的帮助文档。 6）在 http://localhost:8000/ 查看CruiseControl JMX console （ Java Management Extensions ， 用来管理和监视CruiseControl运行）。 二 CruiseControl安装后文件 1）如图 2）解释 *ant，对ant script进行支持； *etc/lib，包含jetty，jetty是免费的，用来提供web server和javax.servlet容器，而且支持Web Sockets, OSGi, JMX, JNDI, JASPI, AJP 和其他一些集成； *webapps，包含了build result JSP和dashborad等； *log/projects/artifacts,是使用cruisecontrol进行build时的log，source code，binary/master等目录； *Cruisecontrol.bat/cruisecontrol.sh是用来启动cruisecontrol的脚本； *config.xml/dashboard-config.xml包含了所有的cruisecontrol运行所需要的配置； 三 CruiseControl启动 cruisecontrol.bat REM Set this if you're using SSH-based CVS REM set CVS_RSH= REM Uncomment the following line if you have OutOfMemoryError errors REM set CC_OPTS=-Xms128m -Xmx256m REM The root of the CruiseControl directory. The key requirement is that this is the parent REM directory of CruiseControl's lib and dist directories. REM By default assume they are using the batch file from the local directory. REM Acknowledgments to Ant Project for this batch file incantation REM %~dp0 is name of current script under NT set CCDIR = %~dp0 :checkJava if not defined JAVA_HOME goto noJavaHome set JAVA_PATH = \" %JAVA_HOME%\\bin\\java \" set CRUISE_PATH = %JAVA_HOME% \\ lib \\ tools . jar goto setCruise :noJavaHome echo WARNING: You have not set the JAVA_HOME environment variable . Any tasks relying on the tools . jar file ( such as \" <javac> \" ) will not work properly . set JAVA_PATH = java :setCruise set LIBDIR = %CCDIR%lib set LAUNCHER = %LIBDIR% \\ cruisecontrol-launcher . jar set JETTY_LOGS = %CCIDR%logs set EXEC = %JAVA_PATH% %CC_OPTS% -Djavax . management . builder . initial = mx4j . server . MX4JMBeanServerBuilder \" -Djetty.logs=%JETTY_LOGS% \" -jar \" %LAUNCHER% \" %* -jmxport 8000 -webport 8080 -rmiport 1099 echo %EXEC% %EXEC% 1）%~dp0 为cruisecontrol.bat的目录。也即cruisecontrol解压以后的根目录。 2）必须安装JavaSDK和定义JAVA_HOME。 四 CruiseControl的手动启动 通过执行jar也可以启动CruiseControl，命令如下：java -jar lib/cruisecontrol-launcher.jar 如果你指定-jmxport和-rmiport或者其中之一，JMX server将会被启动，可以被用来控制CruiseControl（强制启动project builds，监视project的状态，改变log level，等等）。 如果你指定了-webport参数，web server将会被启动来运行CruiseControl reporting程序。 注意所有的参数都是可选的，如果你只是想使用config.xml来在当前工作目录启动CruiseControl，且不想运行JMX来控制CruiseControl，你不需要设置任何参数。 大部分用户指定-jmxport参数，因为JMX提供的html接口更方便。 五 CruiseControl启动选项 1）标准选项 -configfile filename 相对路径来指定CruiseControl的配置文件，默认为\"config.xml\". -dashboardurl url 指定dashboard的home page的地址。默认为 http://localhost:8080/dashboard . -postinterval [number] 指定build loop发布builds information到dashboard的间隔时间，单位为秒。默认为5秒。 -postenabled [true|false] 指定是否要build loop发布builds information到dashboard。默认为是。 -debug 改变CruiseControl内部的log4j的等级为debug。注意这里的修改不会导致你的ANT script的log等级为ant -debug，如果你需要请在config.xml中指定。 -help or -? 打印帮助信息。 -log4jconfig url 指定log4j的config的url。 2）web相关选项 （jetty的具体的设置，可以通过 /cruisecontrol-bin-2.8.3/etc目录下的jetty.xml来设置） -jettyxml filename Jetty配置文件。默认为jetty.xml。 -webport port Jetty的port number。Jetty不会启动除非指定此选项或-webapppath。默认地设置为8080。 -webapppath path cruisecontrol.war文件的路径. Defaults to ./webapps/cruisecontrol. -ccname name 与此CruiseControl关联的一个逻辑名字。此名字将被显示在reporting application的status page。 3）JMX相关选项 -agentutil [true|false] 指定是否load JMX build agent utility。默认为true。 -jmxport [port number] 指定JMX http adapter的port number。这将激活CruiseControl JMX的admin功能。如果不指定默认为8000，你可以通过 http://localhost:8000/ 来访问jmx，或者通过Control panel jsp tab 或 jsp reporting application来访问。 -rmiport [port number] 指定jmx rim adapter 和 connector server的port number。这将激活CruiseControl JMX的admin功能。如果不指定默认为1099，如果没有rmi registry运行在指定的port，则你的registry将被启动。 -xslpath directory 指定JMX http adapter使用的自定义XSLT文件的位置。正常地，CruiseControl使用安装默认的。 -user user id 指定JMX http adapter的user。指定后必须使用此用户来使用JMX web interface。 -password password 指定JMX的user的password。 完！","tags":"中文"},{"url":"http://www.ciandcd.com/buildrelease-managementccnetjia-gou.html","title":"[BuildRelease Management]CC.NET架构","text":"From: http://www.cnblogs.com/itech/archive/2010/10/23/1859165.html 一 CC.NET的操作流程 1) 等待Trigger的唤醒； 2）从Source Control System查询上次build以后的修改列表； 3）如果任何修改被发现或是Trigger触发类型为 'force the build' ： 3.1）为build产生一个label number； 3.2）按照prebuild tasks的指定顺序运行prebuild tasks，如果有error则终止build； 3.3）从Source Control System获得source code； 3.4）按照build tasks的指定顺序运行build tasks，如果有error则终止build； 3.5）如果需要对Source Control System的repository进行label； 3.6）运行publisher tasks； 4）返回1）重新循环； 执行过程的源代码，参考： \\trunk\\project\\core\\Project.cs \\trunk\\project\\core\\IntegrationRunner.cs 二 CC.NET的主要部件 三 CC.NET的扩展 可以使用如下方式对CC.NET的功能扩展： 1）PlugIn，是对build过程的自定义和扩展，需要实现ITask接口，需要在project的config文件中引用； 2）Extension，是对CC.NET本身的功能的扩展，需要实现ICruiseServerExtension接口，需要修改ccnet.exe.config或ccservice.exe.config来引用extension； 四 CC.NET扩展的主要接口 完！","tags":"中文"},{"url":"http://www.ciandcd.com/jenkinscha-jian-zhi-dashboardhe-wall-display.html","title":"Jenkins插件之Dashboard和wall display","text":"From: http://www.cnblogs.com/itech/archive/2011/11/22/2259044.html#FeedBack 一 dashboard插件 Dashboard View 用来自定义自己的主页，例如对下列的jenkins的主页 自定义dashboard来只显示自己感兴趣的job： 二 Wall display 用来将jobs的状态更加直观地显示在大屏幕上。 例如将view MyDashboard显示在大屏幕上如下： +在MyDashboard状态下点击wall display进入 完！","tags":"中文"},{"url":"http://www.ciandcd.com/buildrelease-managementhudsoncha-jian.html","title":"[BuildRelease Management]hudson插件","text":"From: http://www.cnblogs.com/itech/archive/2011/02/27/1966503.html SCP plugin — 该插件允许你使用 SFTP (SSH) 协议上传一些构件到仓库站点。 Violations — 该插件为 checkstyle 、 pmd 、 cpd 、 findbugs 、 fxcop 、 stylecop 和 simian 等静态代码分析工具生成报告。 Backlog Plugin Hudson 中集成 Backlog 该插件在中集成 MSTest Plugin — 该插件允许您发布 MSTest 的测试结果。 Crap4J Plugin Crap4J ( eclipse 插件 ) 读取 \" 无用方法 \" 报告。 Hudson 将生成无用百分率趋势报告，并提供有关如何更改它们的详细信息。 该插件从测试代码质量的插件读取无用方法报告。将生成无用百分率趋势报告，并提供有关如何更改它们的详细信息。 JIRA Plugin Atlassian JIRA Hudson 。 该插件整合 FindBugs Plugin FindBugs ( Bug 的 Eclipse 插件工具 ) 的分析结果，并以视图的方式呈现已发现的警告。 该插件主要收集项目模块中静态分析源代码中可能会出现的插件工具的分析结果，并以视图的方式呈现已发现的警告。 Trac Plugin Hudson 中集成 Edgewall Trac ( Wiki 以及软件开发过程中的问题跟踪系统，使用 Python 编写 ) 。 该插件在中集成增强版的以及软件开发过程中的问题跟踪系统，使用编写 Cppcheck Plugin CppCheck ( C/C++ 代码分析工具 ) 生成趋势报告。 该插件为静态的代码分析工具生成趋势报告。 java.net uploader Plugin java.net 任务库 Hudson 有能力发送构件到 java.net 。 该插件使用，以使有能力发送构件到 Plot Plugin — 该插件为 Hudson 提供通用的测绘 ( 或图表 ) 的能力。 Google Calendar Plugin — This plugin publishes build records over to Google Calendar Checkstyle Plugin Checkstyle ( ) 的分析结果，并以视图的方式呈现已发现的警告。 该插件主要收集项目模块中自动化代码检查工具的分析结果，并以视图的方式呈现已发现的警告。 JavaNCSS+Plugin JavaNCSS 该插件允许您使用构建报告工具。 SLOCCount Plugin SLOCCount 25 种不同的语言统计代码行的数量，包括 C/C++ 、 Ada 、 COBOL 、 Fortran 、 SQL 、 Ruby 、 Python 等等。 该插件能为生成趋势报告，它是一个开源程序，能为超过种不同的语言统计代码行的数量，包括等等。 JavaTest Report Plugin — 该插件把 JavaTest( 一个通过 Sun 公司发布的 TCK 应用的框架 ) 中解析成 XML 结果文件，并以此方式显示它们。 Emma Plugin Hudson 中集成 EMMA code coverage reports ( JAVA 代码覆盖率的开源工具 ) 。 该插件在中集成检测和报告代码覆盖率的开源工具 Warnings Plugin — This plugin generates the trend report for compiler warnings in the console log or in log files. Template Project Plugin — 该插件可以让您使用另一个项目中的构建人、发布人和 SCM 设置。 Japex Plugin Hudson 增加了 Japex Hudson 能够显示其趋势报告和其他有用的数据。 该插件为增加了支持，以使能够显示其趋势报告和其他有用的数据。 PMD Plugin PMD ( ) 的分析结果，并以视图的方式呈现已发现的警告。 该插件主要收集项目模块中程序代码检查工具的分析结果，并以视图的方式呈现已发现的警告。 SVN Publisher — This plugin allows you to upload artifacts to a subversion repository. This is done via a delete/import of the items requested. Task Scanner Plugin — 该插件为开放任务扫描工作区文件，并生成一个趋势报告。 Polarion Plugin Hudson 中集成 WebClient for SVN Web 的程序，通过 Polarion Subversion 的实现接口。 该插件在中集成，它是一个开源的、基于的程序，通过作为的实现接口。 IRC Plugin — 该插件在您选择的 IRC 频道中安装 Hudson IRC 机器人，您可以通过 IRC 获得通知，并通过 IRC 与 Hudson 互动。 Klaros-Testmanagement Plugin — Integrates Hudson with Klaros-Testmanagement by publishing the test results of a hudson build to the Klaros-Testmanagement application. JSUnit plugin — This plugin allows you publish JSUnit test results Clover Plugin Hudson 中集成 Clover code coverage reports ( ) 。 该插件是在中集成代码覆盖测试分析工具 Hudson will generate and track code coverage across time. This plugin can be used without the need to modify your build.xml. Cobertura Plugin Cobertura Hudson 将生成覆盖率趋势报告。 该插件允许您从中获取代码覆盖率报告。将生成覆盖率趋势报告。 xUnit Plugin — 该插件允许你发布测试工具的测试结果报告。 Jabber Plugin Jabber 即时消息协议集成在 Hudson 中。注意您也同样需要安装 。 即时消息协议集成在中。注意您也同样需要安装 instant-messaging plugin The Continuous Integration Game plugin — 该插件为使用者在哪些地方改善了构建 ( 作业 ) 的质量引进了一种策略。","tags":"中文"},{"url":"http://www.ciandcd.com/jenkinsde-pei-zhi.html","title":"Jenkins的配置","text":"From: http://www.cnblogs.com/itech/archive/2011/11/04/2236230.html 1 修改jenkins的根目录，默认地在C:\\Documents and Settings\\AAA\\.jenkins 。 .jenkins ├─jobs │ └─JavaHelloWorld │ ├─builds │ │ ├─2011-11-03_16-48-17 │ │ ├─2011-11-03_16-49-05 │ │ ├─2011-11-03_16-49-29 │ │ ├─2011-11-03_17-01-49 │ │ └─2011-11-03_17-11-42 │ └─workspace │ ├─build │ │ ├─classes │ │ │ └─oata │ │ └─jar │ └─src │ └─oata ├─plugins ├─usercontent ├─war 可以通过设置环境变量来修改，例如： set JENKINS_HOME=C:\\jenkins 然后重新启动jenkins。 2 备份和恢复jenkins 只需要备份JENKINS_HOME下的所有文件和文件夹，恢复的时候需要先停止jenkins。 3 移动，删除或修改jobs 对于移动或删除jobs，只需要简单地移动或删除%JENKINS_HOEM%\\jobs目录。 对于修改jobs的名字，只需要简单地修改%JENKINS_HOEM%\\jobs下对应job的文件夹的名字。 对于不经常使用的job，只需要对%JENKINS_HOEM%\\jobs下对应的jobs的目录zip或tar后存储到其他的地方。 4 可以在jenkins的url中执行一些命令来操作jenkins，如下 http://[jenkins-server]/[command] 命令可以为： exit shutdown jenkins restart restart jenkins reload to reload the configuration --httpPort=$HTTP_PORT，用来设置jenkins运行时的web端口。 --httpsPort=$HTTP_PORT，表示使用https协议。 --httpListenAddress=$HTTP_HOST，用来指定jenkins监听的ip范围，默认为所有的ip都可以访问此jenkins server。 5 Jenkins 启动时的命令行参数 6 修改jenkins的timezone 如果jenkins所在的server的timezone不同于用户的timezone，这时候需要修改jenkins的timezone，需要在jenkins启动的时候增加下列参数-Dorg.apache.commons.jelly.tags.fmt.timeZone=TZ 7 最好通过一个脚本来启动jenkins，确保jenkins每次都运行在相同的环境下，例如 startjenkins.bat set JENKINS_HOME=c:\\jenkins cd /d %JENKINS_HOME% java -jar %JENKINS_HOME%\\jenkins.war --httpPort=8000 8 jenkins在后台运行 如果jenkins是部署在servlet容器中，例如apache，tomcat中。因为servlet容器一般都在后台运行了，所以jenkins也就已经在后台运行了。 对于windows用户需要在jenkins的管理页面中点击insall as windows service来将jenkins部署为service。 但是感觉比较好的方法还是手动将启动jenkins的脚本部署为windows service，从而可以更灵活地设置更多的参数。 9 jenkins的系统信息 可以在jenkins的管理页面下的系统信息中，查看所有的jenkins的信息，例如jenkins的启动配置，所依赖的系统的环境变量，所安装的plugins。 10 jenkins内置的环境变量 BUILD_NUMBER， 唯一标识一次build，例如23； BUILD_ID，基本上等同于BUILD_NUMBER，但是是字符串，例如2011-11-15_16-06-21； JOB_NAME， job的名字，例如JavaHelloWorld； BUILD_TAG， 作用同BUILD_ID,BUILD_NUMBER,用来全局地唯一标识一此build，例如jenkins-JavaHelloWorld-23； EXECUTOR_NUMBER， 例如0； NODE_NAME，slave的名字，例如MyServer01； NODE_LABELS，slave的label，标识slave的用处，例如JavaHelloWorld MyServer01； JAVA_HOME， java的home目录，例如C:\\Program Files (x86)\\Java\\jdk1.7.0_01； HUDSON_URL = JENKINS_URL， jenkins的url，例如http://localhost:8000/ ； BUILD_URL，build的url 例如http://localhost:8000/job/JavaHelloWorld/23/； JOB_URL， job的url，例如http://localhost:8000/job/JavaHelloWorld/； SVN_REVISION，svn 的revison， 例如4； WORKSPACE，job的当前工作目录，例如c:\\jenkins\\workspace\\JavaHelloWorld； 完！","tags":"中文"},{"url":"http://www.ciandcd.com/jenkinscha-jian-zhi-huan-jing-bian-liang-cha-jian-envinject.html","title":"Jenkins插件之环境变量插件EnvInject","text":"From: http://www.cnblogs.com/itech/archive/2011/11/18/2254188.html#FeedBack 一 Master/Slave的Node Properties 用来定义slave特定的变量，例如很多的命令所在的路径。 二 job中的build parameter 设置后在build启动的时候提示修改也可以使用默认值。例如启动改build的时候决定是build release还是debug。 启动build时提示如下： 三 EnvInject插件 需要手动安装此插件，用来对job定义环境变量，还可以定义的ob的step来在build的过程中修改环境变量，例如为job定义公共的post location： 在job的step中修改变量，例如修改buildplatform的值： 四 运行结果如下： 参考： 完！","tags":"中文"},{"url":"http://www.ciandcd.com/jenkinsde-windows-slavede-pei-zhi.html","title":"Jenkins的Windows Slave的配置","text":"From: http://www.cnblogs.com/itech/archive/2011/11/09/2243025.html 参考： https://wiki.jenkins-ci.org/display/JENKINS/Step+by+step+guide+to+set+up+master+and+slave+machines 一 创建新的Slave 注意Jenkins中slave称为note。 所以下面文章中的slave和node指的是一回事。 1）在Manage Jenkins-->Manage Nodes -->New Node下：输入Node Name，且选择Dumb Slave作为Slave的类型，然后OK。 2）在Slave的配置页面，输入如下： *executors的数量，1或多个； *输入Slave 上的跟目录，例如c:\\jenkins； *Usage选择：Leave this machine for tied jobs only； *Lunch Method选择：Launch slave agents via Java Web Start ； * Avaliablitiy选择：Keep this slave online as much as possible； * 然后保存； 3）在slave所在的机器登录jenkins master，且进入Manage Jenkins-->Manage Nodes-->新建的Note，点击launch，然后安装slave为service如下： 4）安装成功后显示如下： 二 在slave上运行job 对上面的slave增加label，从而表示此slave的用处，且同时对uage选择leave this machine for tied jobs only： 对 Jenkins 构建JavaHelloWorld 中的job修改如下： 选择restrict where this project can be run 且输入note（slave）的label。 另外注意SVN的地址因该正确，jenkins会提示输入svn的用户名和密码。 此时job将会在slave所在的机器运行，当然build所需要的环境要在slave上配置好哦，运行如下： 注意： 对slave系统环境变量的修改，jenkins slave不会立即生效，需要重启jenkins slave service。 例如我在slave上装了ant，设置到path中后仍然找不到，需要restart jenkins slave service。 更多参考： https://wiki.jenkins-ci.org/display/JENKINS/Distributed+builds http://community.jboss.org/wiki/HudsonWindowsSlavesSetup 完！","tags":"中文"},{"url":"http://www.ciandcd.com/jenkinscha-jian-zhi-trigger.html","title":"Jenkins插件之trigger","text":"From: http://www.cnblogs.com/itech/archive/2011/11/17/2252647.html 一 Jenkins内置的trigger插件 1） build after other projects are built 可以设置多个依赖的jobs，当任意一个依赖的jobs成功后启动此build。 多个依赖的jobs间使用,隔开。 2） Trigger builds remotely (e.g., from scripts) 在Authentication Token中指定TOKEN_NAME，然后可以通过连接JENKINS_URL/job/JOBNAME/build?token=TOKEN_NAME来启动build。 3） build periodically 在schedule中设置，语法类似于cron中语法。 4） Poll SCM 在schedule中设置时间间隔来抓取SCM server，如果有新的修改，则启动build。 所以这里的作用相当于continous build。 二 其他的trigger插件 需要手动安装插件。 更多： https://wiki.jenkins-ci.org/display/JENKINS/Plugins","tags":"中文"},{"url":"http://www.ciandcd.com/perforce-serverde-linuxde-an-zhuang.html","title":"perforce server的Linux的安装","text":"From: http://www.cnblogs.com/itech/archive/2011/08/19/2146058.html perforce的Linux的安装 一 安装前配置 1) 机器配置 修改机器名 申请静态ip和dns上此ip到机器名的映射。 2) 用户配置 在root下创建用户组，用户和所在的home目录。 创建perforce group ： /usr/sbin/groupadd perforceg 创建perforce user： mkdir /local mkdir /local/perforce /usr/sbin/useradd -m -d /local/perforce -g perforceg perforce 修改perforce user的密码： /usr/bin/passwd perforce (输入123) 查看创建的用户perforce的信息： id perforce uid=504(perforce) gid=504(perforceg) groups=504(perforceg) 修改home目录的owner： chown perforce:perforceg /local/perforce 3) perforce server 的目录 su - perforce mkdir /local/perforce/p4root mkdir /local/perforce/log mkdir /local/perforce/journal [通常地p4root，log，journal需要挂载到不同的物理硬盘，来避免灾难事故的影响] 4）设置用户的su和sudo为不需要密码 参考： 设置su和sudo为不需要密码 二 下载和安装 0) 安装好后的文件和目录如下： -bash-3.2$ pwd /local/perforce -bash-3.2$ ls -l 总计 3120 drwxr-xr-x 2 perforce perforceg 4096 2011-02-15 Desktop drwxr-xr-x 2 perforce perforceg 4096 08-18 19:01 journal drwxr-xr-x 2 perforce perforceg 4096 08-18 18:51 log -rwxr-xr-x 1 perforce perforceg 777652 08-18 18:37 p4 -rwxr-xr-x 1 perforce perforceg 2381536 08-18 18:45 p4d drwxr-xr-x 2 perforce perforceg 4096 08-19 13:02 p4root -rwxr-xr-x 1 perforce perforceg 982 08-19 13:39 p4server001 -rw-r--r-- 1 perforce perforceg 366 08-19 13:28 p4server001.ini 1）安装 在perforce的download页面上找到P4D和p4的Linux下载文件。 chmod +x p4d chmod +x p4 将p4d和p4拷贝到/local/perforce下 2）p4d的配置文件 # This will be configure file for p4 server , # and it need be $ USERHOME /$ P4SERVERNAME . ini USER = perforce USERHOME =/ local / perforce P4USER = perforce P4PASSWD = 123 P4SERVERNAME = p4server001 p4 =$ USERHOME / p4 p4d =$ USERHOME / p4d P4PORT = 10.148 . 151.98 : 1666 P4ROOT =$ USERHOME / p4root JOURNAL =$ USERHOME / journal LOG =$ USERHOME / log / p4server001 . log TRACKLEVEL = 1 SERVERLEVEL = 2 3) 启动脚本/local/perforce/p4server001,将p4server001拷贝到/etc/init.d下，命令为cp /local/perforce/p4server001 /etc/init.d/。 /local/perforce/p4server001.ini3) 启动脚本/local/perforce/p4server001,将p4server001拷贝到/etc/init.d下，命令为cp /local/perforce/p4server001 /etc/init.d/。 参考：http://www.xueai.org/2010/11/110.html # ! / bin / bash # # chkconfig: 1235 80 80 # description: p4server001 daemon # PATH =/ sbin: / bin: / usr / bin: / usr / sbin # Source in the configs ... . / local / perforce / p4server001 . ini SU = \" su $USER -c \" LOCKFILE =$ USERHOME /$ P4SERVERNAME . lock start () { STARTCMD = \" $p4d -d -v server=$SERVERLEVEL,track=$TRACKLEVEL -p $P4PORT -r $P4ROOT -L $LOG -J $JOURNAL \" $ SU \" $STARTCMD \" 2 > & 1 touch $ LOCKFILE $ SU \" logger p4d starting \" echo \" p4d was started on \" `hostname -s` } stop () { STOPCMD = \" $p4 -p $P4PORT -u $P4USER admin stop \" echo $ P4PASSWD | $ p4 -p $ P4PORT -u $ P4USER login $ SU \" $STOPCMD \" 2 > & 1 rm $ LOCKFILE $ SU \" logger p4d stopping \" echo \" p4d was stopped on \" `hostname -s` } status () { if [ -f $ LOCKFILE ] ; then echo p4d status: running else echo p4d status: stopped fi } case \" $1 \" in ' start ' ) start ;; 'stop' ) stop ;; 'status' ) status ;; * ) echo \" Usage: p4server001 { start | stop | status } \" ;; esac 4） 启动p4d $su - perforce -bash-3.2$ /etc/init.d/p4server001 start Perforce Server starting... p4d was started on machinie 也可以使用命令来启动： /sbin/service p4server001 start 5） 查看 /etc/init.d/p4server001 status p4d status: running 或 ps -ef | grep p4d perforce 30898 1 0 14:04 ? 00:00:00 /local/perforce/p4d -d -v server=2,track=1 -p 10.148.151.98:1666 -r /local/perforce/p4root -L /local/perforce/log/p4server001.log -J /local/perforce/journal/journal ps -ef | grep p4d 至此可以在远程的机器上使用p4v或p4来访问此p4server了。 三 优化 1) 配置为daemon，当机器重启后能自动运行 使用chkconfig，脚本前必须有如下的行： #!/bin/bash # # chkconfig:1235 80 80 # description: p4server001 daemon # 安装chkconfig，命令为yum install chkconfig 将上面的p4server001配置为daemon，命令为： su - /sbin/chkconfig --add p4server001 安装后查看： /sbin/chkconfig --list | grep p4server001 p4server001 0:关闭 1:启用 2:启用 3:启用 4:关闭 5:启用 6:关闭 四 配置inetd 或xinetd， 使p4 server只有有request的时候才启动,需要使用p4d 的-i参数支持inetd，如果使用inetd或xinetd，则不需要前面的daemon的配置了，因为不需要p4d在机器启动的时候就启动。 http://kb.perforce.com/article/45/running-a-perforce-server-from-inetd-on-unix http://hints.macworld.com/article.php?story=20041112174525470 参考：http://kb.perforce.com/article/45/running-a-perforce-server-from-inetd-on-unix 在文件中/etc/services 中增加: p4server001 1666/tcp #netview-aix-6 1666/udp # netview-aix-6 p4server001 1666/tcp # Perforce Server #netview-aix-6 1666/tcp # netview-aix-6 然后创建 /etc/xinetd.d/p4server001 ，且内容为： service p4server001 { socket_type = stream wait = no user = perforce server = /local/perforce/p4d server_args = -i -L /local/perforce/log/p4server001.log -r /local/perforce/p4root -p 10.148.151.98:1888 -J /local/perforce/journal/journal disable = no } 可以在远程的机器上登录10.148.151.98:1888，然后在server上查看进程ps -ef | grep p4d，如下： perforce 11778 11776 0 19:20 ? 00:00:00 p4d -i -L /local/perforce/log/p4server001.log -r /local/perforce/p4root -p 10.148.151.98:1888 -J /local/perforce/journal/journal 注意： xinetd监听1666端口，然后当1666端口有请求时启动的p4d运行在1888端口。（这里不知道为啥想设置server运行在1666的时候老是起不来） 完！","tags":"中文"},{"url":"http://www.ciandcd.com/cruisecontrol-gai-nian.html","title":"[CruiseControl] 概念","text":"From: http://www.cnblogs.com/itech/archive/2010/05/20/1732704.html#FeedBack 一 CruiseControl CruiseControl既是一个Continous integration工具，也是一个创建自定义的Continous build process的框架。 cruisecontrol被设置高度可扩展，可以使用plugin对cruisecontrol的功能无限扩展。目前它已经包含非常多的plugins，例如与source control的交互， 各种build technologies的支持，email和im来提醒和通知用户。标准的CruiseControl的发布版就已经包含了大量的plugins（3rd party tools）。 CruiseControl使用Java实现，现在已经被用于大量的projects。CruiseControl支持非常多的builders，例如 Ant , NAnt , Maven , Phing , Rake , and Xcode , and exec 来执行所有的其他的命令行和脚本. CruiseControl是开源的且基于BSD-style的license发布。 同时CruiseControl也被翻译为了 .NET and Ruby 版本。 主页： http://cruisecontrol.sourceforge.net/ 二 CruiseControl组件 二 CruiseControl组件 CruiseControl由主要的3个模块组成： 1）Build Loop: 系统的核心，它触发build cycles和通过各种发布技术来将build结果通知所有的用户。触发的时机可以是定时地，或根据Source Control Management上的代码的改变来触发。它通过xml来配置build过程中的task，task是由对应的plugin来实现的。 2）Jsp Report: 允许用户浏览build的log和访问build的结果。 3）Dashboard: 对所有的project的状态提供更加可视化，直观的表现。 另外可以使用remote技术（HTTP，RMI）来控制和监视CruiseControl的Build Loop，但是默认处于安全的考虑是关闭的。 CruiseControl还可以使用distributed package来支持分布式build。 组件关系： 二 CruiseControl运行 1）CruiseControl 2）Continous Integration 完！","tags":"中文"},{"url":"http://www.ciandcd.com/buildreleasekua-ping-tai-gou-jian-gong-ju-scons.html","title":"[BuildRelease]跨平台构建工具SCons","text":"From: http://www.cnblogs.com/itech/archive/2011/09/14/2176243.html 一 SCons SCons是下一代的开源的软件构建工具。 主页： http://www.scons.org/ 参考： http://www.angelfire.com/linux/skip/Articles/SConsExamples.html 二 Scons特点 特点如下： 1）配置文件是python脚本（使用功能强大的真实的编程语言解决build的问题）。 2）内置的C/C++/Fortran的可信的，自动的依赖分析，不在需要make depend或make clean来得到所有的依赖关系。依赖关系分析很容易使用用户定义的dependency scanner扩展到其他的语言和文件类型。 3）内置的支持c/c++/d/java/fortran/yacc/lex/qt/swig和构建tex和latex文档。很容易通过用户定义的builder来支持其他的语言和文件类型。 4）build从源码管理工具（SVN。。。）或其他的build脚本（ant。。。）。 5）内置支持从源码管理工具fetch代码，例如sccs，rcs，cvs，bitkeeper和perforce。 6）内置支持所有的Microsoft VisualStudio的所有版本，包括产生dsp，dsw，sln和vcproj文件。 7）可信的使用MD5签名来检测build的changes，同时也支持配置为传统的时间戳来检测changes。 8）改进了并行build，像make -j，同时运行N个job，不决定于目录的继承结构。 9）集成像autoconf的find #include 文件，libraries，functions和typedefs。 10）所有的依赖的Global view。 11）能够share built files in a cache to speed up multiple builds, 像CCache但是不局限于C/C++。 12）从一开始就设计为跨平台，所以支持所有的系统（known to work on Linux, other POSIX systems (including AIX, *BSD systems, HP/UX, IRIX and Solaris), Windows NT, Mac OS X, and OS/2.）。 三 实例hello 1）文件： D:\\SCONS │ └─sample hello.c SConstruct 2）hello.c #include < stdio.h > int main() { printf( \" Hello World. \" ); } 3）SConstruct Program( ' hello.c ' ) 4）build 四 other 1）Builder： Program(),Library(),SharedLibrary(),Object(),Other Language Builders。program(''hello.c)。 2) Construction environment: 环境变量。env=Environment(CC = 'gcc', CCFLAGS = '-O2')。 3) SConstruct: 跟目录下的SConstruct file。 4）SConscript：子目录下的Sconscript file。 完！","tags":"中文"},{"url":"http://www.ciandcd.com/jenkinscha-jian-zhi-perforcefang-wen.html","title":"Jenkins插件之Perforce访问","text":"From: http://www.cnblogs.com/itech/archive/2011/11/15/2249723.html Perforce Plugin ，在Jenkins的管理页面的插件管理下面安装Perforce插件，然后重启Jenkins。 一 使用perforce插件来build 对job的设置如下图： job执行后的log如下： 可以看到Jenkins在执行的过程中创建了新的clientspec，新的clientspec是拷贝自上面参数workspace设置的clientspec，且修改了新的clientroot目录，其中的view是来自上面参数view->mapping中的设置。 如下： 二 使用perforce插件的poll功能来触发build 配置如下： 查看如下： 三 使用perforce插件在Jenkins中查看最新的修改 四 使用perforce的label功能来对成功的build进行label 五 使用perforce插件的自动label功能 更多的插件： https://wiki.jenkins-ci.org/display/JENKINS/Plugins 完！","tags":"中文"},{"url":"http://www.ciandcd.com/jenkinscha-jian-zhi-vshpere-cloud.html","title":"Jenkins插件之VShpere Cloud","text":"From: http://www.cnblogs.com/itech/archive/2011/11/21/2257038.html 如果我们使用VShpere 来管理所有的build机器，则使用VSphere Cloud 插件使得虚拟机的管理更加简单，且能够更好地利用VSphere的资源。 VShphere Cloud插件使得Jenkins可以控制VMWare VShpere中的虚拟机。可以配置Jenkins的slave为虚拟机，且可以指定snapshot的名字。Jenkins将自动地恢复到设置的snapshot，然后启动虚拟机作为slave来开始build。在build结束后Jenkins将自动地关闭slave，且恢复到指定的snapshot。 1）首先需要配置VShpere server，在Jenkins的Configure System中，如下： 用户需要启动关闭和恢复虚拟机的权限。 VShphere Cloud插件使得Jenkins可以控制VMWare VShpere中的虚拟机。可以配置Jenkins的slave为虚拟机，且可以指定snapshot的名字。Jenkins将自动地恢复到设置的snapshot，然后启动虚拟机作为slave来开始build。在build结束后Jenkins将自动地关闭slave，且恢复到指定的snapshot。1）首先需要配置VShpere server，在Jenkins的Configure System中，如下：用户需要启动关闭和恢复虚拟机的权限。 2） 创建新的slave来使用VSphere中的VM， + 完！","tags":"中文"},{"url":"http://www.ciandcd.com/chi-xu-ji-cheng-di-er-ban-lai-zi-martin-fowler.html","title":"持续集成（第二版）[来自：Martin Fowler]","text":"From: http://www.cnblogs.com/itech/archive/2010/07/12/1775785.html 持续集成（第二版） 作者： Martin Fowler 译者： 雷镇 持续集成 是一种软件开发实践。在持续集成中，团队成员频繁集成他们的工作成果，一般每人每天至少集成一次，也可以多次。每次集成会经过自动构建（包括自动测试）的 检验，以尽快发现集成错误。许多团队发现这种方法可以显著减少集成引起的问题，并可以加快团队合作软件开发的速度。这篇文章简要介绍了持续集成的技巧和它 最新的应用。 最后更改于： 2006年5月1日 目录 用持续集成构建特性 持续集成实践 只维护一个源码仓库 自动化 build 让你的build自行测试 每人每天都要向mainline提交代码 每次提交都应在集成计算机上重新 构建 mainline 保持快速 build 在模拟生产环境中进行测试 让每个人都能轻易获 得最新的可执行文件 每个人都能看到进度 自动化部署 持续集成的益处 引入持续集成 最后的思考 延伸阅读 我还可以生动记起第一次看到大型软件工程的情景。我当时在一家大型英国电子公司的QA部门实习。我的经理带我熟悉公司环境，我们进到一间巨大的，充满了压抑感和格子间的的仓库。我被告知这个项目已经 开发了好几年，现在正在集成阶段，并已经集成了好几个月。我的向导还告诉我没人知道集成要多久才能结束。从此我学到了软件开发的一个惯例：集成是一个很耗 时并难以预测的过程。但是事实并非总是如此，我的 ThoughWorks 同事所做的项目，以及很多其它遍布世界各地的软件项目，都不会把集成当回事。任何一个开发者本地的代码和项目共享基准代码的差别仅仅只有几小时的工作而已，而且这只要几分钟的时间就可以被集成回去。任何集成错误都可以很快被发现，并被快速修复。这鲜明的差别并非源于昂贵和复杂的工具。其中的精华蕴含于一个简单的实践：使用统一的代码仓库并频繁集成（通常每天一次）。 我还可以生动记起第一次看到大型软件工程的情景。我当时在一家大型英国电子公司的QA部门实习。我的经理带我熟悉公司环境，我们进到一间巨大的，充满了压抑感和格子间的的仓库。我被告知这个项目已经 开发了好几年，现在正在集成阶段，并已经集成了好几个月。我的向导还告诉我没人知道集成要多久才能结束。从此我学到了软件开发的一个惯例：集成是一个很耗 时并难以预测的过程。但是事实并非总是如此，我的 ThoughWorks 同事所做的项目，以及很多其它遍布世界各地的软件项目，都不会把集成当回事。任何一个开发者本地的代码和项目共享基准代码的差别仅仅只有几小时的工作而已，而且这只要几分钟的时间就可以被集成回去。任何集成错误都可以很快被发现，并被快速修复。这鲜明的差别并非源于昂贵和复杂的工具。其中的精华蕴含于一个简单的实践：使用统一的代码仓库并频繁集成（通常每天一次）。 当我向别人介绍持续集成方法时，人们通常会有 两种反应：\"这（在我们这儿）不管用\"和\"做了也不可能有什么不同\"。但如果他们真的试过了，就会发现持续集成其实比听起来要简单，并且能给开发过程带来 巨大的改变。因此第三种常见的反应是：\"我们就是这么做的，做开发怎可能不用它呢？\" \"持续集成\"一词来源于 极限编程（Extreme Programming），作为它的12个实践之一出现。当我开始在 ThoughWorks 开始顾问职业生涯时，我鼓励我所参与的项目使用这种技巧。Matthew Foemmel 将我抽象的指导思想转化为具体的行动。我们看到了项目从少而繁杂的集成进步到我所描述的不当回事。Metthew和我将我们的经验写进了这篇论文的第一版 里。这篇论文后来成了我网站里最受欢迎的文章之一。 尽管持续集成不需要什么特别的工具，我们发现使用一个持续集成服务器软件还是很有效果 的。最出名的持续集成服务器软件是 CruiseControl，这是一个开源工具，最早由 ThoughWorks 的几个人开发，现在由社区维护。之后还有许多其他持续集成服务器软件出现，有些是开源的，有些则是商业软件，比如 ThoughtWorks Studio 的 尽管持续集成不需要什么特别的工具，我们发现使用一个持续集成服务器软件还是很有效果 的。最出名的持续集成服务器软件是 CruiseControl，这是一个开源工具，最早由 ThoughWorks 的几个人开发，现在由社区维护。之后还有许多其他持续集成服务器软件出现，有些是开源的，有些则是商业软件，比如 ThoughtWorks Studio 的 Cruise 用持续集成构建特性 对我来说，解释持续集成最简单的方法就是用一个简单的例子来示范开发一个小 feature。现在假设我要完成一个软件的一部分功能，具体任务是什么并不重要，我们先假设这个 feature 很小，只用几个小时就可以完成。（我们稍后会研究更大的任务的情况。） 一开始，我将已集成的源代码复制一份到 本地计算机。这可以通过从源码管理系统的 mainline 上 check out 一份源代码做到。 如果 你用过任何源代码管理系统，理解上面的文字应该不成问题。但如果你没用过，可能会有读天书的感觉。所以我们先快速解释一个这些概念。源代码管理系统将项目 的所有源代码都保存在一个\"仓库（repository）\"中。系统的当前状态通常被称为\"mainline\"。开发者随时都可以把mainline复制 一份到他们自己的计算机，这个过程被称为\"check out\"。开发者计算机上的拷贝被称为\"工作拷贝（working copy）\"。（绝大部分情况下，你最终都会把工作拷贝的内容提交到mainline上去，所以两者实际上应该差不多。） 现在我拿到了工作拷贝，接下来需要做一些事情来完成任务。这包括修改产品代码和添加修改自动化测试。在持续集成中，软件应该包含完善的可自动运行 的测试，我称之为自测试代码。这一般需要用到某一个流行的 XUnit 测试框架。 一旦完成了修改，我就会在 自己的计算机上启动一个自动化 build。这会将我的工作拷贝中的源代码编译并链接成为一个可执行文件，并在之上运行自动化测试。只有当所有的 build 和测试都完成并没有任何错误时，这个 build 过程才可以认为是成功的。 当我 build 成功后，我就可以考虑将改动提交到源码仓库。但麻烦的情况在于别人可能已经在我之前修改过 mainline。这时我需要首先把别人的修改更新到我的工作拷贝中，再重新做 build。如果别人的代码和我的有冲突，就会在编译或测试的过程中引起错误。我有责任改正这些问题，并重复这一过程，直到我的工作拷贝能通过 build 并和 mainline 的代码同步。 一旦我本地的代码能通过 build，并和 mainline 同步，我就可以把我的修改提交到源码仓库。 然而，提交完代码不表示就完事大吉了。我们还要 做一遍集成 build，这次在集成计算机上并要基于 mainline 的代码。只有这次 build 成功了，我的修改才算告一段落。因为总有可能我忘了什么东西在自己的机器上而没有更新到源码仓库。只有我提交的改动被成功的集成了，我的工作才能结束。这 可以由我手工运行，也可以由 Cruise 自动运行。 如果两个开发者的修改存在冲突，这通常会被第二个人提 交代码前本地做 build 时发现。即使这时侥幸过关，接下来的集成 build 也会失败掉。不管怎样，错误都会被很快检测出来。此时首要的任务就是改正错误并让 build 恢复正常。在持续集成环境里，你必须尽可能快地修复每一个集成 build。好的团队应该每天都有多个成功的 build。错误的 build 可以出现，但必须尽快得到修复。 这样做的结果是你总能得到一个稳定的软件，它可能有一些 bug，但可以正常工作。每个人都基于相同的稳定代码进行开发，而且不会离得太远，否则就会不得不花很长时间集成回去。Bug被发现得越快，花在改正上的 时间就越短。 持续集成实践 从上面的故事我们大概了解了持续集 成是如何在我们的日常工作中发挥作用的。但让一切正常运行起来还需要掌握更多的知识。我接下来会集中讲解一下高效持续集成的关键实践。 只维护一个源码仓库 在软件项目里需要很多文件协调一致才能 build 出产品。跟踪所有这些文件是一项困难的工作，尤其是当有很多人一起工作时。所以，一点也不奇怪，软件开发者们这些年一直在研发这方面的工具。这些工具称为 源代码管理工具，或配置管理，或版本管理系统，或源码仓库，或各种其它名字。大部分开发项目中它们是不可分割的一部分。但可惜的是，并非所有项目都是如 此。虽然很罕见，但我确实参加过一些项目，它们直接把代码存到本地驱动器和共享目录中，乱得一塌糊涂。 所以， 作为一个最基本的要求，你必须有一个起码的源代码管理系统。成本不会是问题，因为有很多优秀的开源工具可用。当前较好的开源工具是 Subversion 。（更 老的同样开源的 CVS 仍被广泛使用，即使是 CVS 也比什么都不用强得多，但 Subversion 更先进也更强大。）有趣的是，我从与开发者们的交谈中了解到，很多商业源代码管理工具其实不比 Subversion 更好。只有一个商业软件是大家一致同意值得花钱的，这就是 Perforce 。 一旦你有了源代码管理系统，你要确保所有人都知道到哪里去取代码。不应出现这样的问题：\"我应该到哪里去找xxx文件？\" 所有东西都应该存在源码仓库里。 即便对于用了源码仓库的团队，我还是观察到一个很普遍的错误，就是他们没有把 所有东西都放在源码仓库里。一般人们都会把代码放进去，但还有许多其它文件，包括测试脚本，配置文件，数据库Schema，安装脚本，还有第三方的库，所 有这些build时需要的文件都应该放在源码仓库里。我知道一些项目甚至把编译器也放到源码仓库里（用来对付早年间那些莫名其妙的C++编译器很有效）。 一个基本原则是：你必须能够在一台干净的计算机上重做所有过程，包括checkout和完全build。只有极少量的软件需要被预装在这台干净机器上，通 常是那些又大又稳定，安装起来很复杂的软件，比如操作系统，Java开发环境，或数据库系统。 你必须把 build需要的所有文件都放进源代码管理系统，此外还要把人们工作需要的其他东西也放进去。IDE配置文件就很适合放进去，因为大家共享同样的IDE配 置可以让工作更简单。 版本控制系统的主要功能之一就是创建 branch 以管理开发流。这是个很有用的功能，甚至可以说是一个基础特性，但它却经常被滥用。你最好还是尽量少用 branch。一般有一个mainline就够 了，这是一条能反映项目当前开发状况的 branch。大部分情况下，大家都应该从mainline出发开始自己的工作。（合理的创建 branch 的 理由主要包括给已发布的产品做维护和临时性的实验。） 一般来说，你要把build依赖的所有文件放进代码管理 系统中，但不要放build的结果。有些人习惯把最终产品也都放进代码管理系统中，我认为这是一种坏味道——这意味着可能有一些深层次的问题，很可能是无 法可靠地重新build一个产品。 自动化build 通常来 说，由源代码转变成一个可运行的系统是一个复杂的过程，牵扯到编译，移动文件，将 schema 装载到数据库，诸如此类。但是，同软件开发中的其它类似任务一样，这也可以被自动化，也必须被自动化。要人工来键入各种奇怪的命令和点击各种对话框纯粹是 浪费时间，也容易滋生错误。 在大部分开发平台上都能找到自动化 build 环境的影子。比如 make，这在 Unix 社区已经用了几十年了，Java 社区也开发出了 Ant，.NET 社区以前用 Nant，现在用 MSBuild。不管你在什么平台上，都要确保只用一条命令就可以运行这些脚本，从而 build 并运行系统。 一 个常见的错误是没有把所有事都放进自动化 build。比如：Build 也应该包括从源码仓库中取出数据库 schema 并在执行环境中设置的过程。我要重申一下前面说过的原则：任何人都应该能从一个干净的计算机上 check out 源代码，然后敲入一条命令，就可以得到能在这台机器上运行的系统。 Build 脚本有很多不同的选择，依它们所属的平台和社区而定，但也没有什么定势。尽管大部分的 Java 项目都用 Ant，还是有一些项目用 Ruby（Ruby Rake 是一个不错的 build 脚本工具）。我们也曾经用 Ant 自动化早期的 Microsoft COM 项目，事实证明很有价值。 一个大型 build 通常会很耗时，如果只做了很小的修改，你不会想花时间去重复所有的步骤。所以一个好的 build 工具应该会分析哪些步骤可以跳过。一个通用的办法是比较源文件和目标文件的修改时间，并只编译那些较新的源文件。处理依赖关系要麻烦一些：如果一个目标文 件修改了，所有依赖它的部分都要重新生成。编译器可能会帮你处理这些事情，也可能不会。 根据你的需要，你可能 会想 build 出各种不同的东西。你可以同时 build 系统代码和测试代码，也可以只 build 系统代码。一些组件可以被单独 build。Build 脚本应该允许你在不同的情况中 build 不同的 target。 我们许多人都用 IDE，许多 IDE 都内置包含某种 build 管理功能。然而，相应的配置文件往往是这些 IDE 的专有格式，而且往往不够健壮，它们离了 IDE 就无法工作。如果只是 IDE 用户自己一个人开发的话，这还能够接受。但在团队里，一个工作于服务器上的主 build 环境和从其它脚本里运行的能力更重要。我们认为，在 Java 项目里，开发者可以用自己的 IDE 做 build，但主 build 必须用 Ant 来做，以保证它可以在开发服务器上运行。 让你的build自行测试 传统意义上的 build 指编译，链接，和一些其它能让程序运行起来的步骤。程序可以运行并不意味着它也工作正常。现代静态语言可以在编译时检测出许多 bug，但还是有更多的漏网之鱼。 一种又快又省的查 bug 的方法是在 build 过程中包含自动测试。当然，测试并非完美解决方案，但它确实能抓住很多 bug——多到可以让软件真正可用。极限编程（XP）和测试驱动开发（TDD）的出现很好地普及了自测试代码的概念，现在已经有很多人意识到了这种技巧的 价值。 经常读我的著作的读者都知道我是 TDD 和 XP 的坚定追随者。但是我想要强调你不需要这两者中任何一个就能享受自测试代码的好处。两者都要求你先写测试，再写代码以通过测试，在这种工作模式里测试更多 着重于探索设计而不是发现 bug。这绝对是一个好方法，但对于持续集成而言它并不必要，因为这里对自测试代码的要求没有那么高。（尽管我肯定会选择用 TDD 的方式。） 自测试代码需要包含一套自动化测试用例，这些测试用例可以检查大部分代码并找出 bug。测试要能够从一条简单的命令启动。测试结果必须能指出哪些测试失败了。对于包含测试的 build，测试失败必须导致 build 也失败。 在过去的几年里，TDD 的崛起普及了开源的 XUnit 系列工具 ，这些工具用作以上用途非常理想。对于我们在 ThoughWorks 工作的人来说，XUnit 工具已经证明了它们的价值。我总是建议人们使用它们。这些最早由 Kent Beck 发明的工具使得设置一个完全自测试环境的工作变得非常简单。 毋庸置疑，对于自动测试的工作而言，XUnit 工具只是一个起点。你还必须自己寻找其他更适合端对端测试的工具。现在有很多此类工具，包括 FIT ， Selenium ， Sahi ， Watir ， FITnesse ， 和许多其它我无法列在这里的工具。 当然你不能指望测试发现所有问题。就像人们经常说的：测试通过不能证明没有 bug。然而，完美并非是你要通过自测试 build 达到的唯一目标。经常运行不完美的测试要远远好过梦想着完美的测试，但实际什么也不做。 每人每天都要向 mainline 提交代码 集成的主要工作其实是沟 通。集成可以让开发者告诉其他人他们都改了什么东西。频繁的沟通可以让人们更快地了解变化。 让开发者提交到 mainline 的一个先决条件是他们必须能够正确地 build 他们的代码。这当然也包括通过 build 包含的测试。在每个提交迭代里，开发者首先更新他们的工作拷贝以与 mainline 一致，解决任何可能的冲突，然后在自己的机器上做 build。在 build 通过后，他们就可以随便向 mainline 提交了。 通过频繁重复上述过程，开发者可以发现 两个人之间的代码冲突。解决问题的关键是尽早发现问题。如果开发者每过几个小时就会提交一次，那冲突也会在出现的几个小时之内被发现，从这一点来说，因为 还没有做太多事，解决起来也容易。如果让冲突待上几个星期，它就会变得非常难解决。 因为你在更新工作拷贝时也 会做 build，这意味着你除了解决源代码冲突外也会检查编译冲突。因为 build 是自测试的，你也可以查出代码运行时的冲突。后者如果在一段较长的时间还没被查出的话会变得尤其麻烦。因为两次提交之间只有几个小时的修改，产生这些问题 只可能在很有限的几个地方。此外，因为没改太多东西，你还可以用 diff-debugging 的技巧来找 bug。 总的来说，我 的原则是每个开发者每天都必须提交代码。实践中，如果开发者提交的更为频繁效果也会更好。你提交的越多，你需要查找冲突错误的地方就越少，改起来也越快。 频繁提交客观上会鼓励开发者将工作分解成以小时计的小块。这可以帮助跟踪进度和让大家感受到进展。经常会有人一开始根 本无法找到可以在几小时内完成的像样的工作，但我们发现辅导和练习可以帮助他们学习其中的技巧。 每次提交都 应在集成计算机上重新构建 mainline 使用每日提交的策略后，团队就能得到很多经过测试的 build。这应该意味着 mainline 应该总是处于一种健康的状态。但在实践中，事情并非总是如此。一个原因跟纪律有关，人们没有严格遵守在提交之前在本地更新并做 build 的要求。另一个原因是开发者的计算机之间环境配置的不同。 结论是你必须保证日常的 build 发生在专用的集成计算机上，只有集成 build 成功了，提交的过程才算结束。本着\"谁提交，谁负责\"的原则，开发者必须监视 mainline 上的 build 以便失败时及时修复。一个推论是如果你在下班前提交了代码，那你在 mainline build 成功之前就不能回家。 我知道主要有两种方法可以使用：手动 build，或持续集成服务器软件。 手动 build 描述起来比较简单。基本上它跟提交代码之前在本地所做的那次 build 差不多。开发者登录到集成计算机，check out 出 mainline 上最新的源码（已包含最新的提交），并启动一个集成 build。他要留意 build 的进程，只有 build 成功了他的提交才算成功。（请查看 Jim Shore 的 描述 。） 持续集成服务器软件就像一个监视着源码仓库的监视器。每 次源码仓库中有新的提交，服务器就会自动 check out 出源代码并启动一次 build，并且把 build 的结果通知提交者。这种情况下，提交者的工作直到收到通知（通常是 email）才算结束。 在 ThoughtWorks，我们都是持续集成服务器软件的坚定支持者，实际上我们引领了 CruiseControl 和 CruiseControl.NET 最 早期的开发，两者都是被广泛使用的开源软件。此后，我们还做了商业版的 Cruise 持续集成服务器。我们几乎在每一个项目里都会用持续集成服务器，并且对结果非常满意。 不是每个人都会用持续集成服务器。Jim Shore 就 清楚地表达 了为什么他更偏好手动的办法。我同意他的看法中的持续集成并不仅仅是安装几个软件而已，所有的实践 都必须为了能让持续集成更有效率。但同样的，许多持续集成执行得很好的团队也会发现持续集成服务器是个很有用的工具。 许多组织根据安排好的日程表做例行 build，如每天晚上。这其实跟持续集成是两码事，而且做得远远不够。持续集成的最终目标就是要尽可能快地发现问题。Nightly build 意味着 bug 被发现之前可能会待上整整一天。一旦 bug 能在系统里呆这么久，找到并修复它们也会花较长的时间。 做好持续集成的一个关键因素是一旦 mainline 上的 build 失败了，它必须被马上修复。而在持续集成环境中工作最大的好处是，你总能在一个稳定的基础上做开发。mainline 上 build 失败并不总是坏事，但如果它经常出错，就意味着人们没有认真地在提交代码前先在本地更新代码和做 build。当 mainline 上 build 真的失败时，第一时间修复就成了头等大事。为了防止在 mainline 上的问题，你也可以考虑用 pending head 的方法。 当团队引入持续集成时，这通常是最难搞定的事情之一。在初期，团队会非常难以接 受频繁在 mainline 上做 build 的习惯，特别当他们工作在一个已存在的代码基础上时更是如此。但最后耐心和坚定不移的实践常常会起作用，所以不要气馁。 保持快速 build 持续集成的重点就是快速反馈。没有什么比缓慢的 build 更能危害持续集成活动。这里我必须承认一个奇思怪想的老家伙关于 build 快慢标准的的玩笑（译者注：原文如此，不知作者所指）。我的大部分同事认为超过1小时的 build 是不能忍受的。团队们都梦想着把 build 搞得飞快，但有时我们也确实会发现很难让它达到理想的速度。 对大多数项目来说，XP 的10分钟 build 的指导方针非常合理。我们现在做的大多数项目都能达到这个要求。这值得花些力气去做，因为你在这里省下的每一分钟都能体现在每个开发者每次提交的时候。持 续集成要求频繁提交，所以这积累下来能节省很多时间。如果你一开始就要花1小时的时间做 build，想加快这个过程会相当有挑战。即使在一个从头开始的新项目里，想让 build 始终保持快速也是很有挑战的。至少在企业应用里，我们发现常见的瓶颈出现在测试时，尤其当测试涉及到外部服务如数据库。 也许最关键的一步是开始使用分阶段build（staged build）。 分阶段 build （也被称作 build 生产线 ）的基本想法是多个 build 按一定顺序执行。向 mainline 提交代码会引发第一个 build，我称之为提交 build（commit build）。 提交 build 是当有人向 mainline 提交时引发的 build。提交 build 要足够快，因此它会跳过一些步骤，检测 bug 的能力也较弱。提交 build 是为了平衡质量检测和速度，因此一个好的提交 build 至少也要足够稳定以供他人基于此工作。 一旦提交 build 成功，其他人就可以放心地基于这些代码工作了。但别忘了你还有更多更慢的测试要做，可以另找一台计算机来运行运行这些测试。 一个简单的例子是两阶段 build。第一阶段会编译和运行一些本地测试，与数据库相关的单元测试会被完全隔离掉（stub out）。这些测试可以运行得非常快，符合我们的10分钟指导方针。但是所有跟大规模交互，尤其是真正的数据库交互的 bug 都无法被发现。第二阶段的 build 运行一组不同的测试，这些测试会调用真正的数据库并涉及更多的端到端的行为。这些测试会跑上好几小时。 这种情况下，人们用第一阶段作为提交 build，并把这作为主要的持续集成工作。第二阶段 build 是 次级build ，只有 在需要的时候才运行，从最后一次成功的提交 build 中取出可执行文件作进一步测试。如果次级 build 失败了，大家不会立刻停下手中所有工作去修复，但团队也要在保证提交 build 正常运行的同时尽快修正 bug。实际上次级 build 并非一定要正常运行，只要 bug 都能够被检查出来并且能尽快得到解决就好。在两阶段 build 的例子里，次级 build 经常只是纯粹的测试，因为通常只是测试拖慢了速度。 如果次级 build 检查到了 bug，这是一个信号，意味着提交 build 需要添加一个新测试了。你应该尽可能把次级 build 失败过的测试用例都添加到提交 build 中，使得提交 build 有能力验证这些 bug。每当有 bug 绕过提交测试，提交测试总能通过这种方法被加强。有时候确实无法找到测试速度和 bug 验证兼顾的方法，你不得不决定把这个测试放回到次级 build 里。但大部分情况下都应该可以找到合适加入提交 build 的测试。 上面这个例子是关于两阶段 build，但基本原则可以被推广到任意数量的后阶段 build。提交 build 之后的其它 build 都可以同时进行，所以如果你的次级测试要两小时才能完成，你可以通过用两台机器各运行一半测试来快一点拿到结果。通过这个并行次级 build 技巧，你可以向日常 build 流程中引入包括性能测试在内的各种自动化测试。（当我过去几年内参加 Thoughtworks 的各种项目时，我碰到了很多有趣的技巧，我希望能够说服一些开发者把这些经验写出来。） 在模拟生产环境中进 行测试 测试的关键在于在受控条件下找出系统内可能在实际生产中出现的任何问题。这里一个明显的因素是生产系 统的运行环境。如果你不在生产环境做测试，所有环境差异都是风险，可能最终造成测试环境中运行正常的软件在生产环境中无法正常运行。 自然你会想到建立一个与生产环境尽可能完全相同的测试环境。用相同的数据库软件，还要同一个版本；用相同版本的操作系统；把所有生产环 境用到的库文件都放进测试环境中，即使你的系统没有真正用到它们；使用相同的IP地址和端口；以及相同的硬件； 好 吧，现实中还是有很多限制的。如果你在写一个桌面应用软件，想要模拟所有型号的装有不同第三方软件的台式机来测试显然是不现实的。类似的，有些生产环境可 能因为过于昂贵而无法复制（尽管我常碰到出于经济考虑拒绝复制不算太贵的环境，结果得不偿失的例子）。即使有这些限制，你的目标仍然是尽可能地复制生产环 境，并且要理解并接受因测试环境和生产环境不同带来的风险。 如果你的安装步骤足够简单，无需太多交互，你也许 能在一个模拟生产环境里运行提交 build。但事实上系统经常反应缓慢或不够稳定，这可以用 test double 来解决。结果常常是提交测试为了速度原因在一个假环境内运行，而次级测试运行在模拟真实的生产环境中。 我注意到越来越多人用虚拟化来搭建测试环境。虚拟机的状态可以被保存，因此安装并测试最新版本的build相对简单。此外，这可以让你 在一台机器上运行多个测试，或在一台机器上模拟网络里的多台主机。随着虚拟化性能的提升，这种选择看起来越来越可行。 让每个人都能轻易获得最新的可执行文件 软件开发中最困难的部分是确定你的软件行为符合预期。我们 发现事先清楚并正确描述需求非常困难。对人们而言，在一个有缺陷的东西上指出需要修改的地方要容易得多。敏捷开发过程认可这种行为，并从中受益。 为了以这种方式工作，项目中的每个人都应该能拿到最新的可执行文件并运行。目的可以为了 demo，也可以为了探索性测试，或者只是为了看看这周有什么进展。 这做起来其实相当简单：只要找到一个大家 都知道的地方来放置可执行文件即可。可以同时保存多份可执行文件以备使用。每次放进去的可执行文件应该要通过提交测试，提交测试越健壮，可执行文件就会越 稳定。 如果你采用的过程是一个足够好的迭代过程，把每次迭代中最后一个 build 放进去通常是明智的决定。Demo 是一个特例，被 demo 的软件特性都应该是演示者熟悉的特性。为了 demo 的效果值得牺牲掉最新的 build，转而找一个早一点但演示者更熟悉的版本。 每个人都能看到进度 持续集成中最重要的是沟通。你需要保证每个人都能轻易看到系统的状态和最新的修改。 沟通的最重要的途 径之一是 mainline build。如果你用 Cruise，一个内建的网站会告诉你是否正有 build 在进行，和最近一次 mainline build 的状态。许多团队喜欢把一个持续工作的状态显示设备连接到 build 系统来让这个过程更加引人注目，最受欢迎的显示设备是灯光，绿灯闪亮表示 build 成功，红灯表示失败。一种常见的选择是红色和绿色的 熔岩灯 ，这不仅仅指示 build 的状态，还能指示它停留在这个状态的时间长短，红灯里出现气泡表示 build 出问题已经太长时间了。每一个团队都会选择他们自己的 build 传感器。如果你的选择带点幽默性和娱乐性效果会更好（最近我看到有人在实验跳舞兔）。 即使你在使用手动持续集 成，可见程度依然很重要。Build 计算机的显示器可以用来显示 mainline build 的状态。你很可能需要一个 build 令牌放在正在做 build 那人的桌子上（橡皮鸡这种看上去傻傻的东西最好，原因同上）。有时人们会想在 build 成功时弄出一点噪音来，比如摇铃的声音。 持续集成服务器软件的网页可以承载更多信息。Cruise 不仅显示谁在做 build，还能指出他们都改了什么。Cruise 还提供了一个历史修改记录，以便团队成员能够对最近项目里的情况有所了解。我知道 team leader喜欢用这个功能了解大家手头的工作和追踪系统的更改。 使用网站的另一大优点是便于那些 远程工作的人了解项目的状态。一般来说，我倾向于让项目中发挥作用的成员都坐在一起工作，但通常也会有一些外围人员想要了解项目的动态。如果组织想要把多 个项目的 build情况聚合起来以提供自动更新的简单状态时，这也会很有用。 好的信息展示方式不仅仅依赖于 电脑显示器。我最喜欢的方式出现于一个中途转入持续集成的项目。很长时间它都无法拿出一个稳定的 build。我们在墙上贴了一整年的日历，每一天都是一个小方块。每一天如果 QA 团队收到了一个能通过提交测试的稳定 build，他们都会贴一张绿色的贴纸，否则就是红色的贴纸。日积月累，从日历上能看出 build 过程在稳定地进步。直到绿色的小方块已经占据了大部分的空间时，日历被撤掉了，因为它的使命已经完成了。 自 动化部署 自动化集成需要多个环境，一个运行提交测试，一个或多个运行次级测试。每天在这些环境之间频繁拷贝 可执行文件可不轻松，自动化是一个更好的方案。为实现自动化，你必须有几个帮你将应用轻松部署到各个环境中的脚本。有了脚本之后，自然而然的结果是你也要 用类似的方式部署到生产环境中。你可能不需要每天都部署到生产环境（尽管我见过这么做的项目），但自动化能够加快速度并减少错误。它的代价也很低，因为它 基本上和你部署到测试环境是一回事。 如果你部署到生产环境，你需要多考虑一件事情：自动化回滚。坏事情随时可 能发生，如果情况不妙，最好的办法是尽快回到上一个已知的正常状态。能够自动回滚也会减轻部署的压力，从而鼓励人们更频繁地部署，使得新功能更快发布给用 户。（Ruby on Rails 社区开发了一个名为 Capistrano 的工具，是这类工具很好的代表。） 我还在服 务器集群环境中见过滚动部署的方法，新软件每次被部署到一个节点上，在几小时时间内逐步替换掉原有的软件。 参见相关文章： 进 化式数据库设计 许多人在频繁发布时都遇到的一个障碍是数据库的迁移。数据库的改动很麻烦，因为你不能仅仅修改 schema，你还要保证数据本身也被顺利迁移。这篇文章描述了我的同事 Pramod Sadalage 自动化重构和迁移数据库的技巧。这篇文章只是一个简单的记录，其中的信息在 Pramod 和 Scott Amblers 关于数据库重构的书中有更详细的阐述[ambler-sadalage]。 在 web 应用开发中，我碰到的一个有趣的想法是把一个试验性的 build 部署到用户的一个子集。团队可以观察这个试验 build 被使用的情况，以决定是否将它部署到全体用户。你可以在做出最终决定之前试验新的功能和新的 UI。自动化部署加上良好的持续集成的纪律是这项工作的基础。 持续集成的益处 我认为持续集成最显著也最广泛的益处是降低风险。说到这里，我的脑海中还是会浮现出第一段描述的早期软件项目。他们已经到了一个漫长项 目的末期（至少他们期望如此），但还是不知道距离真正的结束有多远。 延迟集成的问题在于时间难以估计，你甚至 无法得知你的进展。结果是你在项目最紧张的阶段之一把自己置入了一个盲区，此时即使没有拖延（这很罕见）也轻松不了多少。 持续集成巧妙的解决了这个问题。长时间的集成不再存在，盲区被彻底消除了。在任何时间你都知道你自己的进展，什么能运转，什么不能运转，你系统里 有什么明显的 bug，这些都一目了然。 Bug 让人恶心，它摧毁人的自信，搞乱时间表，还破坏团队形象。已部署软件里的 bug 招致用户的怒气。未完成软件里的 bug 让你接下来的开发工作受阻。 持续集成不能防止 bug 的产生，但它能明显让寻找和修改 bug 的工作变简单。从这个方面看，它更像自测试代码。如果你引入 bug 后能很快发现，改正也会简单得多。因为你只改了系统中很小的一部分，你无需看很多代码就能找到问题所在。因为这一小部分你刚刚改过，你的记忆还很新鲜，也 会让找 bug 的工作简单不少。你还可以用 差异调试 ——比较当前版本和之前没有 bug 的版本。 Bug 也会积累。你的 bug 越多，解决掉任何一个都会越困难。这部分原因是 bug 之间的互相作用，你看到的失败实际上是多个问题叠加的结果，这使得检查其中任何一个问题都更加困难。还有部分原因是心理层面的因素，当人们面对大量 bug 时，他们寻找和解决 bug 的动力就会减弱。《Pragmatic Programmer》一书中称之为\" 破窗综合症 \"。 使用持续集成的项目的通常结果是 bug 数目明显更少，不管在产品里还是开发过程中都是如此。然而，我必须强调，你受益的程度跟你测试的完善程度直接相关。其实建立测试系统并非想象中那么困难， 但带来的区别却显而易见。一般来说，团队需要花一定时间才能把 bug 数量减少到理想的地步。做到这一点意味着不断添加和改进测试代码。 如果你用了持续集成，你就解决了频繁部署的最大障碍之一。频繁部署很有价值，因为它可以让你的用户尽快用到新功能，从而快速提供反馈， 这样他们在开发过程中可以有更多的互动。这可以帮助打破我心目中成功的软件开发最大的障碍——客户与开发团队之间的障碍。 引入持续集成 看到这里，你一定想要尝试一下持续集成了。但是从哪里开始呢？我在上面描述了一整套 的实践，这些可以让你体验到所有的好处。但你也不必一开始就照单全收，你有自己的选择余地，基本上取决于你的环境和团队的特性。我们也从过去的实践中吸取 了一些经验和教训，做好下面这些事会对于你的持续集成运作有重要的意义。 最早的几步之一是实现 build 自动化。把所有需要的东西都放进版本控制系统里，这样你就可以用一条命令 build 整个系统。这对许多项目而言不是什么小任务，这对于其他东西正常工作非常重要。刚开始你可能只是偶尔需要的时候做一个 build，或者只是做一个自动的 nightly build。当你还没有开始持续集成时，自动 nightly build 也是一个不错的开始。 其次是引入一些自动化测试到你的 build 中。试着指出主要出错的地方，并要让自动化测试暴露这些错误。建立又快又好的测试集合会比较困难，特别在已存在的项目中，建立测试需要时间。你必须找个地 方开始动手，就像俗话说的，罗马不是一天建成的。 还要试着加快提交 build 的速度。虽然需要几个小时 build 的持续集成也比什么都没有好，但能做到传说中的10分钟会更好。这通常要对你的代码动一些大手术，以剥离对运行缓慢那部分的依赖。 如果你刚刚开始一个新项目，从一开始就用持续集成。对build时间保持关注，当慢于10分钟时就立即采取行动。通过快速行动，你可以 在代码变得太大之前做一些必要的架构调整。 比所有事情都重要的是寻找帮助。找一个以前做过持续集成的人来帮 你。像所有新技巧一样，当你不知道最终结果怎样的时候会非常难以实施。请一个导师（mentor）可能会花些钱，但如果你不做，你会付出时间和生产效率损 失的代价。（免责声明/广告：是的，我们 ThoughtWorks 在这个领域提供咨询服务。不管怎样，我们曾经犯过你可能会犯的大多数错误。） 最后的思考 在我和 Matt 写完最初那篇论文后的几年，持续集成已经成为了软件开发的一个主流方法。ThoughtWorks 的项目很少有不用到它的。我们也能看到世界各地的人们在用持续集成。与极限编程中一些充满争议的实践不同，我很少听到关于持续集成的负面消息。 如果你还没用持续集成，我强烈建议你试一下。如果你已经在做了，可能这篇文章中的某些方法可以帮你得到更好的效果。过 去几年中，我们已经了解了很多关于持续集成的知识，我希望还有更多的知识可以让我们学习和提高。 延伸阅读 本文篇幅所限，只能覆盖部分内容。想了解持续集成的更多细节，我建议看一下 Paul Duvall 的 书 （这本书得到了Jolt大奖）。目前为止还没有多少关于分阶段build的文章，但有一篇 Dave Farley 发表在 《ThoughtWorks 文选》 中的文章还不错（你也可以在 这里 找到）。","tags":"中文"},{"url":"http://www.ciandcd.com/ji-yu-yun-duan-de-kai-fa-ping-tai-team-foundation-service.html","title":"基于云端的开发平台Team Foundation Service","text":"From: http://www.cnblogs.com/itech/archive/2013/05/08/3068023.html Team Foundation Service 是微软提供的云端的开发平台。 主页： http://tfs.visualstudio.com 。 类似于github，与github的区别是： 1） github创建private的project是需要是付费用户， Team Foundation Service提供5个用户以内免费创建private的project； 2）Team Foundation Service的源码管理server可以是Team Foundation Server或者是git，Team Foundation Service 提供更好的与VisualStudio的集成，但是如果是使用git作为源码server的话，也可以支持其他的工具和平台，例如eclipse，MAC OS等； 3） Team Foundation Service与github都提供了源码管理和defects的管理，但是Team Foundation Server提供了更多的开发flow的支持，例如agile和scrum等支持，将来还会提供build/test的持续集成的支持； 如果单纯地只是创建opensource的源码托管的话，可以选择： 1） github （支持github） 2） googlecode （支持svn和hg） 3）codeplex （支持Team Foundation Server和git） 4）sourceforge （好久没有用了，只改是支持svn） 平时使用visualstudio进行开发的，或者几个人接个私活一起做project的时候，可以试下Team Foundation Service。 我注册的用来测试的： https://itech001.visualstudio.com 完！","tags":"中文"},{"url":"http://www.ciandcd.com/cruisecontrolzai-windowsshang-pei-zhi-shi-li.html","title":"[CruiseControl]在Windows上配置实例","text":"From: http://www.cnblogs.com/itech/archive/2010/07/19/1780762.html#FeedBack 一 CruiseControl安装 1) 安装JAVA SDK，设置JAVA_HOME环境变量，且将%JAVA_HOME%\\bin加入path环境变量。 2) 下载CruiseControl，例如2.8.3版本为cruisecontrol-bin-2.8.3.zip，解压到C:\\CruiseControl。 3) CruiseControl的binary安装中已经包含了ant builder / jetty（提供http server and jmx 对webapps下的cruisecontrol[result/report JSP page] 和 dashboard支持） ,安装后如下： 二 配置 以CruiseControl自带实例connectfour为例，配置文件如下： 1）config.xml < cruisecontrol > < project name =\"connectfour\" > < listeners > < currentbuildstatuslistener file =\"logs/${project.name}/status.txt\" /> </ listeners > < bootstrappers > < antbootstrapper anthome =\"apache-ant-1.7.0\" buildfile =\"projects/${project.name}/build.xml\" target =\"clean\" /> </ bootstrappers > < modificationset quietperiod =\"30\" > <!-- touch any file in connectfour project to trigger a build --> < filesystem folder =\"projects/${project.name}\" /> </ modificationset > < schedule interval =\"300\" > < ant anthome =\"apache-ant-1.7.0\" buildfile =\"projects/${project.name}/build.xml\" /> </ schedule > < log > < merge dir =\"projects/${project.name}/target/test-results\" /> </ log > < publishers > < onsuccess > < artifactspublisher dest =\"artifacts/${project.name}\" file =\"projects/${project.name}/target/${project.name}.jar\" /> </ onsuccess > </ publishers > </ project > </ cruisecontrol > 2）dashboard-config.xml <? xml version=\"1.0\" encoding=\"UTF-8\" ?> < dashboard > < buildloop logsdir =\"\" artifactsdir =\"\" /> < features allowforcebuild =\"\" /> < trackingtool projectname =\"\" baseurl =\"\" keywords =\"\" /> < subtabs > < subtab class =\"net.sourceforge.cruisecontrol.dashboard.widgets.ErrorsAndWarningsMessagesWidget\" /> </ subtabs > </ dashboard > 三 运行 1) 运行CruiseControl.bat来启动CruiseControl， REM CruiseControl.bat REM Set this if you're using SSH-based CVS REM set CVS_RSH= REM Uncomment the following line if you have OutOfMemoryError errors REM set CC_OPTS=-Xms128m -Xmx256m REM The root of the CruiseControl directory. The key requirement is that this is the parent REM directory of CruiseControl's lib and dist directories. REM By default assume they are using the batch file from the local directory. REM Acknowledgments to Ant Project for this batch file incantation REM %~dp0 is name of current script under NT set CCDIR = %~dp0 :checkJava if not defined JAVA_HOME goto noJavaHome set JAVA_PATH = \" %JAVA_HOME%\\bin\\java \" set CRUISE_PATH = %JAVA_HOME% \\ lib \\ tools . jar goto setCruise :noJavaHome echo WARNING: You have not set the JAVA_HOME environment variable . Any tasks relying on the tools . jar file ( such as \" <javac> \" ) will not work properly . set JAVA_PATH = java :setCruise set LIBDIR = %CCDIR%lib set LAUNCHER = %LIBDIR% \\ cruisecontrol-launcher . jar set JETTY_LOGS = %CCIDR%logs set EXEC = %JAVA_PATH% %CC_OPTS% -Djavax . management . builder . initial = mx4j . server . MX4JMBeanServerBuilder \" -Djetty.logs=%JETTY_LOGS% \" -jar \" %LAUNCHER% \" %* -jmxport 8000 -webport 8080 -rmiport 1099 echo %EXEC% %EXEC% 2) CruiseControl的启动log 2010 - 07 - 19 04 : 42 : 51 , 712 [main ] INFO Main - CruiseControl Version 2.8 . 3 Compiled on January 24 2010 2134 2010 - 07 - 19 04 : 42 : 51 , 805 [main ] INFO XMLConfigManager - reading settings from config file [C: \\ CruiseControl \\ config . xml] 2010 - 07 - 19 04 : 42 : 52 , 446 [main ] INFO CruiseControlController - projectName = [connectfour] 2010 - 07 - 19 04 : 42 : 52 , 446 [main ] INFO XMLConfigManager - using settings from config file [C: \\ CruiseControl \\ config . xml] 2010 - 07 - 19 04 : 42 : 52 , 477 [main ] WARN ProjectConfig - No previously serialized project found [C: \\ CruiseControl \\ connectfour . ser] , forcing a build . 2010 - 07 - 19 04 : 42 : 52 , 477 [main ] INFO Project - Project connectfour starting 2010 - 07 - 19 04 : 42 : 52 , 477 [main ] INFO Project - Project connectfour: idle 2010 - 07 - 19 04 : 42 : 52 , 509 [Project connectfour thread] INFO Project - Project connectfour started 2010 - 07 - 19 04 : 42 : 52 , 540 [Project connectfour thread] INFO Project - Project connectfour: in build queue 2010 - 07 - 19 04 : 42 : 53 , 665 [main ] INFO CruiseControlControllerAgent - Starting HttpAdaptor with CC-Stylesheets 2010 - 07 - 19 04 : 42 : 53 , 743 [main ] INFO CruiseControlControllerAgent - starting httpAdaptor 2010 - 07 - 19 04 : 42 : 54 , 071 [main ] INFO CruiseControlControllerAgent - starting rmiRegistry 2010 - 07 - 19 04 : 42 : 56 , 384 [main ] INFO CruiseControlControllerAgent - starting connectorServer 2010 - 07 - 19 04 : 43 : 18 , 555 [BuildQueueThread] INFO BuildQueue - BuildQueue started 2010 - 07 - 19 04 : 43 : 18 , 555 [BuildQueueThread] INFO BuildQueue - now adding to the thread queue: connectfour 2010 - 07 - 19 04 : 43 : 18 , 618 [Thread- 21 ] INFO Project - Project connectfour: bootstrapping 2010 - 07 - 19 04 : 43 : 18 , 665 [Thread- 21 ] INFO ProjectController - connectfour Controller: build progress event: bootstrapping 2010 - 07 - 19 04 : 43 : 23 , 102 [Thread- 23 ] INFO ScriptRunner - Buildfile: projects \\ connectfour \\ build . xml 2010 - 07 - 19 04 : 43 : 23 , 321 [Thread- 23 ] INFO ScriptRunner - 2010 - 07 - 19 04 : 43 : 23 , 321 [Thread- 23 ] INFO ScriptRunner - clean: 2010 - 07 - 19 04 : 43 : 23 , 462 [Thread- 23 ] INFO ScriptRunner - [delete] Deleting directory C: \\ CruiseControl \\ projects \\ connectfour \\ target 2010 - 07 - 19 04 : 43 : 23 , 993 [Thread- 23 ] INFO ScriptRunner - 2010 - 07 - 19 04 : 43 : 23 , 993 [Thread- 23 ] INFO ScriptRunner - BUILD SUCCESSFUL 2010 - 07 - 19 04 : 43 : 23 , 993 [Thread- 23 ] INFO ScriptRunner - Total time : 0 seconds 2010 - 07 - 19 04 : 43 : 24 , 196 [Thread- 21 ] INFO AntBootstrapper - Bootstrap successful . 2010 - 07 - 19 04 : 43 : 24 , 212 [Thread- 21 ] INFO Project - Project connectfour: checking for modifications 2010 - 07 - 19 04 : 43 : 24 , 212 [Thread- 21 ] INFO ProjectController - connectfour Controller: build progress event: checking for modifications 2010 - 07 - 19 04 : 43 : 24 , 509 [Thread- 21 ] INFO Project - Project connectfour: No modifications found , build not necessary . 2010 - 07 - 19 04 : 43 : 24 , 509 [Thread- 21 ] INFO Project - Project connectfour: Building anyway , since build was explicitly forced . 2010 - 07 - 19 04 : 43 : 24 , 509 [Thread- 21 ] INFO Project - Project connectfour: now building 2010 - 07 - 19 04 : 43 : 24 , 509 [Thread- 21 ] INFO ProjectController - connectfour Controller: build progress event: now building 2010 - 07 - 19 04 : 43 : 25 , 305 [Thread- 25 ] INFO ScriptRunner - Buildfile: projects \\ connectfour \\ build . xml 2010 - 07 - 19 04 : 43 : 25 , 540 [Thread- 25 ] INFO ScriptRunner - ccAntProgress -- clean 2010 - 07 - 19 04 : 43 : 25 , 602 [Thread- 25 ] INFO ScriptRunner - ccAntProgress -- compile 2010 - 07 - 19 04 : 43 : 25 , 665 [Thread- 25 ] INFO ScriptRunner - [ mkdir ] Created dir : C: \\ CruiseControl \\ projects \\ connectfour \\ target \\ classes 2010 - 07 - 19 04 : 43 : 26 , 446 [Thread- 25 ] INFO ScriptRunner - [javac] Compiling 10 source files to C: \\ CruiseControl \\ projects \\ connectfour \\ target \\ classes 2010 - 07 - 19 04 : 43 : 34 , 118 [Thread- 25 ] INFO ScriptRunner - ccAntProgress -- sleep 2010 - 07 - 19 04 : 43 : 34 , 149 [Thread- 25 ] INFO ScriptRunner - [ echo ] Sleeping for a while so you can see the build in the new dashboard 2010 - 07 - 19 04 : 44 : 34 , 805 [Thread- 25 ] INFO ScriptRunner - ccAntProgress -- test 2010 - 07 - 19 04 : 44 : 35 , 555 [Thread- 25 ] INFO ScriptRunner - [ mkdir ] Created dir : C: \\ CruiseControl \\ projects \\ connectfour \\ target \\ test-classes 2010 - 07 - 19 04 : 44 : 40 , 290 [Thread- 25 ] INFO ScriptRunner - [javac] Compiling 2 source files to C: \\ CruiseControl \\ projects \\ connectfour \\ target \\ test-classes 2010 - 07 - 19 04 : 45 : 08 , 384 [Thread- 25 ] INFO ScriptRunner - [ mkdir ] Created dir : C: \\ CruiseControl \\ projects \\ connectfour \\ target \\ test-results 2010 - 07 - 19 04 : 45 : 10 , 243 [Thread- 25 ] INFO ScriptRunner - [junit] Running net . sourceforge . cruisecontrol . sampleproject . connectfour . CellTest 2010 - 07 - 19 04 : 45 : 10 , 243 [Thread- 25 ] INFO ScriptRunner - [junit] Testsuite: net . sourceforge . cruisecontrol . sampleproject . connectfour . CellTest 2010 - 07 - 19 04 : 45 : 14 , 196 [Thread- 25 ] INFO ScriptRunner - [junit] Tests run : 1 , Failures: 0 , Errors: 0 , Time elapsed: 3.953 sec 2010 - 07 - 19 04 : 45 : 14 , 196 [Thread- 25 ] INFO ScriptRunner - [junit] Tests run : 1 , Failures: 0 , Errors: 0 , Time elapsed: 3.953 sec 2010 - 07 - 19 04 : 45 : 14 , 212 [Thread- 25 ] INFO ScriptRunner - [junit] 2010 - 07 - 19 04 : 45 : 14 , 805 [Thread- 25 ] INFO ScriptRunner - [junit] Running net . sourceforge . cruisecontrol . sampleproject . connectfour . PlayingStandTest 2010 - 07 - 19 04 : 45 : 14 , 805 [Thread- 25 ] INFO ScriptRunner - [junit] Testsuite: net . sourceforge . cruisecontrol . sampleproject . connectfour . PlayingStandTest 2010 - 07 - 19 04 : 45 : 14 , 837 [Thread- 25 ] INFO ScriptRunner - [junit] Tests run : 10 , Failures: 0 , Errors: 0 , Time elapsed: 0.032 sec 2010 - 07 - 19 04 : 45 : 14 , 837 [Thread- 25 ] INFO ScriptRunner - [junit] Tests run : 10 , Failures: 0 , Errors: 0 , Time elapsed: 0.032 sec 2010 - 07 - 19 04 : 45 : 14 , 837 [Thread- 25 ] INFO ScriptRunner - [junit] 2010 - 07 - 19 04 : 45 : 14 , 852 [Thread- 25 ] INFO ScriptRunner - ccAntProgress -- jar 2010 - 07 - 19 04 : 45 : 15 , 149 [Thread- 25 ] INFO ScriptRunner - [jar] Building jar: C: \\ CruiseControl \\ projects \\ connectfour \\ target \\ connectfour . jar 2010 - 07 - 19 04 : 45 : 15 , 430 [Thread- 25 ] INFO ScriptRunner - ccAntProgress -- all 2010 - 07 - 19 04 : 45 : 15 , 430 [Thread- 25 ] INFO ScriptRunner - 2010 - 07 - 19 04 : 45 : 15 , 446 [Thread- 25 ] INFO ScriptRunner - BUILD SUCCESSFUL 2010 - 07 - 19 04 : 45 : 15 , 446 [Thread- 25 ] INFO ScriptRunner - Total time : 1 minute 49 seconds 2010 - 07 - 19 04 : 45 : 16 , 259 [Thread- 21 ] INFO ProjectController - connectfour Controller: build result event: build successful 2010 - 07 - 19 04 : 45 : 16 , 259 [Thread- 21 ] INFO Project - Project connectfour: merging accumulated log files 2010 - 07 - 19 04 : 45 : 16 , 274 [Thread- 21 ] INFO ProjectController - connectfour Controller: build progress event: merging accumulated log files 2010 - 07 - 19 04 : 45 : 16 , 415 [Thread- 21 ] INFO Project - Project connectfour: build successful 2010 - 07 - 19 04 : 45 : 16 , 493 [Thread- 21 ] INFO Project - Project connectfour: publishing build results 2010 - 07 - 19 04 : 45 : 16 , 493 [Thread- 21 ] INFO ProjectController - connectfour Controller: build progress event: publishing build results 2010 - 07 - 19 04 : 45 : 17 , 009 [Thread- 21 ] INFO Project - Project connectfour: idle 2010 - 07 - 19 04 : 45 : 17 , 009 [Thread- 21 ] INFO ProjectController - connectfour Controller: build progress event: idle 2010 - 07 - 19 04 : 45 : 17 , 087 [Project connectfour thread] INFO Project - Project connectfour: next build in 5 minutes 2010 - 07 - 19 04 : 45 : 17 , 087 [Project connectfour thread] INFO Project - Project connectfour: waiting for next time to build 2010 - 07 - 19 04 : 45 : 17 , 087 [Project connectfour thread] INFO ProjectController - connectfour Controller: build progress event: waiting for next time to build 3）connectfour的ant的log Buildfile: projects \\ connectfour \\ build . xml ccAntProgress -- clean ccAntProgress -- compile [ mkdir ] Created dir : C: \\ CruiseControl \\ projects \\ connectfour \\ target \\ classes [javac] Compiling 10 source files to C: \\ CruiseControl \\ projects \\ connectfour \\ target \\ classes ccAntProgress -- sleep [ echo ] Sleeping for a while so you can see the build in the new dashboard ccAntProgress -- test [ mkdir ] Created dir : C: \\ CruiseControl \\ projects \\ connectfour \\ target \\ test-classes [javac] Compiling 2 source files to C: \\ CruiseControl \\ projects \\ connectfour \\ target \\ test-classes [ mkdir ] Created dir : C: \\ CruiseControl \\ projects \\ connectfour \\ target \\ test-results [junit] Running net . sourceforge . cruisecontrol . sampleproject . connectfour . CellTest [junit] Testsuite: net . sourceforge . cruisecontrol . sampleproject . connectfour . CellTest [junit] Tests run : 1 , Failures: 0 , Errors: 0 , Time elapsed: 3.953 sec [junit] Tests run : 1 , Failures: 0 , Errors: 0 , Time elapsed: 3.953 sec [junit] [junit] Running net . sourceforge . cruisecontrol . sampleproject . connectfour . PlayingStandTest [junit] Testsuite: net . sourceforge . cruisecontrol . sampleproject . connectfour . PlayingStandTest [junit] Tests run : 10 , Failures: 0 , Errors: 0 , Time elapsed: 0.032 sec [junit] Tests run : 10 , Failures: 0 , Errors: 0 , Time elapsed: 0.032 sec [junit] ccAntProgress -- jar [jar] Building jar: C: \\ CruiseControl \\ projects \\ connectfour \\ target \\ connectfour . jar ccAntProgress -- all BUILD SUCCESSFUL Total time : 1 minute 49 seconds 4）build后通过curisecontrol page（report JSP）查看结果 5）build后通过dashborad来查看结果 6）build后通过dashboard的build tab来查看build统计 7）通过JMX来管理 完！","tags":"中文"},{"url":"http://www.ciandcd.com/perforce-pythonde-shi-yong.html","title":"perforce python的使用","text":"From: http://www.cnblogs.com/itech/archive/2011/09/14/2176455.html#FeedBack 一 p4的API支持几乎所有的常用的语言 1）查看p4 API 主页： http://www.perforce.com/perforce/loadsupp.html#api （包含下载 和 p4script.pdf） 2）支持几乎所有的语言： Perforce C/C++ API Perforce Java API Perforce Perl API Perforce Ruby API Perforce Python API Perforce PHP API Perforce Objective-C API P4COM, a COM Interface to the Perforce C++ API for Windows 二 p4python 1）安装（以windows为例） 下载和安装python2.6；（可以安装activepython） 下载和安装p4python26；（ ftp://ftp.perforce.com/perforce/r09.2/bin.ntx86/p4python26.exe ） 2）安装后（C:\\Python26\\Lib\\site-packages） 文件：P4.py + P4.pyc + P4API.pyd + P4Python-2009.2-py2.6.egg-info。其中P4API.pyd为python对p4的extension模块，里面定义了 P4API.P4Adapter可惜看不到源码，P4.py 里面包含了p4 class的定义，p4class从P4API.P4Adapter继承。 3）p4.py 在p4.py中定义了 • P4 • P4Exception • DepotFile • Revision • Integration • MergeData • Spec 其中主要的p4 class定义如下： import P4API class P4(P4API.P4Adapter): \"\"\" Use this class to communicate with a Perforce server Instances of P4 will use the environment settings (including P4CONFIG) to determine the connection parameters such as P4CLIENT and P4PORT. This attributes can also be set separately before connecting. To run any Perforce commands, users of this class first need to run the connect() method. It is good practice to disconnect() after the program is complete. \"\"\" # Constants useful for exception_level # RAISE_ALL: Errors and Warnings are raised as exceptions (default) # RAISE_ERROR: Only Errors are raised as exceptions # RAISE_NONE: No exceptions are raised, instead False is returned RAISE_ALL = 2 RAISE_ERROR = 1 RAISE_NONE = 0 def __init__ (self, * args, ** kwlist): P4API.P4Adapter. __init__ (self, * args, ** kwlist) def __del__ (self): if self.debug > 3 : print >> sys.stderr, \" P4.__del__() \" # store the references to the created lambdas as a weakref to allow Python # to clean up the garbage. |The lambda as a closure stores a reference to self # which causes a circular reference problem without the weakref def __getattr__ (self, name): if name.startswith( \" run_ \" ): cmd = name[len( \" run_ \" ):] return lambda * args: self.run(cmd, * args) elif name.startswith( \" delete_ \" ): cmd = name[len( \" delete_ \" ):] return lambda * args: self.run(cmd, \" -d \" , * args) elif name.startswith( \" fetch_ \" ): cmd = name[len( \" fetch_ \" ):] return lambda * args: self.run(cmd, \" -o \" , * args)[0] elif name.startswith( \" save_ \" ): cmd = name[len( \" save_ \" ):] return lambda * args: self. __save (cmd, * args) elif name.startswith( \" parse_ \" ): cmd = name[len( \" parse_ \" ):] return lambda * args: self.parse_spec(cmd, * args) elif name.startswith( \" format_ \" ): cmd = name[len( \" format_ \" ):] return lambda * args: self.format_spec(cmd, * args) else : raise AttributeError, name def __save (self, cmd, * args): self.input = args[0] return self.run(cmd, \" -i \" , args[ 1 :]) def __repr__ (self): state = \" disconnected \" if self.connected(): state = \" connected \" return \" P4 [%s@%s %s] %s \" % \\ (self.user, self.client, self.port, state) def identify(cls): return P4API.identify() identify = classmethod(identify) def run(self, * args): \" Generic run method \" return P4API.P4Adapter.run(self, * self. __flatten (args)) def run_submit(self, * args): \" Simplified submit - if any arguments is a dict, assume it to be the changeform \" nargs = list(args) form = None for n, arg in enumerate(nargs): if isinstance( arg, dict): self.input = arg nargs.pop(n) nargs.append( \" -i \" ) break return self.run( \" submit \" , * nargs) def run_login(self, * args): \" Simple interface to make login easier \" self.input = self.password return self.run( \" login \" , * args) def run_password( self, oldpass, newpass ): \" Simple interface to allow setting of the password \" if ( oldpass and len(oldpass) > 0 ): self.input = [ oldpass, newpass, newpass ] else : self.input = [ newpass, newpass ] return self.run( \" password \" ) # # run_filelog: convert \"p4 filelog\" responses into objects with useful # methods # # Requires tagged output to be of any real use. If tagged output it not # enabled then you just get the raw data back # def run_filelog( self, * args ): raw = self.run( ' filelog ' , args ) if ( not self.tagged): # untagged mode returns simple strings, which breaks the code below return raw result = [] for h in raw: r = None if isinstance( h, dict ): df = DepotFile( h[ \" depotFile \" ] ) for n, rev in enumerate( h[ \" rev \" ]): # Create a new revision of this file ready for populating r = df.new_revision() # Populate the base attributes of each revision r.rev = int( rev ) r.change = int( h[ \" change \" ][ n ] ) r.action = h[ \" action \" ][ n ] r.type = h[ \" type \" ][ n ] r.time = datetime.datetime.utcfromtimestamp( int( h[ \" time \" ][ n ]) ) r.user = h[ \" user \" ][ n ] r.client = h[ \" client \" ][ n ] r.desc = h[ \" desc \" ][ n ] if \" digest \" in h: r.digest = h[ \" digest \" ][ n ] if \" fileSize \" in h: r.fileSize = h[ \" fileSize \" ][ n ] # Now if there are any integration records for this revision, # add them in too if ( not \" how \" in h) or (n >= len(h[ \" how \" ]) or h[ \" how \" ][n] == None): continue else : for m, how in enumerate( h[ \" how \" ][ n ] ): file = h[ \" file \" ][ n ][ m ] srev = string.lstrip(h[ \" srev \" ][ n ][ m ], ' # ' ) erev = string.lstrip(h[ \" erev \" ][ n ][ m ], ' # ' ) if srev == \" none \" : srev = 0 else : srev = int( srev ) if erev == \" none \" : erev = 0 else : erev = int( erev ) r.integration( how, file, srev, erev ) else : r = h result.append( df ) return result def run_print(self, * args): raw = self.run( ' print ' , args) result = [raw.pop(0), \"\" ] for line in raw: result[ - 1 ] += line return result def run_resolve(self, * args, ** kargs): myResolver = Resolver() if \" resolver \" in kargs: myResolver = kargs[ \" resolver \" ] savedResolver = self.resolver self.resolver = myResolver result = self.run( \" resolve \" , args) self.resolver = savedResolver return result def __flatten (self, args): result = [] if isinstance(args, tuple) or isinstance(args, list): for i in args: result.extend(self. __flatten (i)) else : result.append(args) return tuple(result) def __enter__ ( self ): return self def __exit__ ( self, exc_type, exc_val, exc_tb ): if self.connected(): self.disconnect() return True def connect( self ): P4API.P4Adapter.connect( self ) return self @contextmanager def while_tagged( self, t ): old = self.tagged self.tagged = t yield self.tagged = old @contextmanager def at_exception_level( self, e ): old = self.exception_level self.exception_level = e yield self.exception_level = old @contextmanager def saved_context( self , ** kargs): \"\"\" Saves the context of this p4 object and restores it again at the end of the block \"\"\" saved_context = {} for attr in self. __members__ : saved_context[attr] = getattr(self, attr) for (k,v) in kargs.items(): setattr( self, k, v) yield # now restore the context again. Ignore AttributeError exception # Exception is expected because some attributes only have getters, no setters for (k,v) in saved_context.items(): try : setattr( self, k, v ) except AttributeError: pass # expected for server_level and p4config_file 三 实例 1）p4info.py from P4 import P4,P4Exception p4 = P4() p4.port = \" localhost:1666 \" p4.user = \" AAA \" p4.password = \" aaa \" p4.client = \" TestProject_AAA \" try : p4.connect() info = p4.run( \" info \" ) for key in info[0]: print key, \" = \" , info[0][key] p4.disconnect() except P4Exception: for e in p4.errors: print e 结果： 2）p4sync.py from P4 import P4, P4Exception p4 = P4() p4.port = \" localhost:1666 \" p4.user = \" AAA \" p4.password = \" aaa \" p4.client = \" TestProject_AAA \" try : p4.connect() p4.exception_level = 1 # ignore \"File(s) up-to-date\" files = p4.run_sync() for file in files: for key in file.keys(): print key, \" : \" ,file[key] print \" ---------- \" except P4Exception: for e in p4.errors: print e finally : p4.disconnect() 结果： 3）p4submit.py from P4 import P4 p4 = P4() p4.host = \" localhost \" p4.port = \" 1666 \" p4.user = \" AAA \" p4.password = \" aaa \" p4.client = \" TestProject_AAA \" p4.connect() changeSpec = p4.run_change( \" -o \" ) change = changeSpec[0] change[ \" Description \" ] = \" Autosubmitted changelist \" p4.run_edit( \" //depot/TestProject/addfile.txt \" ) change[ \" Files \" ] = [ \" //depot/TestProject/addfile.txt \" ] p4.input = change p4.run_submit( \" -i \" ) p4.disconnect() 所有的p4 command 都可以使用p4.run_command()或p4.run(command)来调用。 四 超级详细的帮助文档： http://www.perforce.com/perforce/doc.current/manuals/p4script/03_python.html 完！","tags":"中文"},{"url":"http://www.ciandcd.com/jenkinscha-jian-zhi-deploy.html","title":"Jenkins插件之Deploy","text":"From: http://www.cnblogs.com/itech/archive/2011/11/21/2257487.html#FeedBack deploy插件： Deploy Plugin deploy插件支持将War/Jar部署到远程的应用服务器上，例如Tomcat,JBoss,Glassfish。 正在寻找或开发.NET web 应用的自动发布插件。 如何回滚或重新部署先前的build： 0） 需要被deploy的job的结果要存档，例如JavaHelloWorld的设置如下： 1） 安装Copy Artifact Plugin； 2） 创建一个job，在需要的时候手动启动，new job -> build a free-style software project，例如创建DeployJavaHelloWorld来deploy JavaHelloWorld的结果； 3） 配置job，且build参数的类型为\"Build selector for Copy Artifact\", 且copy artifact build 步骤使用\"Specified by build parameter\"来选择build； + 4） 增加post-build action来deploy 从其他的job拷贝来的artifact； 现在你可以启动job然后输入build number来选择redploy那个build。 完！","tags":"中文"},{"url":"http://www.ciandcd.com/p4-add-mu-lu.html","title":"p4 add 目录","text":"From: http://www.cnblogs.com/itech/archive/2013/04/08/3008971.html#FeedBack 貌似p4 add ...挺好用的。 我们知道p4中没有对目录的版本控制，在p4 add的时候只能是文件，所以如果想一次把一个目录add到p4上，需要以下方法： Linux/unix 中p4 add files： find . -type f -print | p4 -x - add Linux/unix 中p4 add symbollinks： find . -type l -print | p4 -x - add -t symlink 增加除了目录的所有文件： find . ! -type d | p4 -x - add windows中的dos命令： dir /b /s /a-d | p4 -x - add Mac的MPWshell中： files -f -q -r -s | p4 -x - add 或者一般的方法： p4 add * */* */*/* 最后如果你使用p4v的话，很简单，右键文件夹mark for add 或open for add 。","tags":"中文"},{"url":"http://www.ciandcd.com/p4-view-mappingji-qi-te-shu-zi-fu.html","title":"p4 view mapping及其特殊字符","text":"From: http://www.cnblogs.com/itech/archive/2013/04/09/3009592.html#FeedBack p4 view mapping及其特殊字符 p4 中三种view ：client views, branch views, and label views. 注意： 1）如果view中对同一个文件有多次mapping，则后面的覆盖前面的mapping。 2）以-开始的mapping，用来排除文件的mapping，一般用来排除一些目录下的某些子目录或文件。 3）在client views中可以在mapping的最前面有+，表示叠加的效果，不同于一般的覆盖。 文件或路径中的空格： //depot/v1/... \"//ws/version one/...\" \"//depot/document 2/...\" //ws/document2/... \"-//depot/document 2/file2\" //ws/document2/file2 文件或路径中的其他特殊字符： Character ASCII expansion @ %40 # %23 * %2A % %25 client view实例： Client View Sample Mapping Full client workspace mapped to entire depot //depot/... //ws/... Full client workspace mapped to part of depot //depot/dir/... //ws/... Some files in the depot aremapped to a different part of the client workspace //depot/... //ws/... //depot/rel1/... //ws/release1/... Some files in the depot areexcluded from the client workspace //depot/dir/... //ws/... -//depot/dir/exclude/... //ws/dir/exclude/... Files in the client workspace are mapped to different names than their depot names. //depot/dir/old.* //ws/renamed/new.* Portions of filenames in the depot are rearranged in the client workspace //depot/dir/%%1.%%2 //ws/dir/%%2.%%1 The files do not map the same way in each direction. The second line takes precedence, and the first line is ignored. //depot/dir1/... //ws/build/... //depot/dir2/... //ws/build/... An overlay mapping is used to map files from more than one depot directory into the same place in the workspace. //depot/dir1/... //ws/build/... +//depot/dir2/... //ws/build/.. The Perforce system allows the use of three wildcards: Wildcard Meaning * Matches all characters except slashes within one directory. ... Matches all files under the current working directory and all subdirectories. (matches anything, including slashes, and does so across subdirectories) %%1 - %%9 Positional specifiers for substring rearrangement in filenames, when used in views. Expression Matches J* J Files in the current directory starting with */help help in current subdirectories All files calledin current subdirectories ./... All files under the current directory and its subdirectories ./....c .c All files under the current directory and its subdirectories, that end in /usr/bruno/... /usr/bruno All files under //bruno_ws/... bruno_ws All files in the workspace or depot that is named //depot/... All files in the depot //... All files in all depots","tags":"中文"},{"url":"http://www.ciandcd.com/jazz-team-blog-try-out-the-new-configuration-management-features-in-clm-60-rc1.html","title":"Jazz Team Blog  Try out the new configuration management features in CLM 6.0 RC1!","text":"From: https://jazz.net/blog/index.php/2015/05/14/try-out-the-new-configuration-management-features-in-clm-6-0-rc1/ The recent release of Collaborative Lifecycle Management (CLM) 6.0 RC1 signals the release candidate (RC) phase of development, which means that the official CLM 6.0 release is in its final stages. The 6.0 release offers new and exciting configuration management capabilities. To learn more about these capabilities, read the Getting started with configuration management help topic or watch the Introduction to configuration management video, which is part of a larger video series . You can try the configuration management capabilities in an existing cloud sandbox , or you can evaluate them in your own environment by downloading CLM 6.0 RC1 , reading the considerations , and obtaining an activation key. Tim Feeney Executive IT Specialist","tags":"ciandcd"},{"url":"http://www.ciandcd.com/webinar-recording-whats-new-in-teamcity-9.html","title":"Webinar Recording: What's New in TeamCity 9","text":"From: http://blog.jetbrains.com/blog/2015/01/26/webinar-recording-whats-new-in-teamcity-9/ The recording of our recent webinar with Wes Higbee, What's New in TeamCity 9 , is now available on JetBrains YouTube Channel . In this webinar, Wes goes over the new features of TeamCity 9, namely: rearranging projects with Project Import; storing settings in VCS; creating and editing Custom Charts; running builds in Cloud Agents; as well as some other improvements. Below are a selection of some of the most frequently asked questions. Versioned Settings: Q: When is this feature going to be available with Perforce? A: We're considering support for Subversion and Perforce in TeamCity 9.1, but can't make any guarantees at this point. Q: What about TFS? A: TFS might be supported in the future but after Subversion and Perforce. Q: Given your VCS support priorities for new features, are Git, Mercurial and TFS your recommended VCS combinations with TeamCity? A: Apart from storing settings in VCS, Git, Mercurial, Subversion and Perforce, they are all supported greatly with TFS catching up. CVS, Vault and ClearCase support can be a bit limited. Q: How does TeamCtiy handle it if somebody corrupts the configuration in the repository? A: If there are errors while applying changes then TeamCity will not change project settings and will show an error. Q: Is it possible to use branches, e.g. to test a change in the build configuration before making it productive? A: Not yet, but we definitely want to add this feature. Q: How does the VCS know which branch to use? A: It uses default branch specified in VCS root. Q: If you want to revert to an earlier version, you need to find the desired version from within the version control system. That is, there's no GUI way to see the previous version from TeamCity perspective? A: There is a changelog tab where you can see all the changes. Q: What about parameters? Are they synced too? If not, the new features could replace templates, couldn't they? A: All the settings are synced form the VCS as if you edit the settings right in the TeamCity data directory (or change them in UI). Q: If a have sync on the root project, are all sub-projects synced too? Is it recommended to have VCS for the sub projects, or a VCS for the root project only? Or are both recommended? A: By default sub projects will be placed into the same version control. If you don't want it for some projects, you can disable synchronization in them. Project Import Q: Can I import a TC8 backup in TC9? It would make testing easier A: No, both servers should have the same version. 9.0 and 9.0.x are compatible, but 8.1 and 9.0 not. Cloud Agents Q: How does that work out with the licensing system? I guess the number of agents licensed its the hard limit of active agents, including from the cloud? A: Yes. The total number of agents (real and virtual) connected at any given time should not exceed the total agent licenses limit. When there are enough agent licences, TeamCity automatically authorizes new cloud agents and unauthorizes the stopped ones. Q: How do we create the VM image?Is a build image template available for the Azure VM image? A: There are no templates with TeamCity agents for now Q: Can we configure the \"on-demand\" agents to use the same VM with more than one agent, until that max out? i.e. 3 agents per Azure VM. A: TeamCity assumes there is a single agent per instance. The recommended setup is to have a single agent per machine. Q: Can the on-demand agents be used with on-premise vCenter? A: Yes we have vSphere plugin doing the same. Q: Does the agent spun up hang around for a while or is it immediately removed? A: There is an idle timeout setting to shutdown the agent instance after. When stopped, the agent is deleted from TeamCity list of agents. Q: EC2 cloud agents auto-shutdown policy is not the best it could be meaning that it doesn't take into account that you always get charged for an hour of usage of an EC2 instance. Any plans to improve this? A: Yes, we do have plans to adjust the shut down to the hour limit. You are welcome to vote for https://youtrack.jetbrains.com/issue/TW-9680 . Custom Charts Q: Is it possible to have time instead of build number on the X axis? A: No, this is not possible Q: I have a configuration that has different snapshot dependencies, is there any way to show the build time from beginning to end, meaning when the first dependency run to the end? A: No, statistics charts operate per build. Theoretically you can calculate this time and report it as statistic value in the last build of the chain. Q: Can you export the stats data? A: Yes, there is a \"download\" action on the chart. Also available via REST API. Q: Does TeamCity have possibility to show charts with time that took to run every test? A: On the build's Tests tab, you can see the duration chart for each test. As well on the Test history page. Wes Higbee is passionate about helping companies achieve remarkable results with technology and software. He's had extensive experience developing software and working with teams to improve how software is developed to meet business objectives. Wes launched Full City Tech to leverage his expertise to help companies rapidly deliver high quality software to delight customers. He has a strong background in using Continuous Integration with TeamCity to bring quality to the table. is passionate about helping companies achieve remarkable results with technology and software. He's had extensive experience developing software and working with teams to improve how software is developed to meet business objectives. Wes launchedto leverage his expertise to help companies rapidly deliver high quality software to delight customers. He has a strong background in using Continuous Integration with TeamCity to bring quality to the table. Follow TeamCity updates on our blog , Twitter @TeamCity , and our product pages .","tags":"ciandcd"},{"url":"http://www.ciandcd.com/jetbrains-night-in-munich-recap-raffle-winners-and-recording.html","title":"JetBrains Night in Munich Recap, Raffle Winners and Recording","text":"From: http://blog.jetbrains.com/blog/2015/04/29/jetbrains-night-in-munich-recap-raffle-winners-and-recording/ In March 2015, we announced an evening event at our JetBrains office in Munich where we would show our guests how to Use ReSharper Effectively, and Perform Exploratory Code Reviews with Upsource. The level of interest was so great that we decided to hold an additional night to accommodate the volume of demand. We could have filled a much larger venue, but we wanted to provide an opportunity to mingle with the team in a relaxed informal atmosphere. In hindsight, this was a good decision. Over two nights, March 24th and 25th, 120 participants gathered at JetBrains office to see in action Upsource and ReSharper Ultimate (ReSharper, dotTrace, dotMemory and dotCover). The feedback that we received on location and through a follow-up survey were overwhelmingly positive, so much so that we are currently planning to extend the same concept with 3 hour in-depth workshops . JetBrains, and our Munich team in particular, would like to thank all of the participants for their time, great conversations and overall positive sentiment that contributed to making the evenings successful. As promised, today we are announcing the winners of our free personal license raffle , along with their product of choice: Maik Heller – dotCover Michael Baur – IntelliJ IDEA Lastly, we would like to share the recorded session featuring Matt Ellis ( @citizenmatt ), Using ReSharper Effectively. Enjoy the video and we hope to meet you at an upcoming event near you! Develop with Pleasure! - The JetBrains Team","tags":"ciandcd"},{"url":"http://www.ciandcd.com/cardiff-silicon-valley-comes-to-wales.html","title":"Cardiff: Silicon Valley comes to Wales","text":"From: http://blog.devopsguys.com/2015/05/05/cardiff-silicon-valley-comes-to-wales/ We've been set up in Cardiff, South Wales for nearly six months now. Every week it becomes more and more apparent that this city is fast becoming an exciting IT and technical hub; an attractive area for emerging and experienced tech talent alike. The term ‘Silicwm Valley' is being bandied about as more and more tech start-ups spring up in, or near, the city centre. Companies like DevOpsGuys, Cardiff Start, Indycube, Method 4, BBC Cymru's Roath Lock studios and a huge collection of digital and design agencies are choosing Cardiff as their base. It seems to be a logical step; the community is small enough to be interconnected, influential and supportive, but large enough to allow for the freedom to develop, expand and learn from the huge range of related industries in the immediate area. With several major universities in and around the city the wealth of talent is growing and Cardiff is taking the reins and nurturing Welsh talent and ability; a very different picture from several years ago where work in Wales was hard to come by and the majority of experienced IT professionals were obliged to seek work further afield, in London or Cambridge. Founder James Smith says: \"Cardiff has historically been built on industry, from the days of exporting coal. It's also frequently voted one of the top places to live and work in the UK, so it's no wonder that this tradition is developing and changing shape with the emergence of the tech industry – Cardiff is moving with the times. \"We've set up DevOpsGuys in Cardiff in order to be a part of this development. We wanted to provide opportunities for people in Wales – there's so much skill here. Plus we are working with international companies and forming partnerships with industry giants across the world; this is a great opportunity to share some of the home-grown Welsh talent, create unique, fulfilling career opportunities and forge connections all over the world. It's a really exciting time.\" The movement has been supported by the Welsh Government, with DevOpsGuys receiving funding to grow as a business and provide career opportunities in the Welsh capital. Meet-ups, tech events, talks and conferences all taking place in the city, give related, but wildly diverse businesses a chance to meet, mix, talk, share thoughts; ideas flow freely, business connections are forged easily and some new and interesting work is emerging. We're excited about Agile Cymru – the first event of its kind in Cardiff – this summer. There seems to be something new to see, do, read, visit, look at or enjoy every week! We're excited to see where Cardiff will take the DevOpsGuys and the future of the UK tech industry.","tags":"devops"},{"url":"http://www.ciandcd.com/devops-and-the-digital-supply-chain.html","title":"DevOps and the Digital Supply Chain","text":"From: http://blog.devopsguys.com/2015/05/06/devops-and-the-digital-supply-chain/ What is the \"Digital Supply Chain\" and why is it important to your organisation and to DevOps as a practice? The concept of the \"Digital Supply Chain\" is a different way of looking at the SDLC and the Continuous Delivery \"pipeline\" that we feel makes it easier for traditional organisations to understand the criticality of software delivery (and by extension DevOps) in the modern world. Any organisation that deals with physical goods understands the concept of the supply chain . They are intimately familiar with ideas like supply chain management , supply chain optimisation and, most importantly, they understand the economics of inventory in the supply chain e.g the carrying cost of inventory . So what is the \"Digital Supply Chain\"? The current definitions of the digital supply chain are anchored in the \"New Media\" sector and focus on digital assets like music, video etc \"The \" digital supply chain \" is a \" new media \" term which encompasses the process of the delivery of digital media, be it music or video, by electronic means, from the point of origin (content provider) to destination (consumer).\" – Wikipedia The Wikipedia article references above breaks it down into a number of discrete steps as shown in Figure 1 below. If we contrast this with our SDLC Continuous Delivery Pipeline (Figure 2) we can see that many of the steps are directly analogous – we are creating digital assets (code) which we then \"compress\" (i.e. the Build/Integrate process), which we then subject to Quality Control (Test), we store in a Digital Asset Management system (e.g. like Nexus or Artifactory), we tag it with metadata (e.g. what release/version we're deploying) and when then deploy it out to servers, CDN's, the AppStore or wherever. Once your customers grasp the idea that software is a digital asset and that carrying excess inventory and delays in moving these digital assets along the supply chain is costing them money it can be a lightbulb moment for many organisations. Software assets can depreciate over time. Indeed \" technical debt \" can be looked at as the \"cost of deprecation\" of your software assets over time. Code that is \"stuck\" in your Digital Supply Chain waiting for your next release (as source code in Git, or as artefacts in an artefact repository) represents a capital investment in \"digital assets\" held as \"digital inventory\" and having it sat on the digital shelf in your digital warehouse is costing you money is analogous to the carrying cost of inventory for physical inventory. Sure, the warehousing costs of a digital asset – your latest idea transformed into software code – is fairly trivial compared to the costs of physical warehousing BUT the \" opportunity cost \" is very real. Each digital software asset represents a significant investment in time & money by your designers, developers, testers, project managers etc and it doesn't start generating a return on that investment until it gets to the end of your digital supply chain and into the hands of your customers. DevOps then becomes a way to optimise your digital supply chain to ensure that we: only build the right things (reducing waste and optimising our digital inventory), Supplier management (by improving the relationships between Dev, Test, Ops etc we ensure that we are getting the best from all of the \"suppliers\" in our digital supply chain) improving our logistics to get our digital assets in the hand of our customers (by automating testing, release and deployment to accelerate the movement of the digital assets from left to right) Constantly seeking \"flow\" across the supply chain (the 1st way of DevOps!) Gathering metrics along the supply chain to give us insight into the bottlenecks (the M in the C.A.L.M.S model) So next time you're talking with people in the business try out the \"Digital Supply Chain\" analogy and see if it works for you – we'd love to hear your feedback! -TheOpsMgr","tags":"devops"},{"url":"http://www.ciandcd.com/sponsored-dog-walk.html","title":"Sponsored DOG Walk","text":"From: http://blog.devopsguys.com/2015/05/08/sponsored-dog-walk/ The DevOpsGuys team are pulling up our hiking socks to raise cash for SSNAP and Countess Mountbatten Hospice this summer with a 13 mile walk across Wales. TrekFest it's no mean feat. We cross the highest peaks in the Beacons and South Wales including Pen y Fan (886m), Corn Du (873m), Cribyn (795m) and Fan y Big (719m). http://www.trekfest.org.uk/ Click here to donate now through our Just Giving Page Our first charity is special since last year, one of our brave team members was diagnosed with terminal cancer. Unfortunately, and with great sadness we know now they are losing their fight – even after enduring endless rounds of chemotherapy and surgery. The amazing staff at Countess Mountbatten Hospice are proving specialist palliative (end of life) care to many fighting a losing battles against advanced stage cancer. They also support their families and loved ones. We'd love to show our thanks and support by raising money on their behalf. It's with a tear in our eye and sadness in our hearts, that we chose our second charity to support. Last year, little Ollie lost his fight for his life after being born with a heart defect. He was 4 days old. His parents have displayed so much courage during an immensely difficult time during which the amazing doctors and nurses, support by the SSNAP team provided them with much needed support. It is their wish to continue to champion SSNAP, who provide support for the sick newborns and their parents at new born intensive care unit at The John Radcliffe Hospital, Oxford as so, we'll match whatever we raise through our JustGiving page as a donation to SSNAP. These amazing charities are truly special to us here DevOpsGuys. Please support us in raising funds for these brilliant charities who have done so much to support our close friends and employees. The team have signed up to complete the distance in six hours. The trek takes place in the Brecon Beacons and covers the highest peaks in South Wales: Pen y Fan, Corn Du and Fan y Big. The Beacons are a training ground for the SAS so, while the DOGs will have their work cut out for them, they're more than up for the challenge: \"I can't wait to get out there\" says office manager Rhian Owen. \"It's such a great opportunity to work together as a team to achieve personal goals and to raise money for good causes – it's going to be brilliant!\" We've set up a Just Giving page, so you can show your support here. It's a chance to donate to some good causes and get the DevOpsGuys and gals out from behind their screens and into the beautiful Welsh wilderness – come on y'all, dig deep! Click here to donate now through our Just Giving Page","tags":"devops"},{"url":"http://www.ciandcd.com/devopsguys-announce-redgate-partnership.html","title":"DevOpsGuys announce RedGate partnership","text":"From: http://blog.devopsguys.com/2015/05/11/devopsguys-partner-redgate/ More and more companies are now considering source control, continuous integration, and automated deployment for their database. To help them adopt each of these stages of Database Lifecycle Management (DLM), Redgate Software has launched a new partner program. Redgate Certified Consultants are now being trained in the USA, Europe and Australia – and many of them will be familiar to SQL Server professionals. The advantages of implementing any stage of DLM are many. Just as with Application Lifecycle Management (ALM), it speeds up the introduction of new features, and makes deployments reliable and error-free. But even though Redgate tools are designed to plug into the tools companies already use for their application d evelopment, questions can arise during the implementation process. As Dan Wood of Northwest Cadence says: \"How can we take a system designed to develop, build and deliver applications and make it work with databases as well? It is a question that has plagued a vast majority of the clients I have worked with over the past few years.\" To address this issue, Redgate's new partner program is training expert consultants like Dan Wood in DLM – and giving them the tools and support they need to help clients on-site, or in training sessions. The list of Certified Consultants is growing and already includes familiar faces like Ike Ellis and Northwest Cadence in the USA, The DevOpsGuys and Skelton Thatcher in the UK, and WARDY IT Solutions in Australia. As John Theron of Redgate points out, the advantages are clear. \"Redgate has spent a lot of time and effort joining the dots in DLM, and making it possible with a suite of dedicated tools, alongside learning materials and resources. The partner program complements this with a group of experts on the ground able to help companies on-site, and provide training in a series of public workshops.\" In places as far apart as Washington, London, San Diego, Philadelphia, Northern Ireland, and Baton Rouge, database professionals are how being trained in source control, continuous integration, and automated deployment for the database. A measure of the success the training is already achieving can be found in the reaction from database professionals like Jim Dorame. A Database Systems Manager for a large scale educational assessment corporation in the Greater Minneapolis area, he reviewed a continuous integration training day on his blog . \"This tool makes the job of the DBA easier as there will be little doubt that the database is in a consistent and correct state. This alone makes me smile, I cannot tell you how many times I've been executing a release and there was a piece missing that caused a failure.\" Further information about the training opportunities available can be found on the Redgate training pages .","tags":"devops"},{"url":"http://www.ciandcd.com/increase-your-elk-herd-with-consulio.html","title":"Increase your ELK herd with Consul.io","text":"From: http://blog.devopsguys.com/2015/05/18/increase-your-elk-herd-with-consul-io/ Originally posted on DevOps Is Common Sense... : At work, I recently had a need to put in place a scalable logging solution based around the ELK stack. Issues with Multicast networking aside, Elasticsearch scales pretty well on its own without the need for any additional overheads, however discovering whether a node is online or not and connecting only to available nodes can be tricky. Scaling Logstash can be tricky, but it basically involves adding more Logstash servers to the mix and pointing them at your Elasticsearch cluster by defining multiple hosts in your Logstash configuration. Kibana (like most web applications) can only have one Elasticsearch host defined in the config, so scaling out Kibana is more difficult. The above raises the question – how do I know which Elasticsearch node to point my configuration at if I don't know whether they are there or not. The answer came in the form of consul.io . If you've not looked at…","tags":"devops"},{"url":"http://www.ciandcd.com/steve-thairs-qcon-talk-now-available-online.html","title":"Steve Thair's QCon Talk – now available online","text":"From: http://blog.devopsguys.com/2015/05/20/steve-thairs-qcon-talk-now-available-online/ DevOps and the Need for Speed, the talk from our very own Steve Thair is now available online. You can check it out here . Steve's just spoken at Krakow's Atmosphere Conference . Stay tuned for more, coming soon.","tags":"devops"},{"url":"http://www.ciandcd.com/the-ops-mgr-at-qcon-2015.html","title":"The Ops Mgr at QCon 2015","text":"From: http://blog.devopsguys.com/2015/06/16/the-ops-mgr-at-qcon-2015/ \"When you're in a startup, the divide between Dev and Ops is normally the width of the desk…it's far easier to collaborate in that small environment. In larger enterprises not only are they in different buildings but they're in different countries with different cultures and different languages…\" The DOG Ops Manager Steve Thair chats to Manuel Pais at QCon 2015. Steve Thair at QCon to hear Steve talk about Enterprise DevOps; taking the first steps on the road to DevOps and cultural change.","tags":"devops"},{"url":"http://www.ciandcd.com/dogwalking-in-brecon.html","title":"DOGWalking in Brecon","text":"From: http://blog.devopsguys.com/2015/06/17/dogwalking-in-brecon/ So, aching, tired and happy the DOGs returned from TrekFest 2015 in the Welsh mountains having raised £1,143.00 for the Countess Mountbatten Hospice and SSNAP – two charities close to the heart of the team. That's 114% of our initial target, so a huge thank you to everyone who donated so generously. The weather could not have been better for a long, scenic ramble in one of the UKs most beautiful spots; beautifully sunny with a refreshing breeze kept the team going. We completed the trek in approximately 5 and a half hours, just in time for a piece of cake and a glass of celebratory champagne at the finish line. Everyone had a thoroughly enjoyable time and we're all looking forward to the next DOG adventure!","tags":"devops"},{"url":"http://www.ciandcd.com/dogs-at-digital-2015.html","title":"DOGs at Digital 2015","text":"From: http://blog.devopsguys.com/2015/06/18/dogs-at-digital-2015/ Last week the DevOpsGuys headed up to Newport's Celtic Manor to take part in Digital 2015 – the Welsh Government's initiative to bring digital innovators and business professionals together. The 2-day event saw more than 2,000 delegates and 140 speakers. DevOpsGuy co-founder Steve Thair says: \"These initiatives are invaluable to the digital sector because they expose the wide variety of digital and technological services that are available in South Wales to business professionals who can use them to take online business services to the next level. It's a relaxed environment where people can chat and form connections that will have a direct impact on the future of business and technology in Wales.\" The diverse range of speakers at the event included Microsoft, the WRU, Amazon and the DVLA. The opportunity to discuss the needs of businesses directly with those running them is invaluable. This dialogue can lead to collaborative projects and further development of the burgeoning tech industry in the area. The team are excited to see more events like this one springing up in the near future. Look out for us at the up-coming Agile Cymru in the Wales Millennium Centre on the 7th and 8th of July.","tags":"devops"},{"url":"http://www.ciandcd.com/what-is-important-for-an-it-ops-to-team-more-effectively-with-preproduction-teams-devops.html","title":"What is important for an IT Ops to team more effectively with preproduction teams? #DevOps","text":"From: http://blog.devopsguys.com/2015/06/23/what-is-important-for-an-it-ops-to-team-more-effectively-with-preproduction-teams-devops/ \"DevOps can present IT Operations teams with new ‘customers' in development and test. What traditional or new tools and technologies are most likely to be important for IT Ops to team more effectively with preproduction teams? What information does IT Ops need to pass right to left and which tools are most likely to aid in that?\" The short answer is \"A whiteboard marker, a pad of Post-It notes and a couple of pizzas\" :-) That answer is a bit tongue-in-cheek, but there is a serious side to it; whilst new tools can be an important part of DevOps (particularly in A utomation) you can get started in changing your C ulture and improving your S haring with very simple tools i.e. the aforementioned whiteboard marker, Post-It notes and pizza. Start to break down the silos by getting key people in a room with some blank walls and whiteboards and start sharing information, mapping out your value stream and trying to find out, collaboratively, where the bottlenecks in your existing processes are. Once you've identified your key constraints then fire up Google and start searching for the tools to solve your problems (or visit a site like DevOpsBookmarks ). DevOpsGuys, like most organisations, have our own \"Opinionated Stack\" – we like the Atlassian Toolset for managing our Agile workflow, TeamCity or Jenkins as our CI tool of choice, Ansible as our configuration management tool for Linux, Powershell DSC for Windows, AppDynamics as our APM tool, Redgate for our Database Lifecycle Management (DLM) and so on. We partner with many of these companies now because we've \"dogfooded\" the products internally and with our customers and they've worked well for our use cases. We always \"try before we buy\" and we \"try before we partner\" too because, as they say, \"your mileage may vary\" (YMMV). This comes back to fostering a culture of experimentation – give something a try and see what works for you. We started off using Atlassian HipChat as our chat tool and we really liked it. Then we tried Slack and we liked that one more, so we switched. YMMV. One additional point worth mentioning – the premise of the question is flawed! They aren't customers they're colleagues. There isn't a silo of \"Us\" (IT Ops=supplier) versus \"Them\" (Everyone Else=customer). We are supposed to be breaking down these silos to create cross-functional, multi-disciplinary, product-based teams. Development, Test, IT Security, Networks shouldn't be silos any more – they are people in our team, sitting over the desk from us, attending our daily standups, eating our pizza :-) The Q&A above is part of material prepared as our contribution to an CA ebook on \"Agile Operations\". We wrote our thoughts on 6 questions, of which 4 will be used in the ebook, scheduled to come out in August 2015. We'll post the remaining 2 questions with our answers onto the blog over the next 2 weeks.","tags":"devops"},{"url":"http://www.ciandcd.com/mobile-application-development-primer.html","title":"Mobile Application Development Primer","text":"From: http://devops.linuxjournal.com/collaborative-development/mobile-application-development-primer-0 Industries of all varieties have begun to realize that the target audiences for their business applications have shifted in massive numbers from the use of traditional personal computers, such as desktops and laptops, to using mobile devices such as smart phones and tablets for accessing the internet and for obtaining the information they seek. This applies if the intended audience for the application is a direct customer of the enterprise (Business-to-Consumer apps, or \"B2C\"), or if the targeted user is an employee or business partner (\"B2E\" and \"B2B\", or Business-to-Employee and Business-to-Business apps). Across the globe, more people are now using mobile devices that they can carry with them wherever they go, and which are more user friendly and intuitive to use, as their primary means of obtaining information and requesting services over the internet.","tags":"devops"},{"url":"http://www.ciandcd.com/ten-answers-regarding-mobile-app-testing.html","title":"Ten Answers Regarding Mobile App Testing","text":"From: http://devops.linuxjournal.com/develop-deploy/ten-answers-regarding-mobile-app-testing This white paper digs deep into the reasons testing mobile apps is fundamentally harder than traditional web or desktop applications. A collaboration by Tina Zhuo and Dennis Schultz from IBM along with Yoram Mizrachi from Perfecto Mobile and John Montgomery from uTest, these experts explore the complexities of mobile test environments, the value of the mobile device cloud, the unique role crowd sourcing can play, and how teams can leverage automation to help deliver quality apps.","tags":"devops"},{"url":"http://www.ciandcd.com/ibm-cloudant-the-do-more-nosql-data-layer.html","title":"IBM Cloudant: The Do-More NoSQL Data Layer","text":"From: http://devops.linuxjournal.com/cloud/ibm-cloudant-do-more-nosql-data-layer Cloudant represents a strategic acquisition by IBM® that extends the company's Big Data and Analytics portfolio to include a fully managed, NoSQL cloud service. Cloudant simplifies the development cycle for creators of fast-growing web and mobile applications, by alleviating the burdens of mundane database administration tasks. Developers are then able to focus on building the next generation of systems of engagement – social and mobile applications – without losing time, money, or sleep managing their database infrastructure and growth. Critically, Cloudant is an enterprise-ready service that supports this infrastructure with guaranteed performance and availability. Built atop a CouchDB-based NoSQL data layer, Cloudant's fully managed database-as-a-service (DBaaS) enables applications and their developers to be more agile. As a part of its data layer, clients have access to multi-master replication and mobile device synchronization capabilities for occasionally connected devices. Applications can take advantage of Cloudant's advanced real-time indexing for ad hoc full text search via Apache Lucene, online analytics via MapReduce, and advanced geospatial querying. Mobile applications can use a durable replication protocol for offline sync and global data distribution, as well as a geo-load balancing capability to ensure cross-data center availability and optimal performance. Cloudant's RESTful web-based API, flexible schema, and capacity to scale massively are what empower clients to deliver applications to market faster in a cost-effective, DBA-free service model. This IBM Redbooks® Solution Guide describes the IBM Cloudant features.","tags":"devops"},{"url":"http://www.ciandcd.com/smart-planning-for-smarter-infrastructure.html","title":"Smart Planning for Smarter Infrastructure","text":"From: http://devops.linuxjournal.com/collaborative-development/smart-planning-smarter-infrastructure A smart Infrastructure project is a large system of systems. This paper looks at how a systems engineering approach can benefit an organization planning smart infrastructure projects.","tags":"devops"},{"url":"http://www.ciandcd.com/smarter-quality-management-the-fast-track-to-competitive-advantage.html","title":"Smarter quality management: The fast track to competitive advantage","text":"From: http://devops.linuxjournal.com/continuous-testing/smarter-quality-management-fast-track-competitive-advantage This paper introduces quality management (QM), a practical, multi-disciplined approach to software delivery that helps reduce time to market without sacrificing quality in the outcome.","tags":"devops"},{"url":"http://www.ciandcd.com/improve-quality-and-increase-your-business-agility-through-automated-testing-test-automation-solutions-for-financial-services-organizations.html","title":"Improve quality and increase your business agility through automated testing. Test automation solutions for Financial Services Organizations","text":"From: http://devops.linuxjournal.com/collaborative-development/improve-quality-and-increase-your-business-agility-through-automated IBM Rational test automation capabilities and tools can help software integrators, developers and testers alike overcome the challenges of testing banking, insurance and financial market applications.","tags":"devops"},{"url":"http://www.ciandcd.com/seven-ways-to-reduce-waste-and-accelerate-software-delivery.html","title":"Seven ways to reduce waste and accelerate software delivery","text":"From: http://devops.linuxjournal.com/collaborative-development/seven-ways-reduce-waste-and-accelerate-software-delivery This paper will explore best practices for identifying and eliminating seven types of waste: Waiting, handoffs and task switching, motion, extra processes, extra features, partially completed work and defects.","tags":"devops"},{"url":"http://www.ciandcd.com/deployment-automation-basics.html","title":"Deployment Automation Basics","text":"From: http://devops.linuxjournal.com/urban-code/deployment-automation-basics Why should we bother automating deployments in the first place? What should the scope of the automation effort be? How do we get started? This white paper provides a solid introduction to these topics.","tags":"devops"},{"url":"http://www.ciandcd.com/7-things-i-didnt-expect-to-hear-at-gartners-it-ops-summit.html","title":"7 Things I Didn't Expect to Hear at Gartner's IT Ops Summit","text":"From: http://java.dzone.com/articles/7-things-i-didn't-expect-hear Last week's Gartner IT Operations Strategies & Solutions Summit in Orlando, Fla., was exactly what you'd expect—a place to talk about the IT operations issues impacting some of the largest companies in the world. Even so, there were a few interesting surprises. Among them: 1. Bi-modal is big. Not everyone will succeed. Gartner continued to tell its customers to employ two modes of IT —a traditional, slower moving capability for older, typically internal systems of record; and a high-speed, experimental one for new, typically customer-facing Web and mobile apps. \"This is a time of experimentation and innovation,\" said Gartner VP and distinguished analyst Chris Howard in his opening keynote. Organizations can't ignore that there are multiple speeds and they should participate in all. Gartner managing VP Ronni Colville added that by 2017, 75% of IT orgs will have this \"bi-modal\" IT capability. See also: Bi-Modal IT: Gartner Endorses Both Disruptive and Conservative Approaches to Technology However, \"50% will make a mess of it,\" Colville said. Why? Not necessarily because of technology failings, but more often because of a lack of people skills. 2. IT success is all about people. Donna Scott , also a Gartner VP and distinguished analyst, told her keynote audience that \"you will be judged on agility, speed, and innovation.\" However, the biggest problems Gartner sees for infrastructure and operations team engagement and innovation are lack of time, company culture that's not conducive to these approaches, and a lack of business skills in IT. More than half of the people responding to an in-room poll said \"people\" are the part of IT ops that must change first. Not technology. Gartner research director George Spafford underscored similar issues in large organizations trying to use DevOps at scale: people and \"human factors\" are the biggest concerns from his in-room poll. All these probably contributed to hiring best-selling author Daniel Pink as a keynote speaker on the opening day of the conference. His focus? Not IT or architecture. Instead, he pounded home the importance of influencing people and selling internally. 3. Big orgs are trying DevOps. But the issues are different at scale. In numerous sessions I saw many hands go up when analysts asked, \"Who here is trying DevOps?\" Clearly, the approach is getting traction in large companies. But there's lots of learning still to do. In fact, that was Spafford's biggest bit of advice. \"Always be learning,\" he said, \"trying to see what works and what breaks, especially at scale.\" And, even once you've had some initial success, keep learning. \"If you've done ‪DevOps, stay humble,\" he advised. 4. Looking to innovative organizations for ideas … analytics on the rise. Many sessions addressed how large organizations are taking on ideas fostered by smaller, more risk-tolerant companies, and offered advice for doing so successfully. In addition to multiple discussions of DevOps, an entire session was devoted to establishing your own \"Genius Bar®—a \"walk-up IT support center\" as explained in this CIO article . As at previous conferences, Gartner research VP Cameron Haight ran several sessions on lessons learned from firms running massive, Web-scale IT systems. \"You need lots of data … and access to it inexpensively,\" he said. Some commercial monitoring companies (New Relic included!) got a shout out for taking the lessons of Web scale IT to heart in their offerings. In addition, Haight said, \"Analytics are increasingly important for application performance monitoring given the huge amount of data now available.\" 5. Cloud: Enterprises want it, but aren't very good at it yet. Gartner research director Dennis Smith talked through the enterprise's interest in cloud computing. A huge majority of his in-room poll wanted some mix of both public and private cloud, while only 9% wanted to use only a private cloud environment and a measly 4% were looking to move entirely to the public cloud. The most popular choice (41%) was an 80/20 split between private and public cloud infrastructure. \"Enterprises don't make the dean's list,\" for cloud usage, Smith said, earning no more than a C average in his opinion. Large organizations are doing well at visibility, governance, and delivering standardized stacks, he said, but are less skilled at optimizing for these new environments. Still, Smith said the trends point toward enterprises improving on all fronts. 6. Cloud security can be better than yours. Importantly, Gartner VP and distinguished analyst Neil MacDonald gave the cloud a vote of confidence: noting that, for a variety of reasons, \"Well-managed public cloud can be more secure than your own data center.\" For example, on-premise software can pose serious security risks, he said, because of \"deployment lag\" where customers are stuck using software releases with unpatched security vulnerabilities. With a cloud-based Software-as-a-Service (SaaS), security updates can be more quickly rolled out to all customers. But cloud security can be different, requiring a shift to information-level security from OS-level security. Best practices include doing away with a huge pool of all-powerful sysadmins in favor of JEA, or \"just enough administration,\" where sysadmins have just enough privileges to do their job, and no more. An analogous security practice for compute resources is \"least privilege,\" where apps and microservices can't talk to each other unless they specifically need to do so. Audience polling supported MacDonald's optimistic view of cloud security, which suggests that large enterprises may struggle less with their cloud policies moving forward. 7. Containers: Try 'em! Ahead of this week's DockerCon in San Francisco, Gartner devoted significant airtime to educating the audience on containers and microservices. My summary of ‪Gartner VP and distinguished analyst Tom Bittman's advice on containers was simple: Try 'em. Now. Complement them with VMs. ‪And Docker (the company) is important, but not the be-all and end-all in this space. Bittman (copping to some deja vu from Gartner presentations he made on server virtualization 13 years ago) noted that while virtualization has been focused on admin and ops functions, containers are focused on value for developers. But because containers are well suited for driving up VM utilization for workloads that share the same OS, we can expect to see more combinations of containers and server virtualization. Finally, Bittman underscored that Gartner doesn't see containers having much impact on premise, but making a huge difference in the cloud. That doesn't necessarily fit with what's been shown in other research, such as this 2015 State of Containers Survey sponsored by VMblog.com and StackEngine, so we'll want to watch how this plays out. This is all a lot to digest. The Gartner IT Operations Strategies & Solutions Summit acknowledges the importance of dealing with existing IT systems and practices as well as promising new technologies and thinking, and tries to point a way forward. In fact, Haight had a very good quote about microservices that I thought also served to wrap up the entire event: \"If you want to run with the big dogs, you need to rethink application architecture,\" he said. That can be very difficult for an enterprise to fully implement … but also very appealing. Note: Al Sargent contributed to this post. All product and company names herein may be trademarks of their registered owners. Server , tortoise and hare , business team , and cloud security images courtesy of Shutterstock.com .","tags":"devops"},{"url":"http://www.ciandcd.com/why-is-git-better-than-mercurial.html","title":"Why is Git better than Mercurial?","text":"From: http://java.dzone.com/articles/why-git-better-mercurial Which of the two main distributed version control systems (DVCS), Mecurial and GIT, is better and why? This is an old argument, with forum posts galore about which is better; with users of both arguing their case, however this argument never seems to be resolved. This is because the feature sets are so similar, and after researching the various strengths and weaknesses it became clear that they both have clear advantages for certain situations, – different use cases. Below we have listed both pros and cons for both Git and Mercurial in order for our readers to be able to make their own decision as to what is best for them, after all user requirements and team requirements differ; there is no one size fits all. Commonly found comments about Git GIT is overly complex in every sense of the word, in particular the information model and the command line syntax as well as the help documentation, none of it is easy to understand. The statement \"if you don't understand the functionality don't use it, just use it like subversion\" just doesn't fit for the simple reason that most commands lead to further commands, where simple actions can require complex actions to undo or refine. The decision as to whether to use Git on a development project or not, is primarily a question of how knowledgeable of Git the individuals are within the team, a single weak link can bring down the entire team. The obvious example of this is seen in the version control of Git. It is simply put, – unsafe, – prone to accidental alteration by users (potentially affecting the entire team), therefore a high minimum knowledge requirement is necessary. Consider a Git test, what should the pass rate be? 70%, 80% or 90%? Continuing on the overly complex nature of Git's design, the obvious point is that it puts control firmly in the project managers hands or whoever is maintaining the codebase, you might wonder why that is bad? The problem is that the majority of Git users are coders (contributors) and what they need most of all is a clean interface which is where Git fails to deliver, putting functionality over usability, favoring the maintainer over the contributor. Git is different but not necessarily better than Mercurial It is often said that working with Git is much better then Mercurial. The two clearly have pros and cons but one is not necessarily better than the other is. It is all depends on your knowledge and expectations. The method by which both Git and Mercurial handle history, is essentially just a directed acyclic graph. However, Mercurial provides you with a simple linear history that can cause confusion due to the lack of information, whereas Git enables you to follow the history backwards but this is complicated and hard to follow, therefore this can cause confusion. It is often an argument that Git handles branches better than Mercurial without any particular reason. As a Mercurial user, you might see two branches (named branches and repository clones) and all your changes will belong to a named branch. The confusion starts if you have many with the same name. Branches structure in Git enables you to avoid putting code where you did not want. Git enforces technical excellence; – if you do not know everything about Git then you should not be using Git else you are a danger to others work. Simply put, a Git team is an expert team. Git is more powerful for larger projects, one good example is the functionality provided by \"partial checkouts\". Last, but not least, the biggest non-technical different between Mercurial and Git is the marketing. Git seems to be better marketed these days and that clearly makes many people use Git.","tags":"devops"},{"url":"http://www.ciandcd.com/create-a-maven-archetype.html","title":"Create a Maven archetype","text":"From: http://java.dzone.com/articles/create-maven-archetype After creating our project structures from the basic maven archetypes since starting to use Maven I thought it was time to start automating the process. It wasn't too painful but not quite as straight forward as I hoped either so I thought it was worth writing an article about my experience. Example Here's an example of the code which I've got working, so feel free to start from there. Click on the link on my website post to download (it should anchor so that the zip file link is at the top of the page) Link to zip I found it easiest to create a Java Project and copy the files into there and then convert the project to Maven project but whatever takes your fancy. Using property names The hard part of creating the archetype was getting the property names to show throughout the code. I had followed instructions on other websites that had mentioned the __my-property-name__ convention to get this to work. However after following the instructions on the Maven website I found that updating the files from the resulting project didn't work. After some more searching I found out about the archetype-metadata.xml file which appears to be quite important. Once this was added into the system I found that my property names started resolving to their expected values. It also gave me an opportunity to add additional parameters like the projectName used in the example. I found that if you want to place property names in a filename or directory name then you needed to use the __my-property-name__ convention but inside the file you need to use the ${my-property-name} convention. The archetype-metadata.xml file also meant that I could do away with the archetype.xml file that was suggested in the Maven tutorial. With this file you had to say exactly which files you wanted to include into your project, however by specifying **/* for the include tag of the archetype-metadata.xml file it included all of the files inside that folder without having to update the config each time the files changed. Velocity and If Statements The maven archetype build uses the velocity templating engine which means that you have a lot of control over what you can do. I'm not going to go through all the details of what Velocity does as there's a perfectly good website for that here . What I will have a quick chat about is the if statement functionality. This is really powerful as I had initially started creating seperate archetypes depending on the type of project I was creating. I then quickly realised that a lot of these shared the same structure and files so managed to merge most of them together into a single archetype by using a mix of a new project type required parameter (as mentioned about above) and the velocity if statement. An example of the statement is shown below, the keyword #else can also be used. #if( ${projectType} == \"cms\" || ${projectType} == \"ecommerce\" ) private int myNewVariable; #end Adding empty folders This one had me stuck for a little while as I was trying to add empty folders into the archetype-resources section but they were not appearing in the generated project. The answer is to include them as a fileset in the archetype-metadata.xml as I have done in the example archetype with the \"/src/main/webapp\" folder. <fileSet filtered=\"true\" encoding=\"UTF-8\"> <directory>src/main/webapp</directory> </fileSet> Create a project from the archetype I use Eclipse as my development environment, if you are using NetBeans or others I imagine this part will be slightly different. In order to create the archetype from the project it was simply a case of right clicking the POM file and choosing to run Maven install. Creating a new project from the archetype however was slightly more tricky as when you go to choose the archetype from the list it's not there. I had to add an archetype using the 'Add Archetype' button on the page where you would normally select the archetype from the list. For the example I just put in com.aplos for the Archetype Group Id, example-archetype for the Archetype Artifact Id and 1.0 for the Version, I left the repository url field blank and Eclipse found the archetype. Conclusion Following these steps should allow you to create a wide range of project structures and base files. I still cannot see how to populate the group id and package name for the project to the default of com.aplos. However I'm happy for now and if I learn the other parts I'll be sure to update this article, or if you know I'd be grateful to hear from you.","tags":"devops"},{"url":"http://www.ciandcd.com/investing-in-your-infrastructure.html","title":"Investing in Your Infrastructure","text":"From: http://java.dzone.com/articles/investing-your-infrastructure Having mentored and invested in startups I have come to learn what works and doesn't websites. The reality of getting out of startup mode and scaling takes vision and the ability to anticipate how you may need to pivot. There are two main reasons why startups do not scale. The first is a lack of experience and mentorship. This is closely followed by a lack of a working capital. An effective entrepreneur understands not only how to pivot but how to utilize capital investment. This article aims to demystify the need for investing in site infrastructure. When scaling a business there are several huge issues that executives tend to forget. whether you are a startup or a fortune 500 company your website is the organizations public face. Maintaining a clean and secure site will help to avoid deep routed problems that could potentially destroy not only you site by your reputation. Hackers can be devastating and end up costing you millions. Another aspect of your site to consider is funnel optimization. By making this a priority you will be able to effective guide customers step by step into a conversion. American With Disabilities Act (ADA) The ADA can be a freighting legal area for many entrepreneurs. Many think that ADA only applies to physical boundaries such as implementing ramps for wheelchairs and accessible bathroom stalls. However this is far from the case. Few people know that the digital world also counts. Is your website written in HTML5? If not you are in violation of ADA. This newest version of the HTML coding language allows for an audible version of a web site for those facing impaired vision. However there are some quick an easy ways to gain some ADA points to allow for more leeway in other more long-term solutions. Implementing closed captioning into your promotional videos will allow for increased accessibility for those facing an auditory disability. Investing in infrastructure is not a single step but a constant to building a good company culture where employees can feel proud of where they work and remain safe. Please comment on this article if you have any additional suggestions.","tags":"devops"},{"url":"http://www.ciandcd.com/how-to-debug-your-maven-build-with-eclipse.html","title":"How to Debug Your Maven Build with Eclipse","text":"From: http://java.dzone.com/articles/how-debug-your-maven-build When running a Maven build with many plugins (e.g. the jOOQ or Flyway plugins ), you may want to have a closer look under the hood to see what's going on internally in those plugins, or in your extensions of those plugins. This may not appear obvious when you're running Maven from the command line, e.g. via: C:\\Users\\jOOQ\\workspace>mvn clean install Luckily, it is rather easy to debug Maven. In order to do so, just create the following batch file on Windows: @ECHO OFF IF \"%1\" == \"off\" ( SET MAVEN_OPTS= ) ELSE ( SET MAVEN_OPTS=-Xdebug -Xnoagent -Djava.compile=NONE -Xrunjdwp:transport=dt_socket,server=y,suspend=y,address=5005 ) Of course, you can do the same also on a MacOS X or Linux box, by using export intead of SET . Now, run the above batch file and proceed again with building: C:\\Users\\jOOQ\\workspace>mvn_debug C:\\Users\\jOOQ\\workspace>mvn clean install Listening for transport dt_socket at address: 5005 Your Maven build will now wait for a debugger client to connect to your JVM on port 5005 (change to any other suitable port). We'll do that now with Eclipse. Just add a new Remote Java Application that connects on a socket, and hit \"Debug\": That's it. We can now set breakpoints and debug through our Maven process like through any other similar kind of server process. Of course, things work exactly the same way with IntelliJ or NetBeans. Once you're done debugging your Maven process, simply call the batch again with parameter off : C:\\Users\\jOOQ\\workspace>mvn_debug off C:\\Users\\jOOQ\\workspace>mvn clean install And your Maven builds will no longer be debugged. Happy debugging!","tags":"devops"},{"url":"http://www.ciandcd.com/how-to-get-your-servers-out-of-coma-first-steps-in-capacity-planning-and-management.html","title":"How to get your servers out of coma – First steps in capacity planning and management","text":"From: http://java.dzone.com/articles/how-get-your-servers-out-coma 30% of servers are sitting \"comatose\" . Out of personal experience, I'm pretty sure that one of the major reasons for that is how tedious it is to have your server's load balanced accordingly. Why? Allow me to explain. The bad part about capacity management and planning in the past was that it was tedious work collecting all the required host-related metrics. But those days are behind us. Have a look at this sortable table view of monitored-host metrics provided by Ruxit : Sorting hosts by metric type can greatly simplify capacity planning Sorting hosts by metric type can greatly simplify capacity planning Having the ability to sort your hosts (both virtual and physical) by metric type helps you to easily see if your infrastructure can handle more load. Having such detailed metrics at your fingertips is a simple first step toward knowing if your hosts are chronically overloaded or under-utilized. If you discover that your hosts are running at less than full capacity you may have the opportunity to save some money by, for example, automatic scaling of VM instances. On the other hand, if your hosts suffer from overload, their performance will be negatively affected. This leads to increased response times and, ultimately, lost revenue. For more on this point, see the Kissmetrics article How Loading Time Affects Your Bottom Line . Know the differences between virtual and physical hosts Virtual hosts can max out their allocated resources while the the physical hosts they reside on sit largely idle with plenty of available capacity. Remember there are physical constraints hehind every virtual host. Remember there are physical constraints hehind every virtual host. There are other reasons besides maxed out physical-host resources that can cause a VM to sit idle. For example, a physical host may not be able to assign more resources to a VM because one or more other VMs have already claimed all the resources. In such cases, it's important to keep a watchful eye on both your physical and your virtual hosts and have integrated metrics and graphs available for comparison. It's no fun correlating these numbers manually though. Again, Ruxit can save you hours of manual number crunching here. Now, with all the tedious work behind you you're ready for … The amazing part The amazing part begins once you've mastered the challenge of identifying your potentially tunable hosts. For me, benchmarking and load testing applications and watching them utilize hardware resources is the really fun part. And with virtualization technology at hand, it's easy to find out if your applications scale better horizontally or vertically. When it comes to load testing, there are loads of tools available. Google can deliver a list of open source load-testing tools if you're interested: Try us out Give Ruxit a try! Watch your environment under normal load, place some load on your infrastructure, and then see how your resource utilization changes. I'd love to hear your findings. Does your environment scale better horizontally or vertically? I'm looking forward to hearing your stories!","tags":"devops"},{"url":"http://www.ciandcd.com/devops-leadership-series-compliance-testing-and-rugged.html","title":"DevOps Leadership Series: Compliance, Testing, and Rugged","text":"From: http://java.dzone.com/articles/devops-leadership-series-1 This past week, I headed to London for the Rugged DevOps event, where I had the chance to catch up with a few more industry thought leaders. First, I caught up with Gareth Rushgrove from Puppet Labs, who also runs the DevOps Weekly newsletter. In this episode of the DevOps Leadership Series, Gareth explains why the importance of security tests within DevOps practices is going to be a big topic for years to come. Gareth anticipates that \"In five years time we will look back on this and ask ‘why were we not always doing this'?\" I then caught up with Helen Beal , Head of DevOps at Ranger4, where she discussed the importance of DevOps and security. While the two ultimately need to work hand and hand, she voiced concerns about DevOps practices sometimes circumventing controls that are essential to a business's safety. She also said that DevOps supports security in a number of ways: from making things consistent to relying on more automation. Finally, I caught up with Justin Arbuckle , Vice President, EMEA & Chief Enterprise Architect at Chef. He discusses key trends he expects to see in over the next 9-12 months regarding compliance and security. Justin explains that compliance as a core driver of DevOps is something that will start to shape our conversations over the next year, whereas the two have previously been seen as contradictory. He believes that we will see that high velocity organizations are able to improve compliance continuously, and tells us to look out for the changing role of the security officer in 2015. Next up in the series, I head to the United States Capitol, Washington DC, for DevOps Days DC . NOTE : If you have missed any of the other videos from this series, you can find them here . (We're up to 15 so far).","tags":"devops"},{"url":"http://www.ciandcd.com/cameron-of-gartner-talks-devops-devops-days-austin.html","title":"Cameron of Gartner talks DevOps – DevOps Days Austin","text":"From: http://java.dzone.com/articles/cameron-gartner-talks-devops Last month at DevOps Days Austin I did a series of interviews with a variety of speakers and attendees. One of the attendees I chatted with was Cameron Haight of Gartner. For the past five years Cameron has been writing about, and advising clients on, DevOps. I caught some time with Cameron to get his thoughts. Some of the ground Cameron covers : How Cameron came to cover the DevOps movement. What changes has he seen in the community over the past five years. How does Cameron see DevOps evolving as it moves into the mainstream and where it fits within the larger transformation enterprises are undergoing. Stay tuned for the final interview in this series starring the one and only John Willis . Extra-credit reading Pau for now…","tags":"devops"},{"url":"http://www.ciandcd.com/blue-green-deployment-with-a-single-database.html","title":"Blue-Green Deployment With a Single Database","text":"From: http://java.dzone.com/articles/blue-green-deployment-single A blue-green deployment is a way to have incremental updates to your production stack without downtime and without any complexity for properly handling rolling updates (including the rollback functionality) I don't need to repeat this wonderful explanation or Martin Fowler's original piece . But I'll extend on them. A blue-green deployment is one where there is an \"active\" and a \"spare\" set of servers. The active running the current version, and the spare being ready to run any newly deployed version. The \"active\" and \"spare\" is slightly different than \"blue\" and \"green\", because one set is always \"blue\" and one is always \"green\", while the \"active\" and \"spare\" labels change. On AWS, for example, you can script the deployment by having two child stacks of your main stacks – active and spare (indicated by a stack label), each having one (or more) auto-scaling group for your application layer, and a script that does the following (applicable to non-AWS as well): push build to an accessible location (e.g. s3) set the spare auto-scaling group size to the desired value (the spare stays at 0 when not used) make it fetch the pushed build on startup wait for it to start run sanity tests switch DNS to point to an ELB in front of the spare ASG switch the labels to make the spare one active and vice versa set the previously active ASG size to 0 The application layer is stateless , so it's easy to do hot-replaces like that. But (as Fowler indicated) the database is the most tricky component. If you have 2 databases, where the spare one is a slave replica of the active one (and that changes every time you switch), the setup becomes more complicated. And you'll still have to do schema changes. So using a single database, if possible, is the easier approach, regardless of whether you have a \"regular\" database or a schemaless one. In fact, it boils down to having your application modify the database on startup, in a way that works with both versions. This includes schema changes – table (or the relevant term in the schemaless db) creation, field addition/removal and inserting new data (e.g. enumerations). And it can go wrong in many ways, depending on the data and datatypes. Some nulls, some datatype change that makes a few values unparseable, etc. Of course, it's harder to do it with a regular SQL database. As suggested in the post I linked earlier, you can use stored procedures (which I don't like), or you can use a database migration tool . For a schemaless database you must do stuff manually, but but fewer actions are normally needed – you don't have to alter tables or explicitly create new ones, as everything is handled automatically. And the most important thing is to not break the running version. But how to make sure everything works? test on staging – preferably with a replica of the production database (automatically) run your behaviour/acceptance/sanity test suites against the not-yet-active new deployment before switching the DNS to point to it. Stop the process if they fail. Only after these checks pass, switch the DNS and point your domain to the previously spare group, thus promoting it to \"active\". Switching can be done manually, or automatically with the deployment script. The \"switch\" can be other than a DNS one (as you need a low TTL for that). It can be a load-balancer or a subnet configuration, for example – the best option depends on your setup. And while it is good to automate everything, having a few manual steps isn't necessarily a bad thing. Overall, I'd recommend the blue-green deployment approach in order to achieve zero downtime upgrades. But always make sure your database is properly upgraded, so that it works with both the old and the new version.","tags":"devops"},{"url":"http://www.ciandcd.com/notes-from-troy-hunts-hack-yourself-first-workshop.html","title":"Notes from Troy Hunt's Hack Yourself First Workshop","text":"From: http://java.dzone.com/articles/notes-troy-hunts-hack-yourself Troy Hunt ( @troyhunt , blog ) had a great, very hands-on 2-day workshop about webapp security at NDC Oslo. Here are my notes. Highlights – resources Personal security and privacy https://www.entropay.com/ – a Prepaid Virtual Visa Card mailinator.com – tmp email f-secure VPN https://www.netsparker.com/ – scan a site for issues (insecure cookies, framework disclosure, SQL injection, …) (lot of $k) Site security https://report-uri.io/ – get reports when CSP rules violated; also displays CSP headers for a site in a human-friendly way https://securityheaders.io/ check quality of headers wrt security free SSL – http://www.startssl.com/ , https://www.cloudflare.com/ (also provides web app firewall and other protections) ; SSL quality check: https://www.ssllabs.com/ssltest/ https://letsencrypt.org/ – free, automated, open Certificate Authority (Linux Found., Mozilla) Breaches etc. http://arstechnica.com/security/2015/06/hack-of-cloud-based-lastpass-exposes-encrypted-master-passwords/ https://twitter.com/jmgosney – one of ppl behind http://passwordscon.org . http://password-hashing.net experts panel. Team Hashcat. http://arstechnica.com/security/2012/12/25-gpu-cluster-cracks-every-standard-windows-password-in-6-hours/ To follow ! http://krebsonsecurity.com/ ! http://www.troyhunt.com/ ! https://www.schneier.com/ ! https://twitter.com/mikko (of F-Secure) also great [TED] talks kevin mitnick (jailed for hacking; twitter, books) Books http://www.amazon.com/We-Are-Anonymous-LulzSec-Insurgency/dp/0316213527 – easy read, hard to put down http://www.amazon.com/Ghost-Wires-Adventures-Worlds-Wanted/dp/1441793755 – about Mitnick's hacking, social engineering, living on the run ? http://www.amazon.com/Art-Intrusion-Exploits-Intruders-Deceivers/dp/0471782661/ Mitnick: http://www.amazon.com/Art-Deception-Controlling-Element-Security/dp/076454280X/ – social engineering Other https://www.xssposed.org/ See https://www.drupal.org/SA-CORE-2014-005 https://www.youtube.com/watch?v=Qvhdz8yE_po – Havij example http://www.troyhunt.com/2013/07/everything-you-wanted-to-know-about-sql.html , http://www.troyhunt.com/2010/05/owasp-top-10-for-net-developers-part-1.html , http://www.troyhunt.com/2012/12/stored-procedures-and-orms-wont-save.html , Googlee: find config files with SA access info: `inurl:ftp inurl:web.config filetype:config sa` https://scotthelme.co.uk/hardening-your-http-response-headers/ and https://securityheaders.io/ https://developer.mozilla.org/en-US/docs/Web/Security/Public_Key_Pinning – prevent MITM wappalyzer chrome plugin displaying info about the server and client that can be detected (jQuery, NewRelic, IIS, win OS, …) http://www.troyhunt.com/2015/05/do-you-really-want-bank-grade-security.html http://www.troyhunt.com/2012/05/everything-you-ever-wanted-to-know.html tool: https://github.com/gentilkiwi/mimikatz extract plaintexts passwords, hash, PIN code and kerberos tickets from memory on Windows Notes HackYourselfFirst.troyhunt.com – an example app with many vulnerabilities Note: maximizing your browser window will share info about your screen size, which might help to identify you haveibeenpwned.com – Troy's online DB of hacked accounts Tips check robots.txt to know what to access Example Issues no https on login page insecure psw requirements cookies not secure flag => sent over http incl. AuthCookie) psw sent in clear text in confirm email user enumeration, f.eks. an issue with AdultFriendFinder – entry someone's email to login to find out whether they've an account post illegal chars, get them displayed => injection no anti-automation (captcha) login confirm. email & autom. creating 1m accounts => sending 1m emails => pisses ppl off, likely increase one's spam reputation (=> harder to send emails) brute-force protection? ### XSS Reflected XSS: display unescaped user input Encoding context: HTML, JS, CSS … have diff. escape sequences for the same char (e.g. <) – look at where they're mixed Check the encoding consistency – manual encoding, omitting some chars JS => load ext resources, access cookies, manipulate the DOM Task: stal authCookie via search ### SQL injection Error-based injection: when the DB helps us by telling us what is wrong -> use ti learn more and even show some data Ex.: http://hackyourselffirst.troyhunt.com/Make/10?orderby=supercarid <—— supercarid is a column name orderby=(select * from userprofile) … learn about DB sructure, force an exception that shows the valueex.: (select top 1 cast(password) as int from userprofile) => \"Conversion failed for the nvar value ‘passw0rd …'\" Tips think of SQL commands that disclose structure: sys.(tables,columns), system commands enumerate records: nest queries: select top X ows asc then top 1 rows from that desc write out how you think the query works / is being constructed internally cast things to invalid types to disclose values in err msgs (or implicit cast due to -1 ..) #### Defenses whitelist input data types (id=123 => onlyallow ints) enumerable values – check against an appropr. whitelist if the value is stored – who uses it, how? making query/insertion safe permissions: give read-only permissions as much as possible; don't use admin user from your webapp ### Mobile apps Look at HTTP req for sensitive data – creds, account, … Apps may ignore certificate validations In your app: param tampering, auth bypass, direct object refs Weak often: airlines, small scale shops, fast foods, … Tips certificate pining – the app has the fingerprint of the server cert. hardcoded and doesn't trust even \"valid\" MITM certificate (banks, dropbox, …)x ### CSRF Cross-Site Request Forgery = make the user send a request => their auth cookie included async Ajax req to another site forbidden but that doesn't apply to normal post Protection anti-forgery tags ### Understanding fwrk disclosure http://www.shodanhq.com/ -> search for \"drupal 7\" -> pwn How disclosed: headers familiar signs – jsessionid cookie for java, … The default error and 404 responses may help to recognize the fwr HTML code (reactid), \".do\" for Sttruts implicit: order of headers (Apache x IIS), paths (capitalized?), response to improper HTTP version/protocol, => likely still possible to figure out the stack but not possible to simple search for fwrk+version ### Session hijacking Steal authentication cookie => use for illegal requests. Persistence over HTTP of auth., session: cookie, URL (but URL insecure – can be shared) Session/auth ID retrieval: insecure transport, referrer, stored in exceptions, XSS Factors limiting hijacking: short duration expiry, keyed to client device / IP (but IPs may rotate, esp, on mobile devices => be very cautious) DAY 2 ——– ### Cracking passwords Password hashing: salt: so that 2 ppl choosing the same psw will have a different hash => cracking is # salts * # passwords inst. of just N has cracking tips: character space Dictionary: passw0rd, … Mutations: manipulation and subst. of characters Tips: 1Password , LastPass, …. GPU ~ 100* faster than CPU #### Ex: Crack with hashcat common psw dict + md5-hashed passwords => crack ./hashcat-cli64.bin –hash-type=0 StratforHashes.txt hashkiller.com.dic # 23M psw dict -> Recovered.: 44 326/860 160 hashes [obs duplications] in 4 min (speed 135.35k plains) Q: What dictionary we use? Do we apply any mutations to it? ### Account enumeration = Does XY have an account? Multiple vectors (psw reset, register a new user with the same e-mail, …) Anti-automation: is there any? It may be inconsistent across vectors Does it matter? (<> privacy needs) How to \"ask\" the site and how to identify + and – responses? Timing attacks: distinguish positive x negative response based on the latency differing between the two ### HTTPS Confidentiality, Integrity, Authenticity Traffic hijacking: [a href=\"https://www.wifipineapple.com/\"]https://www.wifipineapple.com/ – wifi hotspot with evil capabilities monitor probe requests (the phone looks for networks it knows), present yourself as one of those, the phone connects autom. (if no encryption) Consider everything sent over HTTP to be compromised Look at HTTPS content embedded in untrusted pages (iframes, links) – e.g. payment page embedded in http Links HSTS Preload – tell Chrome, FF that your site should only be ever loaded over HTTPS – https://hstspreload.appspot.com/ https://www.owasp.org/index.php/HTTP_Strict_Transport_Security header ### Content Scurity Policy header https://developer.chrome.com/extensions/contentSecurityPolicy See e.g. https://haveibeenpwned.com/ headers w/o CSP anything can be added to the page via a reflected XSS risk Anyth, can be added to the DOM downstream (on a proxy) With CSP the browser will only load resources you white-list; any violations can be reported Use e.g. https://report-uri.io/home/generate to create it and the report to watch for violations to fine tune it. ### SQL injection cont'd (Yesterday: Error-Based) #### Union Based SQLi Modify the query to union whatever other data and show them. More data faster than error-based inj. Ex.: http://hackyourselffirst.troyhunt.com/CarsByCylinders?Cylinders=V12 : V12 -> `V12′ union select voteid, comments collate SQL_Latin1_General_CP1_CI_AS from vote– ` #### Blind Boolean (laborious) Blind inj.: We can't always rely on data being explicitly returned to the UI => ask a question, draw a conclusion about the data. Ex: http://hackyourselffirst.troyhunt.com/Supercar/Leaderboard?orderBy=PowerKw&asc=false -> ordedby => case when (select count(*) from userprofile) > 1 then powerkw else topspeedkm end Extract email: Is ascii of the lowercase char #1 < ascii of m ? Automation: SqlMap #### Time based blind injection When no useful output returned but yes/no responses differ significantly in how much time they take. F.ex. ask the db to delay the OK response. MS SQL: IF ‘b' > ‘a' WAITFOR DELAY '00:00:05′ ### Brute force attacks Are there any defences? Often not How are defences impl? block the req resources block the src IP rate limit (by src IP) ### Automation penetration testing apps and services such as Netsparker, WhiteHatSec targets identification: shodan, googledorks, randowm crawling think aout the actions that adhere to a pattern – sql injection, fuzzing (repeat a req. trying diff. values for fields – SQLi, …), directory enumeration automation can be used for good – test your site tip: have autom. penetration testing (and perhaps static code analysis) as a part fo your build pipeline Task: Get DB schema using sqlmap (see python2.7 sqlmap.py –help) ### Protection Intrusion Detection System (IDS) – e.g. Snort Web Application Firewall (WAF) – e.g. CloudFare ($20/m)","tags":"devops"},{"url":"http://www.ciandcd.com/what-different-security-testing-methodologies-are-out-there.html","title":"What Different Security Testing Methodologies Are Out There?","text":"From: http://java.dzone.com/articles/what-different-security Every business has unique characteristics that set it apart from other organizations, even within the same industry. For this reason, it shouldn't be surprising that there's not a one-size-fits-all approach to app security testing. Each company has certain protection expectations and regulations to adhere to, making it essential to find the best way to achieve these goals. Here are a few examples of security testing methodologies available for quality assurance teams to leverage: Black box With black box testing, QA professionals put themselves in the shoes of the hacker and attempt to break the app through various attack vectors. This methodology can yield a lot of information and help better secure the program from actual threats. A white paper by Security Innovation noted that software testers first analyze the system's architecture and business model to identify any security vulnerabilities. Looking over the software logic in this way can uncover subtle security and privacy issues that may not have been noticed otherwise, such as defects in design, input, system dependency, authentication, cryptography and information disclosure. \"Although white box code inspection is good for analyzing static behavior, only black box exploratory testing can determine the dynamic behavior of how a system is implemented and used, the coupling between systems and the interactions of the distributed systems,\" Security Innovation wrote. Dynamic For QA teams that like to execute code, dynamic testing is the approach for them. This methodology checks the running application for how it behaves and responds to a variety of inputs. This is done to ensure that the product meets up with established regulations and is giving the expected outcomes. IBM noted that dynamic analysis is especially useful to identify code coverage, as it can discover bugs in paths that have gone untested. While dynamic testing can be manual work for testers, it can also yield significant information that will help mitigate defects and produce quality products. Static In contrast to dynamic testing, static approaches directly review the source code, often through an automated test management solution . TechTarget contributor Michael Cobb noted that this methodology occurs at the implementation phase, rather than when the app is running, and often helps mitigate vulnerabilities involved with industry compliance standards. Automation in this area can reduce the amount of time it takes to complete these tasks. However, it may not be able to detect sophisticated threats, which can be supplemented by dynamic security testing . \"A thorough source code review has an advantage over dynamic testing,\" Cobb wrote. \"Nothing is hidden from analysts during a source code review, so they can examine exactly how data flows through a program. By solving the problem at the code level, static testing reduces the number of security-related design and coding defects, and the severity of any defects that make it through to the release version, thus dramatically improving the overall security of the application.\" There are a number of security testing methodologies that organizations can pursue, and designing a combination of approaches may result with a solution that's best for their needs. Using these strategies, companies can better ensure the protection of sensitive information while providing users with the software testing tools needed to succeed.","tags":"devops"},{"url":"http://www.ciandcd.com/how-to-keep-rest-api-credentials-secure.html","title":"How to Keep REST API Credentials Secure","text":"From: http://java.dzone.com/articles/how-keep-rest-api-credentials If you are building mobile apps then you are connecting to some REST API. For example, if you want to resolve an address to a latitude/longitude information to display on a map, you might use the If you are building mobile apps then you are connecting to some REST API. For example, if you want to resolve an address to a latitude/longitude information to display on a map, you might use the Google Geocoding API https://maps.googleapis.com/maps/api/geocode/json?address=San Francisco,CA&key=AIzaSyDvFMYGjeR02RH Google Geocoding API: If you are invoking the API from the client, then the API key also has to be present on the client. But, this is also the problem. It's very easy to look at the app source in the browser and get access to the API key. If someone has access to your API key, they can send requests on your behalf (without you knowing), and use up your request quota. Even if you are building a hybrid app, it's still the same problem. A hybrid app is HTML/JavaScript inside a native wrapper, it's possible to download the app, un-package it and gain access to API keys or any sensitive information stored in the app. Even native apps are not immune to this. For example, an Android app is just a Java application and a Java application can be de-compiled to view the original source. The next image shows how to get access to an API key in the browser: A good solution is to never expose the API key (or any other sensitive data) on the client. How do you do that? You keep the API key and any other sensitive information on the server. Appery.io Secure Proxy (part of Backend Services) enables app developers to keep sensitive app data on the server. Your API keys or any other data is never exposed on the client. Watch this 5-minute video on how to use Secure Proxy: Before using the Secure Proxy, you need to store the data on the server. To store the data you are going to use the Appery.io Database. It's as simple as creating a collection with two columns. The first column is the value name, the second column is the actual value. This is how the database looks when storing the API key for Google Geocoding API: As this key is stored on the server, no one (but you) has access to it. You can store other data as well such as URLs, tokens or anything else that shouldn't be exposed on the client. The next step is to setup the proxy that will use the information stored in the database. This step is also very simple, this is how it looks: You give the proxy a name and then link it to a database which stores your data. The above proxy is linked to Secrets_db database, Credentials collection, and secretName , secretValue columns. The last step is to link a REST API service to the proxy. In the service editor you select the secure proxy created: then in the Request tab you reference the API key stored in the database (the name stored in secretName column): and that's it. When the API service is invoked, the call will go through the secure proxy (server) where the API key will substituted: For web apps, you can add an extra layer of security by specifying from which page URLs the proxy should accept requests: The proxy will only accept requests from page URLs listed in the table. Another option to keep API keys private is to invoke the API from the server using Server Code , I will cover this in another post. Setting up an using the Appery.io Secure Proxy is simple. It provides a very important feature by allowing to keep sensitive and private data on the server, never exposing it on the client, and adding an extra security layer to your app.","tags":"devops"},{"url":"http://www.ciandcd.com/managing-outsourced-quality-assurance-teams.html","title":"Managing Outsourced Quality Assurance Teams","text":"From: http://java.dzone.com/articles/managing-outsourced-quality Business leaders like to be in control of every aspect of their operations, but if any element is outsourced, that sense of governance becomes much more difficult to maintain. Outsourcing can be appealing to organizations looking for talent at reasonable costs - it just takes significant planning to pull off successfully. When a team is in a different location from the company, there are a number of challenges that must be overcome. We will look at what some of the biggest issues are and how you can appropriately manage an outsourced quality assurance team. Obstacles of outsourcing While outsourcing can have a lot of advantages for businesses, the number of roadblocks can be intimidating for many enterprises. Here are the biggest challenges to prepare for: Information sharing: When you don't see individuals every day, it can often be easy to forget to relay important messages. Communication in this situation is imperative, as it could affect the overall operation of the application. As soon as a request is given, there should be seamless transfer of knowledge to ensure that everyone is on the same page and that the project proceeds as expected. This will eliminate any redundancies and lower the overall development cost. Engagement: Many outsourced teams have a difficult time becoming personally involved with their projects. This will also affect their ability to collaborate effectively with other teams in the business, ultimately hurting program quality. Organizations must have a strategy to keep these individuals motivated and provide them with the tools to succeed. Technology/skills: An outsourced team may use different pieces of technology or have skills separate from what the organization was looking for. For example, if the company really wants to move to agile software development , but the outsourced group still uses waterfall methods, that could create problems down the road for their software development initiatives. Similarly, the business must ensure that the outsourced individuals have the skills necessary to meet corporate goals and spur innovation through their app testing. How to regain control Although total governance of outsourced assets won't be possible, there are still some things that organizations can do to take control of these teams and ensure that they're fulfilling business objectives. Australian outsourcer Beepo suggested a consistent schedule for gathering feedback and using technology like the cloud and test management tools. Let's dissect each of these ideas. Many outsourced teams are often left by the wayside when it comes to communication. This can reasonably lead to mistakes being made and leave the members feeling apathetic toward their work. However, by setting up regular video conferences and requesting feedback, outsourced individuals can feel empowered to express their opinions and become a larger part of their projects. This will also help build trust and motivate teams to collaborate more. Using tools like test management and the cloud can also be helpful when working with an outsourced team due to the fact that they provide a singular platform for all users. This means that the outsourced and in-house teams can be working on the same project at the same time, with any changes being made in real time. This will not only reduce redundancies, but it creates accountability and ensures that tasks are being addressed according to their priority. Considerations to make when outsourcing Whether you're looking to outsource, or simply make your outsourced team better, there are some key items to address. Ashok Mani from AppLabs noted that organizations must look into a provider's engagement models, mobilization efforts, communication plans, security and scalability. These elements will be essential to clear up before trying to manage a team. \"While organizations are deriving value from outsourcing software development , outsourced software testing will maximize returns from their investments and provide the right level of objectivity and rigor required to create a high-quality product,\" Mani stated. \"If an independent QA and testing service provider is chosen whose focus is on ensuring quality products/systems are implemented, benefits will be fully maximized.\" Outsourcing a quality assurance team is going to have a few challenges for businesses. But by preparing for these obstacles, they will be able to manage the outsourced group more effectively. Having a communication plan and technology available will be essential to working well with the team and improving development operations.","tags":"devops"},{"url":"http://www.ciandcd.com/lastpass-breach-password-security-and-reason.html","title":"LastPass Breach, Password Security, and Reason","text":"From: http://java.dzone.com/articles/lastpass-breach-password LastPass , the password manager that lets you manage your passwords between different devices, was recently hacked . From this there has been a fair amount of FUD circulating and not enough rational thought. With that in mind, this seems like a good time to talk about password security and LastPass with some rational ideas. Since we can't get rid of passwords just yet we need to manage them well. 1. LastPass Detected The Breach No useful system is impenetrable. Computers not connected to the Internet, that don't even have a network card, have been hacked across an air gap using their speakers and mic. The most up to date systems still suffer from zero-day exploits . Two of the elements of an organization that takes security seriously are keeping certain pieces of data separate and detecting when a breach occurs. From the LastPass announcement of the breach we can see these two things in action. While some information was obtained the actual vaults of passwords were not downloaded. And, they detected there was a problem and enough monitoring in place to distinguish what was effected. I can't overstate how nice that is to hear. Many organizations won't detect if they have been breached. Even many of those that could detect a breach wouldn't be able to tell you what was affected. That's right, many of the places you put personal information couldn't do what LastPass did. 2. LastPass Responded To The Breach Even though the password vaults were not taken LastPass is having everyone change their vault password. They detected the problem and are going the extra mile to protect their users. Now, let's consider an alternative option. Consider a 1Password or KeePass user who stores their information in Dropbox or a similar service. A malicious program on one of their systems could have taken their vault and sent it to an attacker. Those users would not have known. Or, the service could have been hacked but since it's not password specific who would have suggested changing the master password? I'm not trying to defend LastPass. It's a matter of considering the alternatives and the security measures around them. Is a 1Password or KeePass alternative setup actually more secure in practice? 3. Different Passwords For Different Sites There's a good reason to have a different password or passphrase for different sites. You can't trust that a site you submit it to will store it securely so that it won't be misused to access other sites. With all the sites we connect to we it's difficult to remember a different password for each site. Congratulations if you can do that. For the rest of us we need a system to help. This is where a password manager is useful. That is, until we can stop using passwords for something better. So, use a password manager if you can't otherwise have a different password for each site. It's more secure than using the same password everywhere. Note, I'm not recommending a particular password manager on purpose. Use a good one. 4. Security Is Not About Perfection There is no such thing as perfect security. Security needs to be practical. For example, for most people it's more security to use a password manager than to use the same password everywhere. Neither is perfect but when you weigh the differences the password manager comes out as more secure. In a distributed device world where we need passwords on more than one system it's good to go with a system that does this for you. A system that focuses on security and handling issues that come up. You could roll your own solution. But, will it be more secure? For most people the professional solution is the more secure one. When considering password security choose the one that's more secure for you rather than seeking the perfect option. 5. When Not To Use A Password Manager There are some places I would recommend not using a password manager. For example, I would recommend not using one for your financial sites. Those few places that are very important use a passphrase. 6. Encrypt Your Password Store In our multi-device world you'll likely need to share your password datastore between devices. And, any device can be hacked even if you don't need to use multiple devices. Imagine a virus on a computer looking for your password excel file and uploading that to someone bad. It happens. Use an encrypted datastore. This is why password managers are important. They are designed to store your data in an encrypted manner. This way, if someone gets your data store they will have a very hard time reading it. Before they can get to anything they'll need to break the encryption which isn't so easy. That means, even if an attacker had gotten the password vaults from LastPass, which they didn't, they would not have been able to read the data in them. 7. Limit Your Attack Vector One of the problems with LastPass is that they are a known password manager. That makes them a target if someone wants to try and get passwords. Alternatives that store their distributed information in general purpose systems pose a different attack vector. For example, if you use 1Password or KeePass and store your information in Dropbox you can still be hacked. Dropbox has been externally hacked in the past and other applications can access your Dropbox folder. Using alternatives to LastPass doesn't mean you won't be attacked. Take a few minutes and consider the attack vectors of the different solutions you're considering and how each of those will detect a breach and respond to that. For example, I could self host my encrypted file on the Internet somewhere. This would be managed by me and wouldn't be a known system for someone to target. But, the IPv4 addresses are regularly checked for known vulnerabilities so attackers and get onto a system a poke around. That is the entire IPv4 space, which is still the only space routable for all things, is regular checked. Will I keep everything on that system up to date? Will I detect if someone broke into the system? Will I respond appropriately? All of this needs to be taken into account. Final Thoughts On LastPass I'm not trying to defend LastPass. I'm trying to give a little more of a holistic picture of security. It's complicated and any alternatives to a LastPass or LastPass-like solutions need to have their security considered. Viewing the options with security and attacks in mind keeps everything in perspective.","tags":"devops"},{"url":"http://www.ciandcd.com/does-devops-reduce-technical-debt-or-make-it-worse.html","title":"Does DevOps Reduce Technical Debt--or Make it Worse?","text":"From: http://java.dzone.com/articles/does-devops-reduce-technical-0 DevOps can help reduce technical debt in some fundamental ways. Continuous Delivery/Deployment First, building a Continuous Delivery/Deployment pipeline , automating the work of migration and deployment, will force you to clean up inconsistencies and holes in configuration and code deployment, and inconsistencies between development, test and production environments. And automated Continuous Delivery and Infrastructure as Code gets rid of dangerous one-of-a-kind snowflakes and configuration drift caused by making configuration changes and applying patches manually over time. Which makes systems easier to setup and manage, and reduces the risk of an un-patched system becoming the target of a security attack or the cause of an operational problem . A CD pipeline also makes it easier, cheaper and faster to pay down other kinds of technical debt. With Continuous Delivery/Deployment, you can test and push out patches and refactoring changes and platform upgrades faster and with more confidence. Positive Feedback The Lean feedback cycle and Just-in-Time prioritization in DevOps ensures that you're working on whatever is most important to the business. This means that bugs and usability issues and security vulnerabilities don't have to wait until after the next feature release to get fixed. Instead, problems that impact operations or the users will get fixed immediately. Teams that do Blameless Post-Mortems and Root Cause(s) Analysis when problems come up will go even further, and fix problems at the source and improve in fundamental and important ways. But there's a negative side to DevOps that can add to technical debt costs. Erosive Change Michael Feathers' research has shown that constant, iterative change is erosive : the same code gets changed over and over, the same classes and methods become bloated (because it is naturally easier to add code to an existing method or a method to an existing class), structure breaks down and the design is eventually lost. DevOps can make this even worse. DevOps and Continuous Delivery/Deployment involves pushing out lots of small changes, running experiments and iteratively tuning features and the user experience based on continuous feedback from production use. Many DevOps teams work directly on the code mainline, \" branching in code \" to \" dark launch \" code changes, while code is still being developed, using conditional logic and flags to skip over sections of code at run-time. This can make the code hard to understand, and potentially dangerous: if a feature toggle is turned on before the code is ready, bad things can happen . Feature flags are also used to run A/B experiments and control risk on release, by rolling out a change incrementally to a few users to start. But the longer that feature flags are left in the code, the harder it is to understand and change . There is a lot of housekeeping that needs to be done in DevOps: upgrading the CD pipeline and making sure that all of the tests are working; maintaining Puppet or Chef (or whatever configuration management tool you are using) recipes; disciplined, day-to-day refactoring ; keeping track of features and options and cleaning them up when they are no longer needed, getting rid of dead code and trying to keep the code as simple as possible. Microservices and Technology Choices Microservices are a popular architectural approach for DevOps teams. This is because loosely-coupled Microservices are easier for individual teams to independently deploy, change, refactor or even replace . And a Microservices-based approach provides developers with more freedom when deciding on language or technology stack: teams don't necessarily have to work the same way, they can choose the right tool for the job, as long as they support an API contract for the rest of the system. In the short term there are obvious advantages to giving teams more freedom in making technology choices. They can deliver code faster, quickly try out prototypes, and teams get a chance to experiment and learn about different technologies and languages. But Microservices \" are not a free lunch \". As you add more services, system testing costs and complexity increase. Debugging and problem solving gets harder. And as more teams choose different languages and frameworks, it's harder to track vulnerabilities, harder to operate, and harder for people to switch between teams. Code gets duplicated because teams want to minimize coupling and it is difficult or impossible to share libraries in a polyglot environment. Data is often duplicated between services for the same reason, and data inconsistencies creep in over time. Negative Feedback There is a potentially negative side to the Lean delivery feedback cycle too. Constantly responding to production feedback, always working on what's most immediately important to the organization, doesn't leave much space or time to consider bigger, longer-term technical issues, and to work on paying off deeper architectural and technical design debt that result from poor early decisions or incorrect assumptions. Smaller, more immediate problems get fixed fast in DevOps. Bugs that matter to operations and the users can get fixed right away instead of waiting until all the features are done, and patches and upgrades to the run-time can be pushed out more often. Which means that you can pay off a lot of debt before costs start to compound. But behind-the-scenes, strategic debt will continue to add up. Nothing's broke, so you don't have to fix anything right away. And you can't refactor your way out of it either, at least not easily. So you end up living with a poor design or an aging technology platform, slowly slowing down your ability to respond to changes, to come up with new solutions. Or forcing you to continue filling in security holes as they come up, or scrambling to scale as load increases. DevOps can reduce technical debt. But only if you work in a highly disciplined way. And only if you raise your head up from tactical optimization to deal with bigger, more strategic issues before they become real problems.","tags":"devops"},{"url":"http://www.ciandcd.com/get-back-up-and-try-again-retrying-in-python.html","title":"Get Back Up and Try Again: Retrying in Python","text":"from:http://python.dzone.com/articles/get-back-and-try-again I don't often write about tools I use when for my daily software development tasks. I recently realized that I really should start to share more often my workflows and weapons of choice. One thing that I have a hard time enduring while doing Python code reviews, is people writing utility code that is not directly tied to the core of their business. This looks to me as wasted time maintaining code that should be reused from elsewhere. So today I'd like to start with retrying , a Python package that you can use to… retry anything. It's OK to fail Often in computing, you have to deal with external resources. That means accessing resources you don't control. Resources that can fail, become flapping, unreachable or unavailable. Most applications don't deal with that at all, and explode in flight, leaving a skeptical user in front of the computer. A lot of software engineers refuse to deal with failure, and don't bother handling this kind of scenario in their code. In the best case, applications usually handle simply the case where the external reached system is out of order. They log something, and inform the user that it should try again later. In this cloud computing area, we tend to design software components with service-oriented architecture in mind. That means having a lot of different services talking to each others over the network. And we all know that networks tend to fail, and distributed systems too. Writing software with failing being part of normal operation is a terrific idea. Retrying In order to help applications with the handling of these potential failures, you need a plan. Leaving to the user the burden to \"try again later\" is rarely a good choice. Therefore, most of the time you want your application to retry. Retrying an action is a full strategy on its own, with a lot of options. You can retry only on certain condition, and with the number of tries based on time (e.g. every second), based on a number of tentative (e.g. retry 3 times and abort), based on the problem encountered, or even on all of those. For all of that, I use the retrying library that you can retrieve easily on PyPI . retrying provides a decorator called retry that you can use on top of any function or method in Python to make it retry in case of failure. By default, retry calls your function endlessly until it returns rather than raising an error. import random from retrying import retry @retry def pick_one(): if random.randint(0, 10) != 1: raise Exception(\"1 was not picked\") This will execute the function pick_one until 1 is returned by random.randint . retry accepts a few arguments, such as the minimum and maximum delays to use, which also can be randomized. Randomizing delay is a good strategy to avoid detectable pattern or congestion. But more over, it supports exponential delay, which can be used to implement exponential backoff , a good solution for retrying tasks while really avoiding congestion. It's especially handy for background tasks. @retry(wait_exponential_multiplier=1000, wait_exponential_max=10000) def wait_exponential_1000(): print \"Wait 2&#94;x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards\" raise Exception(\"Retry!\") You can mix that with a maximum delay, which can give you a good strategy to retry for a while, and then fail anyway: # Stop retrying after 30 seconds anyway >>> @retry(wait_exponential_multiplier=1000, wait_exponential_max=10000, stop_max_delay=30000) ... def wait_exponential_1000(): ... print \"Wait 2&#94;x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards\" ... raise Exception(\"Retry!\") ... >>> wait_exponential_1000() Wait 2&#94;x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards Wait 2&#94;x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards Wait 2&#94;x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards Wait 2&#94;x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards Wait 2&#94;x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards Wait 2&#94;x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/usr/local/lib/python2.7/site-packages/retrying.py\", line 49, in wrapped_f return Retrying(*dargs, **dkw).call(f, *args, **kw) File \"/usr/local/lib/python2.7/site-packages/retrying.py\", line 212, in call raise attempt.get() File \"/usr/local/lib/python2.7/site-packages/retrying.py\", line 247, in get six.reraise(self.value[0], self.value[1], self.value[2]) File \"/usr/local/lib/python2.7/site-packages/retrying.py\", line 200, in call attempt = Attempt(fn(*args, **kwargs), attempt_number, False) File \"<stdin>\", line 4, in wait_exponential_1000 Exception: Retry! A pattern I use very often, is the ability to retry only based on some exception type. You can specify a function to filter out exception you want to ignore or the one you want to use to retry. def retry_on_ioerror(exc): return isinstance(exc, IOError) @retry(retry_on_exception=retry_on_ioerror) def read_file(): with open(\"myfile\", \"r\") as f: return f.read() retry will call the function passed as retry_on_exception with the exception raised as first argument. It's up to the function to then return a boolean indicating if a retry should be performed or not. In the example above, this will only retry to read the file if an IOError occurs; if any other exception type is raised, no retry will be performed. The same pattern can be implemented using the keyword argument retry_on_result , where you can provide a function that analyses the result and retry based on it. def retry_if_file_empty(result): return len(result) <= 0 @retry(retry_on_result=retry_if_file_empty) def read_file(): with open(\"myfile\", \"r\") as f: return f.read() This example will read the file until it stops being empty. If the file does not exist, an IOError is raised, and the default behavior which triggers retry on all exceptions kicks-in – the retry is therefore performed. That's it! retry is really a good and small library that you should leverage rather than implementing your own half-baked solution!","tags":"devops"},{"url":"http://www.ciandcd.com/continuous-integration-and-delivery-with-docker.html","title":"Continuous Integration and Delivery with Docker","text":"from:http://java.dzone.com/articles/continuous-integration-and-0 Written by Jaroslav Holub for The Codeship Blog . Continuous delivery is all about reducing risk and delivering value faster by producing reliable software in short iterations. As Martin Fowler says , you actually do continuous delivery if: Your software is deployable throughout its lifecycle. Your team prioritizes keeping the software deployable over working on new features. Anybody can get fast, automated feedback on the production readiness of their systems any time somebody makes a change to them. You can perform push-button deployments of any version of the software to any environment on demand. Containerization of software allows us to further improve on this process. The biggest improvements are in speed and in the level of abstraction used as a cornerstone for further innovations in this field. In this post, I'll show you how to set up a continuous delivery pipeline using Docker. We'll see how using this tool for Linux containers as part of the continuous delivery pipeline lets us nicely encapsulate the build process of a service. It also lets us deploy any revision with a few simple steps. I'll mainly use the term continuous delivery in this article, because it stands for the full circle of steps leading to our ultimate goal. However, continuous integration is the most substantial part of continuous delivery. Continuous Integration with Docker Let's take a Hello World web server written in Go as an example service. You can find all the code used in this example here: https://github.com/ContainerSolutions/cd-with-docker-tutorial The continuous integration setup consists of: running unit tests building the Docker image that we use to build our service running the build container and compiling our service building the Docker image that we run and deploy pushing the final image to a Docker registry Automated testing Running tests in this example is as trivial as it should be: go test Building Docker image The core of a single service integration is making the end artifact — Docker image in our case. Because I've deliberately chosen the compiled language Go in this example, we need to build an executable file as part of our integration process. We'll eventually place the executable file inside this Docker image. Now one might think that we would build our web server executable file using build tools installed on the host dedicated to continuous integration and then somehow copy the binary to the Docker image. But this is a no-no in the containerized world. Let's do it all in containers. That way, we won't rely on any build tools installed on hosts, and it'll make the whole setup easily reproducible and encapsulated. Building an executable file can be part of a single Docker image build process together with runtime environment setup. Or we can separate the two. Having everything in a single build process, we would end up with extra content (build process leftovers) in our Docker image filesystem, even if we clean it afterwards in separate RUN commands within the Dockerfile. Some people use tricks to create, manipulate, and remove unwanted stuff in a single RUN command. Although it's sometimes handy, I can't generally recommend it; in my opinion this adds to Dockerfile complexity. Of course, there are situations where you might want to retain your sources and all in the end artifact. The approach I recommend, however, is to create separate \"build\" and \"distribution\" Dockerfiles. Use Dockerfile.build to do the heavy lifting during building the software, and use Dockerfile.dist to create the distributable Docker image, as light and clean as possible. The following is Dockerfile.build . As you can see, once we run the build file, we create the container from a golang image, compile our example service, and output the binary. FROM golang:1.4 RUN mkdir -p /tmp/build ADD hello-world.go /tmp/build/ WORKDIR /tmp/build RUN go build hello-world.go CMD tar -czf - hello-world In Dockerfile.dist , we only use this binary and run it on runtime: FROM debian:jessie RUN mkdir /app ADD build.tar.gz /app/ ENTRYPOINT /app/hello-world Our build.sh script — the essential part of our continuous integration pipeline — then looks like this: # !/bin/sh docker build -t hello-world-build -f Dockerfile.build . docker run hello-world-build > build.tar.gz docker build -t hello-world -f Dockerfile.dist . As you can see, these three simple Docker commands get us a clean, small Hello-World Docker image that's ready to be deployed and run on demand. Once both images used in the FROM clauses are pulled and cached locally, our build process will be a matter of milliseconds or at most a few seconds, with a very small resources footprint. Storing Docker images Once our build process artifact is created, we want to push it to Docker Registry, where it will be available for deployments. Please note that tagging images properly is very important. Docker ecosystems suffer from the usage of \"latest\" tag. If you use a unique tag for every new image, then all your image versions will be easily accessible for deployment in the future. We can choose whether we want to use our own Docker Registry or rely on Docker Hub . On Docker Hub, you can store public or private repositories of images. It's also the first place people would look for your images (if you want anyone to look for them). Your own Docker Registry on the other hand gives you full control over your images storage, performance, and security. More advanced setups might combine both approaches. This way you can tag the new image with an appropriate tag and push it to a public hub (replace your_username and your_tag with actual values): # !/bin/sh docker tag hello-world:latest your_username/hello-world:your_tag docker push your_username/hello-world:your_tag Continuously Delivered Containers Once we have our Docker images building pipeline working and images nicely stashed in a repository, we definitely want to get our service deployed. How you deploy your applications depends on your infrastructure or cloud provider. A few cloud providers support Docker images in their APIs these days (e.g., Amazon EC2 Container Service , Digital Ocean , or Giant Swarm ). You can further leverage the power of containerized applications with resource abstraction tools like Apache Mesos (read more about running containers on Mesos ) or Google Kubernetes that let you deploy and manage containers in their own ways. In case of our Hello World example, deploying remotely means running the following command remotely on a target machine with Docker installed on it: # !/bin/sh docker stop hello-production docker run --rm -p 8000:80 --name hello-production hello-world Beyond Continuous Delivery with Docker Using containerized software does not inherently mean one is implementing microservices. However, containers enable this architectural pattern because they encourage developers to split their monoliths based on separation of concerns. Microservices also promote communication between containerized components over a plain network using standardized and easily replaceable tubes. To learn more about microservices and why they might be a good architectural pattern for your software project, I recommend Building Microservices by Sam Newman. A continuous delivery pipeline with containerized software also allows you to set up a new kind of testing environment; subsets of (micro)services are deployed in small clusters that represent the system under test running with some parts intentionally disabled or disconnected. Creation of such a matrix of deployments and programming against it has little to no additional costs in terms of a continuous integration time. It does have a dramatic impact on the stability and resilience of software in production. Such a testing system allows teams to get ready to deal with any kind of Chaos Monkey .","tags":"devops"},{"url":"http://www.ciandcd.com/enabling-dataops-with-easy-log-analytics.html","title":"Enabling DataOps with Easy Log Analytics","text":"from:http://java.dzone.com/articles/enabling-dataops-easy-log DataOps is becoming an important consideration for organizations. Why? Well, DataOps is about making sure data is collected, analyzed, and available across the company – i.e. Ops insight for your decision-making systems like Hubspot, Tableau, Salesforce and more. Such systems are key to day-to-day operations and in many cases are as important as keeping your customer facing systems up and running. If you think about it, today every online business is a data driven business! Everyone is accountable to have up to the minute answers on what is happening across their systems. You can't do this reliably without having DataOps in place. We have seen this trend across our own customer base at Logentries where more and more customers using log data to implement DataOps across their organization . Using log data for DataOps allows you to perform the following: Troubleshoot your systems managing your data by identifying errors and correlating data sources Get notified when one of these systems is experiencing issues via real time alerts or anomaly detection Analyze how these systems are used by the organization Logentries has always been great at 1 and 2 above, and this week we have enhanced Logentries to now allow you to perform easier and more powerful analytics with our n ew easy-to-use SQL like query language – Logentries QL (LEQL) . LEQL is designed to make analyzing your log data dead simple. There are too many log management tools that are built around complex query languages and require data scientists to operate. Logentries is all about making log data accessible to anyone. With LEQL you are going to be able to use analytical functions like CountUnique, Min, Max, GroupBy, Sort…A number of our users have already been testing these out via our beta program. One great example is how Pluralsight has been using Logentries to manage and understand the usage of their Tableau environment . For example: Calculating the rate of errors over the the past 24 hours e.g. using LEQL Count function Understanding user usage patterns e.g. using GroupBy to understand queries performed grouped by different users Sorting the data to find the most popular queries and how long they are taking Being able to answer these types of questions enables DataOps teams to understand where they need to invest time going forward. For example, do I need to add capacity to improve query performance? Are internal teams having a good user experience or are they getting a lot of errors when they try to access data? At Logentries we are all about making the power of log data accessible to everyone and as we do this we are constantly seeing cool new use cases when using logs. If you have some cool use cases do let us know!","tags":"devops"},{"url":"http://www.ciandcd.com/how-to-make-sure-your-mobile-app-is-secure.html","title":"How to Make Sure Your Mobile App is Secure","text":"from:http://java.dzone.com/articles/how-make-sure-your-mobile-app Mobile app development has become vital for enterprises as they look to support new devices (phones, tablets, wearables, etc.) for internal use while also reaching out to their increasingly mobile customers. This approach makes sense: According to a comScore report , the number of mobile Internet users outnumbered desktop ones for the first time at some point in late 2013, and has since achieved significant separation. Many companies have responded to this change by implementing bring-your-own-device policies and building mobile apps that complement their full websites, mobile Web presence and/or desktop applications. Watch out for pitfalls in mobile apps: General risks and the recent Starbucks example However, both BYOD policies and mobile app development require due diligence around cybersecurity if they are to be worthwhile. Safety starts with well-designed applications that are strongly authenticated, do not leak sensitive data and are safe from popular attack vectors like brute-force password guessing. Unfortunately, many apps still have a long way to go on these fronts. An early 2014 study from MetaIntell discovered that 92 percent of the top 500 most popular Android apps at the time created privacy risks due to data leakage . Wary of leaky apps as well as what kinds of information users put into them, enterprises have understandably been concerned about the impact of mobile apps on their operations and BYOD initiatives. Security is often the biggest barrier to effective BYOD, and justifiably so considering that barely more than 40 percent of employees are required to have a security tool installed, according to Webroot. To get a sense of what could go wrong with today's mobile apps, consider what recently happened to Starbucks. The company's app is a mainstay on many phones, and at one time it accounted for the bulk of all mobile payments made in North America. The issue that arose over the last few months involved unauthorized card reloads and apparent account hijackings. The causes may have been mixed, with poor password management on the part of users possibly exacerbated by exploitation of the app's auto-reload feature and an April 2015 outage of the coffee chain's point-of-sale systems. At the end of the day, Starbucks implemented additional security questions and has been urged to add two-factor authentication into the app to prevent erroneous transactions. Catching mobile app security issues with a test management solution As we can see, mobile app security is multifactorial, requiring best efforts on the parts of end users, developers and infrastructure/network providers. For enterprises, the best approach to ensuring long-term security is to catch potential vulnerabilities early and often with a test management system. A test management solution supports both automated and manual testing , and receiving updates in real-time offers you the ability to make important decisions once issues arise. Regardless of how many tests, sprints and projects your company is running, all of them should be conveniently viewed from a lone interface, enabling a single source of truth that keeps your mobile app development initiatives on track.","tags":"devops"},{"url":"http://www.ciandcd.com/ode-to-a-workstation.html","title":"Ode to a Workstation","text":"from:http://java.dzone.com/articles/ode-workstation Every now and then I get work done in the home office. I've written previously about my setup, but after churning out some solution design today, I sat back and really took some time to appreciate the workspace. I'm really pleased with the configuration, it's probably the best setup I've had in years. The desk is a former QLD police desk from the 1940s, so it wasn't built for modern computers – not a problem, the cables run down the back which is just a minor annoyance. The keyboard and mouse are gaming varieties so that they perform well – the old Sennheiser (RF) wireless headset has been with me since 2006 and still works very well. The wooden clock ( recently reviewed ) acts as external speakers, a Bluetooth receiver and has a built in microphone so it can be used as a hands-free option for conference calls. It also features Qi wireless charging capability and also features a thermostat. Under the second monitor is a HDD caddy which supports USB3, and features 4 bays which can be used in parallel. I try to keep the desk reasonably neat, and there's plenty of space so it doesn't get too cluttered. I have a nice view out the window to a small courtyard which gets early morning sun.","tags":"devops"},{"url":"http://www.ciandcd.com/ensure-software-security-by-understanding-the-attack-surface.html","title":"Ensure Software Security by Understanding the Attack Surface","text":"from:http://java.dzone.com/articles/ensure-software-security For many organizations, it seems like cyberattacks can come from anywhere, at any time. This sense is heightened by the number of endpoints in play that could be vulnerable to threats. Quality assurance teams must ensure that they have the data on hand to keep these risks at bay. By gathering information on current dangers, companies can better understand the attack surface and establish safeguards. Breaking down elements in play The attack surface contains all possible vulnerabilities - known and unknown - that may exist across your infrastructure, and sums up your risk of exposure. While the attack surface may seem like one big scary entity, it's actually made up of several parts. Tripwire broke considerations down into software, network and human attack surfaces to make this large picture easier to manage. QA professionals should approach the attack surface this way in order to ensure that all aspects are accommodated for rather than being overwhelmed by the big picture. Everything from coding to devices and human error must be considered when gathering information and preparing for potential threats. Analyze data and act on it Testing results can be a critical indicator of what types of vulnerabilities may be present within a program. The Open Web Application Security Project noted that an attack surface analysis will help QA and developers better understand what they're up against and build in security accordingly. During this evaluation, they must determine high risk areas of code, what functions should be reviewed for defects and when the attack surface has changed. This last consideration will be especially critical as further tests and adjustments will be needed to secure the software. Anything that an organization does could affect the attack surface, which means that it will have to be constantly monitored. QA teams need to ask what's changed, how it's different from before and what potential holes were opened in the process. This will help keep the attack surface visibly mapped out, making it easy to strategize how to protect the business, its employees and customers. Reduce the noise While a breach is certainly possible, that doesn't mean it should be easy for attackers to gain entry into business systems. Organizations can reduce their attack surface by decreasing the amount of noise within their infrastructure. Accuvant pointed out that doing this will reduce an attack's operating surface , minimizing the likelihood of malicious access. QA teams can use tactics like configuration management, exploit analysis, patching, sandboxing and secure application development to effectively reduce or eliminate the impact of a vulnerability. \"Integrating these strategies into your security program make it much harder for exploits to attack your organization's systems,\" Accuvant stated. \"By reducing your adversaries' operating surface, you are effectively limiting their attack surface.\" The threat of a vulnerability is a very real concern for businesses. By gathering information on what types of attacks are becoming prevalent and understanding how they can affect company software, QA teams can prepare for these risks and protect their users from the growing attack surface.","tags":"devops"},{"url":"http://www.ciandcd.com/reducing-risk-through-security-qa-automation.html","title":"Reducing Risk Through Security Qa Automation","text":"from:http://java.dzone.com/articles/reducing-risk-through-security Organizations are under constant pressure to protect their critical assets from cyberattacks that have plagued a wide variety of industries. However, there is currently no set method of how to ensure that company applications will be safe from these threats. Quality assurance teams have implemented a wide range of approaches to ensure security, but manually executing all of these cases can be time-consuming and lead to potential vulnerabilities. For this reason, QA should look into security automation to reduce risks and improve overall program capabilities . Have realistic expectations When building security into the software development life cycle, there are numerous benefits businesses can see, including seamless protection integration and awareness of team members. An AT&T white paper noted that automated vulnerability scanning can be a great first step for QA teams to implement as it can easily and quickly identify commonly occurring issues . At the same time, however, it's not foolproof, since it cannot detect more sophisticated defects like authentication issues or business logic vulnerabilities. That being said, security QA automation can be a major asset to development efforts and can reduce overall risk, but will still require other tools like manual testing to fully evaluate the threat landscape. After the app has been released, automation can often be essential for finding threats, while enabling QA teams to focus on current projects that are still underway. This helps lower the potential risk across the board while still ensuring that each program gets the attention it needs, no matter where it is in its life cycle. Tools for the job There are a number of resources that QA teams can utilize to test the security of their projects. TechTarget contributor Michael Cobb noted that automated QA verification is often executed through code analysis and vulnerability testing . Both of these assets can quickly find errors that may be easily missed during manual evaluations. This alone helps significantly reduce risks to app functionality and security capabilities while ensuring that QA teams are eliminating common vulnerabilities. These tools paired with human testers can effectively find issues and better protect their projects for the future. \"Despite advances in computer automation, humans are still superior at ensuring applications are developed securely, probably because the best challenge is posed by humans, notably those who can think as an attacker would,\" Cobb wrote. \"However, human work is often more effective if a framework guides it.\" Relying on QA for better security Even if QA teams leverage automated tools for security needs, they must still have an understanding of how these tests work and be able to execute them. Chiron Professional Journal noted that while QA professionals may not often be security experts, having the tools on hand can help them perform the necessary processes and mitigate critical risks. \"Let's be clear here – we're not expecting a QA analyst to be able to cobble together a complicated script to evade an anti-cross-site scripting library … but we should reasonably expect that the analyst can either effectively use a tool, or follow a well-documented process that has varying tests and permutations allowing the analyst to think for themselves and flag questionable results for review by the security experts,\" the Chiron Professional Journal stated.","tags":"devops"},{"url":"http://www.ciandcd.com/why-we-need-continuous-integration.html","title":"Why We Need Continuous Integration","text":"from:http://java.dzone.com/articles/why-we-need-continuous Introduction Continuous integration is a practice that helps developers deliver better software in a more reliable and predictable manner. This article deals with the problems developers face while writing, testing and delivering software to end users. Through exploring continuous integration, we will cover how we can overcome these issues. The Problem First, we will take a look at the source of the problem, which lies in the software development cycle. Next, we will cover some of the change conflicts that can take place during that process, and finally we will explore the main factors that can make these problems escalate, followed by an explanation of how continuous integration solves these issues. The Source of the Problem Let's take a look at what a traditional software development cycle looks like. Each developer gets a copy of the code from the central repository. The starting point is usually the latest stable version of the application. All developers begin at the same starting point, and work on adding a new feature or fixing a bug. Each developer makes progress by working on their own or in a team. They add or change classes, methods and functions, shaping the code to meet their needs, and eventually they complete the task they were assigned to do. Meanwhile, the other developers and teams continue working on their own tasks, changing the code or adding new code, solving the problems they have been assigned. If we take a step back and look at the big picture, i.e. the entire project, we can see that all developers working on a project are changing the context for the other developers as they are working on the source code. As teams finish their tasks, they copy their code to the central repository. There are two scenarios that can take place at this point. The code in the central repository is unchanged The code is the same as the initial copy. If this is the case, things are simple, because the system is unchanged. All the ideas we had about the system still stand. This is always the case if you are the only developer working on the application and if you have finished your work before the other members of your team. Either way, things are looking good for you. The system you have created and tested can be delivered to users without additional changes. The code in the central repository has changed The second scenario is that the application you have been working on has changed, and you discover this at the point when you try to copy your code over to the central repository. Changes in the code may or may not be in conflict with the ones you've made. If there are conflicts, you need to resolve them in order to be able to successfully deliver your code to the users. In this case, things could get complicated. Next, we'll explore the types of conflicts that can happen and what you may need to do to resolve them. Change Conflicts There are several types of change conflicts that can occur when integrating code. Here are some of the most common ones. We'll start with the simplest scenarios, and gradually explore the more complex ones. The implementation details have changed - You refactored a method, but so did the developer that has already integrated their code into the central repository. The behavior of the method is the same in all three implementations. You will need to pick the version that will stay, and remove the other implementations. You can even come up with a fourth implementation. This is a simple type of conflict, which you can usually resolve within a few minutes. The APIs you have been relying on have changed - For instance, the behavior of a certain method has changed. This could affect your code in a number of ways — from minor changes that you might need to make, to major structural changes. There is no silver bullet in such cases. You will need to carefully study the changes and make all the fixes. An entire subsystem of the application behaves in a different way - in such cases you will almost certainly be facing a partial, if not a full rewrite of your solution. If this is the case, you will probably need to speak with all the developers working on the application, because such a significant change should not happen without letting the rest of the team know about it. These and a number of other issues could come up, caused by various factors. Different versions of frameworks, libraries, databases are another potential source of conflicts. Once you have updated your code so it can be compiled or interpreted, you also need to remember to repeat all the tests that you have previously ran. These examples show that the amount of work needed to solve a problem that was initially assigned to a developer can easily double. Escalating Factors Here are some of the main factors that can make these problems escalate. The size of the team working on the project. The number of changes that are being pushed back into the main repository is proportional to the number of people on the project. This makes the process of integrating code into the main repository significantly harder. The amount of time passed since the developer got the latest version of the code from the central repository. As time passes, other people working on the same project are integrating more and more of their work, and changing the context in which your code needs to run. Sometimes the changes in the main repository are so big that it's easier to do a complete rewrite of your solution. A large number of changes in the system make integration events more complex and can have a huge effect on the productivity of the team. Such situations are even referred to as \"integration hell\". This process has a number of other negative consequences for your business. Testing and fixing bugs can take forever. Your releases are running late. Teams are stressed out because of long and unpredictable release cycles, and morale deteriorates. Solution: Integrate Continuously The solution to the problem of managing a large number of changes in big integration events is conceptually simple. We need to split these big integration events into much smaller integration events. This way, developers need to deal with a much smaller number of changes, which are easier to understand and manage. To keep integration events small and easily manageable, we need them to happen often. A couple of times a day is ideal. The practice of doing small integrations often is called Continuous Integration . The idea is simple, but at the same time it often appears to be impossible to implement in practice. This is because changing the process requires us to change some of our own habits, and changing habits is difficult. The Practice of Continuous Integration In order to avoid the previously described issues, developers need to integrate their partially complete work back into the main repository on a daily basis, or even a couple of times a day. To accomplish this, they first need to pull in all the changes added to the main repository while they were working on the code. They also must make sure that their code will work once it is integrated into the main repository. The only way to ensure this is to test every feature of the application. What first comes into mind when we start considering continuous integration is that the developers would need to spend half of their time every day testing the code in order not to break the code in the main repository for everyone else. This is why the prerequisite for continuous integration is having an automated test suite. Automated tests take away the burden of the manual, repetitive, and error-prone testing process from the developers. They also make the entire testing process much quicker. A computer can replace hours of manual testing with just minutes of automated testing. Behavior-driven and test-driven development are techniques that help developers write clean, maintainable code while writing tests at the same time. Testing techniques are out of the scope of this article, and you can read more about them in other articles on Semaphore Community . Tests make sense only if they are executed every time the source code changes, without exception. A continuous integration service such as Semaphore CI is a tool which can automate this process by monitoring the central code repository and running tests on every change in the source code. Apart from running tests, they also collect test results and communicate those results to the entire team working on the project. The result of continuous integration is so important that many teams have a rule to stop working on their current task if the version in the central repository is broken. They join the team which is working on fixing the code until tests are passing again. The role of a continuous integration service is to improve the communication between developers by communicating the status of a project's source code. How to Adopt Continuous Integration Continuous integration as a practice makes a big contribution to improving the development process, but also calls for essential changes in the everyday development routine. Adopting it comes with challenges that are easy to overcome if the process is introduced gradually. One of the biggest challenges teams face is the lack of an automated testing suite. A good recipe for overcoming this situation is to start adding automated tests for all new features as they are being developed. At the same time, the developer working on a bug fix should also work to cover the related code with tests. Whenever a bug is reported, the team should first write a failing test to demonstrate the existence of bug. Once the fix is created, the tests should pass. Over time, the automated tests suite gradually becomes more comprehensive, and the developers begin relying on it more and more. Adopting a continuous integration service to communicate the status of the tests to the entire team in the early stages of a project is also important, because it raises awareness of the project status among team members. Conclusion Introducing continuous integration and automated testing into the development process changes the way software is developed from the ground up. It requires effort from all team members, and a cultural shift in the organization. Big changes in the workflow are not easy to pull off quickly. Changes have to be introduced gradually, and all team members and stakeholders need to be on board with the idea. Educating team members about the practice of continuous integration practice and building the automated tests suite needs to be done systematically. Once the first steps have been taken, the process usually continues on its own, as both developers and stakeholders begin seeing the benefits of automated testing suites and the peace of mind that this practice brings to the entire team. Article originally posted on the Semaphore Community .","tags":"devops"},{"url":"http://www.ciandcd.com/how-to-monitor-a-java-ee-datasource.html","title":"How to Monitor a Java EE DataSource","text":"from:http://java.dzone.com/articles/how-monitor-java-ee-datasource Introduction FlexyPool is an open-source framework that can monitor a DataSource connection usage. This tool come out of necessity, since we previously lacked support for provisioning connection pools. FlexyPool was initially designed for stand-alone environments and the DataSource proxy configuration was done programmatically. Using Spring bean aliases , we could even substitute an already configured DataSource with the FlexyPool Metrics-aware proxy alternative. Java EE support Recently, I've been asked about supporting Java EE environments and in the true open-source spirit, I accepted the challenge. Supporting a managed environment is tricky because the DataSource is totally decoupled from the application-logic and made available through a JNDI lookup. One drawback is that we can't use automatic pool sizing strategies, since most Application Servers return a custom DataSource implementation (which is closely integrated with their in-house JTA transaction manager solution), that doesn't offer access to reading/writing the connection pool size. While the DataSource might not be adjustable, we can at least monitor the connection usage and that's enough reason to support Java EE environments too. Adding declarative configuration Because we operate in a managed environment, we can no longer configure the DataSource programmatically, so we need to use the declarative configuration support. By default, FlexyPool looks for the flexy-pool.properties file in the current Class-path. The location can be customized using the flexy.pool.properties.pathSystem property , which can be a: URL (e.g. file:/D:/wrk/vladmihalcea/flexy-pool/flexy-pool-core/target/test-classes/flexy-pool.properties) File system path (e.g. D:\\wrk\\vladmihalcea\\flexy-pool\\flexy-pool-core\\target\\test-classes\\flexy-pool.properties) Class-path nested path (e.g. nested/fp.properties) The properties file may contain the following configuration options: Parameter nameDescription flexy.pool.data.source.unique.name Each FlexyPool instance requires a unique name so that JMX domains won't clash flexy.pool.data.source.jndi.name The JNDI DataSource location flexy.pool.data.source.jndi.lazy.lookup Whether to lookup the DataSource lazily (useful when the target DataSource is not available when the FlexyPoolDataSource is instantiated) flexy.pool.data.source.class.name The DataSource can be instantiated at Runtime using this Class name flexy.pool.data.source.property.* If the DataSource is instantiated at Runtime, each flexy.pool.data.source.property.${java-bean-property} will set the java-bean-property of the newly instantiated DataSource (e.g. flexy.pool.data.source.property.user=sa) flexy.pool.adapter.factory Specifies the PoolAdaptorFactory, in case the DataSource supports dynamic sizing. By default it uses the generic DataSourcePoolAdapter which doesn't support auto-scaling flexy.pool.metrics.factory Specifies the MetricsFactory used for creating Metrics flexy.pool.metrics.reporter.log.millis Specifies the metrics log reported interval flexy.pool.metrics.reporter.jmx.enable Specifies if the jmx reporting should be enabled flexy.pool.metrics.reporter.jmx.auto.start Specifies if the jmx service should be auto-started (set this to true in Java EE environments) flexy.pool.strategies.factory.resolver Specifies a ConnectionAcquiringStrategyFactoryResolver class to be used for obtaining a list of ConnectionAcquiringStrategyFactory objects. This should be set only if the PoolAdaptor supports accessing the DataSource pool size. Hibernate ConnectionProvider Most Java EE applications already use JPA and for those who happen to be using Hibernate, we can make use of the hibernate.connection.provider_class configuration property for injecting our proxy DataSource. Hibernate provides many built-in extension points and the connection management is totally configurable. By providing a custom ConnectionProvider we can substitute the original DataSource with the FlexyPool proxy. All we have to do is adding the following property to our persistence.xml file: <property name=\"hibernate.connection.provider_class\" value=\"com.vladmihalcea.flexypool.adaptor.FlexyPoolHibernateConnectionProvider\"/> Behind the scenes, this provider will configure a FlexyPoolDataSource and use it whenever a new connection is requested: private FlexyPoolDataSource<DataSource> flexyPoolDataSource; @Override public void configure(Map props) { super.configure(props); LOGGER.debug( \"Hibernate switched to using FlexyPoolDataSource \"); flexyPoolDataSource = new FlexyPoolDataSource<DataSource>( getDataSource() ); } @Override public Connection getConnection() throws SQLException { return flexyPoolDataSource.getConnection(); } Instantiating the actual DataSource at runtime If you're not using Hibernate, you need to have the FlexyPoolDataSource ready before the EntityManagerFactory finishes bootstrapping: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <persistence version=\"2.0\" xmlns=\"http://java.sun.com/xml/ns/persistence\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\" http://java.sun.com/xml/ns/persistence http://java.sun.com/xml/ns/persistence/persistence_2_0.xsd\"> <persistence-unit name=\"persistenceUnit\" transaction-type=\"JTA\"> <provider>org.hibernate.jpa.HibernatePersistenceProvider</provider> <jta-data-source>java:global/jdbc/flexypool</jta-data-source> <properties> <property name=\"hibernate.hbm2ddl.auto\" value=\"update\"/> <property name=\"hibernate.show_sql\" value=\"true\"/> <property name=\"hibernate.dialect\" value=\"org.hibernate.dialect.HSQLDialect\"/> <property name=\"hibernate.transaction.jta.platform\" value=\"org.hibernate.service.jta.platform.internal.SunOneJtaPlatform\"/> </properties> </persistence-unit> </persistence> While in a production Java EE environment we use an Application server specific DataSource configuration, for simplicity sake, I'm going to configure the FlexyPooldataSource using the DataSourceDefinition annotation: @DataSourceDefinition( name = \"java:global/jdbc/flexypool\", className = \"com.vladmihalcea.flexypool.FlexyPoolDataSource\") @Stateless public class FlexyPoolDataSourceConfiguration {} We now need to pass the actual DataSource properties to FlexyPool and this is done through the flexy-pool.properties configuration file: flexy.pool.data.source.unique.name=unique-name flexy.pool.data.source.class.name=org.hsqldb.jdbc.JDBCDataSource flexy.pool.data.source.property.user=sa flexy.pool.data.source.property.password= flexy.pool.data.source.property.url=jdbc:hsqldb:mem:test flexy.pool.metrics.reporter.jmx.auto.start=true The actual DataSource is going to be created by the FlexyPoolDataSource on start-up. Locating the actual DataSource from JNDI If the actual DataSource is already configured by the Application Server, we can instruct FlexyPool to fetch it from JNDI. Let's say we have the following DataSource configuration: @DataSourceDefinition( name = \"java:global/jdbc/default\", className = \"org.hsqldb.jdbc.JDBCDataSource\", url = \"jdbc:hsqldb:mem:test\", initialPoolSize = 3, maxPoolSize = 5 ) @Stateless public class DefaultDataSourceConfiguration {} To proxy the JNDI DataSource, we need to configure FlexyPool like this: flexy.pool.data.source.unique.name=unique-name flexy.pool.data.source.jndi.name=java:global/jdbc/default flexy.pool.metrics.reporter.jmx.auto.start=true The FlexyPoolDataSource is defined alongside the actual DataSource: @DataSourceDefinition( name = \"java:global/jdbc/flexypool\", className = \"com.vladmihalcea.flexypool.FlexyPoolDataSource\") @Stateless public class FlexyPoolDataSourceConfiguration {} The JPA will have to fetch the FlexyPoolDataSource instead of the actual one: <jta-data-source>java:global/jdbc/flexypool</jta-data-source> In TomEE , because the DataSourceDefinitions are not lazily instantiated, the actual DataSource might not be available in the JNDI registry when the FlexyPoolDataSource definition is processed. For this, we need to instruct FlexyPool to dely the JNDI lookup until the DataSource is actually requested: flexy.pool.data.source.jndi.lazy.lookup=true Conclusion The last time I used Java EE was in 2008, on a project that was using Java EE 1.4 with EJB 2.1. After 7 years of using Spring exclusively, I'm pleasantly surprised by the Java EE experience. Arquillian is definitely my favourite add-on, since integration testing is of paramount importance in enterprise applications. CDI is both easy and powerful and I'm glad the dependency injection got standardised. But the best asset of the Java EE platform is the community itself. Java EE has very strong community, willing to give you a hand when in need. I'd like to thank Steve Millidge (Founder of Payara and C2B2) for giving me some great tips on designing the FlexyPool Java EE integration, Alex Soto , Antonio Goncalves and all the other Java EE members whom I had some very interesting conversations on Twitter.","tags":"devops"},{"url":"http://www.ciandcd.com/better-and-fewer-suppliers-2015-software-supply-chain-report.html","title":"Better and Fewer Suppliers (2015 Software Supply Chain Report)","text":"from:http://devops.com/2015/06/19/better-fewer-suppliers-2015-software-supply-chain-report/ That Supplier is Better For You Since releasing the 2015 State of the Software Supply Chain Report, there has been a lot of great discussion across the industry on best practices for managing the complexity introduced by the volume and velocity of the components used across your software supply chain. Today I want to focus on the huge ecosystem of open source projects (\"suppliers\") that feed a steady stream of innovative components into our software supply chains. In the Java ecosystem alone, there are now over 108,000 suppliers of open source components. Across all component types available to developers (e.g., RubyGems, NuGet, npm, Bower, PyPI, etc.), estimates now reach over 650,000 suppliers of open source projects. However, like in traditional manufacturing, not all suppliers deliver parts of comparable quality and integrity. My latest research, the 2015 State of the Software Supply Chain Report , shows that some open source projects use restrictive licenses and vulnerable sub-components, while other projects are far more diligent at updating the overall quality of their components. Choosing the best and fewest suppliers can improve the quality and integrity of the applications we deliver to our customers. While I am hosting a webinar next week to share many of the detailed report findings, I wanted to share a few of the more meaningful stats here. Your 7,600 Suppliers My research for the report revealed many new perspectives on \"suppliers\" across the software supply chains. First of all, I saw that the average large development organization consumed over 240,000 open source components last year — sourced from over 7,600 open source projects. On the surface, the huge reliance on open source projects is a great thing. Development teams have chosen to not write those pieces themselves, but have sourced the needed components from outside suppliers. This practice speeds development, enables more innovation, and ensures time-to-release goals are achieved. The use of open source is so prolific today, few of us could ever imagine reducing the use of those components and their suppliers in the future. At the same time that we benefit from open source, our high paced, high volume consumption practices don't allow us the time needed to do the due diligence on the suppliers or open source projects where we source our component parts from. For example, of the 240,000 average component downloads in 2014, the same businesses sourced an average of 15,000 components that included known security vulnerabilities. In many cases, developers were downloading vulnerable component versions, when safer versions of those same components were available from the open source projects. While no one intends to download components with known vulnerabilities, the problem is exacerbated due to the lack the visibility into a better recommended version. Fewer Suppliers, Less Context Switching Choosing an open source project supplier should be considered an important strategic decision in organizations because changing a supplier (\"open source project\") used is far more effort than swapping out a specific component. Like traditional suppliers, open source projects have good and bad practices impacting the overall quality of their component parts. Traditional manufacturing supply chains intentionally select specific parts from approved suppliers. They also rely on formalized sourcing and procurement practices. This practice also focuses the organization on using the best and fewest suppliers — an effort that improves quality, reduces context switching, and also accelerates mean time to repair when defects are discovered. One industry example from the report describes how Toyota manages 125 suppliers for their Prius to help sustain competitive advantages over GM who manages over 800 suppliers for the Chevy Volt. By contrast, development teams working with software supply chains often rely on an unchecked variety of supply, where each developer or development team can make their own sourcing and procurement decisions. The effort of managing over 7,600 suppliers introduces a drag on development and is contrary to their need to develop faster as part of agile, continuous delivery and devops practices. Coming to Terms When you come to terms with the volume of consumption and the massive ecosystem of suppliers you can source your components from, you quickly realize it is impossible to address this issue with a manual review process. Any organizations clutching to these outdated manual practices are will continue to be outgunned by the velocity by their software supply chains. Just as traditional manufacturing supply chains have turned to automation, software development teams need to take the same approach by further automating their software supply chains. Information about suppliers and the quality of their projects needs to be made available to developers at the time they are selecting components. Information about the latest versions, features, licenses, known vulnerabilities, popularity of versions being used, and the cadence of new releases should be made available to developers in an automated way. Automating the availability of this information about suppliers can lead to better and fewer suppliers being used. Be sure to read the full 2015 State of the Software Supply Chain Report for more information about open source suppliers and organizations sourcing practices. The report also highlights current and best practices being used in organizations that are managing their use of suppliers that feed their software supply chains. To hear more about the overall report findings and industry best practices, please join me on Wednesday, June 24th (1pm ET) for our webinar .","tags":"devops"},{"url":"http://www.ciandcd.com/internaps-devops-culture-privatestack-cd-read-on-draw-your-own-conclusions.html","title":"Internap's DevOps Culture: PrivateStack + CD = ? [Read On & Draw Your Own Conclusions]","text":"from:http://devops.com/2015/06/19/internaps-devops-culture-privatestack-cd-read-draw-conclusions/ Engineers at Internap , a hosting company and public cloud vendor designed a new OpenStack-based cloud platform and development environment. DevOps.com tells the story of Internap's DevOps cultural evolution, which grew virally, interwoven with the company's development of PrivateStack. Challenges in Internap's Common Development Environment Challenges that slowed Internap's cloud development process triggered a hunger for change and for a new / altered development scheme. \"The Internap public cloud product is essentially composed of micro services that make cloud resources available to our customers,\" says Mathieu Mitchell, Senior Software Developer, Internap. Internap engineering teams tested their cloud services simultaneously with each affecting the other and transferring adverse effects to production or pre-production. Internap needed to let the billing team test their services, integrated with a duplicate of the production software and environment, without affecting the virtualization team. These challenges were rooted in a commonly shared development environment for all the engineering teams and team members, which introduced tedium for engineers as they worked to build and deploy stable code. When sharing a common development environment, each engineer's tests depended on environment consistency and reliability. When engineers / developers tested two changes at the same time, there was no way to tell which change introduced a regression. The most reasonable way to address this prior to PrivateStack was to push all changes into the environment and dedicate specific engineers to identify and fix the issues that occurred. The challenge with dedicating specific engineers to troubleshoot these regressions was that they had a diminished context to work with when compared to the original developer's understanding. The team that introduced a change needed feedback from the environment and to shoulder responsibility for fixing the issue. This would result in a stable codebase and the ability to write more thorough, automated tests. \"This is why we created PrivateStack–to allow us to independently test a single change, integrated with other services, in an environment that is the equivalent of a private production setup,\" says Mitchell. PrivateStack enables development teams to innovate while ensuring that changes behave correctly in production. This in turn enables CD and speeds development. DevOps Culture Leads Teams to Success As Internap developed PrivateStack, engineering observed a DevOps belief system spreading across the project and the teams. \"At first, it was only a minority among us, mostly people with an Agile background, who advocated the Continuous Delivery approach while we worked on this project. We really believe in delivering value to our customers as quickly as possible. To achieve Continuous Delivery, we needed people to understand that automating everything was the way to go,\" says Mitchell. As more engineers realized that they were the solution, they became increasingly motivated to enhance processes, create the new development environment, and use CD to develop the Internap public cloud product. Results with PrivateStack PrivateStack enables Internap teams to share the same code and system configurations while individual developers have each their own private production environment to test their software without affecting others. \"We heavily leverage virtualization to be able to recreate our environments, since buying racks and racks of hardware to make this available to all of our developers would be cost prohibitive,\" says Mitchell. With PrivateStack, Internap isolates production issues during development, keeping R&D efficient and slashing pre-production troubleshooting time. \"PrivateStack also improves our time-to-market, is much less costly, and most importantly, reduces the chance of any issues reaching the customer,\" says Mitchell. DevOps Culture Wrap Up To finish the PrivateStack project and foster a DevOps culture at the same time, Internap's internal core of CD true believers lead by example. Mitchell and his colleagues adhered to consistent principles while hearing out other team members on their concerns. This helped them to encourage the larger engineering department including billing and virtualization teams to adopt a DevOps and CD frame of mind. \"We dedicated two people to drive this initiative, along with a few others who believed in the Continuous Delivery approach but were not involved directly,\" says Mitchell. That was enough to get the job done. Now the vast majority of the engineering department is targeting CD. \"We are eager to share our PrivateStack platform with the open source community to enable other developers to run production-like environments for development in their day-to-day operations,\" says Mitchell.","tags":"devops"},{"url":"http://www.ciandcd.com/two-paths-to-metal-devops-cloud-like-api-driven-cluster-building.html","title":"Two paths to metal devops: cloud-like API driven & cluster building","text":"from:http://devops.com/2015/06/19/two-paths-metal-devops-cloud-like-api-driven-cluster-building/ I've been seeing a rising interest in metal DevOps fueled by containers and scale-out data center platforms (like Hadoop, Ceph & OpenStack) that run at the metal level. While I see this is a growing general trend ( Packet , Internap , RackSpace , OpenStack Ironic , MaaS ), I'm going to stay firmly within my wheelhouse and use OpenCrowbar as my reference here. Building on the API-driven metal features of OpenCrowbar, this has translated into two paths for workloads to run on metal: 1) \"Cloudify\" the metal using APIs from tools like Chef Provision , SaltStack Libcloud , Docker Machine , Cloud Foundry BOSH . These tools have clients that target cloud APIs like OpenStack and Amazon. These same clients work against cloud are easily ported to Crowbar's APIs. Five years ago, conventional wisdom was that we'd need a universal cloud API; however, practice has shown it's not very difficult to wrap APIs in a way that does not reduce every cloud to a least common denominator. 2) DevOps deploy the workload using hand-offs to tools like Chef, Saltstack, Puppet or Ansible. This approach leverages the community scripts (Cookbooks, Modules, Playbooks) for the workload with the critical ability to create a tuned environment and inject the needed parameters directly into the scripts. A critical lesson we learned going from Crowbar v1 to v2 was for our scripts to have crisp attribute input/output boundary to avoid embedding environmental knowledge into the code. While I'm casting this in Crowbar terms, I see this approach to metal as coming into the market by force fuels by a desire for containers-on-metal and devops-on-metal. Let's look at some of the unique and shared use-cases for each approach: Metal API Both Metal Cluster Easy Cloud to Metal Migration Minimal Tool Customization Portability of DevOps Scripts Take advantage of power cycling Enables constant refresh cycles Leverage Hardware features Advanced Network topologies In either case, you have to handle bespoke (hipster word for custom) steps in the provisioning flow that are unique to the your operational needs. Our experience is that each site (even each server!) is unique in some incremental way. For example, one site may require teamed networks with VLANs while another requires flat networks with an SDN layer. These differences are not mistakes or errors : the reality of physical ops and individual operational choices mean that there are a lot of valid configurations. Rather than attempt the Sisyphean task of enforced conformity, we work to abstract differences so that they can be ignored when they are not material. In the end, the choices are not mutually exclusive. Metal APIs are often faster but harder to optimize. You can use them to get started quickly and then invest time to optimize a cluster for long term operations. The underlying physical orchestration can support both. Are you looking at getting closer to metal? Which of the options above makes the most sense to you? I'd love to hear about your use-cases, architecture and configuration requirements.","tags":"devops"},{"url":"http://www.ciandcd.com/clusterhq-and-devopscom-survey-show-containers-poised-for-mass-adoption.html","title":"ClusterHQ and DevOps.com survey show Containers poised for mass adoption","text":"from:http://devops.com/2015/06/18/clusterhq-and-devops-com-survey-show-containers-poised-for-mass-adoption/ DevOps.com and ClusterHQ, conducted a survey on Container usage that shows an overwhelming majority of users have either already using, testing or investigating Container usage. With 285 respondents representing a wide range of organizations, it shows that Containers will be part of many production environments in the very near future. Currently, only 38 percent of respondents reported using containers in production environments, but that number is projected to increase 69 percent over the next 12 months as organizations find new ways to address important barriers to adoption. It verified that Docker is overwhelmingly the container of choice, with 92% of respondents having used or investigated it, followed by LXC (32%) a distant second, but still far ahead of Rocket (21%). To access the complete survey and report visit https://clusterhq.com/assets/pdfs/state-of-container-usage-june-2015.pdf . Companies ranging in size from small organizations with 1 to 500 employees (69%), to mid-size companies with 501-2,500 personnel (12%), all the way up to large enterprises with over 2,500 employees (19%) are represented in the survey. This demonstrates that containers are being embraced by all businesses from the startup stage to Fortune 500 companies. Respondents came predominantly from Development, Operations and DevOps teams. QA and security teams were a smaller share. The survey revealed how container technologies are being used today, as well as research-based insights while providing clues as to where the industry is trending. From the ClusterHQ release on the survey: The survey also revealed insights into what is perceived to be the primary barriers to container adoption. Security seems to be emerging as a consistent concern throughout the DevOps community in these times of never ending breaches throughout the world: Security — 61% Data Management — 53% Networking — 51% Skills and Knowledge — 48% Persistent Storage — 48% Data Management capabilities also emerged as essential to the success of container strategies and that the vast majority of organizations want to run databases as well as additional services in containers. When asked to rate how important data management is to container strategies, 66 percent reported it as a critical or important gating factor, 29 percent ranked it as moderately important and only 5 percent reported that it carries no importance. Over 70% of respondents said they would like to run a database or other stateful service in their container environments. Respondents were also asked which specific features of container data management they considered to be most important, selecting the \"integration of data management capabilities into existing container workflows and tools\" as their first choice, with \"seamless movement of data between dev, test and production environments\" a close second. MySQL (53%), Redis (52%), PostgreSQL (50%), and Elasticsearch (43%) were reported as the top four most frequently used stateful services. Containers have become known for portability and flexibility, the survey reveals that organizations are using them in different infrastructures but most frequently in on-premises data centers (57%), followed by Amazon Web Services (52%). So I think it is safe to say Containers are rapidly becoming one of the staples of development and are here to stay to stay in the foreseeable future. DevOps.com along with ElasticBox are currently conducting another survey on \"What is DevOps to you?\" One in 50 respondents wins a $50 dollar Amazon gift card and one grand prize winner will win a new 3DR Drone. Take a few minutes to help us with this survey .","tags":"devops"},{"url":"http://www.ciandcd.com/git-simple-feature-branch-workflow.html","title":"Git Simple Feature Branch Workflow","text":"from:http://java.dzone.com/articles/git-simple-feature-branch In my previous post , I wrote about git work flows. Now I will going to try out simple ' Feature Branch Workflow '. 1. I pull down the latest changes from mastergit checkout mastergit pull origin master2. I make branch to make changes git checkout -b new-feature3. Now I am working on the feature4. I keep my feature branch fresh and up to date with the latest changes in master, using 'rebase'Every once in a while during the development update the feature branch with the latest changes in master.git fetch origingit rebase origin/masterIn the case where other devs are also working on the same shared remote feature branch, also rebase changes coming from it:git rebase origin/new-featureResolving conflicts during the rebase allows me to have always clean merges at the end of the feature development.5. When I am ready I commit my changesgit add -pgit commit -m \"my changes\"6. rebasing keeps my code working, merging easy, and history clean.git fetch origingit rebase origin/new-featuregit rebase origin/masterBelow two points are optional6.1 push my branch for discussion (pull-request)git push origin new-feature6.2 feel free to rebase within my feature branch, my team can handle it!git rebase -i origin/master Few point that can be happen in developing phase. Another new feature is needed and it need some commits from my new branch 'new-feature' that new feature need new branch and few commits need to push to it and clean from my branch. 7.1 Creating x-new-feature branch on top of 'new-feature' git checkout -b x-new-feature new-feature 7.2 Cleaning commits //revert a commit git revert --no-commit //reverting few steps a back from current HEAD git reset --hard HEAD~2 7.3 Updating the git //Clean new-feature branch git push origin HEAD --force 1. I pull down the latest changes from mastergit checkout mastergit pull origin master2. I make branch to make changes git checkout -b new-feature3. Now I am working on the feature4. I keep my feature branch fresh and up to date with the latest changes in master, using 'rebase'Every once in a while during the development update the feature branch with the latest changes in master.git fetch origingit rebase origin/masterIn the case where other devs are also working on the same shared remote feature branch, also rebase changes coming from it:git rebase origin/new-featureResolving conflicts during the rebase allows me to have always clean merges at the end of the feature development.5. When I am ready I commit my changesgit add -pgit commit -m \"my changes\"6. rebasing keeps my code working, merging easy, and history clean.git fetch origingit rebase origin/new-featuregit rebase origin/masterBelow two points are optional6.1 push my branch for discussion (pull-request)git push origin new-feature6.2 feel free to rebase within my feature branch, my team can handle it!git rebase -i origin/masterAnother new feature is needed and it need some commits from my new branch 'new-feature' that new feature need new branch and few commits need to push to it and clean from my branch.7.1 Creating x-new-feature branch on top of 'new-feature'git checkout -b x-new-feature new-feature7.2 Cleaning commits//revert a commitgit revert --no-commit//reverting few steps a back from current HEADgit reset --hard HEAD~27.3 Updating the git//Clean new-feature branchgit push origin HEAD --force","tags":"devops"},{"url":"http://www.ciandcd.com/know-thy-mvn-plugins-keeping-ones-sanity-amidst-open-source-version-hell.html","title":"Know Thy MVN Plugins: Keeping One's Sanity Amidst Open Source Version Hell","text":"from:http://java.dzone.com/articles/know-thy-mvn-plugins-or Problem #1: Everyone knows that keeping up with versions is tough. This is the reason tools such as OpenLogic come to exist. Some companies pay, some companies develop home grown solutions, some live with the version which are getting older every day. Problem #2: When multiple open source projects once consumed by your product can bring in different versions of the same library. One never knows which will be picked up at run-time and behavior on the developer's box based on the Murphy's Law will be different from the server run-time. This does not have to be such an ordeal. With the power of maven plugins this can be solved relatively easy. Versions Maven Plugin and Maven Enforcer Plugin to the rescue! Versions Maven Plugin will keep versions up-to-date and as you probably guessed from the name Maven Enforcer plugin will be guarding against multiple versions of the maven artifact in the build package produced. Let's see how one introduced enforcer into the mix first. The code below can go in the parent pom.xml of the project, or global parent of the projects if one exists. <project …> … <plugins> … <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-enforcer-plugin</artifactId> <version>1.4</version> <executions> <execution> <id>enforce-versions</id> <goals> <goal>enforce</goal> </goals> <configuration> <rules> <requireMavenVersion> <version>[2.2.*,)</version> </requireMavenVersion> <requireJavaVersion> <version>[1.7.*,)</version> </requireJavaVersion> <DependencyConvergence/> </rules> </configuration> </execution> </executions> </plugin> … </plugins> … </project> For each dependency version collision one has to pick the version to keep and exclude the ones that are mismatched. See maven help page for details. If one wants absolute guarantee of the version used, this is the only way to go. An example of excluded dependencies will be: <project …> … <dependencies> … <dependency> <groupId>com.lordofthejars</groupId> <artifactId>nosqlunit-mongodb</artifactId> <version>${nosqlunit.veresion}</version> <scope>test</scope> <exclusions> <exclusion> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> </exclusion> <exclusion> <groupId>com.github.fakemongo</groupId> <artifactId>fongo</artifactId> </exclusion> <exclusion> <groupId>org.mongodb</groupId> <artifactId>mongo-java-driver</artifactId> </exclusion> </exclusions> </dependency> … </dependencies> … </project> From the open source developer side , producing two flavors of the package for consumption with and without dependencies can make world a better place. This is the difference of maven scope ‘provided' vs. default scope ‘compile'. Apache Maven Shade Plugin can be used to produce the version with the dependencies to be used there no conflicts can arise along with the version that is artifact version independent and can be used without version conflicts. Once all conflicts are resolved and one version of each artifact is in the final build product guaranteed, one should check if ones project is up to date. It is a good practice to do the check at the beginning of the new version. To check for outdates dependencies, and plugins, and bring those up to date in the controlled manner (manually) one can execute: mvn versions:display-dependency-updates mvn versions:display-plugin-updates mvn versions:display-property-updates As an alternative one can let the plugin update the versions by executing following mvn versions:use-latest-releases mvn versions:update-properties This last mvn command I am going to share is a bonus for the dedicated reader. It allows changing versions across the project and its modules painlessly: mvn versions:set -DgenerateBackupPoms=false​ -DnewVersion=<version>","tags":"devops"},{"url":"http://www.ciandcd.com/nohuphe-disown-itech.html","title":"nohup和disown - iTech","text":"from:http://www.cnblogs.com/itech/p/4535572.html Many system administrators make a practice of using GNU Screen or tmux to manage jobs running in the terminal. If you have a long-running job that you want to \"detach\" from the terminal, you can simply use your terminal multiplexer to do it. But what if you don't use tmux or Screen, or you just forgot? For those times, there's nohup and disown. Have a long-running job you want to \"detach\" from the terminal? Don't use tmux or Screen? If you haven't started the job yet, nohup is an easy to use option, and if you must stop a job in the middle, there's disown. If you haven't started the job yet, there's nohup. Short for \"no hangup,\" nohup will detach the program from the current shell and send its output to nohup.out. If you quit the shell or whatever, the process will continue to run until it completes. Ah, but what if you forget to use nohup, or if you didn't expect to be leaving the computer but get called away? Then there's disown. The use of disown is a bit more complex. While the command is running, use Ctrl-z to stop it and then use bg to put it in the background. Then you'll use disown %n where n is the job number (jobspec). And, of course, you can find the job number using the jobs command. Run jobs again to verify that the job has been detached -- and you can use ps or top to verify that the job is actually still running.","tags":"中文"},{"url":"http://www.ciandcd.com/linuxmu-lu-de-quan-xian-itech.html","title":"linux目录的权限 - iTech","text":"from:http://www.cnblogs.com/itech/p/4533980.html Answer: When applying permissions to directories on Linux, the permission bits have different meanings than on regular files. The write bit allows the affected user to create, rename, or delete files within the directory, and modify the directory's attributes The read bit allows the affected user to list the files within the directory The execute bit allows the affected user to enter the directory, and access files and directories inside The sticky bit states that files and directories within that directory may only be deleted or renamed by their owner (or root)","tags":"中文"},{"url":"http://www.ciandcd.com/ke-yi-ti-dai-puttyde-sshke-hu-duan-itech.html","title":"可以替代putty的ssh客户端 - iTech","text":"from:http://www.cnblogs.com/itech/p/4532665.html 1. Bitvise SSH Client http://www.putty.org/ Bitvise SSH Client is an SSH and SFTP client for Windows. It is developed and supported professionally by Bitvise. The SSH Client is robust, easy to install, easy to use, and supports all features supported by PuTTY, as well as the following: graphical SFTP file transfer; single-click Remote Desktop tunneling; auto-reconnecting capability; dynamic port forwarding through an integrated proxy; an FTP-to-SFTP protocol bridge. Bitvise SSH Client is free for personal use , as well as for individual commercial use inside organizations. You can download Bitvise SSH Client here . 2. MobaXterm X server and SSH client http://mobaxterm.mobatek.net/ MobaXterm is your ultimate toolbox for remote computing . In a single Windows application, it provides loads of functions that are tailored for programmers, webmasters, IT administrators and pretty much all users who need to handle their remote jobs in a more simple fashion. MobaXterm provides all the important remote network tools (SSH, X11, RDP, VNC, FTP, MOSH, ...) and Unix commands (bash, ls, cat, sed, grep, awk, rsync, ...) to Windows desktop, in a single portable exe file which works out of the box. More info on supported network protocols There are many advantages of having an All-In-One network application for your remote tasks, e.g. when you use SSH to connect to a remote server, a graphical SFTP browser will automatically pop up in order to directly edit your remote files. Your remote applications will also display seamlessly on your Windows desktop using the embedded X server . See demo You can download and use MobaXterm Home Edition for free. If you want to use it inside your company, you should consider subscribing to MobaXterm Professional Edition: this will give you access to much more features, professional support and \"Customizer\" software. Features comparison done","tags":"中文"},{"url":"http://www.ciandcd.com/4ge-ke-yi-fa-song-wan-zheng-dian-zi-you-jian-de-ming-ling-xing-gong-ju-itech.html","title":"4个可以发送完整电子邮件的命令行工具 - iTech","text":"from:http://www.cnblogs.com/itech/p/4530294.html 今天的文章里我们会讲到一些使用 Linux 命令行工具来发送带附件的电子邮件的方法。它有很多用处，比如在应用程序所在服务器上，使用电子邮件发送 一个文件过来，或者你可以在脚本中使用这些命令来做一些自动化操作。在本文的例子中，我们会使用foo.tar.gz文件作为附件。 有不同的命令行工具可以发送邮件，这里我分享几个多数用户会使用的工具，如 mailx 、 mutt 和 swaks 。 我们即将呈现的这些工具都是非常有名的，并且存在于多数Linux发行版默认的软件仓库中，你可以使用如下命令安装： 在 Debian / Ubuntu 系统 apt - get install mutt apt - get install swaks apt - get install mailx apt - get install sharutils 在基于Red Hat的系统，如 CentOS 或者 Fedora yum install mutt yum install swaks yum install mailx yum install sharutils 1) 使用 mail / mailx mailx 工具在多数Linux发行版中是默认的邮件程序，现在已经支持发送附件了。如果它不在你的系统中，你可以使用上边的命令安装。有一点需要注意，老版本的mailx可能不支持发送附件，运行如下命令查看是否支持。 $ man mail 第一行看起来是这样的： mailx [- BDdEFintv ~] [- s subject ] [- a attachment ] [- c cc - addr ] [- b bcc - addr ] [- r from - addr ] [- h hops ] [- A account ] [- S variable [= value ]] to - addr . . . 如果你看到它支持 -a 的选项（-a 文件名，将文件作为附件添加到邮件）和 -s 选项（-s 主题，指定邮件的主题），那就是支持的。可以使用如下的几个例子发送邮件。 a) 简单的邮件 运行 mail 命令，然后 mailx 会等待你输入邮件内容。你可以按回车来换行。当输入完成后，按Ctrl + D， mailx 会显示EOT表示结束。 然后 mailx 会自动将邮件发送给收件人。 $ mail user@example . com HI , Good Morning How are you EOT b) 发送有主题的邮件 $ echo \"Email text\" | mail - s \"Test Subject\" user@example . com -s 的用处是指定邮件的主题。 c) 从文件中读取邮件内容并发送 \" user@example . com < /path/ to / file \"message send from file $ mail d) 将从管道获取到的 echo 命令输出作为邮件内容发送 $ echo \"This is message body\" | mail - s \"This is Subject\" user@example . com e) 发送带附件的邮件 $ echo \" Body with attachment \"| mail -a foo.tar.gz -s \" attached file \" user@example.com -a 选项用于指定附件。 2) mutt Mutt是类Unix系统上的一个文本界面邮件客户端。它有20多年的历史，在Linux历史中也是一个很重要的部分，它是最早支持进程打分和多线程处理的客户端程序之一。按照如下的例子来发送邮件。 a) 带有主题，从文件中读取邮件的正文，并发送 $ mutt - s \"Testing from mutt\" user@example . com < /tmp/ message . txt b) 通过管道获取 echo 命令输出作为邮件内容发送 $ echo \"This is the body\" | mutt - s \"Testing mutt\" user@example . com c) 发送带附件的邮件 $ echo \"This is the body\" | mutt - s \"Testing mutt\" user@example . com - a / tmp / foo . tar . gz d) 发送带有多个附件的邮件 $ echo \"This is the body\" | mutt - s \"Testing\" user@example . com - a foo . tar . gz – a bar . tar . gz 3) swaks Swaks（Swiss Army Knife，瑞士军刀）是SMTP服务上的瑞士军刀，它是一个功能强大、灵活、可编程、面向事务的SMTP测试工具，由John Jetmore开发和维护。你可以使用如下语法发送带附件的邮件： $ swaks - t \" foo@bar.com \" -- header \"Subject: Subject\" -- body \"Email Text\" -- attach foo . tar . gz 关于Swaks一个重要的地方是，它会为你显示整个邮件发送过程，所以如果你想调试邮件发送过程，它是一个非常有用的工具。 它会给你提供了邮件发送过程的所有细节，包括邮件接收服务器的功能支持、两个服务器之间的每一步交互。 （LCTT 译注：原文此处少了 sharutils 的相关介绍，而多了 uuencode 的介绍。） 4) uuencode 邮件传输系统最初是被设计来传送7位编码（类似ASCII）的内容的。这就意味这它是用来发送文本内容，而不能发会使用8位的二进制内容（如程序文件或者图片）。 uuencode （\"UNIX to UNIX encoding\"，UNIX之间使用的编码方式）程序用来解决这个限制。使用 uuencode ，发送端将二进制格式的转换成文本格式来传输，接收端再转换回去。 我们可以简单地使用 uuencode 和 mailx 或者 mutt 配合，来发送二进制内容，类似这样： $ uuencode example . jpeg example . jpeg | mail user@example . com Shell脚本：解释如何发送邮件 #!/bin/bash FROM = \"\" SUBJECT = \"\" ATTACHMENTS = \"\" TO = \"\" BODY = \"\" # 检查文件名对应的文件是否存在 function check_files () { output_files = \"\" for file in $1 do if [ - s $file ] then output_files = \"${output_files}${file} \" fi done echo $output_files } echo \"*********************\" echo \"E-mail sending script.\" echo \"*********************\" echo # 读取用户输入的邮件地址 while [ 1 ] do if [ ! $FROM ] then echo - n - e \"Enter the e-mail address you wish to send mail from:\\n[Enter] \" else echo - n - e \"The address you provided is not valid:\\n[Enter] \" fi read FROM echo $FROM | grep - E '&#94;.+@.+$' > /dev/ null if [ $ ? - eq 0 ] then break fi done echo # 读取用户输入的收件人地址 while [ 1 ] do if [ ! $TO ] then echo - n - e \"Enter the e-mail address you wish to send mail to:\\n[Enter] \" else echo - n - e \"The address you provided is not valid:\\n[Enter] \" fi read TO echo $TO | grep - E '&#94;.+@.+$' > /dev/ null if [ $ ? - eq 0 ] then break fi done echo # 读取用户输入的邮件主题 echo - n - e \"Enter e-mail subject:\\n[Enter] \" read SUBJECT echo if [ \"$SUBJECT\" == \"\" ] then echo \"Proceeding without the subject...\" fi # 读取作为附件的文件名 echo - e \"Provide the list of attachments. Separate names by space. If there are spaces in file name, quote file name with \\\".\" read att echo # 确保文件名指向真实文件 attachments = $ ( check_files \"$att\" ) echo \"Attachments: $attachments\" for attachment in $attachments do ATTACHMENTS","tags":"中文"},{"url":"http://www.ciandcd.com/20ge-you-yong-de-linuxming-ling-xing-ji-qiao-itech.html","title":"20个有用的linux命令行技巧 - iTech","text":"from:http://www.cnblogs.com/itech/p/4512113.html Deleting a HUGE file I had a huge log file 200GB I need to delete on a production web server. My rm and ls command was crashed and I was afraid that the system to a crawl with huge disk I/O load. To remove a HUGE file, enter: > /path/to/file.log # or use the following syntax : > /path/to/file.log # finally delete it rm /path/to/file.log Want to cache console output? Try the script command line utility to create a typescript of everything printed on your terminal. script my.terminal.sessio Type commands: ls date sudo service foo stop To exit (to end script session) type exit or logout or press control-D exit To view type: more my.terminal.session less my.terminal.session cat my.terminal.session Restoring deleted /tmp folder As my journey continues with Linux and Unix shell, I made a few mistakes . I accidentally deleted /tmp folder. To restore it all you have to do is: mkdir /tmp chmod 1777 /tmp chown root:root /tmp ls -ld /tmp Locking a directory For privacy of my data I wanted to lock down /downloads on my file server. So I ran: chmod 0000 /downloads The root user can still has access and ls and cd commands will not work. To go back: chmod 0755 /downloads Password protecting file in vim text editor Afraid that root user or someone may snoop into your personal text files? Try password protection to a file in vim, type: vim +X filename Or, before quitting in vim use :X vim command to encrypt your file and vim will prompt for a password. Clear gibberish all over the screen Just type: reset Becoming human Pass the -h or -H (and other options) command line option to GNU or BSD utilities to get output of command commands like ls, df, du, in human-understandable formats: ls -lh # print sizes in human readable format (e.g., 1K 234M 2G) df -h df -k # show output in bytes, KB, MB, or GB free -b free -k free -m free -g # print sizes in human readable format (e.g., 1K 234M 2G) du -h # get file system perms in human readable format stat -c %A /boot # compare human readable numbers sort -h -a file # display the CPU information in human readable format on a Linux lscpu lscpu -e lscpu -e=cpu,node # Show the size of each file but in a more human readable way tree -h tree -h /boot Show information about known users in the Linux based system Just type: ## linux version ## lslogins ## BSD version ## logins Sample outputs: UID USER PWD-LOCK PWD-DENY LAST-LOGIN GECOS 0 root 0 0 22:37:59 root 1 bin 0 1 bin 2 daemon 0 1 daemon 3 adm 0 1 adm 4 lp 0 1 lp 5 sync 0 1 sync 6 shutdown 0 1 2014-Dec17 shutdown 7 halt 0 1 halt 8 mail 0 1 mail 10 uucp 0 1 uucp 11 operator 0 1 operator 12 games 0 1 games 13 gopher 0 1 gopher 14 ftp 0 1 FTP User 27 mysql 0 1 MySQL Server 38 ntp 0 1 48 apache 0 1 Apache 68 haldaemon 0 1 HAL daemon 69 vcsa 0 1 virtual console memory owner 72 tcpdump 0 1 74 sshd 0 1 Privilege-separated SSH 81 dbus 0 1 System message bus 89 postfix 0 1 99 nobody 0 1 Nobody 173 abrt 0 1 497 vnstat 0 1 vnStat user 498 nginx 0 1 nginx user 499 saslauth 0 1 \"Saslauthd user\" How do I fix mess created by accidentally untarred files in the current dir? So I accidentally untar a tarball in /var/www/html/ directory instead of /home/projects/www/current. It created mess in /var/www/html/. The easiest way to fix this mess: cd /var/www/html/ /bin/rm -f \"$(tar ztf /path/to/file.tar.gz)\" Confused on a top command output? Seriously, you need to try out htop instead of top: sudo htop Want to run the same command again? Just type !!. For example: /myhome/dir/script/name arg1 arg2 # To run the same command again !! ## To run the last command again as root user sudo !! The !! repeats the most recent command. To run the most recent command beginning with \"foo\": !foo # Run the most recent command beginning with \"service\" as root sudo !service The !$ use to run command with the last argument of the most recent command: # Edit nginx.conf sudo vi /etc/nginx/nginx.conf # Test nginx.conf for errors /sbin/nginx -t -c /etc/nginx/nginx.conf # After testing a file with \"/sbin/nginx -t -c /etc/nginx/nginx.conf\", you # can edit file again with vi sudo vi !$ Get a reminder you when you have to leave If you need a reminder to leave your terminal, type the following command: leave +hhmm Where, hhmm - The time of day is in the form hhmm where hh is a time in hours (on a 12 or 24 hour clock), and mm are minutes. All times are converted to a 12 hour clock, and assumed to be in the next 12 hours. Home sweet home Want to go the directory you were just in? Run: cd - Need to quickly return to your home directory? Enter: cd The variable CDPATH defines the search path for the directory containing directories: export CDPATH=/var/www:/nas10 Now, instead of typing cd /var/www/html/ I can simply type the following to cd into /var/www/html path: cd html Editing a file being viewed with less pager To edit a file being viewed with less pager, press v. You will have the file for edit under $EDITOR: less *.c less foo.html ## Press v to edit file ## ## Quit from editor and you would return to the less pager again ## List all files or directories on your system To see all of the directories on your system, run: find / -type d | less # List all directories in your $HOME find $HOME -type d -ls | less To see all of the files, run: find / -type f | less # List all files in your $HOME find $HOME -type f -ls | less Build directory trees in a single command You can create directory trees one at a time using mkdir command by passing the -p option: mkdir -p /jail/{dev,bin,sbin,etc,usr,lib,lib64} ls -l /jail/ Copy file into multiple directories Instead of running: cp /path/to/file /usr/dir1 cp /path/to/file /var/dir2 cp /path/to/file /nas/dir3 Run the following command to copy file into multiple dirs: echo /usr/dir1 /var/dir2 /nas/dir3 | xargs -n 1 cp -v /path/to/file Creating a shell function is left as an exercise for the reader Quickly find differences between two directories The diff command compare files line by line. It can also compare two directories: ls -l /tmp/r ls -l /tmp/s # Compare two folders using diff ## diff /tmp/r/ /tmp/s/ Text formatting You can reformat each paragraph with fmt command. In this example, I'm going to reformat file by wrapping overlong lines and filling short lines: fmt file.txt You can also split long lines, but do not refill i.e. wrap overlong lines, but do not fill short lines: fmt -s file.txt See the output and write it to a file Use the tee command as follows to see the output on screen and also write to a log file named my.log: mycoolapp arg1 arg2 input.file | tee my.log The tee command ensures that you will see mycoolapp output on on the screen and to a file same time.","tags":"中文"},{"url":"http://www.ciandcd.com/devopsguys-at-redgate.html","title":"DevOpsGuys at RedGate","text":"from:http://blog.devopsguys.com/2015/05/01/devopsguys-at-redgate/ The DevOpsGuys headed off on a road trip this week to meet with the RedGate team at their amazing offices in Cambridge. As well as working on some workshop training opportunities and guest blog articles (stay tuned to the DevOpsGuys Blog for some RedGate articles coming soon) the teams got together to brainstorm ideas and share skills. We were able to look at some of their newest tools and we're excited to announce that we will be delivering workshops on RedGate DLM tools at various sessions across the country this summer. We've already implemented these tools for a many of our customers and we're delighted to be able to introduce their qualities, in detail, to a wide range of industry professionals as part of an effective, independent DevOps adoption process. The workshops will be running on: May 20 – Automated Database Deployment, London June 26 – Automated Database Deployment, Belfast July 8 – Database Source Control, London July 24 – Database Source Control, Manchester August 20 – Database Continuous Integration, Cardiff Spaces are limited, so register now to take part in a workshop or request a workshop near you.","tags":"devops"},{"url":"http://www.ciandcd.com/jian-dan-de-proxyzhi-tinyhttpproxypy-itech.html","title":"简单的proxy之TinyHTTPProxy.py - iTech","text":"from:http://www.cnblogs.com/itech/p/3800590.html 如果是在外企工作的话，可以访问美国的机器，这样就可以在美国的机器上为自己装个proxy，然后本地就可以很容易的使用proxy来上网了。 主页： http://www.voidtrance.net/2010/01/simple-python-http-proxy/ 下载： http://www.voidtrance.net/downloads/tiny-proxy-0.3.1.tar.gz 1）很好用，下载然后在后台运行。只依赖于基本的python modules，运行的时候不需要root权限。 2） Chrome中的switchsharper插件的配置： #!/usr/bin/python __doc__ = \"\"\"Tiny HTTP Proxy. This module implements GET, HEAD, POST, PUT and DELETE methods on BaseHTTPServer, and behaves as an HTTP proxy. The CONNECT method is also implemented experimentally, but has not been tested yet. Any help will be greatly appreciated. SUZUKI Hisao 2009/11/23 - Modified by Mitko Haralanov * Added very simple FTP file retrieval * Added custom logging methods * Added code to make this a standalone application \"\"\" __version__ = \"0.3.1\" import BaseHTTPServer, select, socket, SocketServer, urlparse import logging import logging.handlers import getopt import sys import os import signal import threading from types import FrameType, CodeType from time import sleep import ftplib DEFAULT_LOG_FILENAME = \"proxy.log\" class ProxyHandler (BaseHTTPServer.BaseHTTPRequestHandler): __base = BaseHTTPServer.BaseHTTPRequestHandler __base_handle = __base.handle server_version = \"TinyHTTPProxy/\" + __version__ rbufsize = 0 # self.rfile Be unbuffered def handle(self): (ip, port) = self.client_address self.server.logger.log (logging.INFO, \"Request from '%s'\", ip) if hasattr(self, 'allowed_clients') and ip not in self.allowed_clients: self.raw_requestline = self.rfile.readline() if self.parse_request(): self.send_error(403) else: self.__base_handle() def _connect_to(self, netloc, soc): i = netloc.find(':') if i >= 0: host_port = netloc[:i], int(netloc[i+1:]) else: host_port = netloc, 80 self.server.logger.log (logging.INFO, \"connect to %s:%d\", host_port[0], host_port[1]) try: soc.connect(host_port) except socket.error, arg: try: msg = arg[1] except: msg = arg self.send_error(404, msg) return 0 return 1 def do_CONNECT(self): soc = socket.socket(socket.AF_INET, socket.SOCK_STREAM) try: if self._connect_to(self.path, soc): self.log_request(200) self.wfile.write(self.protocol_version + \" 200 Connection established\\r\\n\") self.wfile.write(\"Proxy-agent: %s\\r\\n\" % self.version_string()) self.wfile.write(\"\\r\\n\") self._read_write(soc, 300) finally: soc.close() self.connection.close() def do_GET(self): (scm, netloc, path, params, query, fragment) = urlparse.urlparse( self.path, 'http') if scm not in ('http', 'ftp') or fragment or not netloc: self.send_error(400, \"bad url %s\" % self.path) return soc = socket.socket(socket.AF_INET, socket.SOCK_STREAM) try: if scm == 'http': if self._connect_to(netloc, soc): self.log_request() soc.send(\"%s %s %s\\r\\n\" % (self.command, urlparse.urlunparse(('', '', path, params, query, '')), self.request_version)) self.headers['Connection'] = 'close' del self.headers['Proxy-Connection'] for key_val in self.headers.items(): soc.send(\"%s: %s\\r\\n\" % key_val) soc.send(\"\\r\\n\") self._read_write(soc) elif scm == 'ftp': # fish out user and password information i = netloc.find ('@') if i >= 0: login_info, netloc = netloc[:i], netloc[i+1:] try: user, passwd = login_info.split (':', 1) except ValueError: user, passwd = \"anonymous\", None else: user, passwd =\"anonymous\", None self.log_request () try: ftp = ftplib.FTP (netloc) ftp.login (user, passwd) if self.command == \"GET\": ftp.retrbinary (\"RETR %s\"%path, self.connection.send) ftp.quit () except Exception, e: self.server.logger.log (logging.WARNING, \"FTP Exception: %s\", e) finally: soc.close() self.connection.close() def _read_write(self, soc, max_idling=20, local=False): iw = [self.connection, soc] local_data = \"\" ow = [] count = 0 while 1: count += 1 (ins, _, exs) = select.select(iw, ow, iw, 1) if exs: break if ins: for i in ins: if i is soc: out = self.connection else: out = soc data = i.recv(8192) if data: if local: local_data += data else: out.send(data) count = 0 if count == max_idling: break if local: return local_data return None do_HEAD = do_GET do_POST = do_GET do_PUT = do_GET do_DELETE=do_GET def log_message (self, format, *args): self.server.logger.log (logging.INFO, \"%s %s\", self.address_string (), format % args) def log_error (self, format, *args): self.server.logger.log (logging.ERROR, \"%s %s\", self.address_string (), format % args) class ThreadingHTTPServer (SocketServer.ThreadingMixIn, BaseHTTPServer.HTTPServer): def __init__ (self, server_address, RequestHandlerClass, logger=None): BaseHTTPServer.HTTPServer.__init__ (self, server_address, RequestHandlerClass) self.logger = logger def logSetup (filename, log_size, daemon): logger = logging.getLogger (\"TinyHTTPProxy\") logger.setLevel (logging.INFO) if not filename: if not daemon: # display to the screen handler = logging.StreamHandler () else: handler = logging.handlers.RotatingFileHandler (DEFAULT_LOG_FILENAME, maxBytes=(log_size*(1<<20)), backupCount=5) else: handler = logging.handlers.RotatingFileHandler (filename, maxBytes=(log_size*(1<<20)), backupCount=5) fmt = logging.Formatter (\"[%(asctime)-12s.%(msecs)03d] \" \"%(levelname)-8s {%(name)s %(threadName)s}\" \" %(message)s\", \"%Y-%m-%d %H:%M:%S\") handler.setFormatter (fmt) logger.addHandler (handler) return logger def usage (msg=None): if msg: print msg print sys.argv[0], \"[-p port] [-l logfile] [-dh] [allowed_client_name ...]]\" print print \" -p - Port to bind to\" print \" -l - Path to logfile. If not specified, STDOUT is used\" print \" -d - Run in the background\" print def handler (signo, frame): while frame and isinstance (frame, FrameType): if frame.f_code and isinstance (frame.f_code, CodeType): if \"run_event\" in frame.f_code.co_varnames: frame.f_locals[\"run_event\"].set () return frame = frame.f_back def daemonize (logger): class DevNull (object): def __init__ (self): self.fd = os.open (\"/dev/null\", os.O_WRONLY) def write (self, *args, **kwargs): return 0 def read (self, *args, **kwargs): return 0 def fileno (self): return self.fd def close (self): os.close (self.fd) class ErrorLog: def __init__ (self, obj): self.obj = obj def write (self, string): self.obj.log (logging.ERROR, string) def read (self, *args, **kwargs): return 0 def close (self): pass if os.fork () != 0: ## allow the child pid to instanciate the server ## class sleep (1) sys.exit (0) os.setsid () fd = os.open ('/dev/null', os.O_RDONLY) if fd != 0: os.dup2 (fd, 0) os.close (fd) null = DevNull () log = ErrorLog (logger) sys.stdout = null sys.stderr = log sys.stdin = null fd = os.open ('/dev/null', os.O_WRONLY) #if fd != 1: os.dup2 (fd, 1) os.dup2 (sys.stdout.fileno (), 1) if fd != 2: os.dup2 (fd, 2) if fd not in (1, 2): os.close (fd) def main (): logfile = None daemon = False max_log_size = 20 port = 8000 allowed = [] run_event = threading.Event () local_hostname = socket.gethostname () try: opts, args = getopt.getopt (sys.argv[1:], \"l:dhp:\", []) except getopt.GetoptError, e: usage (str (e)) return 1 for opt, value in opts: if opt == \"-p\": port = int (value) if opt == \"-l\": logfile = value if opt == \"-d\": daemon = not daemon if opt == \"-h\": usage () return 0 # setup the log file logger = logSetup (logfile, max_log_size, daemon) if daemon: daemonize (logger) signal.signal (signal.SIGINT, handler) if args: allowed = [] for name in args: client = socket.gethostbyname(name) allowed.append(client) logger.log (logging.INFO, \"Accept: %s (%s)\" % (client, name)) ProxyHandler.allowed_clients = allowed else: logger.log (logging.INFO, \"Any clients will be served...\") server_address = (socket.gethostbyname (local_hostname), port) ProxyHandler.protocol = \"HTTP/1.0\" httpd = ThreadingHTTPServer (server_address, ProxyHandler, logger) sa = httpd.socket.getsockname () print \"Servering HTTP on\", sa[0], \"port\", sa[1] req_count = 0 while not run_event.isSet (): try: httpd.handle_request () req_count += 1 if req_count == 1000: logger.log (logging.INFO, \"Number of active threads: %s\", threading.activeCount ()) req_count = 0 except select.error, e: if e[0] == 4 and run_event.isSet (): pass else: logger.log (logging.CRITICAL, \"Errno: %d - %s\", e[0], e[1]) logger.log (logging.INFO, \"Server shutdown\") return 0 if __name__ == '__main__': sys.exit (main ())","tags":"中文"},{"url":"http://www.ciandcd.com/perl-du-xie-wen-jian-itech.html","title":"perl 读写文件 - iTech","text":"from:http://www.cnblogs.com/itech/p/3696895.html #http://perlmaven.com/open-and-read-from-files #mode operand create truncate #read < #write > yes yes #append >> yes Case 1: Throw an exception if you cannot open the file: use strict; use warnings; my $filename = 'data.txt'; open(my $fh, '<:encoding(UTF-8)', $filename) or die \"Could not open file '$filename' with the error $!\"; while (my $row = <$fh>) { chomp $row; print \"$row\\n\"; } close($fh); Case 2: Give a warning if you cannot open the file, but keep running: use strict; use warnings; my $filename = 'data.txt'; if (open(my $fh, '<:encoding(UTF-8)', $filename)) { while (my $row = <$fh>) { chomp $row; print \"$row\\n\"; } close($fh); } else { warn \"Could not open file '$filename' $!\"; } Case 3: Read one file into array use strict; use warnings; my $filename = 'data.txt'; open (FILEIN, \"<\", $filename) or die \"Could not open file '$filename' with the error $!\"; my @FileContents = <FILEIN>; for my $l (@FileContents){ print \"$l\\n\"; } close FILEIN; end","tags":"中文"},{"url":"http://www.ciandcd.com/announcing-flowcon.html","title":"Announcing FlowCon","text":"from:http://continuousdelivery.com/2013/05/announcing-flowcon/ Announcing FlowCon I spend quite a lot of time at conferences, and it consistently bothers me that they are so often focused on one particular function: development, testing, UX, systems administration. The point of continuous delivery is to accelerate the rate at which we can learn from each other – and from our customers. That requires everyone involved in the delivery process (including users, product owners and entrepreneurs) to collaborate throughout. So why isn't there a conference which focuses on flow – the emergent property of great teams? So I got together with a bunch of like-minded folks – Elisabeth Hendrickson , Gene Kim , John Esser and Lane Halley – and now there is a conference about creating flow: FlowCon . It's on Friday November 1 in San Francisco , and it's produced by ThoughtWorks and Trifork (creators of the GOTO conferences ). The conference is based around four values: Learning : Our goal is to provide the best possible conference forum for practitioners to learn from each other how to build great products and services. Open Information : We aim to uncover how great products and services are built in real life and make this information freely available to the widest audience possible. Diversity : We believe the technology community – and thus the conference speakers and participants – should reflect the demographics of our customers and the wider world. Spanning boundaries : We believe that the best products and services are created collaboratively by people with a range of skills and experiences. We have put together nearly half of the program , and we're delighted to announce that Adrian Cockcroft , Catherine Courage , Jeff Gothelf and Linda Rising will be giving keynotes. The program is still a work in process (a minimum viable product, if you will). In particular, the after lunch sessions are empty – for a good reason: we want you to speak in those slots . We're looking for people working to create flow in their organization – especially those who: Span multiple roles and work across organizational silos. Work in any of the following areas: a highly regulated environment; a large, traditional enterprise; in the pursuit of social and economic justice. Are willing to share obstacles encountered or mistakes made and how you overcame them – whether cultural or technological. Offer actionable advice \"the rest of us\" can apply today (even if we don't have the resources of Etsy / Amazon / Google). Your talk could be about culture, technology, design, process – the only really important criterion is that it draws on what you've learned about helping to create flow in your organization. If that sounds like you, please submit your proposal . If you know someone who would do a great job, please encourage them to submit. Our submission process is designed to be entirely merit-based, which means that the first round is anonymous. The deadline is midnight Pacific time, Sunday June 23, 2013. Tickets for the conference are now on sale – at $350 if you register before July 31, or $500 if you register afterwards. Whatever your role or domain, you're sure to find inspirational, disruptive thinking that will make you better at creating great products and services. I hope to see you there!","tags":"ciandcd"}]}