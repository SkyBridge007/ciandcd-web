{"pages":[{"text":"about thanks https://www.python.org/ https://pythonhosted.org/feedparser/index.html https://github.com/codelucas/newspaper https://github.com/getpelican/pelican http://twitter.github.com/bootstrap Thanks!!!","url":"http://www.ciandcd.com/pages/guan-yu.html","tags":"pages","title":"关于"},{"text":"ciandcd Continuous Integration and Continuous Delivery Written by ciandcd.com A curated list of awesome tools for: continuous integration continuous delivery software integration devops ciandcd Theory Build And Release System Online Build System Infrastructure dev env Source Code Management Code Review Build Static Check Dynamic Check Performance Analysis Coverage Testing Package Deploy Delivery Provisioning Tools Web Server Applications And Container OS And Shell Database Version Control Useful Sites Conference And Submit Other Awesome Lists Contact Theory The theory for continuous integration and continuous deliver continuousIntegration continuousdelivery software integration devopsdays * ci cheatsheet Build And Release System The system for software build and release, continuous integration and continuous delivery Jenkins An extendable open source continuous integration server BuildForge Automate and accelerate build and release processes ElectricCommander ElectricCommander gives distributed teams shared control and visibility into infrastructure, tool chains and processes. It accelerates and automates the software delivery process to enable agility, predictability and security across many build-test-deploy pipelines Teamcity Ready to work, extensible and developer-friendly build server out of the box bamboo Bamboo does more than just run builds and tests. It connects issues, commits, test results, and deploys so the whole picture is available to your entire product team go Automate and streamline the build-test-release cycle for worry-free, continuous delivery of your product hudson the previous one of Jenkins openbuildservice The Open Build Service (OBS) is a generic system to build and distribute binary packages from sources in an automatic, consistent and reproducible way. You can release packages as well as updates, add-ons, appliances and entire distributions for a wide range of operating systems and hardware architectures buildbot Buildbot is a continuous integration system designed to automate the build/test cycle. By automatically rebuilding and testing the tree each time something has changed, build problems are pinpointed quickly, before other developers are inconvenienced by the failure Parabuild Parabuild is an enterprise software build and release management system that helps software teams to release on time by providing them practically unbreakable release builds and Continuous Integration FinalBuilder Automating your Build process is simple with FinalBuilder. With FinalBuilder you don't need to edit xml, or write scripts. Visually define and debug your build scripts, then schedule them with windows scheduler, or integrate them with Continua CI, Jenkins or any other CI Server VisualBuild Visual Build enables developers and build masters to easily create an automated, repeatable build process cruisecontrol CruiseControl.NET, an Automated Continuous Integration server, implemented using the .NET Framework continuum Apache Continuum™ is an enterprise-ready continuous integration server with features such as automated builds, release management, role-based security, and integration with popular build tools and source control management systems quickbuild GitHub integration. Perforce shelve support. Coverity report rendering. Subversion external change retrieval. Resource access info. Display reasons for waiting steps. Custom build and request columns. Favorite dash board list. Inheritable environment variables.And much more... rexify perl Deployment & Configuration Management Online Build System Online build release system travis-ci ci server for github and bitbuckets cloudbees the Enterprise Jenkins Company elasticbox A DevOps approach that focuses on reusable application components as a service, and enables operations to provide IT as a Service coveralls Track your project's code coverage over time, changes to files, and badge your GitHub repo shippable Hosted continuous integration and deployment service built on docker circleci Continuous Integration for web apps. buildbox Simple self-hosted Continuous Integration drone Open source continuous integration platform built on Docker appveyor Continuous Integration and Deployment service for busy Windows snap-ci Easy builds, deployed when you want codeship Continuous Integration and Delivery made simple solanolabs Hosted continuous integration and deployment githost Painless GitLab CE & CI Hosting testling Automatic browser tests on every push magnum-ci Hosted Continuous Integration and Delivery Platform for private repositories wercker Test and deploy your applications with ease coveralls Track your project's code coverage over time, changes to files, and badge your GitHub repo ship.io Simple, powerful CI for iOS and Android. Re-build, Re-test, Re-deploy. GitLab CI - Based off of ruby. They also provide GitLab, which manages git repositories. IBM DevOps Services - Develop, track, plan, and deploy software onto the IBM Bluemix cloud platform. Infrastructure The hardware,virtual machines, fram management, docker GridWiki wiki page for Grid UGE Univa workload management solutions maximize the value of existing computing resources by efficiently sharing workloads across thousands of servers SGE Grid Engine is typically used on a computer farm or high-performance computing (HPC) cluster and is responsible for accepting, scheduling, dispatching, and managing the remote and distributed execution of large numbers of standalone, parallel or interactive user jobs. It also manages and schedules the allocation of distributed resources such as processors, memory, disk space, and software licenses LSF Platform Load Sharing Facility (or simply LSF) is a workload management platform, job scheduler, for distributed HPC environments. It can be used to execute batch jobs on networked Unix and Windows systems on many different architectures vmwarevshpere VMware vSphere (formerly VMware Infrastructure 4) is VMware's cloud computing virtualization operating system ctrixserver XenServer is the best server virtualization platform for public and private clouds, powering 4 of the 5 largest hosting provider clouds. Built with scale, security and multi-tenancy in mind, XenServer allows for even greater flexibility and cost efficiency miscrosofthyperv microsoft virtualization amazon Scalable, pay-as-you-go compute capacity in the cloud Dev env boxstarter Repeatable, reboot resilient windows environment installations made easy using Chocolatey packages. vagrantup Create and configure lightweight, reproducible, and portable development environments. veewee Easing the building of vagrant boxes Source Code Management Version control and source code management tools git Git is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency perforce Industry's most reliable and proven platform for versioning code, images, documents... everything clearcase IBM Rational ClearCase is a software configuration management solution that provides version control, workspace management, parallel development support, and build auditing mercurial Mercurial is a free, distributed source control management tool. It efficiently handles projects of any size and offers an easy and intuitive interface svn Subversion is an open source version control system gitlab Open source self-hosted Git management software github Powerful collaboration, review, and code management for open source and private development projects. bitbuckets Plant your code in the cloud. Watch it grow. teamfoundationservice Visual Studio Online, based on the capabilities of Team Foundation Server with additional cloud services, is the online home for your development projects. Get up and running in minutes on our cloud infrastructure without having to install or configure a single server. Visual Studio Online connects to Visual Studio, Eclipse, Xcode, and other Git clients to support development for a variety of platforms and languages phabricator Phabricator is a collection of open source web applications that help software companies build better software. * IBM DevOps Services - Store, manage, edit, and collaborate on your source code. Then deploy onto the IBM Bluemix cloud platform. Code Review Code review tools codecollaborator Collaborator helps development, testing and management teams work together to produce high quality code crucible Code reviews = quality code. Review code, discuss changes, share knowledge, and identify defects with Crucible's flexible review workflow. It's code review made easy for Subversion, CVS, Perforce, and more reviewboard Review Board takes the pain out of code review, saving you time, money, and sanity so you can focus on making great software codestriker Codestriker is an open-sourced web application which supports online code reviewing. Traditional document reviews are supported, as well as reviewing diffs generated by an SCM (Source Code Management) system and plain unidiff patches getbarkeep a fast, fun way to review code gerrit Gerrit is a web based code review system, facilitating online code reviews for projects using the Git version control system * Codebrag Codebrag is a simple code review tool that makes the process work for your team. Build Build tools gnumake GNU Make is a tool which controls the generation of executables and other non-source files of a program from the program's source files gnuautoconf Autoconf is an extensible package of M4 macros that produce shell scripts to automatically configure software source code packages mozillabuildtools The Mozilla build system, like the rest of the Mozilla codebase, is cross-platform. It uses traditional Unix-style autoconf and make tools to build the various applications (even on non-unix operating systems) scons SCons is an Open Source software construction tool—that is, a next-generation build tool. Think of SCons as an improved, cross-platform substitute for the classic Make utility with integrated functionality similar to autoconf/automake and compiler caches such as ccache. In short, SCons is an easier, more reliable and faster way to build software cmake cmake offers robust, cross-platform software development solutions. Find out how we can help your team efficiently manage the build, test, and package process for your software project msbuild The Microsoft Build Engine is a platform for building applications. This engine, which is also known as MSBuild, provides an XML schema for a project file that controls how the build platform processes and builds software. Visual Studio uses MSBuild, but it doesn't depend on Visual Studio. By invoking msbuild.exe on your project or solution file, you can orchestrate and build products in environments where Visual Studio isn't installed ant Ant can be used to pilot any type of process which can be described in terms of targets and tasks. The main known usage of Ant is the build of Java applications. maven Apache Maven is a software project management and comprehension tool. Based on the concept of a project object model (POM), Maven can manage a project's build, reporting and documentation from a central piece of information. gradle Gradle is build automation evolved. Gradle can automate the building, testing, publishing, deployment and more of software packages or other types of projects such as generated static websites, generated documentation or indeed anything else. ElectricacCelerator Continuous Delivery isn't continuous if builds and tests take too long to complete. ElectricAccelerator speeds up builds and tests by up to 20X, improving software time to market, infrastructure utilization and developer productivity Static Check Software static check tools static tools wiki wiki page coverity Continually measure and improve code quality and security across your development organization fxcop FxCop is an application that analyzes managed code assemblies (code that targets the .NET Framework common language runtime) and reports information about the assemblies, such as possible design, localization, performance, and security improvements cpd Duplicate code can be hard to find, especially in a large project. But PMD's Copy/Paste Detector (CPD) can find it for you sonar SonarQube is an open platform to manage code quality findbugs Find Bugs in Java Programs * checkstyle Checkstyle is a development tool to help programmers write Java code that adheres to a coding standard Dynamic Check Software dynamic check tools * dynamic tools wiki wiki page Performance Analysis Software performance analysis tools * performance tools wiki wiki page Coverage Software testing coverage tools * code coverage wiki wiki page Testing Software testing framework and tools Testingautomation test automation wiki page softwareqatest FAQ page qaforums SQA Forums opensourcetesting open source software testing tools, news and disccussions * selenium Selenium automates browsers Package The tools for software package and installation installshield World's #1 Software Installation Solution-Build Reliable MSI Installers for Windows Applications NSIS NSIS (Nullsoft Scriptable Install System) is a professional open source system to create Windows installers. It is designed to be as small and flexible as possible and is therefore very suitable for internet distribution rpm The RPM Package Manager (RPM) is a powerful command line driven package management system capable of installing, uninstalling, verifying, querying, and updating computer software packages yum Yum is an automatic updater and package installer/remover for rpm systems fpm Effing package management! Build packages for multiple platforms (deb, rpm, etc) with great ease and sanity. wix The most powerful set of tools available to create your Windows installation experience. * packer Packer is a tool for creating identical machine images for multiple platforms from a single source configuration. Deploy The tools for web site deploy jfrog s the first Binary Repository Management solution, Artifactory has changed the way binaries are controlled, stored and managed throughout the software release cycle xl-deploy Agentless, Model-based App Deployment Jenkinsdeployplugin deploy to tomcat bintray The fastest and most reliable way to automate the distribution of your software releases Delivery The tools for software delivery sl-release Orchestrate your Continuous Delivery pipelines. Simple. Flexible. End-to-End archiva Apache Archiva™ is an extensible repository management software that helps taking care of your own personal or enterprise-wide build artifact repository. It is the perfect companion for build tools such as Maven, Continuum, and ANT nexus The use of repository managers (also known as component managers) is helping software development teams achieve simple gains in speed, efficiency, and quality of their operations chocolatey Chocolatey NuGet is a Machine Package Manager, somewhat like apt-get, but built with Windows in mind pulp Pulp is a platform for managing repositories of content, such as software packages, and pushing that content out to large numbers of consumers. herd A single-command bittorrent distribution system, based on Twitter's Murder * murder Large scale server deploys using BitTorrent and the BitTornado library from twitter.com Provisioning Tools Provision tools Puppet Build, destroy and rebuild servers on any public or private cloud Chef Fast, scalable and flexible software for data center automation SaltStack Radically simple configuration-management, application deployment, task-execution, and multi-node orchestration engine ansible Web Server Common used web server apache Apache httpd has been the most popular web server on the Internet since April 1996 nginx A high performance free open source web server powering busiest sites on the Internet tomcat An open source software implementation of the Java Servlet and JavaServer Pages technologies jetty Jetty provides a Web server and javax.servlet container, plus support for SPDY, WebSocket, OSGi, JMX, JNDI, JAAS and many other integrations * HAProxy - Software based load Balancing, SSL offloading and performance optimization, compression, and general web routing. OS And Shell Linux shell, perl, python awesome-shell awesome-python awesome-perl awesome-sysadmin Applications And Container VM application and container docker Docker - An open platform for distributed applications for developers and sysadmins suseapplication tools to create suse applications Database Version Control Database version control system liquibase source control for your database flywaydb Database Migrations Made Easy nextep NeXtep Softwares provides software solutions for the industrialization of your database developments and deployments. Our goal is to increase the productivity of your development teams by taking control of your developments' lifecycle and by automating your deployment and test processes dbdeploy dbdeploy is a Database Change Management tool. It's for developers or DBAs who want to evolve their database design – or refactor their database – in a simple, controlled, flexible and frequent manner * dbmaestro Controlled Database Continuous Delivery is Our Business Useful Sites Other useful pages infoq stackoverflow Conference And Submit Conference and submit * devops submit Other Awesome Lists Other amazingly awesome lists can be found in awesome awesome-awesome awesome-awesomeness sysadmin Contact To add new items about continuous integration and continuous delivery: 1) pull; 2) add issue; 3) send me by email itech001@126.com; 4) qq group 172758282;","url":"http://www.ciandcd.com/pages/awesome.html","tags":"pages","title":"Awesome"},{"text":"from:http://java.dzone.com/articles/3-reasons-why-testing-software The software development life cycle is an extremely intensive process for developers and quality assurance professionals alike. If even one element is neglected, it can delay project schedules and affect user performance. Security is one aspect that must be built in from the inception of any app, and here are a few reasons why: Breaches can cost your business Let's say that an organization uses its application to order and manage inventory, payroll and other operational needs. If a malicious entity were to access this information, it could easily make fraudulent transactions, costing the company more than what was intended. Not to mention it will create a massive headache to set the record straight. TechTarget contributor Peter Gregory noted that this can happen when programs lack audit trails and processes required for secure purchasing. By building in this functionality early on, this type of situation can be avoided, allowing organizations to retain customer trust and money. \"Organizations that fail to involve information security in the life cycle will pay the price in the form of costly and disruptive events,\" Gregory wrote. \"Many bad things can happen to information systems that lack the required security interfaces and characteristics.\" Access to confidential data can be damaging If a business aims to use an app for information sharing and availability, protection must be at the forefront of this project throughout its life cycle. While some data may not be as costly to leak, the loss of confidential reports and documents can severely affect the organization's ability tofunction. QA teams must ensure that security practices are implemented and built upon constantly. TechTarget contributor Nick Lewis noted that firewalls and traditional methods will not be enough to keep targeted attacks at bay. Instead, testing the app for insufficient process validation, abuse of functionality, weak password recovery validation and information leakage will be critical toguarding the program. Analyze initial risk before jumping in One SDLC security practice to observe is a primary risk assessment before the start of a new project. Not all applications are equal, which means each program will be labeled with a different risk level. Some software will be publicly accessible, whereas others will be more business-critical and involve processing sensitive data. These uses will largely determine how much risk would be involved with a breach on such activities. This information will give QA teams a clear picture of the security roadmap needed, and can be implemented. \"Doing the preliminary risk assessment to establish the need for the system helps identify any security show stoppers before too much time and effort goes into the next SDLC phases,\" a SANS white paper stated. \"It also gets the design team thinking about security issues early in the design process.\" Cyberattacks and malware in the headlines have made security more prominent than ever before. By building in protections early in the SDLC, QA teams can ensure that they will be better able tohandle these threats without interruptions to regular business activities.","url":"http://www.ciandcd.com/3-reasons-why-testing-software-security-should-start-early.html","tags":"devops","title":"3 Reasons Why Testing Software Security Should Start Early"},{"text":"from:http://java.dzone.com/articles/organisation-pattern-trunk-based-development Trunk Based Development is a version control strategy in which developers commit their changes to the shared trunk of a source code repository with minimal branching. Trunk Based Development became well known in the mid 2000s as Continuous Integration became a mainstream development practice, and today it is equally applicable to centralised Version Control Systems (VCS) and Distributed Version Control Systems (DVCS). In Trunk Based Development new features are developed concurrently on trunk as a series of small, incremental steps that preserve existing functionality and minimise merge complexity. Features are always released from trunk, and defect fixes are either released from trunk or a short-lived release branch. When development of a feature spans multiple releases its entry point is concealed to ensure the ongoing changes do not impede release cadence. The addition of a new feature can be concealed with a Feature Toggle , which means a configuration parameter or business rule is used to turn a feature on or off at runtime. As shown below a Feature Toggle is turned off while its feature is in development (v1), turned on when its feature is in production (v2), and removed after a period of time (v3). Updates to an existing feature can be concealed with a Branch By Abstraction , which means an abstraction layer is temporarily introduced to encapsulate both the old behaviour in use and the new behaviour in development. As shown below a Branch By Abstraction routes requests to the old behaviour while the new behaviour is in development (v1-v2), reroutes requests to the new behaviour when it is in production (v3), and is removed after a period of time (v4). Trunk Based Development is synonymous with Continuous Integration, which has been described by Jez Humble et al as \" the most important technical practice in the agile canon \". Continuous Integration is a development practice where all members of a team integrate and test their changes together on at least a daily basis, resulting in a shared mindset of collaboration and an always releasable codebase. This is verified by an automated build server continuously building the latest changes, and can include pre- and post-build actions such as code reviews and auto-revert on failure. Consider an organisation that provides an online Company Accounts Service, with its codebase maintained by a team practicing Trunk Based Development and Continuous Integration. In iteration 1 two features are requested – F1 Computations and F2 Write Offs – so the team discuss their concurrent development and decide on a Feature Toggle for F1 as it is a larger change. The developers commit their changes for F1 and F2 to trunk multiple times a day, with F1 tested in its on and off states to verify its progress alongside F2. In iteration 2 more features – F3 Bank Details and F4 Accounting Periods – begin development. F4 requires a different downstream submissions system, so the team design a Branch By Abstraction for submissions to ensure F1 and F3 can continue with the legacy submissions system until F4 is complete. F2 is signed off and released into production with F1 still toggled off at runtime. Some changes for F3 break the build, which triggers an automatic revert and a team discussion on a better design for F3. In iteration 3 a production defect is found in F2, and after the defect is fixed on trunk a release branch is agreed for risk mitigation. An F2.1 release branch is created from the last commit of the F2 release, the fix is merged to the branch, and F2.1 is released into production. F4 continues on trunk, with the submissions Branch By Abstraction tested in both modes. F3 is signed off and released into production using the legacy submissions system. In iteration 4 F1 is signed off and its Feature Toggle is turned on in production following a release. F4 is signed off and released into production, but when the Branch By Abstraction is switched to the new submissions system a defect is found. As a result the Branch By Abstraction is reverted at runtime to the legacy submissions system, and a F4.1 fix is released from trunk. In this example F1, F2, F3, and F4 clearly benefit from being developed by a team collaborating on a single shared code stream. For F1 the team agrees on the why and how of the Feature Toggle, with F1 tested in both its on and off states. For F2 the defect fix is made available from trunk and everyone is aware of the decision to use a release branch for risk mitigation. For F3 the prominence of a reverted build failure encourages people to contribute to a better design. For F4 there is a team decision to create a submissions Branch By Abstraction, with the new abstraction layer offering fresh insights into the legacy system and incremental commits enabling regular feedback on the new approach. Furthermore, when the new submissions system is switched on and a defect is found in F4 the ability to revert at runtime to the legacy submissions means the Company Accounts Service can remain online with zero downtime. This highlights the advantages of Trunk Based Development: Continuous Integration – incremental commits to trunk ensure an always integrated, always tested codebase with minimal integration costs and a predictable flow of features Adaptive scheduling – an always releasable codebase separates the release schedule from development efforts, meaning features can be released on demand according to customer needs Collaborative design – everyone working on the same code encourages constant communication, with team members sharing responsibility for design changes and a cohesive Evolutionary Architecture Operational and business empowerment – techniques such as Feature Toggle and Branch By Abstraction decouple release from launch, providing the operational benefit of graceful degradation on failure and the business benefit of Dark Launching features Breaking down features and re-architecting an existing system in incremental steps requires discipline, planning, and ingenuity from an entire team on a daily basis, and Trunk Based Development can incur a development overhead for some time if multiple technologies are in play and/or the codebase is poorly structured. However, those additional efforts will substantially reduce integration costs and gradually push the codebase in the right direction – as shown by Dave Farley and Jez Humble praising Trunk Based Development for \" the gentle, subtle pressure it applies to make the design of your software better \". A common misconception of Trunk Based Development is that it is slow, as features take longer to complete and team velocity is often lower than expected. However, an organisation should optimise globally for cycle time not locally for velocity, and by mandating a single code stream Trunk Based Development ensures developers work at the maximum rate of the team not the individual, with reduced integration costs resulting in lower lead times. Trunk Based Development is simple, but not easy. It has a steep learning curve but the continuous integration of small changesets into trunk will minimise integration costs, encourage collaborative design, empower runtime operational and business decisions, and ultimately drive the engine of Continuous Delivery. It is for this reason Dave Farley and Jez Humble declared \" we can't emphasise enough how important this practice is in enabling continuous delivery of valuable, working software \".","url":"http://www.ciandcd.com/organisation-pattern-trunk-based-development.html","tags":"devops","title":"Organisation Pattern: Trunk Based Development"},{"text":"from:http://python.dzone.com/articles/get-back-and-try-again I don't often write about tools I use when for my daily software development tasks. I recently realized that I really should start to share more often my workflows and weapons of choice. One thing that I have a hard time enduring while doing Python code reviews, is people writing utility code that is not directly tied to the core of their business. This looks to me as wasted time maintaining code that should be reused from elsewhere. So today I'd like to start with retrying , a Python package that you can use to… retry anything. It's OK to fail Often in computing, you have to deal with external resources. That means accessing resources you don't control. Resources that can fail, become flapping, unreachable or unavailable. Most applications don't deal with that at all, and explode in flight, leaving a skeptical user in front of the computer. A lot of software engineers refuse to deal with failure, and don't bother handling this kind of scenario in their code. In the best case, applications usually handle simply the case where the external reached system is out of order. They log something, and inform the user that it should try again later. In this cloud computing area, we tend to design software components with service-oriented architecture in mind. That means having a lot of different services talking to each others over the network. And we all know that networks tend to fail, and distributed systems too. Writing software with failing being part of normal operation is a terrific idea. Retrying In order to help applications with the handling of these potential failures, you need a plan. Leaving to the user the burden to \"try again later\" is rarely a good choice. Therefore, most of the time you want your application to retry. Retrying an action is a full strategy on its own, with a lot of options. You can retry only on certain condition, and with the number of tries based on time (e.g. every second), based on a number of tentative (e.g. retry 3 times and abort), based on the problem encountered, or even on all of those. For all of that, I use the retrying library that you can retrieve easily on PyPI . retrying provides a decorator called retry that you can use on top of any function or method in Python to make it retry in case of failure. By default, retry calls your function endlessly until it returns rather than raising an error. import random from retrying import retry @retry def pick_one(): if random.randint(0, 10) != 1: raise Exception(\"1 was not picked\") This will execute the function pick_one until 1 is returned by random.randint . retry accepts a few arguments, such as the minimum and maximum delays to use, which also can be randomized. Randomizing delay is a good strategy to avoid detectable pattern or congestion. But more over, it supports exponential delay, which can be used to implement exponential backoff , a good solution for retrying tasks while really avoiding congestion. It's especially handy for background tasks. @retry(wait_exponential_multiplier=1000, wait_exponential_max=10000) def wait_exponential_1000(): print \"Wait 2&#94;x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards\" raise Exception(\"Retry!\") You can mix that with a maximum delay, which can give you a good strategy to retry for a while, and then fail anyway: # Stop retrying after 30 seconds anyway >>> @retry(wait_exponential_multiplier=1000, wait_exponential_max=10000, stop_max_delay=30000) ... def wait_exponential_1000(): ... print \"Wait 2&#94;x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards\" ... raise Exception(\"Retry!\") ... >>> wait_exponential_1000() Wait 2&#94;x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards Wait 2&#94;x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards Wait 2&#94;x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards Wait 2&#94;x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards Wait 2&#94;x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards Wait 2&#94;x * 1000 milliseconds between each retry, up to 10 seconds, then 10 seconds afterwards Traceback (most recent call last): File \"<stdin>\", line 1, in <module> File \"/usr/local/lib/python2.7/site-packages/retrying.py\", line 49, in wrapped_f return Retrying(*dargs, **dkw).call(f, *args, **kw) File \"/usr/local/lib/python2.7/site-packages/retrying.py\", line 212, in call raise attempt.get() File \"/usr/local/lib/python2.7/site-packages/retrying.py\", line 247, in get six.reraise(self.value[0], self.value[1], self.value[2]) File \"/usr/local/lib/python2.7/site-packages/retrying.py\", line 200, in call attempt = Attempt(fn(*args, **kwargs), attempt_number, False) File \"<stdin>\", line 4, in wait_exponential_1000 Exception: Retry! A pattern I use very often, is the ability to retry only based on some exception type. You can specify a function to filter out exception you want to ignore or the one you want to use to retry. def retry_on_ioerror(exc): return isinstance(exc, IOError) @retry(retry_on_exception=retry_on_ioerror) def read_file(): with open(\"myfile\", \"r\") as f: return f.read() retry will call the function passed as retry_on_exception with the exception raised as first argument. It's up to the function to then return a boolean indicating if a retry should be performed or not. In the example above, this will only retry to read the file if an IOError occurs; if any other exception type is raised, no retry will be performed. The same pattern can be implemented using the keyword argument retry_on_result , where you can provide a function that analyses the result and retry based on it. def retry_if_file_empty(result): return len(result) <= 0 @retry(retry_on_result=retry_if_file_empty) def read_file(): with open(\"myfile\", \"r\") as f: return f.read() This example will read the file until it stops being empty. If the file does not exist, an IOError is raised, and the default behavior which triggers retry on all exceptions kicks-in – the retry is therefore performed. That's it! retry is really a good and small library that you should leverage rather than implementing your own half-baked solution!","url":"http://www.ciandcd.com/get-back-up-and-try-again-retrying-in-python.html","tags":"devops","title":"Get Back Up and Try Again: Retrying in Python"},{"text":"from:http://java.dzone.com/articles/continuous-integration-and-0 Written by Jaroslav Holub for The Codeship Blog . Continuous delivery is all about reducing risk and delivering value faster by producing reliable software in short iterations. As Martin Fowler says , you actually do continuous delivery if: Your software is deployable throughout its lifecycle. Your team prioritizes keeping the software deployable over working on new features. Anybody can get fast, automated feedback on the production readiness of their systems any time somebody makes a change to them. You can perform push-button deployments of any version of the software to any environment on demand. Containerization of software allows us to further improve on this process. The biggest improvements are in speed and in the level of abstraction used as a cornerstone for further innovations in this field. In this post, I'll show you how to set up a continuous delivery pipeline using Docker. We'll see how using this tool for Linux containers as part of the continuous delivery pipeline lets us nicely encapsulate the build process of a service. It also lets us deploy any revision with a few simple steps. I'll mainly use the term continuous delivery in this article, because it stands for the full circle of steps leading to our ultimate goal. However, continuous integration is the most substantial part of continuous delivery. Continuous Integration with Docker Let's take a Hello World web server written in Go as an example service. You can find all the code used in this example here: https://github.com/ContainerSolutions/cd-with-docker-tutorial The continuous integration setup consists of: running unit tests building the Docker image that we use to build our service running the build container and compiling our service building the Docker image that we run and deploy pushing the final image to a Docker registry Automated testing Running tests in this example is as trivial as it should be: go test Building Docker image The core of a single service integration is making the end artifact — Docker image in our case. Because I've deliberately chosen the compiled language Go in this example, we need to build an executable file as part of our integration process. We'll eventually place the executable file inside this Docker image. Now one might think that we would build our web server executable file using build tools installed on the host dedicated to continuous integration and then somehow copy the binary to the Docker image. But this is a no-no in the containerized world. Let's do it all in containers. That way, we won't rely on any build tools installed on hosts, and it'll make the whole setup easily reproducible and encapsulated. Building an executable file can be part of a single Docker image build process together with runtime environment setup. Or we can separate the two. Having everything in a single build process, we would end up with extra content (build process leftovers) in our Docker image filesystem, even if we clean it afterwards in separate RUN commands within the Dockerfile. Some people use tricks to create, manipulate, and remove unwanted stuff in a single RUN command. Although it's sometimes handy, I can't generally recommend it; in my opinion this adds to Dockerfile complexity. Of course, there are situations where you might want to retain your sources and all in the end artifact. The approach I recommend, however, is to create separate \"build\" and \"distribution\" Dockerfiles. Use Dockerfile.build to do the heavy lifting during building the software, and use Dockerfile.dist to create the distributable Docker image, as light and clean as possible. The following is Dockerfile.build . As you can see, once we run the build file, we create the container from a golang image, compile our example service, and output the binary. FROM golang:1.4 RUN mkdir -p /tmp/build ADD hello-world.go /tmp/build/ WORKDIR /tmp/build RUN go build hello-world.go CMD tar -czf - hello-world In Dockerfile.dist , we only use this binary and run it on runtime: FROM debian:jessie RUN mkdir /app ADD build.tar.gz /app/ ENTRYPOINT /app/hello-world Our build.sh script — the essential part of our continuous integration pipeline — then looks like this: # !/bin/sh docker build -t hello-world-build -f Dockerfile.build . docker run hello-world-build > build.tar.gz docker build -t hello-world -f Dockerfile.dist . As you can see, these three simple Docker commands get us a clean, small Hello-World Docker image that's ready to be deployed and run on demand. Once both images used in the FROM clauses are pulled and cached locally, our build process will be a matter of milliseconds or at most a few seconds, with a very small resources footprint. Storing Docker images Once our build process artifact is created, we want to push it to Docker Registry, where it will be available for deployments. Please note that tagging images properly is very important. Docker ecosystems suffer from the usage of \"latest\" tag. If you use a unique tag for every new image, then all your image versions will be easily accessible for deployment in the future. We can choose whether we want to use our own Docker Registry or rely on Docker Hub . On Docker Hub, you can store public or private repositories of images. It's also the first place people would look for your images (if you want anyone to look for them). Your own Docker Registry on the other hand gives you full control over your images storage, performance, and security. More advanced setups might combine both approaches. This way you can tag the new image with an appropriate tag and push it to a public hub (replace your_username and your_tag with actual values): # !/bin/sh docker tag hello-world:latest your_username/hello-world:your_tag docker push your_username/hello-world:your_tag Continuously Delivered Containers Once we have our Docker images building pipeline working and images nicely stashed in a repository, we definitely want to get our service deployed. How you deploy your applications depends on your infrastructure or cloud provider. A few cloud providers support Docker images in their APIs these days (e.g., Amazon EC2 Container Service , Digital Ocean , or Giant Swarm ). You can further leverage the power of containerized applications with resource abstraction tools like Apache Mesos (read more about running containers on Mesos ) or Google Kubernetes that let you deploy and manage containers in their own ways. In case of our Hello World example, deploying remotely means running the following command remotely on a target machine with Docker installed on it: # !/bin/sh docker stop hello-production docker run --rm -p 8000:80 --name hello-production hello-world Beyond Continuous Delivery with Docker Using containerized software does not inherently mean one is implementing microservices. However, containers enable this architectural pattern because they encourage developers to split their monoliths based on separation of concerns. Microservices also promote communication between containerized components over a plain network using standardized and easily replaceable tubes. To learn more about microservices and why they might be a good architectural pattern for your software project, I recommend Building Microservices by Sam Newman. A continuous delivery pipeline with containerized software also allows you to set up a new kind of testing environment; subsets of (micro)services are deployed in small clusters that represent the system under test running with some parts intentionally disabled or disconnected. Creation of such a matrix of deployments and programming against it has little to no additional costs in terms of a continuous integration time. It does have a dramatic impact on the stability and resilience of software in production. Such a testing system allows teams to get ready to deal with any kind of Chaos Monkey .","url":"http://www.ciandcd.com/continuous-integration-and-delivery-with-docker.html","tags":"devops","title":"Continuous Integration and Delivery with Docker"},{"text":"from:http://java.dzone.com/articles/enabling-dataops-easy-log DataOps is becoming an important consideration for organizations. Why? Well, DataOps is about making sure data is collected, analyzed, and available across the company – i.e. Ops insight for your decision-making systems like Hubspot, Tableau, Salesforce and more. Such systems are key to day-to-day operations and in many cases are as important as keeping your customer facing systems up and running. If you think about it, today every online business is a data driven business! Everyone is accountable to have up to the minute answers on what is happening across their systems. You can't do this reliably without having DataOps in place. We have seen this trend across our own customer base at Logentries where more and more customers using log data to implement DataOps across their organization . Using log data for DataOps allows you to perform the following: Troubleshoot your systems managing your data by identifying errors and correlating data sources Get notified when one of these systems is experiencing issues via real time alerts or anomaly detection Analyze how these systems are used by the organization Logentries has always been great at 1 and 2 above, and this week we have enhanced Logentries to now allow you to perform easier and more powerful analytics with our n ew easy-to-use SQL like query language – Logentries QL (LEQL) . LEQL is designed to make analyzing your log data dead simple. There are too many log management tools that are built around complex query languages and require data scientists to operate. Logentries is all about making log data accessible to anyone. With LEQL you are going to be able to use analytical functions like CountUnique, Min, Max, GroupBy, Sort…A number of our users have already been testing these out via our beta program. One great example is how Pluralsight has been using Logentries to manage and understand the usage of their Tableau environment . For example: Calculating the rate of errors over the the past 24 hours e.g. using LEQL Count function Understanding user usage patterns e.g. using GroupBy to understand queries performed grouped by different users Sorting the data to find the most popular queries and how long they are taking Being able to answer these types of questions enables DataOps teams to understand where they need to invest time going forward. For example, do I need to add capacity to improve query performance? Are internal teams having a good user experience or are they getting a lot of errors when they try to access data? At Logentries we are all about making the power of log data accessible to everyone and as we do this we are constantly seeing cool new use cases when using logs. If you have some cool use cases do let us know!","url":"http://www.ciandcd.com/enabling-dataops-with-easy-log-analytics.html","tags":"devops","title":"Enabling DataOps with Easy Log Analytics"},{"text":"from:http://java.dzone.com/articles/how-make-sure-your-mobile-app Mobile app development has become vital for enterprises as they look to support new devices (phones, tablets, wearables, etc.) for internal use while also reaching out to their increasingly mobile customers. This approach makes sense: According to a comScore report , the number of mobile Internet users outnumbered desktop ones for the first time at some point in late 2013, and has since achieved significant separation. Many companies have responded to this change by implementing bring-your-own-device policies and building mobile apps that complement their full websites, mobile Web presence and/or desktop applications. Watch out for pitfalls in mobile apps: General risks and the recent Starbucks example However, both BYOD policies and mobile app development require due diligence around cybersecurity if they are to be worthwhile. Safety starts with well-designed applications that are strongly authenticated, do not leak sensitive data and are safe from popular attack vectors like brute-force password guessing. Unfortunately, many apps still have a long way to go on these fronts. An early 2014 study from MetaIntell discovered that 92 percent of the top 500 most popular Android apps at the time created privacy risks due to data leakage . Wary of leaky apps as well as what kinds of information users put into them, enterprises have understandably been concerned about the impact of mobile apps on their operations and BYOD initiatives. Security is often the biggest barrier to effective BYOD, and justifiably so considering that barely more than 40 percent of employees are required to have a security tool installed, according to Webroot. To get a sense of what could go wrong with today's mobile apps, consider what recently happened to Starbucks. The company's app is a mainstay on many phones, and at one time it accounted for the bulk of all mobile payments made in North America. The issue that arose over the last few months involved unauthorized card reloads and apparent account hijackings. The causes may have been mixed, with poor password management on the part of users possibly exacerbated by exploitation of the app's auto-reload feature and an April 2015 outage of the coffee chain's point-of-sale systems. At the end of the day, Starbucks implemented additional security questions and has been urged to add two-factor authentication into the app to prevent erroneous transactions. Catching mobile app security issues with a test management solution As we can see, mobile app security is multifactorial, requiring best efforts on the parts of end users, developers and infrastructure/network providers. For enterprises, the best approach to ensuring long-term security is to catch potential vulnerabilities early and often with a test management system. A test management solution supports both automated and manual testing , and receiving updates in real-time offers you the ability to make important decisions once issues arise. Regardless of how many tests, sprints and projects your company is running, all of them should be conveniently viewed from a lone interface, enabling a single source of truth that keeps your mobile app development initiatives on track.","url":"http://www.ciandcd.com/how-to-make-sure-your-mobile-app-is-secure.html","tags":"devops","title":"How to Make Sure Your Mobile App is Secure"},{"text":"from:http://java.dzone.com/articles/ode-workstation Every now and then I get work done in the home office. I've written previously about my setup, but after churning out some solution design today, I sat back and really took some time to appreciate the workspace. I'm really pleased with the configuration, it's probably the best setup I've had in years. The desk is a former QLD police desk from the 1940s, so it wasn't built for modern computers – not a problem, the cables run down the back which is just a minor annoyance. The keyboard and mouse are gaming varieties so that they perform well – the old Sennheiser (RF) wireless headset has been with me since 2006 and still works very well. The wooden clock ( recently reviewed ) acts as external speakers, a Bluetooth receiver and has a built in microphone so it can be used as a hands-free option for conference calls. It also features Qi wireless charging capability and also features a thermostat. Under the second monitor is a HDD caddy which supports USB3, and features 4 bays which can be used in parallel. I try to keep the desk reasonably neat, and there's plenty of space so it doesn't get too cluttered. I have a nice view out the window to a small courtyard which gets early morning sun.","url":"http://www.ciandcd.com/ode-to-a-workstation.html","tags":"devops","title":"Ode to a Workstation"},{"text":"from:http://java.dzone.com/articles/coming-donkey-apocalypse After a bit of a gap I'm continuing the my series from DevOps Days Austin . After Damon Edwards kicked off the event , Michael Cote of Pivotal took the stage. Cote presented \"The coming donkey apocalypse — what happens when Devops goes mainstream.\" Take a listen (you can find his slides below): Some of the ground that Cote covers: What DevOps as a community needs to focus on next to expand Unicorns (eg Uber and Netflix), Horses (eg top banks) and Donkeys (mainstream organizations) 3 key areas of DevOps to focus on today Culture and process Supporting legacy code Tools and technology Interviews on tap: Cameron Haight – Gartner John Willis – Docker Paul Read – Release Engineering Approaches Extra Credit reading Pau for now…","url":"http://www.ciandcd.com/the-coming-donkey-apocalypse-devops-days-austin.html","tags":"devops","title":"The Coming Donkey Apocalypse: Devops Days Austin"},{"text":"from:http://java.dzone.com/articles/ensure-software-security For many organizations, it seems like cyberattacks can come from anywhere, at any time. This sense is heightened by the number of endpoints in play that could be vulnerable to threats. Quality assurance teams must ensure that they have the data on hand to keep these risks at bay. By gathering information on current dangers, companies can better understand the attack surface and establish safeguards. Breaking down elements in play The attack surface contains all possible vulnerabilities - known and unknown - that may exist across your infrastructure, and sums up your risk of exposure. While the attack surface may seem like one big scary entity, it's actually made up of several parts. Tripwire broke considerations down into software, network and human attack surfaces to make this large picture easier to manage. QA professionals should approach the attack surface this way in order to ensure that all aspects are accommodated for rather than being overwhelmed by the big picture. Everything from coding to devices and human error must be considered when gathering information and preparing for potential threats. Analyze data and act on it Testing results can be a critical indicator of what types of vulnerabilities may be present within a program. The Open Web Application Security Project noted that an attack surface analysis will help QA and developers better understand what they're up against and build in security accordingly. During this evaluation, they must determine high risk areas of code, what functions should be reviewed for defects and when the attack surface has changed. This last consideration will be especially critical as further tests and adjustments will be needed to secure the software. Anything that an organization does could affect the attack surface, which means that it will have to be constantly monitored. QA teams need to ask what's changed, how it's different from before and what potential holes were opened in the process. This will help keep the attack surface visibly mapped out, making it easy to strategize how to protect the business, its employees and customers. Reduce the noise While a breach is certainly possible, that doesn't mean it should be easy for attackers to gain entry into business systems. Organizations can reduce their attack surface by decreasing the amount of noise within their infrastructure. Accuvant pointed out that doing this will reduce an attack's operating surface , minimizing the likelihood of malicious access. QA teams can use tactics like configuration management, exploit analysis, patching, sandboxing and secure application development to effectively reduce or eliminate the impact of a vulnerability. \"Integrating these strategies into your security program make it much harder for exploits to attack your organization's systems,\" Accuvant stated. \"By reducing your adversaries' operating surface, you are effectively limiting their attack surface.\" The threat of a vulnerability is a very real concern for businesses. By gathering information on what types of attacks are becoming prevalent and understanding how they can affect company software, QA teams can prepare for these risks and protect their users from the growing attack surface.","url":"http://www.ciandcd.com/ensure-software-security-by-understanding-the-attack-surface.html","tags":"devops","title":"Ensure Software Security by Understanding the Attack Surface"},{"text":"from:http://java.dzone.com/articles/reducing-risk-through-security Organizations are under constant pressure to protect their critical assets from cyberattacks that have plagued a wide variety of industries. However, there is currently no set method of how to ensure that company applications will be safe from these threats. Quality assurance teams have implemented a wide range of approaches to ensure security, but manually executing all of these cases can be time-consuming and lead to potential vulnerabilities. For this reason, QA should look into security automation to reduce risks and improve overall program capabilities . Have realistic expectations When building security into the software development life cycle, there are numerous benefits businesses can see, including seamless protection integration and awareness of team members. An AT&T white paper noted that automated vulnerability scanning can be a great first step for QA teams to implement as it can easily and quickly identify commonly occurring issues . At the same time, however, it's not foolproof, since it cannot detect more sophisticated defects like authentication issues or business logic vulnerabilities. That being said, security QA automation can be a major asset to development efforts and can reduce overall risk, but will still require other tools like manual testing to fully evaluate the threat landscape. After the app has been released, automation can often be essential for finding threats, while enabling QA teams to focus on current projects that are still underway. This helps lower the potential risk across the board while still ensuring that each program gets the attention it needs, no matter where it is in its life cycle. Tools for the job There are a number of resources that QA teams can utilize to test the security of their projects. TechTarget contributor Michael Cobb noted that automated QA verification is often executed through code analysis and vulnerability testing . Both of these assets can quickly find errors that may be easily missed during manual evaluations. This alone helps significantly reduce risks to app functionality and security capabilities while ensuring that QA teams are eliminating common vulnerabilities. These tools paired with human testers can effectively find issues and better protect their projects for the future. \"Despite advances in computer automation, humans are still superior at ensuring applications are developed securely, probably because the best challenge is posed by humans, notably those who can think as an attacker would,\" Cobb wrote. \"However, human work is often more effective if a framework guides it.\" Relying on QA for better security Even if QA teams leverage automated tools for security needs, they must still have an understanding of how these tests work and be able to execute them. Chiron Professional Journal noted that while QA professionals may not often be security experts, having the tools on hand can help them perform the necessary processes and mitigate critical risks. \"Let's be clear here – we're not expecting a QA analyst to be able to cobble together a complicated script to evade an anti-cross-site scripting library … but we should reasonably expect that the analyst can either effectively use a tool, or follow a well-documented process that has varying tests and permutations allowing the analyst to think for themselves and flag questionable results for review by the security experts,\" the Chiron Professional Journal stated.","url":"http://www.ciandcd.com/reducing-risk-through-security-qa-automation.html","tags":"devops","title":"Reducing Risk Through Security Qa Automation"},{"text":"from:http://java.dzone.com/articles/why-we-need-continuous Introduction Continuous integration is a practice that helps developers deliver better software in a more reliable and predictable manner. This article deals with the problems developers face while writing, testing and delivering software to end users. Through exploring continuous integration, we will cover how we can overcome these issues. The Problem First, we will take a look at the source of the problem, which lies in the software development cycle. Next, we will cover some of the change conflicts that can take place during that process, and finally we will explore the main factors that can make these problems escalate, followed by an explanation of how continuous integration solves these issues. The Source of the Problem Let's take a look at what a traditional software development cycle looks like. Each developer gets a copy of the code from the central repository. The starting point is usually the latest stable version of the application. All developers begin at the same starting point, and work on adding a new feature or fixing a bug. Each developer makes progress by working on their own or in a team. They add or change classes, methods and functions, shaping the code to meet their needs, and eventually they complete the task they were assigned to do. Meanwhile, the other developers and teams continue working on their own tasks, changing the code or adding new code, solving the problems they have been assigned. If we take a step back and look at the big picture, i.e. the entire project, we can see that all developers working on a project are changing the context for the other developers as they are working on the source code. As teams finish their tasks, they copy their code to the central repository. There are two scenarios that can take place at this point. The code in the central repository is unchanged The code is the same as the initial copy. If this is the case, things are simple, because the system is unchanged. All the ideas we had about the system still stand. This is always the case if you are the only developer working on the application and if you have finished your work before the other members of your team. Either way, things are looking good for you. The system you have created and tested can be delivered to users without additional changes. The code in the central repository has changed The second scenario is that the application you have been working on has changed, and you discover this at the point when you try to copy your code over to the central repository. Changes in the code may or may not be in conflict with the ones you've made. If there are conflicts, you need to resolve them in order to be able to successfully deliver your code to the users. In this case, things could get complicated. Next, we'll explore the types of conflicts that can happen and what you may need to do to resolve them. Change Conflicts There are several types of change conflicts that can occur when integrating code. Here are some of the most common ones. We'll start with the simplest scenarios, and gradually explore the more complex ones. The implementation details have changed - You refactored a method, but so did the developer that has already integrated their code into the central repository. The behavior of the method is the same in all three implementations. You will need to pick the version that will stay, and remove the other implementations. You can even come up with a fourth implementation. This is a simple type of conflict, which you can usually resolve within a few minutes. The APIs you have been relying on have changed - For instance, the behavior of a certain method has changed. This could affect your code in a number of ways — from minor changes that you might need to make, to major structural changes. There is no silver bullet in such cases. You will need to carefully study the changes and make all the fixes. An entire subsystem of the application behaves in a different way - in such cases you will almost certainly be facing a partial, if not a full rewrite of your solution. If this is the case, you will probably need to speak with all the developers working on the application, because such a significant change should not happen without letting the rest of the team know about it. These and a number of other issues could come up, caused by various factors. Different versions of frameworks, libraries, databases are another potential source of conflicts. Once you have updated your code so it can be compiled or interpreted, you also need to remember to repeat all the tests that you have previously ran. These examples show that the amount of work needed to solve a problem that was initially assigned to a developer can easily double. Escalating Factors Here are some of the main factors that can make these problems escalate. The size of the team working on the project. The number of changes that are being pushed back into the main repository is proportional to the number of people on the project. This makes the process of integrating code into the main repository significantly harder. The amount of time passed since the developer got the latest version of the code from the central repository. As time passes, other people working on the same project are integrating more and more of their work, and changing the context in which your code needs to run. Sometimes the changes in the main repository are so big that it's easier to do a complete rewrite of your solution. A large number of changes in the system make integration events more complex and can have a huge effect on the productivity of the team. Such situations are even referred to as \"integration hell\". This process has a number of other negative consequences for your business. Testing and fixing bugs can take forever. Your releases are running late. Teams are stressed out because of long and unpredictable release cycles, and morale deteriorates. Solution: Integrate Continuously The solution to the problem of managing a large number of changes in big integration events is conceptually simple. We need to split these big integration events into much smaller integration events. This way, developers need to deal with a much smaller number of changes, which are easier to understand and manage. To keep integration events small and easily manageable, we need them to happen often. A couple of times a day is ideal. The practice of doing small integrations often is called Continuous Integration . The idea is simple, but at the same time it often appears to be impossible to implement in practice. This is because changing the process requires us to change some of our own habits, and changing habits is difficult. The Practice of Continuous Integration In order to avoid the previously described issues, developers need to integrate their partially complete work back into the main repository on a daily basis, or even a couple of times a day. To accomplish this, they first need to pull in all the changes added to the main repository while they were working on the code. They also must make sure that their code will work once it is integrated into the main repository. The only way to ensure this is to test every feature of the application. What first comes into mind when we start considering continuous integration is that the developers would need to spend half of their time every day testing the code in order not to break the code in the main repository for everyone else. This is why the prerequisite for continuous integration is having an automated test suite. Automated tests take away the burden of the manual, repetitive, and error-prone testing process from the developers. They also make the entire testing process much quicker. A computer can replace hours of manual testing with just minutes of automated testing. Behavior-driven and test-driven development are techniques that help developers write clean, maintainable code while writing tests at the same time. Testing techniques are out of the scope of this article, and you can read more about them in other articles on Semaphore Community . Tests make sense only if they are executed every time the source code changes, without exception. A continuous integration service such as Semaphore CI is a tool which can automate this process by monitoring the central code repository and running tests on every change in the source code. Apart from running tests, they also collect test results and communicate those results to the entire team working on the project. The result of continuous integration is so important that many teams have a rule to stop working on their current task if the version in the central repository is broken. They join the team which is working on fixing the code until tests are passing again. The role of a continuous integration service is to improve the communication between developers by communicating the status of a project's source code. How to Adopt Continuous Integration Continuous integration as a practice makes a big contribution to improving the development process, but also calls for essential changes in the everyday development routine. Adopting it comes with challenges that are easy to overcome if the process is introduced gradually. One of the biggest challenges teams face is the lack of an automated testing suite. A good recipe for overcoming this situation is to start adding automated tests for all new features as they are being developed. At the same time, the developer working on a bug fix should also work to cover the related code with tests. Whenever a bug is reported, the team should first write a failing test to demonstrate the existence of bug. Once the fix is created, the tests should pass. Over time, the automated tests suite gradually becomes more comprehensive, and the developers begin relying on it more and more. Adopting a continuous integration service to communicate the status of the tests to the entire team in the early stages of a project is also important, because it raises awareness of the project status among team members. Conclusion Introducing continuous integration and automated testing into the development process changes the way software is developed from the ground up. It requires effort from all team members, and a cultural shift in the organization. Big changes in the workflow are not easy to pull off quickly. Changes have to be introduced gradually, and all team members and stakeholders need to be on board with the idea. Educating team members about the practice of continuous integration practice and building the automated tests suite needs to be done systematically. Once the first steps have been taken, the process usually continues on its own, as both developers and stakeholders begin seeing the benefits of automated testing suites and the peace of mind that this practice brings to the entire team. Article originally posted on the Semaphore Community .","url":"http://www.ciandcd.com/why-we-need-continuous-integration.html","tags":"devops","title":"Why We Need Continuous Integration"},{"text":"from:https://github.com/blog/2027-codeconf-updates-meet-greet-and-workshop-tickets CodeConf is next week, and I couldn't be more excited to bring the open source community together to exchange ideas and have some fun in Nashville. There are a few updates I'd like to share: On June 24, the day before the conference, we'll be hosting a meet & greet for attendees who would like to register early. This event is free and open to the public, so if you aren't attending CodeConf but live in the Nashville area and would like to stop by, grab a ticket here . We'll be congregating on the second floor of Acme Feed & Seed downtown beginning at 5:30pm The workshop schedule has been updated, and I have opened up more space in each session for those interested. If you'd like to snag one of the newly available tickets, go for it! There's still time to grab a CodeConf ticket. Take a look at the website for the full schedule of sessions, workshops, and sponsors. I hope to see you in Nashville.","url":"http://www.ciandcd.com/codeconf-updates-meet-greet-and-workshop-tickets.html","tags":"scm","title":"CodeConf Updates: Meet & Greet and Workshop Tickets"},{"text":"from:http://java.dzone.com/articles/how-monitor-java-ee-datasource Introduction FlexyPool is an open-source framework that can monitor a DataSource connection usage. This tool come out of necessity, since we previously lacked support for provisioning connection pools. FlexyPool was initially designed for stand-alone environments and the DataSource proxy configuration was done programmatically. Using Spring bean aliases , we could even substitute an already configured DataSource with the FlexyPool Metrics-aware proxy alternative. Java EE support Recently, I've been asked about supporting Java EE environments and in the true open-source spirit, I accepted the challenge. Supporting a managed environment is tricky because the DataSource is totally decoupled from the application-logic and made available through a JNDI lookup. One drawback is that we can't use automatic pool sizing strategies, since most Application Servers return a custom DataSource implementation (which is closely integrated with their in-house JTA transaction manager solution), that doesn't offer access to reading/writing the connection pool size. While the DataSource might not be adjustable, we can at least monitor the connection usage and that's enough reason to support Java EE environments too. Adding declarative configuration Because we operate in a managed environment, we can no longer configure the DataSource programmatically, so we need to use the declarative configuration support. By default, FlexyPool looks for the flexy-pool.properties file in the current Class-path. The location can be customized using the flexy.pool.properties.pathSystem property , which can be a: URL (e.g. file:/D:/wrk/vladmihalcea/flexy-pool/flexy-pool-core/target/test-classes/flexy-pool.properties) File system path (e.g. D:\\wrk\\vladmihalcea\\flexy-pool\\flexy-pool-core\\target\\test-classes\\flexy-pool.properties) Class-path nested path (e.g. nested/fp.properties) The properties file may contain the following configuration options: Parameter nameDescription flexy.pool.data.source.unique.name Each FlexyPool instance requires a unique name so that JMX domains won't clash flexy.pool.data.source.jndi.name The JNDI DataSource location flexy.pool.data.source.jndi.lazy.lookup Whether to lookup the DataSource lazily (useful when the target DataSource is not available when the FlexyPoolDataSource is instantiated) flexy.pool.data.source.class.name The DataSource can be instantiated at Runtime using this Class name flexy.pool.data.source.property.* If the DataSource is instantiated at Runtime, each flexy.pool.data.source.property.${java-bean-property} will set the java-bean-property of the newly instantiated DataSource (e.g. flexy.pool.data.source.property.user=sa) flexy.pool.adapter.factory Specifies the PoolAdaptorFactory, in case the DataSource supports dynamic sizing. By default it uses the generic DataSourcePoolAdapter which doesn't support auto-scaling flexy.pool.metrics.factory Specifies the MetricsFactory used for creating Metrics flexy.pool.metrics.reporter.log.millis Specifies the metrics log reported interval flexy.pool.metrics.reporter.jmx.enable Specifies if the jmx reporting should be enabled flexy.pool.metrics.reporter.jmx.auto.start Specifies if the jmx service should be auto-started (set this to true in Java EE environments) flexy.pool.strategies.factory.resolver Specifies a ConnectionAcquiringStrategyFactoryResolver class to be used for obtaining a list of ConnectionAcquiringStrategyFactory objects. This should be set only if the PoolAdaptor supports accessing the DataSource pool size. Hibernate ConnectionProvider Most Java EE applications already use JPA and for those who happen to be using Hibernate, we can make use of the hibernate.connection.provider_class configuration property for injecting our proxy DataSource. Hibernate provides many built-in extension points and the connection management is totally configurable. By providing a custom ConnectionProvider we can substitute the original DataSource with the FlexyPool proxy. All we have to do is adding the following property to our persistence.xml file: <property name=\"hibernate.connection.provider_class\" value=\"com.vladmihalcea.flexypool.adaptor.FlexyPoolHibernateConnectionProvider\"/> Behind the scenes, this provider will configure a FlexyPoolDataSource and use it whenever a new connection is requested: private FlexyPoolDataSource<DataSource> flexyPoolDataSource; @Override public void configure(Map props) { super.configure(props); LOGGER.debug( \"Hibernate switched to using FlexyPoolDataSource \"); flexyPoolDataSource = new FlexyPoolDataSource<DataSource>( getDataSource() ); } @Override public Connection getConnection() throws SQLException { return flexyPoolDataSource.getConnection(); } Instantiating the actual DataSource at runtime If you're not using Hibernate, you need to have the FlexyPoolDataSource ready before the EntityManagerFactory finishes bootstrapping: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <persistence version=\"2.0\" xmlns=\"http://java.sun.com/xml/ns/persistence\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\" http://java.sun.com/xml/ns/persistence http://java.sun.com/xml/ns/persistence/persistence_2_0.xsd\"> <persistence-unit name=\"persistenceUnit\" transaction-type=\"JTA\"> <provider>org.hibernate.jpa.HibernatePersistenceProvider</provider> <jta-data-source>java:global/jdbc/flexypool</jta-data-source> <properties> <property name=\"hibernate.hbm2ddl.auto\" value=\"update\"/> <property name=\"hibernate.show_sql\" value=\"true\"/> <property name=\"hibernate.dialect\" value=\"org.hibernate.dialect.HSQLDialect\"/> <property name=\"hibernate.transaction.jta.platform\" value=\"org.hibernate.service.jta.platform.internal.SunOneJtaPlatform\"/> </properties> </persistence-unit> </persistence> While in a production Java EE environment we use an Application server specific DataSource configuration, for simplicity sake, I'm going to configure the FlexyPooldataSource using the DataSourceDefinition annotation: @DataSourceDefinition( name = \"java:global/jdbc/flexypool\", className = \"com.vladmihalcea.flexypool.FlexyPoolDataSource\") @Stateless public class FlexyPoolDataSourceConfiguration {} We now need to pass the actual DataSource properties to FlexyPool and this is done through the flexy-pool.properties configuration file: flexy.pool.data.source.unique.name=unique-name flexy.pool.data.source.class.name=org.hsqldb.jdbc.JDBCDataSource flexy.pool.data.source.property.user=sa flexy.pool.data.source.property.password= flexy.pool.data.source.property.url=jdbc:hsqldb:mem:test flexy.pool.metrics.reporter.jmx.auto.start=true The actual DataSource is going to be created by the FlexyPoolDataSource on start-up. Locating the actual DataSource from JNDI If the actual DataSource is already configured by the Application Server, we can instruct FlexyPool to fetch it from JNDI. Let's say we have the following DataSource configuration: @DataSourceDefinition( name = \"java:global/jdbc/default\", className = \"org.hsqldb.jdbc.JDBCDataSource\", url = \"jdbc:hsqldb:mem:test\", initialPoolSize = 3, maxPoolSize = 5 ) @Stateless public class DefaultDataSourceConfiguration {} To proxy the JNDI DataSource, we need to configure FlexyPool like this: flexy.pool.data.source.unique.name=unique-name flexy.pool.data.source.jndi.name=java:global/jdbc/default flexy.pool.metrics.reporter.jmx.auto.start=true The FlexyPoolDataSource is defined alongside the actual DataSource: @DataSourceDefinition( name = \"java:global/jdbc/flexypool\", className = \"com.vladmihalcea.flexypool.FlexyPoolDataSource\") @Stateless public class FlexyPoolDataSourceConfiguration {} The JPA will have to fetch the FlexyPoolDataSource instead of the actual one: <jta-data-source>java:global/jdbc/flexypool</jta-data-source> In TomEE , because the DataSourceDefinitions are not lazily instantiated, the actual DataSource might not be available in the JNDI registry when the FlexyPoolDataSource definition is processed. For this, we need to instruct FlexyPool to dely the JNDI lookup until the DataSource is actually requested: flexy.pool.data.source.jndi.lazy.lookup=true Conclusion The last time I used Java EE was in 2008, on a project that was using Java EE 1.4 with EJB 2.1. After 7 years of using Spring exclusively, I'm pleasantly surprised by the Java EE experience. Arquillian is definitely my favourite add-on, since integration testing is of paramount importance in enterprise applications. CDI is both easy and powerful and I'm glad the dependency injection got standardised. But the best asset of the Java EE platform is the community itself. Java EE has very strong community, willing to give you a hand when in need. I'd like to thank Steve Millidge (Founder of Payara and C2B2) for giving me some great tips on designing the FlexyPool Java EE integration, Alex Soto , Antonio Goncalves and all the other Java EE members whom I had some very interesting conversations on Twitter.","url":"http://www.ciandcd.com/how-to-monitor-a-java-ee-datasource.html","tags":"devops","title":"How to Monitor a Java EE DataSource"},{"text":"from:http://devops.com/2015/06/19/better-fewer-suppliers-2015-software-supply-chain-report/ That Supplier is Better For You Since releasing the 2015 State of the Software Supply Chain Report, there has been a lot of great discussion across the industry on best practices for managing the complexity introduced by the volume and velocity of the components used across your software supply chain. Today I want to focus on the huge ecosystem of open source projects (\"suppliers\") that feed a steady stream of innovative components into our software supply chains. In the Java ecosystem alone, there are now over 108,000 suppliers of open source components. Across all component types available to developers (e.g., RubyGems, NuGet, npm, Bower, PyPI, etc.), estimates now reach over 650,000 suppliers of open source projects. However, like in traditional manufacturing, not all suppliers deliver parts of comparable quality and integrity. My latest research, the 2015 State of the Software Supply Chain Report , shows that some open source projects use restrictive licenses and vulnerable sub-components, while other projects are far more diligent at updating the overall quality of their components. Choosing the best and fewest suppliers can improve the quality and integrity of the applications we deliver to our customers. While I am hosting a webinar next week to share many of the detailed report findings, I wanted to share a few of the more meaningful stats here. Your 7,600 Suppliers My research for the report revealed many new perspectives on \"suppliers\" across the software supply chains. First of all, I saw that the average large development organization consumed over 240,000 open source components last year — sourced from over 7,600 open source projects. On the surface, the huge reliance on open source projects is a great thing. Development teams have chosen to not write those pieces themselves, but have sourced the needed components from outside suppliers. This practice speeds development, enables more innovation, and ensures time-to-release goals are achieved. The use of open source is so prolific today, few of us could ever imagine reducing the use of those components and their suppliers in the future. At the same time that we benefit from open source, our high paced, high volume consumption practices don't allow us the time needed to do the due diligence on the suppliers or open source projects where we source our component parts from. For example, of the 240,000 average component downloads in 2014, the same businesses sourced an average of 15,000 components that included known security vulnerabilities. In many cases, developers were downloading vulnerable component versions, when safer versions of those same components were available from the open source projects. While no one intends to download components with known vulnerabilities, the problem is exacerbated due to the lack the visibility into a better recommended version. Fewer Suppliers, Less Context Switching Choosing an open source project supplier should be considered an important strategic decision in organizations because changing a supplier (\"open source project\") used is far more effort than swapping out a specific component. Like traditional suppliers, open source projects have good and bad practices impacting the overall quality of their component parts. Traditional manufacturing supply chains intentionally select specific parts from approved suppliers. They also rely on formalized sourcing and procurement practices. This practice also focuses the organization on using the best and fewest suppliers — an effort that improves quality, reduces context switching, and also accelerates mean time to repair when defects are discovered. One industry example from the report describes how Toyota manages 125 suppliers for their Prius to help sustain competitive advantages over GM who manages over 800 suppliers for the Chevy Volt. By contrast, development teams working with software supply chains often rely on an unchecked variety of supply, where each developer or development team can make their own sourcing and procurement decisions. The effort of managing over 7,600 suppliers introduces a drag on development and is contrary to their need to develop faster as part of agile, continuous delivery and devops practices. Coming to Terms When you come to terms with the volume of consumption and the massive ecosystem of suppliers you can source your components from, you quickly realize it is impossible to address this issue with a manual review process. Any organizations clutching to these outdated manual practices are will continue to be outgunned by the velocity by their software supply chains. Just as traditional manufacturing supply chains have turned to automation, software development teams need to take the same approach by further automating their software supply chains. Information about suppliers and the quality of their projects needs to be made available to developers at the time they are selecting components. Information about the latest versions, features, licenses, known vulnerabilities, popularity of versions being used, and the cadence of new releases should be made available to developers in an automated way. Automating the availability of this information about suppliers can lead to better and fewer suppliers being used. Be sure to read the full 2015 State of the Software Supply Chain Report for more information about open source suppliers and organizations sourcing practices. The report also highlights current and best practices being used in organizations that are managing their use of suppliers that feed their software supply chains. To hear more about the overall report findings and industry best practices, please join me on Wednesday, June 24th (1pm ET) for our webinar .","url":"http://www.ciandcd.com/better-and-fewer-suppliers-2015-software-supply-chain-report.html","tags":"devops","title":"Better and Fewer Suppliers (2015 Software Supply Chain Report)"},{"text":"from:http://devops.com/2015/06/19/internaps-devops-culture-privatestack-cd-read-draw-conclusions/ Engineers at Internap , a hosting company and public cloud vendor designed a new OpenStack-based cloud platform and development environment. DevOps.com tells the story of Internap's DevOps cultural evolution, which grew virally, interwoven with the company's development of PrivateStack. Challenges in Internap's Common Development Environment Challenges that slowed Internap's cloud development process triggered a hunger for change and for a new / altered development scheme. \"The Internap public cloud product is essentially composed of micro services that make cloud resources available to our customers,\" says Mathieu Mitchell, Senior Software Developer, Internap. Internap engineering teams tested their cloud services simultaneously with each affecting the other and transferring adverse effects to production or pre-production. Internap needed to let the billing team test their services, integrated with a duplicate of the production software and environment, without affecting the virtualization team. These challenges were rooted in a commonly shared development environment for all the engineering teams and team members, which introduced tedium for engineers as they worked to build and deploy stable code. When sharing a common development environment, each engineer's tests depended on environment consistency and reliability. When engineers / developers tested two changes at the same time, there was no way to tell which change introduced a regression. The most reasonable way to address this prior to PrivateStack was to push all changes into the environment and dedicate specific engineers to identify and fix the issues that occurred. The challenge with dedicating specific engineers to troubleshoot these regressions was that they had a diminished context to work with when compared to the original developer's understanding. The team that introduced a change needed feedback from the environment and to shoulder responsibility for fixing the issue. This would result in a stable codebase and the ability to write more thorough, automated tests. \"This is why we created PrivateStack–to allow us to independently test a single change, integrated with other services, in an environment that is the equivalent of a private production setup,\" says Mitchell. PrivateStack enables development teams to innovate while ensuring that changes behave correctly in production. This in turn enables CD and speeds development. DevOps Culture Leads Teams to Success As Internap developed PrivateStack, engineering observed a DevOps belief system spreading across the project and the teams. \"At first, it was only a minority among us, mostly people with an Agile background, who advocated the Continuous Delivery approach while we worked on this project. We really believe in delivering value to our customers as quickly as possible. To achieve Continuous Delivery, we needed people to understand that automating everything was the way to go,\" says Mitchell. As more engineers realized that they were the solution, they became increasingly motivated to enhance processes, create the new development environment, and use CD to develop the Internap public cloud product. Results with PrivateStack PrivateStack enables Internap teams to share the same code and system configurations while individual developers have each their own private production environment to test their software without affecting others. \"We heavily leverage virtualization to be able to recreate our environments, since buying racks and racks of hardware to make this available to all of our developers would be cost prohibitive,\" says Mitchell. With PrivateStack, Internap isolates production issues during development, keeping R&D efficient and slashing pre-production troubleshooting time. \"PrivateStack also improves our time-to-market, is much less costly, and most importantly, reduces the chance of any issues reaching the customer,\" says Mitchell. DevOps Culture Wrap Up To finish the PrivateStack project and foster a DevOps culture at the same time, Internap's internal core of CD true believers lead by example. Mitchell and his colleagues adhered to consistent principles while hearing out other team members on their concerns. This helped them to encourage the larger engineering department including billing and virtualization teams to adopt a DevOps and CD frame of mind. \"We dedicated two people to drive this initiative, along with a few others who believed in the Continuous Delivery approach but were not involved directly,\" says Mitchell. That was enough to get the job done. Now the vast majority of the engineering department is targeting CD. \"We are eager to share our PrivateStack platform with the open source community to enable other developers to run production-like environments for development in their day-to-day operations,\" says Mitchell.","url":"http://www.ciandcd.com/internaps-devops-culture-privatestack-cd-read-on-draw-your-own-conclusions.html","tags":"devops","title":"Internap's DevOps Culture: PrivateStack + CD = ? [Read On & Draw Your Own Conclusions]"},{"text":"from:http://devops.com/2015/06/19/two-paths-metal-devops-cloud-like-api-driven-cluster-building/ I've been seeing a rising interest in metal DevOps fueled by containers and scale-out data center platforms (like Hadoop, Ceph & OpenStack) that run at the metal level. While I see this is a growing general trend ( Packet , Internap , RackSpace , OpenStack Ironic , MaaS ), I'm going to stay firmly within my wheelhouse and use OpenCrowbar as my reference here. Building on the API-driven metal features of OpenCrowbar, this has translated into two paths for workloads to run on metal: 1) \"Cloudify\" the metal using APIs from tools like Chef Provision , SaltStack Libcloud , Docker Machine , Cloud Foundry BOSH . These tools have clients that target cloud APIs like OpenStack and Amazon. These same clients work against cloud are easily ported to Crowbar's APIs. Five years ago, conventional wisdom was that we'd need a universal cloud API; however, practice has shown it's not very difficult to wrap APIs in a way that does not reduce every cloud to a least common denominator. 2) DevOps deploy the workload using hand-offs to tools like Chef, Saltstack, Puppet or Ansible. This approach leverages the community scripts (Cookbooks, Modules, Playbooks) for the workload with the critical ability to create a tuned environment and inject the needed parameters directly into the scripts. A critical lesson we learned going from Crowbar v1 to v2 was for our scripts to have crisp attribute input/output boundary to avoid embedding environmental knowledge into the code. While I'm casting this in Crowbar terms, I see this approach to metal as coming into the market by force fuels by a desire for containers-on-metal and devops-on-metal. Let's look at some of the unique and shared use-cases for each approach: Metal API Both Metal Cluster Easy Cloud to Metal Migration Minimal Tool Customization Portability of DevOps Scripts Take advantage of power cycling Enables constant refresh cycles Leverage Hardware features Advanced Network topologies In either case, you have to handle bespoke (hipster word for custom) steps in the provisioning flow that are unique to the your operational needs. Our experience is that each site (even each server!) is unique in some incremental way. For example, one site may require teamed networks with VLANs while another requires flat networks with an SDN layer. These differences are not mistakes or errors : the reality of physical ops and individual operational choices mean that there are a lot of valid configurations. Rather than attempt the Sisyphean task of enforced conformity, we work to abstract differences so that they can be ignored when they are not material. In the end, the choices are not mutually exclusive. Metal APIs are often faster but harder to optimize. You can use them to get started quickly and then invest time to optimize a cluster for long term operations. The underlying physical orchestration can support both. Are you looking at getting closer to metal? Which of the options above makes the most sense to you? I'd love to hear about your use-cases, architecture and configuration requirements.","url":"http://www.ciandcd.com/two-paths-to-metal-devops-cloud-like-api-driven-cluster-building.html","tags":"devops","title":"Two paths to metal devops: cloud-like API driven & cluster building"},{"text":"from:http://java.dzone.com/articles/does-devops-reduce-technical DevOps can help reduce technical debt in some fundamental ways. Continuous Delivery/Deployment First, building a Continuous Delivery/Deployment pipeline , automating the work of migration and deployment, will force you to clean up inconsistencies and holes in configuration and code deployment, and inconsistencies between development, test and production environments. And automated Continuous Delivery and Infrastructure as Code gets rid of dangerous one-of-a-kind snowflakes and configuration drift caused by making configuration changes and applying patches manually over time. Which makes systems easier to setup and manage, and reduces the risk of an un-patched system becoming the target of a security attack or the cause of an operational problem . A CD pipeline also makes it easier, cheaper and faster to pay down other kinds of technical debt. With Continuous Delivery/Deployment, you can test and push out patches and refactoring changes and platform upgrades faster and with more confidence. Positive Feedback The Lean feedback cycle and Just-in-Time prioritization in DevOps ensures that you're working on whatever is most important to the business. This means that bugs and usability issues and security vulnerabilities don't have to wait until after the next feature release to get fixed. Instead, problems that impact operations or the users will get fixed immediately. Teams that do Blameless Post-Mortems and Root Cause(s) Analysis when problems come up will go even further, and fix problems at the source and improve in fundamental and important ways. But there's a negative side to DevOps that can add to technical debt costs. Erosive Change Michael Feathers' research has shown that constant, iterative change is erosive : the same code gets changed over and over, the same classes and methods become bloated (because it is naturally easier to add code to an existing method or a method to an existing class), structure breaks down and the design is eventually lost. DevOps can make this even worse. DevOps and Continuous Delivery/Deployment involves pushing out lots of small changes, running experiments and iteratively tuning features and the user experience based on continuous feedback from production use. Many DevOps teams work directly on the code mainline, \" branching in code \" to \" dark launch \" code changes, while code is still being developed, using conditional logic and flags to skip over sections of code at run-time. This can make the code hard to understand, and potentially dangerous: if a feature toggle is turned on before the code is ready, bad things can happen . Feature flags are also used to run A/B experiments and control risk on release, by rolling out a change incrementally to a few users to start. But the longer that feature flags are left in the code, the harder it is to understand and change . There is a lot of housekeeping that needs to be done in DevOps: upgrading the CD pipeline and making sure that all of the tests are working; maintaining Puppet or Chef (or whatever configuration management tool you are using) recipes; disciplined, day-to-day refactoring ; keeping track of features and options and cleaning them up when they are no longer needed, getting rid of dead code and trying to keep the code as simple as possible. Microservices and Technology Choices Microservices are a popular architectural approach for DevOps teams. This is because loosely-coupled Microservices are easier for individual teams to independently deploy, change, refactor or even replace . And a Microservices-based approach provides developers with more freedom when deciding on language or technology stack: teams don't necessarily have to work the same way, they can choose the right tool for the job, as long as they support an API contract for the rest of the system. In the short term there are obvious advantages to giving teams more freedom in making technology choices. They can deliver code faster, quickly try out prototypes, and teams get a chance to experiment and learn about different technologies and languages. But Microservices \" are not a free lunch \". As you add more services, system testing costs and complexity increase. Debugging and problem solving gets harder. And as more teams choose different languages and frameworks, it's harder to track vulnerabilities, harder to operate, and harder for people to switch between teams. Code gets duplicated because teams want to minimize coupling and it is difficult or impossible to share libraries in a polyglot environment. Data is often duplicated between services for the same reason, and data inconsistencies creep in over time. Negative Feedback There is a potentially negative side to the Lean delivery feedback cycle too. Constantly responding to production feedback, always working on what's most immediately important to the organization, doesn't leave much space or time to consider bigger, longer-term technical issues, and to work on paying off deeper architectural and technical design debt that result from poor early decisions or incorrect assumptions. Smaller, more immediate problems get fixed fast in DevOps. Bugs that matter to operations and the users can get fixed right away instead of waiting until all the features are done, and patches and upgrades to the run-time can be pushed out more often. Which means that you can pay off a lot of debt before costs start to compound. But behind-the-scenes, strategic debt will continue to add up. Nothing's broke, so you don't have to fix anything right away. And you can't refactor your way out of it either, at least not easily. So you end up living with a poor design or an aging technology platform, slowly slowing down your ability to respond to changes, to come up with new solutions. Or forcing you to continue filling in security holes as they come up, or scrambling to scale as load increases. DevOps can reduce technical debt. But only if you work in a highly disciplined way. And only if you raise your head up from tactical optimization to deal with bigger, more strategic issues before they become real problems.","url":"http://www.ciandcd.com/does-devops-reduce-technical-debt-or-make-it-worse.html","tags":"devops","title":"Does DevOps Reduce Technical Debt – or Make it Worse?"},{"text":"from:http://www.perforce.com/blog/150617/finding-%E2%80%98needle-haystack%E2%80%99-helix-threat-detection Software development projects in bigger companies typically involve large teams collaborating across multiple locations. A large corporation may employ tens of thousands of developers working on thousands of projects over a span of many years. For many companies, developer access to older software projects and files may continue long after the project has been completed, sometimes because of lax processes and stagnant access control policies. Yet, these projects can represent valuable IP worth tens of millions of dollars. In light of the ramifications of a competitor getting ahold of these files, what can companies do to better protect their crown jewels from theft? The answer might be found in the source code management (SCM) or version control tools companies use to drive their development workflows. SCM tools typically track access to key projects and files via audit logs. However, the sheer volume of these logs can overwhelm security teams. A month of log data might yield millions of different interactions with files and projects, making it virtually impossibe to find important clues. Done the right way, however, this approach can bring the real threats to the surface. A recent Fortune article entitled Using Log Data and Machine Learning to Weed out the Bad Guys shares how a large company applied our Helix Threat Detection capabilities to quickly identify data theft. Likening this approach to ‘finding a needle in a haystack,' the article describes how effective it can be to apply behavioral analytics to the audit logs in our Helix Versioning Engine. Leveraging Machine Learning to Establish a Baseline Conventional security tools (e.g., SIEMs) are often rule-based and require time-consuming manual setting of thresholds and iterative tuning of multiple parameters in order to identify anomalous behavior. Yet manually setting alerts to trigger when developers access an arbitrary number of files may be problematic for large projects and can inundate security teams with too many false positives. A better approach is to use machine-learning algorithms and risk-based-behavior-analytics models to audit logs to first establish a baseline understanding of normal behavior. It's possible to create cluster models that group similar users based on their past activities. Continuous self-learning more accurately identifies high-risk events, like someone accessing a project he or she doesn't normally work on, putting a spotlight on threats to an organization's most sensitive assets. Identifying High-Risk Behaviors Once you've establised what's normal behavior, the next step is to apply advanced mathematical models that generate a behavioral risk score. This score represents multiple factors, including the importance of an asset or file, the method of access, the activity (e.g., volume or type), and the user. These behavioral analytics models can then be used to find anomalies by: Comparing access patterns, data usage patterns and data movement patterns against historic behavior Determining similar user patterns across the environment and comparing behavioral patterns between users and groups of users Detecting dissimilar patterns among members of the same project group or job role Comparing individuals against the entire user group To learn more about the behavioral analytics models used in Helix Threat Detection, download the white paper Helix Threat Detection: IP Security and Risk Analytics. To learn more download our white paper: A Unified Approach to Securing and Protecting IP. READ NOW","url":"http://www.ciandcd.com/finding-the-needle-in-a-haystack-with-helix-threat-detection.html","tags":"scm","title":"Finding the ‘Needle in a Haystack' with Helix Threat Detection"},{"text":"from:http://devops.com/2015/06/18/clusterhq-and-devops-com-survey-show-containers-poised-for-mass-adoption/ DevOps.com and ClusterHQ, conducted a survey on Container usage that shows an overwhelming majority of users have either already using, testing or investigating Container usage. With 285 respondents representing a wide range of organizations, it shows that Containers will be part of many production environments in the very near future. Currently, only 38 percent of respondents reported using containers in production environments, but that number is projected to increase 69 percent over the next 12 months as organizations find new ways to address important barriers to adoption. It verified that Docker is overwhelmingly the container of choice, with 92% of respondents having used or investigated it, followed by LXC (32%) a distant second, but still far ahead of Rocket (21%). To access the complete survey and report visit https://clusterhq.com/assets/pdfs/state-of-container-usage-june-2015.pdf . Companies ranging in size from small organizations with 1 to 500 employees (69%), to mid-size companies with 501-2,500 personnel (12%), all the way up to large enterprises with over 2,500 employees (19%) are represented in the survey. This demonstrates that containers are being embraced by all businesses from the startup stage to Fortune 500 companies. Respondents came predominantly from Development, Operations and DevOps teams. QA and security teams were a smaller share. The survey revealed how container technologies are being used today, as well as research-based insights while providing clues as to where the industry is trending. From the ClusterHQ release on the survey: The survey also revealed insights into what is perceived to be the primary barriers to container adoption. Security seems to be emerging as a consistent concern throughout the DevOps community in these times of never ending breaches throughout the world: Security — 61% Data Management — 53% Networking — 51% Skills and Knowledge — 48% Persistent Storage — 48% Data Management capabilities also emerged as essential to the success of container strategies and that the vast majority of organizations want to run databases as well as additional services in containers. When asked to rate how important data management is to container strategies, 66 percent reported it as a critical or important gating factor, 29 percent ranked it as moderately important and only 5 percent reported that it carries no importance. Over 70% of respondents said they would like to run a database or other stateful service in their container environments. Respondents were also asked which specific features of container data management they considered to be most important, selecting the \"integration of data management capabilities into existing container workflows and tools\" as their first choice, with \"seamless movement of data between dev, test and production environments\" a close second. MySQL (53%), Redis (52%), PostgreSQL (50%), and Elasticsearch (43%) were reported as the top four most frequently used stateful services. Containers have become known for portability and flexibility, the survey reveals that organizations are using them in different infrastructures but most frequently in on-premises data centers (57%), followed by Amazon Web Services (52%). So I think it is safe to say Containers are rapidly becoming one of the staples of development and are here to stay to stay in the foreseeable future. DevOps.com along with ElasticBox are currently conducting another survey on \"What is DevOps to you?\" One in 50 respondents wins a $50 dollar Amazon gift card and one grand prize winner will win a new 3DR Drone. Take a few minutes to help us with this survey .","url":"http://www.ciandcd.com/clusterhq-and-devopscom-survey-show-containers-poised-for-mass-adoption.html","tags":"devops","title":"ClusterHQ and DevOps.com survey show Containers poised for mass adoption"},{"text":"from:http://blog.devopsguys.com/2015/06/18/dogs-at-digital-2015/ Last week the DevOpsGuys headed up to Newport's Celtic Manor to take part in Digital 2015 – the Welsh Government's initiative to bring digital innovators and business professionals together. The 2-day event saw more than 2,000 delegates and 140 speakers. DevOpsGuy co-founder Steve Thair says: \"These initiatives are invaluable to the digital sector because they expose the wide variety of digital and technological services that are available in South Wales to business professionals who can use them to take online business services to the next level. It's a relaxed environment where people can chat and form connections that will have a direct impact on the future of business and technology in Wales.\" The diverse range of speakers at the event included Microsoft, the WRU, Amazon and the DVLA. The opportunity to discuss the needs of businesses directly with those running them is invaluable. This dialogue can lead to collaborative projects and further development of the burgeoning tech industry in the area. The team are excited to see more events like this one springing up in the near future. Look out for us at the up-coming Agile Cymru in the Wales Millennium Centre on the 7th and 8th of July.","url":"http://www.ciandcd.com/dogs-at-digital-2015.html","tags":"devops","title":"DOGs at Digital 2015"},{"text":"from:https://github.com/blog/2023-a-closer-look-at-europe Last week we opened our first international office in Japan . This week we thought we'd take a closer look at Europe, which happens to be the largest demographic of GitHub users around the world, representing 36% of site traffic. Around 32 million people visit GitHub each month, and most of this traffic comes from outside of the United States (74% in fact!). The most active countries in Europe are Germany, the United Kingdom, and France, but if we look at users per capita we see a different story -- Sweden, Finland, and the Netherlands lead the way. London, Paris and Stockholm top the list of European cities most active on GitHub. The goals of building better software are universal, and several European organizations are setting the example. Companies like SAP and XS4ALL are driving innovation with software, while The UK Government Digital Services and dozens of other European government agencies and services are developing new ways to serve citizens. Today, around 10% of GitHub employees are based in Europe, with a dozen new faces in the last year alone -- many of whom are focused solely on helping our European customers build great software. A few of us are here in the UK for London Tech Week and EnterConf in Belfast. There will be plenty more meetups ahead if we don't see you there.","url":"http://www.ciandcd.com/a-closer-look-at-europe.html","tags":"scm","title":"A closer look at Europe"},{"text":"from:http://blog.devopsguys.com/2015/06/17/dogwalking-in-brecon/ So, aching, tired and happy the DOGs returned from TrekFest 2015 in the Welsh mountains having raised £1,143.00 for the Countess Mountbatten Hospice and SSNAP – two charities close to the heart of the team. That's 114% of our initial target, so a huge thank you to everyone who donated so generously. The weather could not have been better for a long, scenic ramble in one of the UKs most beautiful spots; beautifully sunny with a refreshing breeze kept the team going. We completed the trek in approximately 5 and a half hours, just in time for a piece of cake and a glass of celebratory champagne at the finish line. Everyone had a thoroughly enjoyable time and we're all looking forward to the next DOG adventure!","url":"http://www.ciandcd.com/dogwalking-in-brecon.html","tags":"devops","title":"DOGWalking in Brecon"},{"text":"from:https://www.gitlab.com/2015/06/17/gitlab-com-and-logjam/ GitLab.com and Logjam We've previously announced security advisory for Logjam vulnerability . In that announcement we've mentioned that GitLab.com is using 1024-bit DH groups to retain compatibility with older Java-based clients. We've updated the default/recommended SSL ciphers for all GitLab installations and implemented new ciphers on GitLab.com. After some reasearch and testing we've decided to change the SSL cipher suite served by the web server/load balancer. This decision was made after weighing on the trade-offs between having the stronger DH params and denying access to Java 6 based clients. Using 2048-bit DHE params Generating the 2048-bit DHE params was advised to help against the Logjam vulnerability. While this is a way to go for most servers, with GitLab.com we have to keep in mind that we have users using older Java-based clients. Adopting the stronger params suites would prevent those users using GitLab.com completely. Although the number of these users is not high, denying them access does not seem like an option. Removing DHE suites DHE suites have a couple of issues: DHE is slow Not all browsers support all the necessary suites One advantage of having DHE together with ECDHE suites is that this allows forward secrecy to all clients. We then turned to investigating how others are handling this issue and we found out that, for example, Google sites mostly do not have DHE suites in their configuration . With this in mind we've tried removing the DHE suites and the result was as follows: All major browsers and clients retain forward secrecy using ECDHE SSL labs score went from B to A There is no forward secrecy for Android 2.3.7, Java 6 and OpenSSL 0.9.8 After considering the trade-offs, we've decided to remove the DHE suites from our cipher suite on GitLab.com. Forward secrecy is now denied for Android 2.3.7, Java 6 and OpenSSL 0.9.8 but we suspect that number of users affected will be extremely low. We have also updated the recommended configurations for omnibus-gitlab packages and GitLab installation from source. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","url":"http://www.ciandcd.com/gitlabcom-and-logjam.html","tags":"scm","title":"GitLab.com and Logjam"},{"text":"from:https://www.gitlab.com/2015/06/17/highlights-to-my-first-remote-job/ Some time ago, GitLab's CEO Sytse wrote The Remote Manifesto . It inspired me to write about it from an employee's perspective. I started working with GitLab 2 months ago and it has been quite an interesting experience to work remotely with a team that's spread out in the world. I have been working from home for a while, with freelance jobs, but it was very different to today, that I am part of a remote team. Here are the highlights of my experience so far: Learning how to communicate It was hard at the beginning to communicate with my team. I hadn't met them in person and if I had a doubt about something, I had to go to Slack to find the person who seemed the nicest and ask him. I was shy to write people to ask for their help all the time, but I learned that it is \"the way\" to learn and to stay communicated in a remote job. I learned not to be afraid to ask and to provide as much context as I could, trying not to include so many links without information. Avoiding interruptions So it is great to have coworkers and being with people, but they usually talk with each other, they are friendly and they get you distracted. I love working from home. I got myself nice headphones and I wear them when I don't want anybody at home to distract me. I think I can focus more and get into my \"zone\" easier. Morning calls in my PJs The first few days I would wake up really early to look fresh and dressed up for my morning call with the team. Then, I realized that Hangouts won't show the difference, so I learned to wake up, fix some coffee, sit down and work in my PJs all morning and then have a late shower during a brake. It is absolutely comfortable and fun, but it's important to learn to be disciplined. No matter how you look or if you are in bed, you need to get into the mindset of working and have a disciplined productive day. It's all about trust before they can see results Trust is a strong word and for me, it is what I feel about having a remote job. They trust that I work and that I use my time wisely. I have been learning to be more efficient, but since GitLab is very technical and I've had to learn so much, my speed has been very slowly increasing. I believe they trust that I am putting all my efforts into my job and that even though nobody is next to me looking at me working, I am doing my job. That makes me work more because when somebody trusts you, you don't want to disappoint them Find a comfortable place where you can be productive I have been traveling for a couple of years and when I started, I would just try to find a comfortable and cheap place to stay at, close to a yoga studio. Now, I find myself looking for an Airbnb with a good wifi connection and a comfortable desk or table. The rest is second. This is very important for my mental and physical health and I believe it's one of the most important things to make sure you have. If I can't find this, I will look for a co-working space or a working cafe. It is important to be able to sit comfortably and stay connected with no distractions. I am able to travel This is my favorite highlight. I used to work in the traditional corporate world, which helped me save some money and start traveling with it. When I was running out of savings, I thought that my travels had to end soon. The fact that I could work remotely was one for the best things that could happen to me, since it allows me to work from anywhere and to see the world. I can also spend quality time with my dog and it lets me take care of her. So, are you thinking about applying for a remote job? Do it! It's awesome! By the way, in Gitlab we're always hiring ! Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","url":"http://www.ciandcd.com/highlights-to-my-first-remote-job.html","tags":"scm","title":"Highlights to My First Remote Job"},{"text":"from:https://github.com/blog/2024-read-only-deploy-keys You can now create deploy keys with read-only access. A deploy key is an SSH key that is stored on your server and grants access to a single GitHub repository. They are often used to clone repositories during deploys or continuous integration runs. Deploys sometimes involve merging branches and pushing code, so deploy keys have always allowed both read and write access. Because write access is undesirable in many cases, you now have the ability to create deploy keys with read-only access. New deploy keys created through GitHub.com will be read-only by default and can be given write access by selecting \"Allow write access\" during creation. Access level can be specified when creating deploy keys from the API as well.","url":"http://www.ciandcd.com/read-only-deploy-keys.html","tags":"scm","title":"Read-only deploy keys"},{"text":"from:http://www.perforce.com/blog/150610/helix-dvcs-initialization-tips-tricks We are all very excited about the new distributed version control system (DVCS) capabilities of Perforce Helix. Here are a few tips for getting started. Keep in mind that in order to use Helix DVCS, you need to have the 2015.1 version of both Helix client P4 and Helix server P4D installed. Some of the commands (e.g., init and clone ) are implemented in P4, so you need the latest version of both executables. The first thing you need to do when you want to use a local Helix server (called a personal server) is to run \"p4 init\". This command will create the personal server for you (in a subdirectory called .p4root) and set up the P4CONFIG and P4IGNORE files, as well. \"p4 init\" also turns your current directory into the client workspace root for your new personal server , which is useful if you already have some files and realize it might be a good thing to version them: p4 init p4 rec p4 submit -n \"Initial checkin\" In the above, \"rec\" is a handy alias for \"reconcile\" to save you typing . If you start a new project from scratch and want to place it in another directory instead, use the \"-d\" option like such: p4 –d path-to-new-project init Case and Unicode Let's take a closer look at the output of the \"p4 init\" command: Matching server configuration from ‘wayfarer-p4d:1666': case-sensitive (-C0), non-unicode (-n) Server sknop-dvcs-1429629213 saved. One might ask: what is case-sensitive and Unicode about? Because the Helix versioning engine supports many platforms, both case sensitive and insensitive, you can choose how your personal server handles case. By default, the Helix versioning engine adopts the case policy of the platform you run it on: insensitive on Mac and Windows, sensitive on Linux and other Unix platforms. Also by default, the Helix versioning engine does no Unicode translation and simply accepts any encoding for file content and metadata. For cross-platform development it is better to put a shared server into Unicode mode. For a personal server you may not care at first what these settings are, but what if you want to push your changes to another server at a later stage? The settings of your personal server have to match the settings on the destination server or there could be chaos, as the destination server will refuse the push if the settings do not match. It is cumbersome to change case sensitivity and Unicode settings after the Helix versioning engine is populated, so it is important to get this right up front. \"p4 init\" will \"guess\" what the standard settings within your enterprise are by connecting to and inquiring with the Helix versioning engine specified by the P4PORT environment variable (or \"perforce:1666\" if that is not set). If you'd rather inquire with a particular server when initializing a personal server, use the \"-p\" option: p4 init –p myserver:1666 Alternately, you can also explicitly set case and Unicode support with the following options: Option Meaning -C0 Case insensitive -C1 Case sensitive -n No Unicode support -xi Unicode support Server and User NameNote well: if you have P4CHARSET defined in your environment and not set to \"none\", a new personal server will automatically be initialized as a Unicode-enabled server. So what is the story with the server and user name? The name of your personal server and client workspace coincide. Although in principle you can have more than one workspace against your personal server, in practice there is rarely any need for it. Locally the name does not matter, but when you push your changes into another server, the changes are linked to your local workspace name. An automatically generated name like \"sknop-dvcs-1429629213\" is highly likely do be unique, but you are free to choose a different name if you so wish by using the \"-c\" option. The same is true for your user name: locally it does not matter and will typically coincide with either your OS user name and/or whatever P4USER is set to, but when pushing to another server the user name becomes important. Take the Perforce workshop for example: my local user name is always \"sknop\", but for the workshop I use \"sven_erik_knop\". If I create a local DVCS server under the user name \"sknop\", submit my changes, set up a remote to the workshop, and push, I'll receive only an error message. Fortunately, the solution is very simple. I add another user to my local server and update my local protection table: p4 user –f sven_erik_knop p4 protect Now I can push my changes under the new user name (I might have to log into the target server first): p4 –u sven_erik_knop push Conclusion A simple \"p4 init\" will create you a new personal server to which you can submit changes, but if you want to push these changes to another server, it makes sense to pay attention to case sensitivity, Unicode support, and workspace and user name. Let me know if you are using our new DVCS features and how you are getting on. My Twitter handle is @p4sven. For a live technical overview of DVCS features in the Helix Versioning engine sign up for our DevTalk Webinar on June 26th.","url":"http://www.ciandcd.com/helix-dvcs-how-to-initialize-like-a-pro.html","tags":"scm","title":"Helix DVCS - How to Initialize Like a Pro"},{"text":"from:http://java.dzone.com/articles/git-simple-feature-branch In my previous post , I wrote about git work flows. Now I will going to try out simple ' Feature Branch Workflow '. 1. I pull down the latest changes from mastergit checkout mastergit pull origin master2. I make branch to make changes git checkout -b new-feature3. Now I am working on the feature4. I keep my feature branch fresh and up to date with the latest changes in master, using 'rebase'Every once in a while during the development update the feature branch with the latest changes in master.git fetch origingit rebase origin/masterIn the case where other devs are also working on the same shared remote feature branch, also rebase changes coming from it:git rebase origin/new-featureResolving conflicts during the rebase allows me to have always clean merges at the end of the feature development.5. When I am ready I commit my changesgit add -pgit commit -m \"my changes\"6. rebasing keeps my code working, merging easy, and history clean.git fetch origingit rebase origin/new-featuregit rebase origin/masterBelow two points are optional6.1 push my branch for discussion (pull-request)git push origin new-feature6.2 feel free to rebase within my feature branch, my team can handle it!git rebase -i origin/master Few point that can be happen in developing phase. Another new feature is needed and it need some commits from my new branch 'new-feature' that new feature need new branch and few commits need to push to it and clean from my branch. 7.1 Creating x-new-feature branch on top of 'new-feature' git checkout -b x-new-feature new-feature 7.2 Cleaning commits //revert a commit git revert --no-commit //reverting few steps a back from current HEAD git reset --hard HEAD~2 7.3 Updating the git //Clean new-feature branch git push origin HEAD --force 1. I pull down the latest changes from mastergit checkout mastergit pull origin master2. I make branch to make changes git checkout -b new-feature3. Now I am working on the feature4. I keep my feature branch fresh and up to date with the latest changes in master, using 'rebase'Every once in a while during the development update the feature branch with the latest changes in master.git fetch origingit rebase origin/masterIn the case where other devs are also working on the same shared remote feature branch, also rebase changes coming from it:git rebase origin/new-featureResolving conflicts during the rebase allows me to have always clean merges at the end of the feature development.5. When I am ready I commit my changesgit add -pgit commit -m \"my changes\"6. rebasing keeps my code working, merging easy, and history clean.git fetch origingit rebase origin/new-featuregit rebase origin/masterBelow two points are optional6.1 push my branch for discussion (pull-request)git push origin new-feature6.2 feel free to rebase within my feature branch, my team can handle it!git rebase -i origin/masterAnother new feature is needed and it need some commits from my new branch 'new-feature' that new feature need new branch and few commits need to push to it and clean from my branch.7.1 Creating x-new-feature branch on top of 'new-feature'git checkout -b x-new-feature new-feature7.2 Cleaning commits//revert a commitgit revert --no-commit//reverting few steps a back from current HEADgit reset --hard HEAD~27.3 Updating the git//Clean new-feature branchgit push origin HEAD --force","url":"http://www.ciandcd.com/git-simple-feature-branch-workflow.html","tags":"devops","title":"Git Simple Feature Branch Workflow"},{"text":"from:http://java.dzone.com/articles/know-thy-mvn-plugins-or Problem #1: Everyone knows that keeping up with versions is tough. This is the reason tools such as OpenLogic come to exist. Some companies pay, some companies develop home grown solutions, some live with the version which are getting older every day. Problem #2: When multiple open source projects once consumed by your product can bring in different versions of the same library. One never knows which will be picked up at run-time and behavior on the developer's box based on the Murphy's Law will be different from the server run-time. This does not have to be such an ordeal. With the power of maven plugins this can be solved relatively easy. Versions Maven Plugin and Maven Enforcer Plugin to the rescue! Versions Maven Plugin will keep versions up-to-date and as you probably guessed from the name Maven Enforcer plugin will be guarding against multiple versions of the maven artifact in the build package produced. Let's see how one introduced enforcer into the mix first. The code below can go in the parent pom.xml of the project, or global parent of the projects if one exists. <project …> … <plugins> … <plugin> <groupId>org.apache.maven.plugins</groupId> <artifactId>maven-enforcer-plugin</artifactId> <version>1.4</version> <executions> <execution> <id>enforce-versions</id> <goals> <goal>enforce</goal> </goals> <configuration> <rules> <requireMavenVersion> <version>[2.2.*,)</version> </requireMavenVersion> <requireJavaVersion> <version>[1.7.*,)</version> </requireJavaVersion> <DependencyConvergence/> </rules> </configuration> </execution> </executions> </plugin> … </plugins> … </project> For each dependency version collision one has to pick the version to keep and exclude the ones that are mismatched. See maven help page for details. If one wants absolute guarantee of the version used, this is the only way to go. An example of excluded dependencies will be: <project …> … <dependencies> … <dependency> <groupId>com.lordofthejars</groupId> <artifactId>nosqlunit-mongodb</artifactId> <version>${nosqlunit.veresion}</version> <scope>test</scope> <exclusions> <exclusion> <groupId>org.slf4j</groupId> <artifactId>slf4j-api</artifactId> </exclusion> <exclusion> <groupId>com.github.fakemongo</groupId> <artifactId>fongo</artifactId> </exclusion> <exclusion> <groupId>org.mongodb</groupId> <artifactId>mongo-java-driver</artifactId> </exclusion> </exclusions> </dependency> … </dependencies> … </project> From the open source developer side , producing two flavors of the package for consumption with and without dependencies can make world a better place. This is the difference of maven scope ‘provided' vs. default scope ‘compile'. Apache Maven Shade Plugin can be used to produce the version with the dependencies to be used there no conflicts can arise along with the version that is artifact version independent and can be used without version conflicts. Once all conflicts are resolved and one version of each artifact is in the final build product guaranteed, one should check if ones project is up to date. It is a good practice to do the check at the beginning of the new version. To check for outdates dependencies, and plugins, and bring those up to date in the controlled manner (manually) one can execute: mvn versions:display-dependency-updates mvn versions:display-plugin-updates mvn versions:display-property-updates As an alternative one can let the plugin update the versions by executing following mvn versions:use-latest-releases mvn versions:update-properties This last mvn command I am going to share is a bonus for the dedicated reader. It allows changing versions across the project and its modules painlessly: mvn versions:set -DgenerateBackupPoms=false​ -DnewVersion=<version>","url":"http://www.ciandcd.com/know-thy-mvn-plugins-keeping-ones-sanity-amidst-open-source-version-hell.html","tags":"devops","title":"Know Thy MVN Plugins: Keeping One's Sanity Amidst Open Source Version Hell"},{"text":"from:https://www.gitlab.com/2015/06/16/feature-highlight-approve-merge-request/ Feature Highlight: Approve Merge Request With less than a week until GitLab 7.12, we've got a nice preview for you today: Merge Request Approvals. Usually you accept a merge request the moment it is ready and reviewed. But in some cases you want to make sure that every merge request is reviewed and signed off by several people before merging it. With GitLab Enterprise Edition 7.12, you can enforce such a workflow that requires multiple reviewers with the new Merge Request Approval feature. To enable approvals, go to project settings page and set the \"Approvals required\" field to a numeric value. For example, if you set it to 3 each merge request has to receive 3 approvals from different people before it can be merged through the user interface. After setting the approval, you will see an Approve button on merge requests, rather than an Accept button. Once the merge request has enough approvals, you will be able to merge it as usual. We'd love to hear what you think of this new feature in the comments below. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","url":"http://www.ciandcd.com/feature-highlight-approve-merge-request.html","tags":"scm","title":"Feature Highlight: Approve Merge Request"},{"text":"from:http://blog.devopsguys.com/2015/06/16/the-ops-mgr-at-qcon-2015/ \"When you're in a startup, the divide between Dev and Ops is normally the width of the desk…it's far easier to collaborate in that small environment. In larger enterprises not only are they in different buildings but they're in different countries with different cultures and different languages…\" The DOG Ops Manager Steve Thair chats to Manuel Pais at QCon 2015. Steve Thair at QCon to hear Steve talk about Enterprise DevOps; taking the first steps on the road to DevOps and cultural change.","url":"http://www.ciandcd.com/the-ops-mgr-at-qcon-2015.html","tags":"devops","title":"The Ops Mgr at QCon 2015"},{"text":"from:http://www.perforce.com/blog/150612/next-round-online-training-helix-dvcs Back in March, we announced our new Helix platform which includes highly anticipated distributed version control (DVCS) capabilities. For some of you, hearing that we now offer DVCS may have been music to your ears. For others, it may have invoked curiosity and added another acronym to your lexicon. Given the recent proliferation of Git-style workflows, we are seeing a lot of developers finding themselves working with their own private, local versioning repositories while collaborating with teammates via the new init/clone/pull/push command set. But did you know that you can now rewrite the history of changes in your personal repository before sharing them? To help you come up to speed with Helix DVCS, we are pleased to announce the availability of a new instructor-led training course. The Helix DVCS course will take place online via Webex and will include hands-on lab exercises within our lab environment. The class is taught by our expert Professional Services consultants who have a lot of experience advising customers. Topics on this new half-day course will include: Why do you need DVCS? Overview of DVCS architecture and workflows Basic DVCS operations How to perform initial setup Working with multiple streams Rewriting history The first class is now scheduled for European customers on June 29, 9am – 1pm, British Summer Time (GMT+1). We will have a DVCS training for North American customers in the weeks that follow. So sign up here and bring along your questions about DVCS. The class does assume that you are already familiar with Perforce Helix, so if you're new to Helix, we also offer introductory courses. Check out our course schedule for details. Any questions, just email training@perforce.com.","url":"http://www.ciandcd.com/next-round-of-online-training-helix-dvcs.html","tags":"scm","title":"Next Round of Online Training: Helix DVCS"},{"text":"from:https://www.gitlab.com/2015/06/12/price_changes/ Changes to Enterprise Edition subscription pricing Today we are announcing two changes to GitLab Enterprise Edition subscription pricing. The changes are intended to better reflect the value of each offering and ensure our subscription options cater to the needs of different organizations. In short, our basic subscription is now $19,10 more expensive, but in 10-user packs. Our Plus subscription is now $100 more affordable. Standard and terms remain unchanged. As of today (June 12, 2015) the following will take affect: Basic Subscriptions will cost $390 per year for a 10-user pack ($39 per user / per year). Current Basic Subscribers will be offered a 25% discount on this new pricing at their next renewal. However, new pricing will apply to subsequent renewals and any additional user packs. Basic subscriptions are now available in 10-user packs, making it slightly more affordable for small teams. Plus Subscriptions will cost $14,900 for a 100-user pack ($149 per user / per year). Current Plus subscribers will receive a prorated refund on the price differece. There are no changes in the software features or service level of Basic or Plus subscriptions, which you can view on our website here . Standard Subscription pricing will also remain unchanged at $4,900 per year for each 100-user pack ($49 per user / per year). All current quotes will be honored until their expiration (60 days from issue date) but the new pricing will apply to any subsequent orders, including renewals. Our goal is to keep GitLab the most affordable enterprise grade development platform available. These changes should not have any significant effect on our ability to achieve that. We felt our Basic plan was underpriced and Plus plan was overpriced. These changes reduce the price difference between them. If you have questions about the changes or about pricing in general, please contact our sales team at sales@gitlab.com. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","url":"http://www.ciandcd.com/changes-to-enterprise-edition-subscription-pricing.html","tags":"scm","title":"Changes to Enterprise Edition Subscription Pricing"},{"text":"from:https://www.gitlab.com/2015/06/12/did-you-install-gitlab-from-source-recently-check-your-git-version/ Did you install GitLab from source? Check your Git version Although the preferred way to install GitLab is to use our omnibus packages , you can also install GitLab Community Edition or Enterprise Edition ‘from source'. If you used this installation method, and if you compiled Git from source in the process then please check whether your Git version defends against Git vulnerability CVE-2014-9390. This issue does not apply to our Omnibus packages (DEB or RPM). Although GitLab itself is not affected by CVE-2014-9390 , a GitLab server may be used to deliver ‘poisoned' Git repositories to users on vulnerable systems. Upgrading Git on your GitLab server stops users from pushing poisoned repositories to your GitLab server. Due to an oversight, the guide for installing GitLab from source still contained instructions telling administrators to install Git 2.1.2 if the version of Git provided by their Linux distribution was too old. Git 2.1.2 does not defend against CVE-2014-9390. If your GitLab server uses /usr/local/bin/git please check your Git version using the instructions in this upgrade guide . Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","url":"http://www.ciandcd.com/did-you-install-gitlab-from-source-check-your-git-version.html","tags":"scm","title":"Did You Install GitLab From Source? Check Your Git Version"},{"text":"from:https://github.com/blog/2022-an-updated-header-just-for-you Navigating what's most important to you on GitHub.com just got a little easier with our updated site header. The new header gives you faster access to your pull requests and issues dashboards from anywhere on the site. If you're unfamiliar with them, these dashboards list all of your open pull requests and issues—as well as those you've been mentioned in or are assigned to—in one place. Use them to stay up to date on what needs to be done across your projects. Lastly, clicking your avatar now opens a new dropdown menu with links to your profile, account settings, and more. As a small bonus, we've also included a new Your stars link for easy access to your starred repositories. Enjoy!","url":"http://www.ciandcd.com/an-updated-header-just-for-you.html","tags":"scm","title":"An updated header, just for you"},{"text":"from:http://www.perforce.com/blog/150610/news-infosecurity-2015-0 I've just returned from my first visit to InfoSecurity 2015 in London . With the launch earlier this year of Perforce Helix Threat Detection this was a great opportunity to review the state of the cyber-security world, hear about the key challenges facing governments, businesses and individuals and review some of the solutions being offered. This couldn't be more timely as, right when I'm writing this, it has been announced that the U.S. government suffered a serious attack earlier this year which compromised the personal details of thousands of federal employees. My particular area of interest is the emerging role of security in DevOps. There are a few key aspects to consider: As a developer what do you need to do and how does that fit with agile and development processes? As a Release Manager/Operations Specialist/DevOps Engineer what do you need to know to roll out and manage secure applications? As a Chief Information Security Officer or Risk Manager what is going on in the development and operations areas that I ought to be concerned about? I haven't got space here to cover all of these topics, but here are a few highlights from the conversations I had at the conference. Development Managers and DevOps specialists are increasingly aware of the need for secure applications. They are concerned that as release cycle times reduce with the adoption of Continuous Delivery they don't reduce security nor slow down deliveries. Some companies are working out how to do this by involving security experts in the earliest stages of sprint planning and ensuring security stories are \"groomed\" to ensure they are properly positioned for priority in their backlogs. They're also adopting tools for automated code and application validation. It was interesting to see an increasing number of tools addressing the need for dynamic security testing. Although the term seems to have been around a few years already, there were a number of people talking about \" Rugged DevOps \" and I think this is an area that will continue to grow. Security experts, especially those involved in IT audits or risk assessments are busier than ever. Some are aware of the potential risks that may exist in their development organizations but I suspect the majority are not. This is the result of two issues. Firstly, they may not fully appreciate the value of the software being developed. They know that they need to protect customer and staff personal data, but they don't necessarily realize that the software is actually their company's competitive differentiator and could be critical if leaked to a competitor. Secondly there is a lot of technology involved that they don't understand. They may be familiar with firewalls, VPNs, email, etc., but developers often bring tools into the business without their knowledge and these tools, such as Subversion or Git are inherently vulnerable. It's increasingly hard to keep track of business documents in a world full of email, cloud file sharing services and BYOD mobiles, but this technical software content is even harder to grasp. I saw a number of tools that try to address some of these problems by monitoring network traffic rather than trying to lock down each application. This generates another problem though – if you're monitoring hundreds or thousands of different file types and communications, it quickly becomes an impossible management challenge. A few tools are trying to address that problem by using analytics to analyze the basic data and infer what looks like suspicious behavior. This helps with the management issue but they still don't understand the context of the data being moved around the organization which makes them inefficient for DevOps. I didn't see anything that was close to Perforce Helix Threat Detection , which focuses on protecting this valuable IP being created by design and development teams. Because it uses the rich data available from the Helix Versioning Engine it understands the context of the files being accessed. It can not only track that a user may be accessing more files than usual (and most tools can't work out what \"normal\" means), but it also understands whether those files are in projects they \"normally\" use or whether they're using the files in ways that are unusual for the user. I'm really looking forward to the webinar Perforce are hosting on June 16th where the Forrester DevOps Analysts, Kirt Bittner, and Security Analyst, Rick Holland will talk about the issues raised above and the solutions to them.","url":"http://www.ciandcd.com/news-from-infosecurity-2015.html","tags":"scm","title":"News from InfoSecurity 2015"},{"text":"from:https://github.com/blog/2020-improved-organization-permissions Organizations have always been the best way for teams to work together and collaborate on code. We're happy to announce major improvements to GitHub organization permissions . These improvements include new customizable member privileges, fine-grained team permissions, and more open communication. The improved permissions system gives your organization the flexibility to work the way you want. Here are just a few highlights: (Opt-in) Members can view and mention all teams, even when they're not on those teams. (Opt-in) Members can create repositories without help from an owner. Members can create new teams to self-organize with the people they work with. Owners can give just the right amount of access to contractors and interns by adding them to repositories without giving them the privileges of organization members. And many more! Learn about GitHub's improved organization permissions . All of these new features give your organization the ability to work together seamlessly without everyone needing to be an owner. Once these features launch, organization owners will be able to turn on new permissions as needed. Simply opt-in when you're ready. Early access We're rolling out our improved permissions system to a select group of users who will be asked to provide feedback over a short survey as part of the program. If you're interested in being one of the first to try it out on GitHub.com, sign your organization up for early access . In the next few months, every organization on GitHub.com will have the improved permissions system.","url":"http://www.ciandcd.com/improved-organization-permissions.html","tags":"scm","title":"Improved organization permissions"},{"text":"from:http://www.perforce.com/blog/150609/living-la-vida-helix-submitting-without-fear One of the common complaints I hear about centralized version control systems is that they are scary. With every commit being immediately visible there is a feeling that you may screw up everything for your co-workers. What's worse is that you generally don't have the power to clean up after yourself. How many of us have had to sheepishly go ask the admin to obliterate something? With P4D (which we now call Helix Versioning Engine) becoming a proper DVCS, you now can manipulate history that has not yet been shared with other people. Than means you can commit to your heart's content, and then sweep through later to keep just the interesting commits. It also means that if you accidentally submit something you can deal with it. Just recently while doing some cleanup work in the Workshop I had just one of these cases. I'd like to walk you through what happened so that you can see how unsubmit and resubmit will help you. Setting the scene A user had reported that a number of files that I had added the day before had all of their line endings mangled. The files were already in the shared server, so I didn't want to run p4 unsubmit there, and anyway I feel it is important for my failures to remain on display for all to see. So I got to work updating the files. p4 fetch Everyting was up-to-date. Next to find the files with the bad line endings. grep -lIUr --color \"&#94;R\" I was lucky and it was just a handful of files. Thankfully turning Windows line endings into Unixones is a piece of cake with P4D. p4 client -o | sed s/LineEnd: local/LineEnd: share | p4 client -i Now to get the files synced with the correct line endings and submitted: p4 sync -f p4 submit -d \"Fixing up some busted line endings that snuck in\" All was well and good until I realized that in my excitement I'd mangled some solution files which probably wanted those '\\R's. Thankfully I hadn't pushed, so I could quickly clean up my mess. p4 changes -m1 p4 unsubmit @ 12345 I identified my last change number, and then unsubmitted it. At this point I had all of my changed files in a shelf. In this case I had only one changelist, but I still decided to use p4 resubmit to apply the change. p4 resubmit makes it easy to reapply the changes in order. p4 resubmit This kicks me into interactive mode. Because there is a lot you can do with resubmit and I always forget the options, I hit '?' to see the list. Specify next action ( l/m/e/c/r/R/s/d/b/v/V/a/q ) or ? for help: ? The following actions are available: c Modify the change description for this change m Merge this change, then submit if no conflicts e Merge this change, then exit for further editing r Interactively resolve this change, then submit if no conflicts a Add (squash) this change into the next unsubmitted change s Skip this change and move on to the next d Delete this change without submitting it b Begin again from the earliest remaining change l List the changes remaining to be processed v View the current change in short form V View the current change with full diffs R Display the status of resolved and unresolved merges q Quit the resubmit operation< ? Display this help. In this case I wanted to resubmit all of the files except the solution files, so I selected e That merged my change back in, but then dropped me back to the command prompt so I could further mangle the files. A quick revert got rid of the changed solution files, and then I used p4 resubmit -Re to resume the resubmit process. p4 revert ....sln p4 resubmit -Re P4D submitted the change again, and cleaned up the shelf for me since I no longer needed it. With that tidied up I was ready to push and share my changes with the community. p4 push Sharing that broken change wouldn't have been the end of the world, but I felt so much more in control being able to clean up those .sln files before pushing out my change. Ever wish you could undo a merge between branches? With p4 unsubmit you can. Helix Versioning Engine gives you a way to safely experiment, modifying history as need be to make sure the changes your coworkers see are the ones you want them to see. Interested in trying it yourself? You're just a download of our Helix Versioning Engine and p4 init away from being able to try this all yourself. If you'd like to push to a shared server the Workshop has been running 2015.1 since beta, and Helix Cloud is also using it. As always we're here to help, so if you have questions, just shout!","url":"http://www.ciandcd.com/living-la-vida-helix-submitting-without-fear.html","tags":"scm","title":"Living la Vida Helix: Submitting Without Fear"},{"text":"from:http://www.perforce.com/blog/150608/helix-swarm-20151-released Swarm is two years old this month!👏 It's rewarding to think that just two years ago, our collaboration engine was only just getting into the hands of our customers. Fast-forward to today, where Swarm plays a big part in the daily workflows of so many innovative companies. With much of the functionality now matured, we wanted to expand Swarm beyond just our English-speaking customers. With the latest release of Helix Swarm, we've translated the product into Japanese. It's available through our exclusive partner in Japan, the TOYO Corporation . TOYO provides expert consulting and support to our Japanese customers, and Swarm joins the Helix Versioning Engine and our popular visual client, P4V , in the suite of Perforce products available in Japanese. Localization Support for Swarm The Swarm team spent the last couple of months creating a localization framework and translating the product and documentation into Japanese. The next languages on our list are Korean and Simplified Chinese. If there's a language you'd like us to add to our list of localizations, please send us your request via the Perforce Swarm Forums or by emailing support and we'll put it on our radar. Aside from the localization support , other new functionalities include: Files and folders are downloadable as ZIP archives Swarm now limits the number of files to display in a committed change to a configurable default of 1000 Configurable timeout sets thresholds for large commits More details can be found in the What's new in 2015.1 section of our user guide.","url":"http://www.ciandcd.com/helix-swarm-20151-released.html","tags":"scm","title":"Helix Swarm 2015.1 Released"},{"text":"from:https://github.com/blog/2019-how-to-undo-almost-anything-with-git One of the most useful features of any version control system is the ability to \"undo\" your mistakes. In Git, \"undo\" can mean many slightly different things. When you make a new commit, Git stores a snapshot of your repository at that specific moment in time; later, you can use Git to go back to an earlier version of your project. In this post, I'm going to take a look at some common scenarios where you might want to \"undo\" a change you've made and the best way to do it using Git. Undo a \"public\" change Scenario: You just ran git push , sending your changes to GitHub, now you realize there's a problem with one of those commits. You'd like to undo that commit. Undo with: git revert <SHA> What's happening: git revert will create a new commit that's the opposite (or inverse) of the given SHA. If the old commit is \"matter\", the new commit is \"anti-matter\"—anything removed in the old commit will be added in the new commit and anything added in the old commit will be removed in the new commit. This is Git's safest, most basic \"undo\" scenario, because it doesn't alter history—so you can now git push the new \"inverse\" commit to undo your mistaken commit. Fix the last commit message Scenario: You just typo'd the last commit message, you did git commit -m \"Fxies bug #42\" but before git push you realized that really should say \"Fixes bug #42\". Undo with: git commit --amend or git commit --amend -m \"Fixes bug #42\" What's happening: git commit --amend will update and replace the most recent commit with a new commit that combines any staged changes with the contents of the previous commit. With nothing currently staged, this just rewrites the previous commit message. Undo \"local\" changes Scenario: The cat walked across the keyboard and somehow saved the changes, then crashed the editor. You haven't committed those changes, though. You want to undo everything in that file—just go back to the way it looked in the last commit. Undo with: git checkout -- <bad filename> What's happening: git checkout alters files in the working directory to a state previously known to Git. You could provide a branch name or specific SHA you want to go back to or, by default, Git will assume you want to checkout HEAD , the last commit on the currently-checked-out branch. Keep in mind: any changes you \"undo\" this way are really gone. They were never committed, so Git can't help us recover them later. Be sure you know what you're throwing away here! (Maybe use git diff to confirm.) Reset \"local\" changes Scenario: You've made some commits locally (not yet pushed), but everything is terrible, you want to undo the last three commits—like they never happened. Undo with: git reset <last good SHA> or git reset --hard <last good SHA> What's happening: git reset rewinds your repository's history all the way back to the specified SHA. It's as if those commits never happened. By default, git reset preserves the working directory. The commits are gone, but the contents are still on disk. This is the safest option, but often, you'll want to \"undo\" the commits and the changes in one move—that's what --hard does. Redo after undo \"local\" Scenario: You made some commits, did a git reset --hard to \"undo\" those changes (see above), and then realized: you want those changes back! Undo with: git reflog and git reset or git checkout What's happening: git reflog is an amazing resource for recovering project history. You can recover almost anything—anything you've committed—via the reflog. You're probably familiar with the git log command, which shows a list of commits. git reflog is similar, but instead shows a list of times when HEAD changed. Some caveats: HEAD changes only. HEAD changes when you switch branches, make commits with git commit and un-make commits with git reset , but HEAD does not change when you git checkout -- <bad filename> (from an earlier scenario—as mentioned before, those changes were never committed, so the reflog can't help us recover those. git reflog doesn't last forever. Git will periodically clean up objects which are \"unreachable.\" Don't expect to find months-old commits lying around in the reflog forever. Your reflog is yours and yours alone. You can't use git reflog to restore another developer's un-pushed commits. So... how do you use the reflog to \"redo\" a previously \"undone\" commit or commits? It depends on what exactly you want to accomplish: If you want to restore the project's history as it was at that moment in time use git reset --hard <SHA> If you want to recreate one or more files in your working directory as they were at that moment in time, without altering history use git checkout <SHA> -- <filename> If you want to replay exactly one of those commits into your repository use git cherry-pick <SHA> Once more, with branching Scenario: You made some commits, then realized you were checked out on master . You wish you could make those commits on a feature branch instead. Undo with: git branch feature , git reset --hard origin/master , and git checkout feature What's happening: You may be used to creating new branches with git checkout -b <name> —it's a popular short-cut for creating a new branch and checking it out right away—but you don't want to switch branches just yet. Here, git branch feature creates a new branch called feature pointing at your most recent commit, but leaves you checked out to master . Next, git reset --hard rewinds master back to origin/master , before any of your new commits. Don't worry, though, they are still available on feature . Finally, git checkout switches to the new feature branch, with all of your recent work intact. Branch in time saves nine Scenario: You started a new branch feature based on master , but master was pretty far behind origin/master . Now that master branch is in sync with origin/master , you wish commits on feature were starting now, instead of being so far behind. Undo with: git checkout feature and git rebase master What's happening: You could have done this with git reset (no --hard , intentionally preserving changes on disk) then git checkout -b <new branch name> and then re-commit the changes, but that way, you'd lose the commit history. There's a better way. git rebase master does a couple of things: First it locates the common ancestor between your currently-checked-out branch and master . Then it resets the currently-checked-out branch to that ancestor, holding all later commits in a temporary holding area. Then it advances the currently-checked-out-branch to the end of master and replays the commits from the holding area after master 's last commit. Mass undo/redo Scenario: You started this feature in one direction, but mid-way through, you realized another solution was better. You've got a dozen or so commits, but you only want some of them. You'd like the others to just disappear. Undo with: git rebase -i <earlier SHA> What's happening: -i puts rebase in \"interactive mode\". It starts off like the rebase discussed above, but before replaying any commits, it pauses and allows you to gently modify each commit as it's replayed. rebase -i will open in your default text editor, with a list of commits being applied, like this: The first two columns are key: the first is the selected command for the commit identified by the SHA in the second column. By default, rebase -i assumes each commit is being applied, via the pick command. To drop a commit, just delete that line in your editor. If you no longer want the bad commits in your project, you can delete lines 1 and 3-4 above. If you want to preserve the contents of the commit but edit the commit message, you use the reword command. Just replace the word pick in the first column with the word reword (or just r ). It can be tempting to rewrite the commit message right now, but that won't work— rebase -i ignores everything after the SHA column. The text after that is really just to help us remember what 0835fe2 is all about. When you've finished with rebase -i , you'll be prompted for any new commit messages you need to write. If you want to combine two commits together, you can use the squash or fixup commands, like this: squash and fixup combine \"up\"—the commit with the \"combine\" command will be merged into the commit immediately before it. In this scenario, 0835fe2 and 6943e85 will be combined into one commit, then 38f5e4e and af67f82 will be combined together into another. When you select squash , Git will prompt us to give the new, combined commit a new commit message; fixup will give the new commit the message from the first commit in the list. Here, you know that af67f82 is an \"ooops\" commit, so you'll just use the commit message from 38f5e4e as is, but you'll write a new message for the new commit you get from combining 0835fe2 and 6943e85 . When you save and exit your editor, Git will apply your commits in order from top to bottom. You can alter the order commits apply by changing the order of commits before saving. If you'd wanted, you could have combined af67f82 with 0835fe2 by arranging things like this: Fix an earlier commit Scenario: You failed to include a file in an earlier commit, it'd be great if that earlier commit could somehow include the stuff you left out. You haven't pushed, yet, but it wasn't the most recent commit, so you can't use commit --amend . Undo with: git commit --squash <SHA of the earlier commit> and git rebase --autosquash -i <even earlier SHA> What's happening: git commit --squash will create a new commit with a commit message like squash! Earlier commit . (You could manually create a commit with a message like that, but commit --squash saves you some typing.) You can also use git commit --fixup if you don't want to be prompted to write a new commit message for the combined commit. In this scenario, you'd probably use commit --fixup , since you just want to use the earlier commit's commit message during rebase . rebase --autosquash -i will launch an interactive rebase editor, but the editor will open with any squash! and fixup! commits already paired to the commit target in the list of commits, like so: When using --squash and --fixup , you might not remember the SHA of the commit you want to fix—only that it was one or five commits ago. You might find using Git's &#94; and ~ operators especially handy. HEAD&#94; is one commit before HEAD . HEAD~4 is four commits before HEAD - or, altogether, five commits back. Stop tracking a tracked file Scenario: You accidentally added application.log to the repository, now every time you run the application, Git reports there are unstaged changes in application.log . You put *.log in the .gitignore file, but it's still there—how do you tell git to to \"undo\" tracking changes in this file? Undo with: git rm --cached application.log What's happening: While .gitignore prevents Git from tracking changes to files or even noticing the existence of files it's never tracked before, once a file has been added and committed, Git will continue noticing changes in that file. Similarly, if you've used git add -f to \"force\", or override, .gitignore , Git will keep tracking changes. You won't have to use -f to add it in the future. If you want to remove that should-be-ignored file from Git's tracking, git rm --cached will remove it from tracking but leave the file untouched on disk. Since it's now being ignored, you won't see that file in git status or accidentally commit changes from that file again. That's how to undo anything with Git. To learn more about any of the Git commands used here, check out the relevant documentation:","url":"http://www.ciandcd.com/how-to-undo-almost-anything-with-git.html","tags":"scm","title":"How to undo (almost) anything with Git"},{"text":"from:https://www.gitlab.com/2015/06/08/implementing-gitlab-ci-dot-yml/ Implementing .gitlab-ci.yml We wrote about why we're replacing GitLab CI jobs with a .gitlab-ci.yml file. As we've started on implementing this large change, we wanted to share the details of that process with you and would love to hear what you think. To recap the previous article : currently you are required to write out your CI jobs in GitLab CI's interface. We're replacing this with a single file .gitlab-ci.yml , that you place in the root of your repository. Schema change Currently, on a push to GitLab, GitLab sends a web-hook to the CI Coordinator. The coordinator creates a build based on the jobs that are defined in its UI, which can then be executed by the connected Runners. In the new schema, GitLab sends the web-hook and the .gitlab-ci.yml contents to the CI Coordinator, which creates builds based on the yml file. In turn, these builds are executed by the Runners as before. Migrating to new style Keeping two different ways of doing things would be a strain on development and support, not to mention confusing. So we're not just deprecating the old style of defining jobs, we're removing it entirely and will migrate existing jobs. Upon upgrading your existing jobs defined in the GitLab CI Coordinator will be converted into a YAML file with the new syntax. You can download this file at any time from the project settings. When the GitLab webhook triggers and doesn't transmit the content from .gitlab-ci.yml , the coordinator will use the converted YAML file instead. This makes migrating to the new style very easy. You can start by simply copy-pasting the contents of the converted YAML file to the root of your repository. Existing projects will continue to build successfully, yet new projects do not have the option to use anything else. An example .gitlab-ci.yml To get an idea of how the .gitlab-ci.yml will look, we've prepared an example for a Ruby on Rails project (such as GitLab itself). Of course, this is due to change as we're still working on this. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # Refs to skip skip_refs: \"deploy*\" # Run before each script # Refs to skip skip_refs: \"deploy*\" # Run before each script before_script: - export PATH=$HOME/bin:/usr/local/bin:/usr/bin:/bin - gem install bundler - cp config/database.yml.mysql config/database.yml - cp config/gitlab.yml.example config/gitlab.yml - touch log/application.log - touch log/test.log - bundle install --without postgres production --jobs $(nproc) - \"bundle exec rake db:create RAILS_ENV=test\" # Parallel jobs, each line is a parallel build jobs: - script: \"rake spec\" runner: \"ruby,postgres\" name: \"Rspec\" - script: \"rake spinach\" runner: \"ruby,mysql\" name: \"Spinach\" tags: true branches: false # Parallel deploy jobs on_success: - \"cap deploy production\" - \"cap deploy staging\" UPDATE Dmitriy and Sytse spend some time thinking about file syntax. Scripting should be simple and memorable. Thats why we come with better proposal: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 before_script: - gem install bundler - bundle install - bundle exec rake db:create rspec: test: \"rake spec\" tags: - ruby - postgres only: - branches spinach: test: \"rake spinach\" tags: - ruby - mysql except: - tags staging: deploy: \"cap deploy stating\" tags: - capistrano - debian except: - stable production: deploy: - cap deploy production - cap notify tags: - capistrano - debian only: - master - /&#94;deploy-.*$/ Contribute GitLab is nothing without its community. Contribute or follow the development in the GitLab CI repository . Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","url":"http://www.ciandcd.com/implementing-gitlab-ciyml.html","tags":"scm","title":"Implementing .gitlab-ci.yml"},{"text":"from:https://www.gitlab.com/2015/06/05/how-gitlab-uses-unicorn-and-unicorn-worker-killer/ How GitLab uses Unicorn and unicorn-worker-killer We just wrote some new documentation on how Gitlab uses Unicorn and unicorn-worker-killer, available on doc.gitlab.com but also included below. We would love to hear from the community if you have other questions so we can improve this documentation resource! Update 19:29 CEST: made link to doc.gitlab.com more specific. Understanding Unicorn and unicorn-worker-killer Unicorn GitLab uses Unicorn , a pre-forking Ruby web server, to handle web requests (web browsers and Git HTTP clients). Unicorn is a daemon written in Ruby and C that can load and run a Ruby on Rails application; in our case the Rails application is GitLab Community Edition or GitLab Enterprise Edition. Unicorn has a multi-process architecture to make better use of available CPU cores (processes can run on different cores) and to have stronger fault tolerance (most failures stay isolated in only one process and cannot take down GitLab entirely). On startup, the Unicorn ‘master' process loads a clean Ruby environment with the GitLab application code, and then spawns ‘workers' which inherit this clean initial environment. The ‘master' never handles any requests, that is left to the workers. The operating system network stack queues incoming requests and distributes them among the workers. In a perfect world, the master would spawn its pool of workers once, and then the workers handle incoming web requests one after another until the end of time. In reality, worker processes can crash or time out: if the master notices that a worker takes too long to handle a request it will terminate the worker process with SIGKILL (‘kill -9'). No matter how the worker process ended, the master process will replace it with a new ‘clean' process again. Unicorn is designed to be able to replace ‘crashed' workers without dropping user requests. This is what a Unicorn worker timeout looks like in unicorn_stderr.log . The master process has PID 56227 below. 1 2 3 4 [2015-06-05T10:58:08.660325 #56227] ERROR -- : worker=10 PID:53009 timeout (61s > 60s), killing [2015-06-05T10:58:08.699360 #56227] ERROR -- : reaped #<Process::Status: pid 53009 SIGKILL (signal 9)> worker=10 [2015-06-05T10:58:08.708141 #62538] INFO -- : worker=10 spawned pid=62538 [2015-06-05T10:58:08.708824 #62538] INFO -- : worker=10 ready Tunables The main tunables for Unicorn are the number of worker processes and the request timeout after which the Unicorn master terminates a worker process. See the omnibus-gitlab Unicorn settings documentation if you want to adjust these settings. unicorn-worker-killer GitLab has memory leaks. These memory leaks manifest themselves in long-running processes, such as Unicorn workers. (The Unicorn master process is not known to leak memory, probably because it does not handle user requests.) To make these memory leaks manageable, GitLab comes with the unicorn-worker-killer gem . This gem monkey-patches the Unicorn workers to do a memory self-check after every 16 requests. If the memory of the Unicorn worker exceeds a pre-set limit then the worker process exits. The Unicorn master then automatically replaces the worker process. This is a robust way to handle memory leaks: Unicorn is designed to handle workers that ‘crash' so no user requests will be dropped. The unicorn-worker-killer gem is designed to only terminate a worker process in between requests, so no user requests are affected. This is what a Unicorn worker memory restart looks like in unicorn_stderr.log. You see that worker 4 (PID 125918) is inspecting itself and decides to exit. The threshold memory value was 254802235 bytes, about 250MB. With GitLab this threshold is a random value between 200 and 250 MB. The master process (PID 117565) then reaps the worker process and spawns a new ‘worker 4' with PID 127549. 1 2 3 4 5 [2015-06-05T12:07:41.828374 #125918] WARN -- : #<Unicorn::HttpServer:0x00000002734770>: worker (pid: 125918) exceeds memory limit (256413696 bytes > 254802235 bytes) [2015-06-05T12:07:41.828472 #125918] WARN -- : Unicorn::WorkerKiller send SIGQUIT (pid: 125918) alive: 23 sec (trial 1) [2015-06-05T12:07:42.025916 #117565] INFO -- : reaped #<Process::Status: pid 125918 exit 0> worker=4 [2015-06-05T12:07:42.034527 #127549] INFO -- : worker=4 spawned pid=127549 [2015-06-05T12:07:42.035217 #127549] INFO -- : worker=4 ready One other thing that stands out in the log snippet above, taken from Gitlab.com, is that ‘worker 4' was serving requests for only 23 seconds. This is a normal value for our current GitLab.com setup and traffic. The high frequency of Unicorn memory restarts on some GitLab sites can be a source of confusion for administrators. Usually they are a red herring . Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","url":"http://www.ciandcd.com/how-gitlab-uses-unicorn-and-unicorn-worker-killer.html","tags":"scm","title":"How GitLab Uses Unicorn and Unicorn-worker-killer"},{"text":"from:http://www.perforce.com/blog/150514/p4python-goes-pip-0 In recent years Python has changed its package manager strategy, and the result is pip . Pip is a powerful package manager that simplifies the creation and consumption of Python packages, turning the Python Package Index into a hub of an ever-growing number of useful packages. P4Python always had to stay away from the package index because it requires binary builds for some platforms. With the advent of the wheel format, this has changed. Wheels are Python packages that can contain binary builds on Windows and OSX, allowing package creators to precompile their packages. P4Python 2015.1 has been uploaded to the Python Package Index. For you this means installing the latest release of P4Python becomes a simple: pip install p4python Easier, isn't it? However, there are a few preparations you need to make before you can run this command the first time successfully: You need to have the right version of Python installed: 2.7, 3.3 or 3.4. Python 2.6 is supported but we have not uploaded binaries for it. You need to install pip. It comes with Python 3.4 and 2.7.9 automatically; everyone else will need to install it, for example from here , and make sure the pip executable is in your PATH. There is currently no binary wheel format available for Linux, so pip will download the source code of P4Python and attempt to build it. This requires ‘python-dev' and ‘build-essential' installed on Debian-based distributions (using apt) and the equivalent on RPM-based (using yum). In order to build P4Python automatically from Pip, setup.py has also gained some new tricks. First of all, it now uses setuptools and not distutils, so you need to have setuptools installed if you want to build it on, say, Python 2.6. If you run setup.py without the –apidir option, setup will now go off and attempt to download the correct P4API binaries from the Perforce FTP site for your platform. The API will be downloaded and unpacked into the temp directory and used automatically by the build process. You can still download the source or the packages, of course, and install P4Python the traditional way if you prefer. I found pip amazingly simple to use, and I hope it will make your life a lot easier when installing P4Python. As usual, if you have any issues or requests, let us know in Perforce Support or ping me on my Twitter handle @p4sven . Happy hacking.","url":"http://www.ciandcd.com/p4python-goes-pip.html","tags":"scm","title":"P4Python goes pip"},{"text":"from:https://github.com/blog/2017-announcing-github-japan GitHub <3s Japan, and today we're excited to announce the formation of GitHub Japan G.K., a subsidiary of GitHub, Inc. Our new office in Tokyo is our first official office outside of the United States. The Japanese developer community GitHub couldn't exist without the Japanese open source community — after all, our site is built on Rails , which is built on Ruby , an open source project started in Japan . Japan has historically been one of the most active countries on GitHub, ranking in the top 10 countries visiting github.com since GitHub was founded in 2008. The thriving software community in Japan keeps growing; in 2014, activity on github.com from Japan increased more than 60 percent from the previous year. GitHub Enterprise in Japan In addition to an active local open source community, Japanese businesses including Hitachi Systems , CyberAgent and GREE are collaborating and building the best software with GitHub Enterprise. To that end, we're also announcing that we'll be partnering locally to provide Japanese language technical support for GitHub Enterprise users, as well as the ability to pay in Japanese Yen in Japan. Stay up to date Keep tabs on everything happening in our Tokyo office by following @GitHubJapan on Twitter and checking out github.co.jp . We'd also love to see you at our meetup in Osaka on June 6 . Yoroshiku-Onegaiitashimasu! 初めましてGitHub Japanです！ GitHub <3s Japan, 本日、私達はGitHub, Inc.の子会社である、GitHub Japan合同会社の設立の発表ができる事をとても光栄に思っております 。東京にオープンした新しいオフィスは、米国外でオープンする初のオフィスになります。 〜日本のデベロッパー・コミュニティにむけて〜 GitHubは、日本で生まれたオープンソース・プロジェクトのRubyで作られたRailsというフレームワークによって開発されており、日本のオープンソース・コミュニティーなしではGitHubは存在しえないと言っては過言ではない程、日本とGitHubは深いつながりがあります。 また、2008年のGitHub設立当初から、日本からgithub.comへのアクセス数は上位10ヶ国に入り続けてきました。そして、日本のユーザーは現在も増加し続けており、2014年の日本ユーザーのGitHub上でのアクティビティは、前年比60％も増加しました。 ～「GitHub Enterprise」の日本展開～ GitHubは広く開かれた開発を支援するオープンソース・プラットフォーム以外にも、全世界で企業向けに「GitHub Enterprise」を提供して参りました。これまで「GitHub Enterprise」は、英語でのサポートのみだったにもかかわらず、日本国内では、 株式会社日立システムズ 、 ヤフー株式会社 、 株式会社サイバーエージェント や グリー株式会社 などの大手企業をはじめとして、多くの先進的な企業にご活用頂いて参りました。そして今回、さらに迅速できめ細かいサービスやサポートを提供するため、GitHubは大手代理店と業務提携を行い、日本語による「GitHub Enterprise」の法人向け導入サポートも開始しました。この販売パートナー提携により、円建て決済や日本語のテクニカルサポートも可能になります。 GitHub の最新の情報を得よう 東京オフィスで何が起こっているか知る為にはTwitterで @GitHubJapan をフォローするか、 github.co.jp にアクセスしてくださいね。そして 大阪で開催されるuser meetup にも是非お越しください！ お待ちしております！. よろしくお願い致します！","url":"http://www.ciandcd.com/announcing-github-japan.html","tags":"scm","title":"Announcing GitHub Japan"},{"text":"from:https://www.gitlab.com/2015/06/04/gitlab-dot-com-outage-on-2015-05-29/ GitLab.com outage on 2015-05-29 GitLab.com suffered an outage from 2015-05-29 01:00 to 2015-05-29 02:34 (times in UTC). In this blog post we will discuss what happened, why it took so long to recover the service, and what we are doing to reduce the likelihood and impact of such incidents. Background GitLab.com is provided and maintained by the team of GitLab B.V., the company behind GitLab. On 2015-05-02 we performed a major infrastructure upgrade, moving GitLab.com from a single server to a small cluster of servers, consisting of a load balancer (running HAproxy), three workers (NGINX/Unicorn/Sidekiq/gitlab-shell) and a backend server (PostgreSQL/Redis/NFS). This new infrastructure configuration improved the responsiveness of GitLab.com, at the expense of having more moving parts. GitLab.com is backed up using Amazon EBS snapshots. To protect against inconsistent snapshots our backup script ‘freezes' the filesystem on the backend server with fsfreeze prior to making EBS snapshots, and ‘unfreezes' the filesystem immediately after. Timeline Italic comments below are written with the knowledge of hindsight 1:00 The GitLab.com backup script is activated by Cron on the backend server. For unknown reasons, the backup script hangs/crashes before or during the ‘unfreeze' of the filesystem holding all user data. 1:07 Our on-call engineer is paged by Pingdom . The on-call engineer tries to diagnose the issue on the worker servers but is unable to diagnose the problem. The issue was on the backend server, not on the workers. 1:30 The on-call engineer decides to call in more help. The other team members with access and knowledge to resolve the issue are all in Europe at this time, where it is 3:30/4:30am. 1:45 A second engineer in Europe has been woken up and takes the lead on the investigation of the outage. More workers are rebooted because they appear to be stuck. It becomes apparent that the workers cannot mount the NFS share which holds all Git repository data. 1:51 One of the engineers notices that the load on the backend server is more than 150. A normal value would be less than 5. 2:10 The engineers give up on running commands on the workers to bring the NFS share back, and start investigating the backend server. The engineers discuss whether they should reboot the backend server but they are unsure if it is safe given that this setup is fairly new. 2:21 The engineers reboot the backend server. The reboot is taking a long time. The AWS ‘reboot' command first tries a soft reboot, and only does a hard reboot after a 4-minute timeout. The soft reboot probably hung when it tried to shut down services that were trying to write to the ‘frozen' disk. 2:30 The backend server has rebooted and the engineers regain SSH access to it. The worker servers are able to mount the NFS share now but GitLab.com is still not functioning because the Postgres database server is not responding. One of the engineers restarts Postgres on the backend server. It may have been that Postgres was still busy performing crash recovery. 2:34 Gitlab.com is available again. Root causes Although we cannot explain what went wrong with the backup script it is hard to come to another conclusion that something did go wrong with it. The length of the outage was caused by insufficient training and documentation for our on-call engineers following the infrastructure upgrade rolled out on May 2nd. Next steps We have removed the freeze/unfreeze steps from our backup script. Because this (theoretically) increases the risk of occasional corrupt backups we have added a second backup strategy for our SQL data. In the future we would like to have automatical validation of our GitLab.com backups. The day before this incident we decided the training was our most important priority. We have started to do regular operations drills in one-on-one sessions with all of our on-call engineers. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","url":"http://www.ciandcd.com/gitlabcom-outage-on-2015-05-29.html","tags":"scm","title":"GitLab.com Outage on 2015-05-29"},{"text":"from:https://www.gitlab.com/2015/06/04/note-on-license-expiration-in-gitlab-7-dot-10-dot-5-ee/ Note on license expiration in GitLab 7.10.5 EE If you're upgrading to GitLab Enterprise Edition 7.11, which introduces licenses keys, you're probably planning to upgrade to 7.10.5 first. This way you are able to upload your license key in advance . One of our customers notified us of a faulty description in the license uploader in GitLab 7.10.5. Upon uploading, the license is checked properly, however the text in the license view in the admin page in GitLab will show: While it should look like this: This only occurs in GitLab 7.10.5 and does not affect functionality. The license information is correctly shown in GitLab 7.11 and up. If you have any questions or comments do not hesitate to comment below or contact support. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","url":"http://www.ciandcd.com/note-on-license-expiration-in-gitlab-7105-ee.html","tags":"scm","title":"Note on License Expiration in GitLab 7.10.5 EE"},{"text":"from:https://github.com/blog/2018-atom-at-codeconf-2015 CodeConf is coming June 25 & 26 to Nashville and will feature the best that the open source community has to offer. We're excited to share that there will be several talks about the Atom ecosystem presented for your enjoyment and edification, kicked off by GitHub CEO Chris Wanstrath . Speakers will include: @bolinfest talking about Nuclide @lee-dohm talking about the Atom community @paulcbetts talking about Slack and Electron We will also be hosting an Atom workshop led by Nathan Sobo , and a lounge where you will be able to meet with the core team and hack on Atom together. Grab your CodeConf and workshop tickets now and we'll see you there in Nashville!","url":"http://www.ciandcd.com/atom-at-codeconf-2015.html","tags":"scm","title":"Atom at CodeConf 2015"},{"text":"from:http://www.perforce.com/blog/150602/hear-forrester-devops-security-analysts-live-webinar-june-16th LIVE WEBINARS on Tuesday, June 16th - Get Best Practices for Secure DevOps from Forrester Analysts The advantages of DevOps and Continuous Delivery have been much touted the last several years: faster application delivery, better product quality, cost savings… the list goes on. What have gotten less attention are the security risks these new practices can place on your intellectual property. Engineering and Operations teams need to understand these security implications. Similarly, Security teams need to better understand modern release practices. Above all, these teams need to work better together. And that's why we're hosting a webinar featuring two principal analysts from Forrester Research who come at this issue from different vantage points. Kurt Bittner focuses on application development and delivery, while Rick Holland focuses on security and risk. During our webinar, Kurt and Rick will share… Perspectives on how modern product delivery practices introduce additional security threats to intellectual property Insights on how DevOps and Security teams can work better together Best practices for bringing greater security to product development and delivery So here it is… a great opportunity to get your Engineering, DevOps and Security teams on the same page. Book a conference room together. Jointly decide what questions to ask during the live Q&A. We look forward to seeing you on our webinar! Achieving Secure DevOps: Overcoming the Risks of Modern Service Delivery LIVE WEBINARS on Tuesday, June 16th REGISTER NOW","url":"http://www.ciandcd.com/hear-from-forrester-devops-and-security-analysts-in-live-webinar-june-16th.html","tags":"scm","title":"Hear from Forrester DevOps and Security Analysts in Live Webinar June 16th"},{"text":"from:https://github.com/blog/2014-filter-pull-requests-by-status When we shipped the new GitHub Issues , we made it easy to scope lists of Issues and Pull Requests with filters like author, date, mentions, and team mentions. With the new status: filter you can now filter the Pull Requests in your repositories by combined status . If you're taking advantage of the Status API , or using an integration that does, try out the new filters: status:success Only pull requests with all successful statuses status:failure Only pull requests that have statuses in the failure or error state status:pending Only pull requests with no statuses or at least one status in the pending state","url":"http://www.ciandcd.com/filter-pull-requests-by-status.html","tags":"scm","title":"Filter Pull Requests by Status"},{"text":"from:https://github.com/blog/2015-focus-on-your-changes-in-github-for-windows GitHub for Windows now makes it even easier to see everything local to your machine, whether it's uncommitted changes or commits you haven't synced yet. One of the things you'll notice when creating commits is the new, compact list of changed files in your working directory. GitHub for Windows shows the number of files that a commit changed and lets you drill down to see what changed in a given file. The updated branch selector now groups your recently used branches so that you can jump straight back in to what you were doing before that pesky hotfix distracted you. We've given branch creation a dedicated place in the toolbar. As a bonus, you can pick which branch to base the new one off. Finally, you can collapse the repository list to reclaim some screen space. If you have GitHub for Windows installed it will automatically update to the latest version. If you don't have it installed, download GitHub for Windows from windows.github.com .","url":"http://www.ciandcd.com/focus-on-your-changes-in-github-for-windows.html","tags":"scm","title":"Focus on your changes in GitHub for Windows"},{"text":"from:https://github.com/blog/2016-support-lgbtq-tech-organizations-with-the-pridetocat-shirt With the purchase of the Pridetocat Shirt you will be assisting Lesbians Who Tech , Maven , and Trans*H4CK to further their work. All proceeds from sales will be donated to these organizations that are helping educate, connect and empower LGBTQ people in tech. This limited edition shirt is available in the GitHub Shop until August 31st. More info about the LGBTQ tech organizations that benefit from the purchase of this shirt: Lesbians Who Tech Lesbians Who Tech is a global community of 9,000 queer women in tech. It exists to provide value to queer women in tech, a demographic that is rarely represented in both the tech community and the LGBTQ community. Trans*H4CK Trans*H4CK is a hackathon and speaker series that tackles social problems by developing new and useful open source tech products that benefit the trans and gender non-conforming communities, while bringing visibility to transgender tech innovators and entrepreneurs. Maven Maven partner with local LGBTQA youth serving organizations and LGBTQA tech professionals to provide free tech camps, workshops, Game Jams/hackathons for the queer youth community.","url":"http://www.ciandcd.com/support-lgbtq-tech-organizations-with-the-pridetocat-shirt.html","tags":"scm","title":"Support LGBTQ tech organizations with the Pridetocat Shirt"},{"text":"from:http://www.perforce.com/blog/150601/five-requirements-securing-protecting-intellectual-property In this age of industrial espionage, insider theft and advanced cyber threats, it is becoming increasingly difficult to secure intellectual property (IP). The stakes are high. In fact, the IP Commission Report estimates annual losses of $300B attributed to IP theft in the United States alone. A Kroll Advisory Systems white paper describes how industrial espionage and insider attacks were responsible for the loss of trade secrets valued at $300M at Dow Chemical; $225M in illicit proceeds of a DuPont competitor that obtained DuPont's Kevlar trade secrets; theft of Space Shuttle, jet and rocket design trade secrets from Rockwell and Boeing and technical IP theft from Motorola. Many, if not most, large organizations are spread across multiple geographies. Their team members generally include not only internal staff but also contractors, service providers and business partners. They typically use an source code management system and content collaboration platform to create and store all their IP. However, collaboration and security are often at odds with each other. Shared source code, designs, specifications, and digital assets can be easy targets for IP theft and leaks. To secure and protect your IP, your collaboration platform must address the following five requirements: 1. Flexible Authentication Industry-standard authentication for LDAP and Active Directory is the minimum for any enterprise platform. More secure environments require custom authentication methods (e.g., two-factor authentication) to reduce the risk of stolen or compromised user credentials. When working with external contractors, service providers or business partners, it's also important to be able to enforce the use of unique user credentials and to avoid using a single shared username or password. 2. Fine-Grained Access Control Project collaboration with internal and external teams requires file-level access control. Most users need access to just the intellectual property they are working on. Being able to employ IP address-specific access and limit access to only authorized locations or users in different regions may be useful when dealing with partner companies or external service providers. For example, this would allow limiting access to external collaborators to a specific section of a repository or set of files that they need to perform their job, based on the network IP address of those external collaborators. 3. Strong Password Security Setting strong password guidelines and enabling password-strengthening options, such as minimum length, maximum login attempts, password reset upon login and password expiration time frames, help reduce the risk of stolen or compromised user credentials. In addition, preventing the storage of passwords in configuration files, the Windows registry or other parts of the system used for authentication may further improve password security. 4. Detailed Audit Logs/Access Tracking Keeping detailed audit logs is useful in determining who accessed which corporate assets when. Audit logs are valuable for monitoring access to corporate assets to determine potential data misuse, security breaches or data theft. Detailed audit logs are also necessary for forensic purposes, audits and regulatory compliance. 5. Automated Threat Detection Even with all security measures in place, you have to assume that your systems have been breached and your IP is always at risk. Your last line of defense must include apparatus for monitoring all IP-related activity within your environment to detect suspicious events. Many existing SIEM security tools attempt to identify security threats using data from traditional security products (e.g., firewall, IDS or anti-malware) or even newer data sources (e.g., OS logs, LDAP/AD, badge data, DNS and/or email or web servers). These tools examine and process massive amounts of data and quickly identify anomalous behavior or outliers that could represent security threats. But this step is insufficient. To fully protect your valuable IP you must track activity and detect threats at the origin—within your collaboration environment itself. A unified approach to securing and protecting your intellectual property addresses these challenges in a comprehensive manner, while informing your Security Operations Center and integrating with your existing security infrastructure. To learn more download our white paper: A Unified Approach to Securing and Protecting IP. READ NOW","url":"http://www.ciandcd.com/five-requirements-for-securing-and-protecting-your-intellectual-property.html","tags":"scm","title":"Five Requirements for Securing and Protecting Your Intellectual Property"},{"text":"from:http://www.perforce.com/blog/150528/helix-versioning-engine-hg-users As I said recently when offering advice for Git users , the new DVCS features of Perforce Helix are easy to use, but making the transition from another system always involves climbing a learning curve. The purpose of today's post is to provide some guidance specifically for Mercurial (Hg) users. Concepts Let's start with some basics. Hg supplies only DVCS features and was developed with a particular eye toward simplifying branching and merging. In contrast, Helix was developed with a more broadly based feature set for a wider variety of workflows/needs. Yet Hg and Helix actually have more in common than one might otherwise expect. Status For starters, work-in-progress is handled similarly by Hg and Helix. Modifications to versioned files are part of your next commit by default, so the only operations an Hg user typically expects to \"tell\" the system about are file adds and deletes. This is usually handled via the \"hg add\" and \"hg remove\" commands, but the \"hg addremove\" command is a third alternative. It adds all new files and removes all missing files previously tracked with a single command. I mention this because the Helix reconcile command achieves a similar end result. The main difference is that while Hg includes modifications automatically, Helix does not. Helix maintains all work-in-progress via a concept known as a changelist, and in particular your DVCS work-in-progress is maintained in a changelist named \"default\". Hg users largely don't need to worry about this. Simply run the reconcile command to prepare for a commit, and it will include all adds, deletes, and modifications in your default changelist. Shelving Hg users have long enjoyed shelving as a feature, previously as an extension and natively since the v2.8 release. Shelving in Helix is similar but differs in that Hg shelves can be named whereas Helix shelves are a unary feature of a changelist, which is numbered and has a description instead. For example, the default changelist has one shelf where work in progress will be saved automatically when switching branches or what not. Paths In Hg parlance, a \"path\" is a \"pointer\" from/to which content may be cloned/pushed, whereas Helix users know these as \"remote specifications\" or just \"remotes\" for short. Further, an Hg \"path\" offers only a small subset of what Helix offers. A Helix remote specification is much more powerful insofar as it includes the ability to map content very selectively. [1] Folders and files may be flexibly remapped, completely changing the hierarchical structure for the local repository as needed. And unlike Hg, a single Helix local repository may also include content from multiple remote specifications (all of whose content may be re-mapped). But that's an advanced feature for a future post. It suffices for now that remotes are much more advanced with Helix. Commit The Hg notion of commit is also quite similar to the Helix submit, with one important difference. Hg users are accustomed to their commits having two separate IDs: (1) a monotonically increasing integer unique and valid only within the scope of the current, local repository, and (2) a forty-character hash value that is both unique and valid in every version of the repository. In contrast, each Helix changelist, submitted or pending, is identified by a single, monotonically increasing integer number. Many developers won't care about these details, but DevOps personnel tasked with tying revision IDs to other integrated systems should be aware of them. Defaults There are also minor differences in the naming conventions. Hg users are accustomed to their default path for push and their default branch name both being \"default\". In contrast, Helix uses \"origin\" for its default remote name and \"main\" for the default branch name. This can be a source of confusion when Hg users first get started and can't seem to switch back to the \"default\" branch; just use \"main\" instead and you'll be fine. Commands Having vaulted the conceptual hurdles, let's look at the commands for the most common tasks. Initializing a new repository requires a \"p4 init\" rather than an \"hg init\", while the usual \"hg addremove\" followed by \"hg commit\" is instead \"p4 rec\" followed by \"p4 submit\". All along the way \"hg status\" is replaced by \"p4 status\". Many of the other commands are equally similar. Take a look: Task Hg Command Helix Equivalent Stage your changes addremove reconcile (rec for short) List all branches branches switch -l Branch and switch to it branch newBranch switch -c newStream Create an empty branch (no equivalent) switch -cm newStream Switch to a branch up branchName switch streamName Clone a repository clone repository clone -p host:port -r remote Clone part of a repository (no equivalent) clone -p host:port -f fileSpecification [2] Commit your work commit submit Initialize a repository init init Merge work from a branch merge branchName merge --from streamName Get latest updates pull update [3] fetch -u -r remoteName -S streamName Push all local commits push push Rewrite history rebase unsubmit / resubmit [4] List all remotes paths remotes Create a new remote (no equivalent) [5] remote newRemoteName View your changes status status As you can see, many of the commands are identical while others are a little different, but those provide what you need to get started. In fact, the above \"cheat sheet\" nicely serves the majority of day-to-day use cases. In conclusion, Hg users should feel right at home with Helix with very little effort, so why not try the industry's first DVCS built with the enterprise in mind? [1] For more information about remote specifications see: http://www.perforce.com/perforce/doc.current/manuals/dvcs/_create_a_remote.html [2] The Helix clone command may be used in a way that Hg cannot: to clone only part of a given shared server, though this has the side effect of bringing it all down in a single branch. [3] Hg separates the act of pulling new work from updating the local repository for philosophical reasons, so you have to execute its two commands compared to Helix' fetch command. [4] For more information about changing local history see: http://www.perforce.com/blog/150401/neatness-counts-cleaning-history-sharing-work [5] Hg users have to edit the .hg/.hgrc file manually and add a new entry to the \"[paths]\" section. All Perforce Version Management & Collaboration Products Are Free for 20 Users. TRY IT NOW","url":"http://www.ciandcd.com/helix-versioning-engine-for-hg-users.html","tags":"scm","title":"Helix Versioning Engine for Hg Users"},{"text":"from:https://github.com/blog/2010-diversity-partners-for-codeconf-2015 When a diverse set of presenters and participants comes together for a conference, everyone benefits from the variety of experiences, perspectives and voices in the room. We realize, however, that ticket costs can sometimes be prohibitive for individuals from underrepresented groups. That's why we've partnered with six fantastic organizations to distribute CodeConf tickets to their members. Each one has a mission to connect, support and/or educate people from backgrounds underrepresented in tech. This will help us build a diverse audience and a great experience for everyone. Our partner groups have shared a little more about their work and upcoming projects, as well as details of how they're distributing their CodeConf tickets. Read on to get inspired, involved, and potentially grab a ticket for yourself, and remember too that you can help on a personal level by choosing to purchase the \"scholarship\" ticket option on the registration page . Black Girls CODE Black Girls CODE is an international non-profit that empowers young women of color to enter the tech space as builders and creators by introducing them to coding and technology. www.blackgirlscode.org Upcoming events: June 19th - 21st: All Girls Hackathon, Oakland June 20th: Oracle Academy's Greenfoot brought to you by Black Girls CODE, Memphis June 20th: Introduction to Javascript Workshop, New York July 24th-26th: All Girls Hackathon, New York July: 1 week summer camps, Raleigh, NC and Washington D.C. August: 1 week summer camps, Bay Area and New York Ticket Details : We will be distributing our tickets directly to the Black Girls CODE network. Coding While Black Coding While Black is headquartered in Chicago; our focus is code education, professional development, and entrepreneurship. We welcome blacks in technology from around the globe to become active members of a growing community that supports, encourages, and connects black technology professionals. Members can post articles, share events, find other black technology professionals to connect with, and enjoy community features. www.codingwhileblack.com Ticket details : Coding While Black is excited to partner in promoting CodeConf to a diverse audience. We will be distributing the tickets by sharing them with members who have volunteered with us and/or have been active participants in our community. Girl Geek Dinner Nashville The Nashville chapter of Girl Geek Dinner was founded with the goal to encourage and inspire Nashville's young women and girls to pursue technology careers. Girl Geek Dinners have grown into an international movement. The ask is simple to sponsors — buy dinner and drinks for girl geeks, invite speakers and encourage networking amongst the girl geeks. www.ggdnashville.com Upcoming events: June 5th: Eventbrite is hosting our next Girl Geek Dinner where they are flying in 3 Software Engineers and a Product Manager to Nashville to give lighting tech talks. Third Thursday of every month: Code & Pinot night is a great opportunity for beginner programmers to come out and get a taste of what programming is like. Bring your favorite wine and learn to code! Food provided. Ticket details : We plan to distribute the tickets to our Girl Geeks through a coding contest and drawing. Coding contest: the first to give the correct answer will get a ticket. Drawing: we will randomly select a winner from contest entries. Lesbians Who Tech Lesbians Who Tech is a global community of 9,000 queer women in tech. It exists to provide value to queer women in tech, a demographic that is rarely represented in both the tech community and the LGBTQ community. We've hosted events in 22 cities worldwide and focus on connecting our community, increasing visibility and increasing women in tech and leadership positions. www.lesbianswhotech.org Upcoming events: Ticket details : We'll be offering the tickets through Lesbians Who Tech chapters in the Midwest Rails Girls Atlanta Rails Girls Atlanta is an encouraging place for women to take the plunge into learning to code. We host monthly meetups and socials where dev-minded ladies can ask questions, learn from others, and get the support they need to be successful. www.meetup.com/Rails-Girls-Atlanta Upcoming events: June 1st: All the Nerdy Ladies Social at Joystick. No agenda, just a time to visit and catch up with other nerdy lady types. Ticket details : We're planning to raffle off the tickets at our May meetup. Trans*H4CK Trans*H4CK is a hackathon and speaker series that tackles social problems by developing new and useful open source tech products that benefit the trans and gender non-conforming communities, while bringing visibility to transgender tech innovators and entrepreneurs. We are planning to launch an online hackathon space this year--stay tuned! http://www.transhack.org Ticket details : We will be distributing our CodeConf tickets through the Trans*H4CK network.","url":"http://www.ciandcd.com/diversity-partners-for-codeconf-2015.html","tags":"scm","title":"Diversity Partners for CodeConf 2015"},{"text":"from:http://www.cnblogs.com/itech/p/4535572.html Many system administrators make a practice of using GNU Screen or tmux to manage jobs running in the terminal. If you have a long-running job that you want to \"detach\" from the terminal, you can simply use your terminal multiplexer to do it. But what if you don't use tmux or Screen, or you just forgot? For those times, there's nohup and disown. Have a long-running job you want to \"detach\" from the terminal? Don't use tmux or Screen? If you haven't started the job yet, nohup is an easy to use option, and if you must stop a job in the middle, there's disown. If you haven't started the job yet, there's nohup. Short for \"no hangup,\" nohup will detach the program from the current shell and send its output to nohup.out. If you quit the shell or whatever, the process will continue to run until it completes. Ah, but what if you forget to use nohup, or if you didn't expect to be leaving the computer but get called away? Then there's disown. The use of disown is a bit more complex. While the command is running, use Ctrl-z to stop it and then use bg to put it in the background. Then you'll use disown %n where n is the job number (jobspec). And, of course, you can find the job number using the jobs command. Run jobs again to verify that the job has been detached -- and you can use ps or top to verify that the job is actually still running.","url":"http://www.ciandcd.com/nohuphe-disown-itech.html","tags":"中文","title":"nohup和disown - iTech"},{"text":"from:https://www.gitlab.com/2015/05/28/gitlab-7-dot-11-dot-4-released/ GitLab 7.11.4 released We've released GitLab 7.11.4 for GitLab CE, EE and CI. It includes the following fixes for CE and EE: Fix rendering of list bullets Force a rel=\"nofollow\" attribute on all external links in markdown For GitLab Enterprise Edition this patch release also fixes a bug in git-annex. This fix was also included in the (unannounced) 7.11.3 patch. Upgrade barometer This is a minor update, without any migrations. No downtime is necessary. Updating To update, check out our update page . Enterprise Edition Interested in GitLab Enterprise Edition? For an overview of feature exclusive to GitLab Enterprise Edition please have a look at the features exclusive to GitLab EE . Access to GitLab Enterprise Edition is included with a subscription . No time to upgrade GitLab yourself? A subscription also entitles to our upgrade and installation services. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","url":"http://www.ciandcd.com/gitlab-7114-released.html","tags":"scm","title":"GitLab 7.11.4 Released"},{"text":"from:https://www.gitlab.com/2015/05/28/gitlab-subscription-terms-have-changed/ GitLab subscription terms have changed In an effort to reduce red tape when subscribing to GitLab Enterprise Edition, we've made some changes to our subscription Terms that make them more digestible to potential customers. Here is a summary of the important changes: Grant of License clause that provides a limited, non-exclusive, non-transferable, non-sublicensable license to use GitLab Enterprise Edition; Clarification that suggestions, feedback and code submitted for inclusion in GitLab, becomes the property of GitLab; Clarification on the payment process, including the ‘true-up' payment model; Inclusion of GitLab's warranty that there are no trojans or the like in GitLab EE; A new paragraph relating to US Government matters; A change in governing law from the Netherlands to California, USA. These updated subscription terms do not apply retroactively. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","url":"http://www.ciandcd.com/gitlab-subscription-terms-have-changed.html","tags":"scm","title":"GitLab Subscription Terms Have Changed"},{"text":"from:http://www.cnblogs.com/itech/p/4533980.html Answer: When applying permissions to directories on Linux, the permission bits have different meanings than on regular files. The write bit allows the affected user to create, rename, or delete files within the directory, and modify the directory's attributes The read bit allows the affected user to list the files within the directory The execute bit allows the affected user to enter the directory, and access files and directories inside The sticky bit states that files and directories within that directory may only be deleted or renamed by their owner (or root)","url":"http://www.ciandcd.com/linuxmu-lu-de-quan-xian-itech.html","tags":"中文","title":"linux目录的权限 - iTech"},{"text":"from:http://www.cnblogs.com/itech/p/4532665.html 1. Bitvise SSH Client http://www.putty.org/ Bitvise SSH Client is an SSH and SFTP client for Windows. It is developed and supported professionally by Bitvise. The SSH Client is robust, easy to install, easy to use, and supports all features supported by PuTTY, as well as the following: graphical SFTP file transfer; single-click Remote Desktop tunneling; auto-reconnecting capability; dynamic port forwarding through an integrated proxy; an FTP-to-SFTP protocol bridge. Bitvise SSH Client is free for personal use , as well as for individual commercial use inside organizations. You can download Bitvise SSH Client here . 2. MobaXterm X server and SSH client http://mobaxterm.mobatek.net/ MobaXterm is your ultimate toolbox for remote computing . In a single Windows application, it provides loads of functions that are tailored for programmers, webmasters, IT administrators and pretty much all users who need to handle their remote jobs in a more simple fashion. MobaXterm provides all the important remote network tools (SSH, X11, RDP, VNC, FTP, MOSH, ...) and Unix commands (bash, ls, cat, sed, grep, awk, rsync, ...) to Windows desktop, in a single portable exe file which works out of the box. More info on supported network protocols There are many advantages of having an All-In-One network application for your remote tasks, e.g. when you use SSH to connect to a remote server, a graphical SFTP browser will automatically pop up in order to directly edit your remote files. Your remote applications will also display seamlessly on your Windows desktop using the embedded X server . See demo You can download and use MobaXterm Home Edition for free. If you want to use it inside your company, you should consider subscribing to MobaXterm Professional Edition: this will give you access to much more features, professional support and \"Customizer\" software. Features comparison done","url":"http://www.ciandcd.com/ke-yi-ti-dai-puttyde-sshke-hu-duan-itech.html","tags":"中文","title":"可以替代putty的ssh客户端 - iTech"},{"text":"from:https://www.gitlab.com/2015/05/27/gitlab-7-dot-10-dot-5-released/ GitLab 7.10.5 released In GitLab 7.11 we have introduced the requirement of a license key for users of GitLab Enterprise Edition. This can cause a moment of downtime when upgrading, as you will need to upload the license key before being able to push to the GitLab instance. With this patch release we're adding a license upload functionality that allows you to upload your license in GitLab 7.10.5, preventing downtime when upgrading to GitLab 7.11 Enterprise Edition. This patch release also includes a fix for GitLab Annex and patches a MySQL vulnerability in GitLab CI. If you are not using GitLab Enterprise Edition, you can skip this patch and go straight to GitLab 7.11 . Upgrade barometer This is a minor update, without any migrations. No downtime is necessary. Updating To update, check out our update page . As Enterprise Edition user, if you want to update to 7.10.5 rather than straight to 7.11, download and install the Omnibus package at the old download location, here. . For installations from source, use this guide . Enterprise Edition Interested in GitLab Enterprise Edition? For an overview of feature exclusive to GitLab Enterprise Edition please have a look at the features exclusive to GitLab EE . Access to GitLab Enterprise Edition is included with a subscription . No time to upgrade GitLab yourself? A subscription also entitles to our upgrade and installation services. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","url":"http://www.ciandcd.com/gitlab-7105-released.html","tags":"scm","title":"GitLab 7.10.5 Released"},{"text":"from:https://github.com/blog/2011-transforming-the-future-of-nasa-with-codeconf-s-ariel-waldman CodeConf 2015 will take place in Nashville on June 25 and 26. Ariel Waldman is one of many incredible speakers that will take the stage at the Bell Tower to share her expertise. We asked her some questions about her experiences at NASA, her vision for the future, and more. Check out her answers below! Q: Why is collaboration important to you, and how do you think it can further scientific exploration and discovery? A: To me, multidisciplinary collaboration is at the heart of furthering scientific exploration and discovery. In my work, I especially focus on unusual collaborations between people from different backgrounds. By having a fresh set of eyes from those who solve problems across a wide range of industries, new concepts emerge and go on to influence scientific processes, communication and discoveries in unexpected ways. Science doesn't require serendipity, but I'd argue it's significantly disadvantaged without it. Q: You spoke at the very first CodeConf in 2011, and we're excited to have you back since you've been doing so much exciting stuff in the meantime. What was your experience of CodeConf 2011, and what are you looking forward to seeing at CodeConf 2015? A: CodeConf was fantastic. The community was so excited to have a wide range of topics at a \"code\" conference and I think it really helped open everyone up to new opportunities and aspirations. I think what made CodeConf 2011, and what will make CodeConf 2015, so special was the unexpected connections people end up drawing and a broadening of how big the universe to play in is. Q: GitHub had the pleasure of hosting Science Hack Day in 2014, and we'll do so again this year. What is the purpose of Science Hack Day, and what were some of the most exciting projects to come out of that event? A: The mission of Science Hack Day is to get excited and make things with science! Science Hack Day is a 2-day-all-night event where anyone excited about making weird, silly or serious things with science comes together in the same physical space to see what they can prototype within 24 consecutive hours. Designers, developers, scientists and anyone who is excited about making things with science are welcome to attend – no experience in science or hacking is necessary, just an insatiable curiosity. One of the projects I loved that came out of the Science Hack Day SF at GitHub last year was an interactive planetarium where you could explore the distance between stars, planets and constellations using your hands via a Kinect. I loved the idea of making a planetarium more physical. Because the code was made open source on GitHub, the project was then hacked on further and installed as a temporary exhibit at the American Museum of Natural History in NYC, where school kids, families and even an astronaut got to try it out! Q: Tell us about NASA Innovative Advanced Concepts (NIAC) . How can we participate in and contribute to the vision of this project? A: NASA Innovative Advanced Concepts (NIAC) is arguably the coolest program at NASA - they fund and nurture all the radical, sci-fi-esque ideas that could one day transform future space missions. Submarines on Titan, human hibernation to Mars, comet hitchhiking space probes, you name it. Some utilize computer science and computer vision techniques – one project analyzes light versus dark areas on the Moon so that a lunar rover could navigate staying in continuous sunlight, thus able to be more efficient by having continuous solar power. The cool thing about NIAC is that they accept proposals from anyone every year around October, so if you have a credible idea you'd like to do further research and prototyping on that could transform a future space mission, you can apply! Follow Ariel on Twitter for more updates on all of her projects, and grab your CodeConf ticket now ! We can't wait to see you in Nashville next month.","url":"http://www.ciandcd.com/transforming-the-future-of-nasa-with-codeconfs-ariel-waldman.html","tags":"scm","title":"Transforming the Future of NASA with CodeConf's Ariel Waldman"},{"text":"from:https://github.com/blog/2007-adding-a-billing-manager-to-your-organization With the new billing manager role, you can invite individuals to manage the billing details of your organization without giving them access to code. The new role enables a user to: Upgrade or downgrade the organization's plan. Update payment details like the credit card on file. View history of past transactions and download receipts. Receive receipts via email. Billing managers won't : Be able to create or access repositories in your organization. See private members of your organization. Be seen in the list of organization members. Leave the payment details to your wonderful finance team, and get back to your code! For more information on adding a billing manager to your organization, check out the help article .","url":"http://www.ciandcd.com/adding-a-billing-manager-to-your-organization.html","tags":"scm","title":"Adding a billing manager to your organization"},{"text":"from:http://www.cnblogs.com/itech/p/4530294.html 今天的文章里我们会讲到一些使用 Linux 命令行工具来发送带附件的电子邮件的方法。它有很多用处，比如在应用程序所在服务器上，使用电子邮件发送 一个文件过来，或者你可以在脚本中使用这些命令来做一些自动化操作。在本文的例子中，我们会使用foo.tar.gz文件作为附件。 有不同的命令行工具可以发送邮件，这里我分享几个多数用户会使用的工具，如 mailx 、 mutt 和 swaks 。 我们即将呈现的这些工具都是非常有名的，并且存在于多数Linux发行版默认的软件仓库中，你可以使用如下命令安装： 在 Debian / Ubuntu 系统 apt - get install mutt apt - get install swaks apt - get install mailx apt - get install sharutils 在基于Red Hat的系统，如 CentOS 或者 Fedora yum install mutt yum install swaks yum install mailx yum install sharutils 1) 使用 mail / mailx mailx 工具在多数Linux发行版中是默认的邮件程序，现在已经支持发送附件了。如果它不在你的系统中，你可以使用上边的命令安装。有一点需要注意，老版本的mailx可能不支持发送附件，运行如下命令查看是否支持。 $ man mail 第一行看起来是这样的： mailx [- BDdEFintv ~] [- s subject ] [- a attachment ] [- c cc - addr ] [- b bcc - addr ] [- r from - addr ] [- h hops ] [- A account ] [- S variable [= value ]] to - addr . . . 如果你看到它支持 -a 的选项（-a 文件名，将文件作为附件添加到邮件）和 -s 选项（-s 主题，指定邮件的主题），那就是支持的。可以使用如下的几个例子发送邮件。 a) 简单的邮件 运行 mail 命令，然后 mailx 会等待你输入邮件内容。你可以按回车来换行。当输入完成后，按Ctrl + D， mailx 会显示EOT表示结束。 然后 mailx 会自动将邮件发送给收件人。 $ mail user@example . com HI , Good Morning How are you EOT b) 发送有主题的邮件 $ echo \"Email text\" | mail - s \"Test Subject\" user@example . com -s 的用处是指定邮件的主题。 c) 从文件中读取邮件内容并发送 \" user@example . com < /path/ to / file \"message send from file $ mail d) 将从管道获取到的 echo 命令输出作为邮件内容发送 $ echo \"This is message body\" | mail - s \"This is Subject\" user@example . com e) 发送带附件的邮件 $ echo \" Body with attachment \"| mail -a foo.tar.gz -s \" attached file \" user@example.com -a 选项用于指定附件。 2) mutt Mutt是类Unix系统上的一个文本界面邮件客户端。它有20多年的历史，在Linux历史中也是一个很重要的部分，它是最早支持进程打分和多线程处理的客户端程序之一。按照如下的例子来发送邮件。 a) 带有主题，从文件中读取邮件的正文，并发送 $ mutt - s \"Testing from mutt\" user@example . com < /tmp/ message . txt b) 通过管道获取 echo 命令输出作为邮件内容发送 $ echo \"This is the body\" | mutt - s \"Testing mutt\" user@example . com c) 发送带附件的邮件 $ echo \"This is the body\" | mutt - s \"Testing mutt\" user@example . com - a / tmp / foo . tar . gz d) 发送带有多个附件的邮件 $ echo \"This is the body\" | mutt - s \"Testing\" user@example . com - a foo . tar . gz – a bar . tar . gz 3) swaks Swaks（Swiss Army Knife，瑞士军刀）是SMTP服务上的瑞士军刀，它是一个功能强大、灵活、可编程、面向事务的SMTP测试工具，由John Jetmore开发和维护。你可以使用如下语法发送带附件的邮件： $ swaks - t \" foo@bar.com \" -- header \"Subject: Subject\" -- body \"Email Text\" -- attach foo . tar . gz 关于Swaks一个重要的地方是，它会为你显示整个邮件发送过程，所以如果你想调试邮件发送过程，它是一个非常有用的工具。 它会给你提供了邮件发送过程的所有细节，包括邮件接收服务器的功能支持、两个服务器之间的每一步交互。 （LCTT 译注：原文此处少了 sharutils 的相关介绍，而多了 uuencode 的介绍。） 4) uuencode 邮件传输系统最初是被设计来传送7位编码（类似ASCII）的内容的。这就意味这它是用来发送文本内容，而不能发会使用8位的二进制内容（如程序文件或者图片）。 uuencode （\"UNIX to UNIX encoding\"，UNIX之间使用的编码方式）程序用来解决这个限制。使用 uuencode ，发送端将二进制格式的转换成文本格式来传输，接收端再转换回去。 我们可以简单地使用 uuencode 和 mailx 或者 mutt 配合，来发送二进制内容，类似这样： $ uuencode example . jpeg example . jpeg | mail user@example . com Shell脚本：解释如何发送邮件 #!/bin/bash FROM = \"\" SUBJECT = \"\" ATTACHMENTS = \"\" TO = \"\" BODY = \"\" # 检查文件名对应的文件是否存在 function check_files () { output_files = \"\" for file in $1 do if [ - s $file ] then output_files = \"${output_files}${file} \" fi done echo $output_files } echo \"*********************\" echo \"E-mail sending script.\" echo \"*********************\" echo # 读取用户输入的邮件地址 while [ 1 ] do if [ ! $FROM ] then echo - n - e \"Enter the e-mail address you wish to send mail from:\\n[Enter] \" else echo - n - e \"The address you provided is not valid:\\n[Enter] \" fi read FROM echo $FROM | grep - E '&#94;.+@.+$' > /dev/ null if [ $ ? - eq 0 ] then break fi done echo # 读取用户输入的收件人地址 while [ 1 ] do if [ ! $TO ] then echo - n - e \"Enter the e-mail address you wish to send mail to:\\n[Enter] \" else echo - n - e \"The address you provided is not valid:\\n[Enter] \" fi read TO echo $TO | grep - E '&#94;.+@.+$' > /dev/ null if [ $ ? - eq 0 ] then break fi done echo # 读取用户输入的邮件主题 echo - n - e \"Enter e-mail subject:\\n[Enter] \" read SUBJECT echo if [ \"$SUBJECT\" == \"\" ] then echo \"Proceeding without the subject...\" fi # 读取作为附件的文件名 echo - e \"Provide the list of attachments. Separate names by space. If there are spaces in file name, quote file name with \\\".\" read att echo # 确保文件名指向真实文件 attachments = $ ( check_files \"$att\" ) echo \"Attachments: $attachments\" for attachment in $attachments do ATTACHMENTS","url":"http://www.ciandcd.com/4ge-ke-yi-fa-song-wan-zheng-dian-zi-you-jian-de-ming-ling-xing-gong-ju-itech.html","tags":"中文","title":"4个可以发送完整电子邮件的命令行工具 - iTech"},{"text":"from:https://www.gitlab.com/2015/05/22/gitlab-7-11-released/ It's the 22nd of the month, so we have a new GitLab release ready! GitLab 7.11 brings more improvements to the look and feel of GitLab, two-factor authentication, a version check and more! Of course we're also releasing GitLab CI 7.11, with a new backup and restore utility, improvements in the UI and other new features. This month's MVP is James Newton (newton on IRC)! James is very active on our #gitlab IRC channel, often helping people out with issues or helping people getting started with GitLab. We're very happy to have James supporting the community and believe that is deserving of a MVP award! Thanks James! Better looking sidebar We changed the look of the sidebar to reflect its function better and make it look more pretty: Clean project dashboard The project dashboard was a good example of design by committee, one GitLab contributor noted. We broomed through it and cleaned it up: Two-factor authentication Keep your code more secure and start using two-factor authentication (2FA)! GitLab has built-in 2FA in both CE and EE now and makes use of the convenient Google Authenticator. All you have to do is go to your Profile > Account and scan the QR code using Google's app. From now on, on login you'll be required to provide the code the app gives you for GitLab. Two-factor authentication only works with the web-UI for now. User roles in comments Now you know who's who in your favorite project. On comments you will see the role of the person in that project: Task lists everywhere Want a task list in the comments? Now you can! Version Check GitLab releases a new version every single month on the 22nd, so we understand that people are not always up to date. We wanted to give you some help with this, so from now on you can quickly see which version of GitLab you have running by visiting the Help or Admin page. It will show if you are up to date and if there is a security release you should have installed. Read more about the version check in our blog post about it. You can turn off the version check under Admin > Settings. License keys for Enterprise Edition GitLab Enterprise Edition used to live in a private repository, which was fine up until now. However, with the addition of our package server, we want to make it easier to start using GitLab Enterprise Edition. Rather than locking up the package repository of GitLab EE, we decided to open up all the code and packages and start using license keys. The code is still proprietary, but now is publicly viewable . This has several advantages. The installation of GitLab EE becomes as easy as installing GitLab CE. You no longer needs access to specific repositories, rather you can download it using the same methods as CE (including AWS/Azure templates, Docker images, etc). In addition, the code for Enterprise Edition is now becoming open to inspect for everyone. This will make it easier to send enhancements and makes it easier to do a trial of Enterprise Edition. Getting organizations to purchase a subscription after their trial expires or at renewal time sometimes took a substantial effort from us. We don't want to raise prices for customers that renew without prompting because we need to invest more time in unresponsive customers. Therefore we decided to introduce license keys that prompt customers automatically. We regret the inconvenience that license keys introduce but we think it is the best solution to keep prices low. True-up model for subscriptions The worst thing about license keys is that they can be very inflexible. Most GitLab installations quickly grow in popularity within the organization. Having to purchase a new license key every time this happens is very inefficient. Also, we noticed that the majority of our customers didn't have a compliant subscription, for us this indicates that having to renew the subscription multiple times a year is very inconvenient. Therefore we will switch to a true-up model that allows you to grow now and pay later. When you get a new license you should get it for your current number of active users. For users that are added during the year you pay half price when you renew. So if you have 100 active users today you get a 100 user subscription. Suppose that when you renew a year from now you have 300 active users. You pay for a 300 user subscription and pay half a year for the 200 users that you added during the year. Getting the license key If you are currently a GitLab customer, you should have received your license key already at the email you registered with your payment. You can also email sales at gitlab dot com to request it at any time. New subscribers will receive their license key automatically. Installing the license key To install the license, vist /admin/license in your GitLab instance as an admin. Here you can upload your .gitlab-license file, which will instantly unlock GitLab Enterprise Edition. You can also download and review your current license here. Please note that we will release GitLab 7.10.5 soon, that will allow you to upload the license key to your GitLab instance before upgrading, to avoid unnecessary downtime. Two-Factor Authentication for LDAP / Active Directory (EE-only) Want to use two-factor authentication together with your LDAP or Active Directory integration? With GitLab Enterprise Edition you can. New GitLab CI Features With the release of GitLab 7.11, we also updated GitLab CI to 7.11. Some changes worth mentioning are an improved runners page, public accessible build and commit pages for public projects , a new backup/restore utility that will backup your CI database and HipChat notifications! Other awesome changes in GitLab CE We can never cover all the new stuff in each GitLab release, but these are worth to have a quick look at as well: Quick quote-reply You can now reply with a quotation by simply selecting text in an issue or merge request and pressing r . It will set the focus to the editing window and have the quoted text already in it! Atom feeds for all! There is now an atom feed for each project! Settings in admin UI We moved default project and snippet visibility settings to the admin web interface. Improved UI for mobile GitLab is now better viewable on mobile! WIP your MRs! If you add WIP or [WIP] (work in progress) to the start of the title of a merge request, it will be protected from merging now. This release has more improvements, including security fixes, please check out the Changelog to see the all named changes. Upgrade barometer Coming from 7.10, the migrations in 7.11 are pretty fast (under 1 minute), but one of them is tricky: we rename any existing users with names ending in a period (‘.'). This migration updates both the database and the filesystem and previous versions of this migration have proven to be fragile. If you have no user namespaces with paths ending in ‘.' in your database and if you trust your users not to create any until after you upgrade to GitLab 7.11 you can perform this upgrade online. If not, we recommend to take downtime (this is what we did for gitlab.com). You can find the current number of affected database records with the following command: 1 sudo gitlab-rails runner \"puts Namespace.where(type: nil).where(%q{path LIKE '%.'}).count\" Installation If you are setting up a new GitLab installation please see the installing GitLab page . Updating Check out our update page . Please note that cookbook-omnibus-gitlab, our Chef cookbook that installs/manages GitLab omnibus packages, does not yet support packages.gitlab.com. See this issue . Enterprise Edition The mentioned EE-only features and things like LDAP group support can be found in GitLab Enterprise Edition. For a complete overview please have a look at the feature list of GitLab EE . Access to GitLab Enterprise Edition is included with a subscription . No time to upgrade GitLab yourself? A subscription also entitles you to our upgrade and installation services. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","url":"http://www.ciandcd.com/gitlab-711-released-with-two-factor-authentication-and-a-publicly-viewable-enterprise-edition.html","tags":"scm","title":"GitLab 7.11 Released With Two-factor Authentication and a Publicly Viewable Enterprise Edition"},{"text":"from:https://github.com/blog/2006-codeconf-2015-speakers-workshops-and-hotel-discount We thought you'd like a preview of what we have in store for CodeConf 2015 in June before discounted ticket sales end on May 25th . We're beyond excited to be welcoming speakers from all over the globe and from companies and organizations of all sizes, who work on every facet of open source technology and represent many different parts of the community. Here's a sneak peak at some of the excellent speakers who will be presenting at CodeConf this year: Eric Levine of Airbnb Casey Rosenthal of Netflix Christine Abernathy of Facebook Corinne Warnshuis of Girl Develop It This is just a sampling of the amazing line-up. Check out the full preview over at the CodeConf site . But wait there's more! Reserve a spot in one of our hands-on workshops led by expert trainers. Space is limited, so register early . Join us the evening before the conference to enjoy the company of the Nashville open source community and grab your badge early. You won't want to miss the food from legendary Hattie B's . Register for a discounted room at the Gaylord Opryland Hotel . Shuttles will depart at convenient intervals so you can easily get to and from the conference. By booking a room at the Opryland instead of a downtown hotel, you save about $100. There are still a few sponsorship opportunities left, including our scholarship program. Check out the prospectus and drop us a line at events@github.com . Find more details about everything we have in store for you on the new codeconf.com . Lastly, we'd like to thank everyone who took the time to send us their ideas. We were overwhelmed by the quality and creativity of the 300+ proposals submitted. You are the heart of CodeConf. Ticket prices go up to $399 on May 25th and we can't wait to see you in Nashville, so what are you waiting for?","url":"http://www.ciandcd.com/codeconf-2015-speakers-workshops-and-hotel-discount.html","tags":"scm","title":"CodeConf 2015: Speakers, Workshops and Hotel Discount"},{"text":"from:https://www.gitlab.com/2015/05/21/security-advisory-for-logjam-vulnerability/ Security advisory for Logjam vulnerability A recently announced Logjam vulnerability allows an attacker to do a man-in-the-middle attack, allowing them to downgrade a TLS connection to 512-bit DH parameters. More details on what that is and means can be found on openssl blog . Impact on GitLab GitLab is using, by default, up-to-date SSL ciphers: Export Cipher Suites are not used. Elliptic-Curve Diffie-Hellman ciphers are used By default, 1024-bit DH groups are used This means that GitLab is safe in principle. When using 1028-bit DH groups there is a small chance that an attacker with nation-state resources could be eavesdropping. If you find this insufficient for your GitLab installation, you can generate 2048-bit DH groups and enable the ssl_dhparam option in NGINX config. Params can be generated with: 1 openssl dhparam -out dhparams.pem 2048 After the dhparams.pem file has been generated you will need to tell Nginx where the file is located: GitLab installations using omnibus-gitlab packages For packages version 7.11.0 and up. Place the dhparams.pem file in /etc/gitlab/ssl/ directory. In /etc/gitlab/gitlab.rb , enable the following setting: 1 nginx [ 'ssl_dhparam' ] = \"/etc/gitlab/ssl/dhparams.pem\" and do sudo gitlab-ctl reconfigure . More information can be found in the omnibus-gitlab nginx documentation . Workaround for packages prior to version 7.11.0 Place the dhparams.pem file in /etc/gitlab/ssl/ directory. In /etc/gitlab/gitlab.rb , enable the following setting: 1 nginx [ 'custom_gitlab_server_config' ] = \"ssl_dhparam /etc/gitlab/ssl/dhparams.pem; \\n \" and run sudo gitlab-ctl reconfigure . GitLab installations from source Place the generated dhparams.pem in a suitable location, for example /etc/nginx/ssl/dhparams.pem . In GitLab nginx config find ssl_dhparam config and set it to ssl_dhparam /etc/nginx/ssl/dhparams.pem; . Reload your nginx config. Impact on GitLab.com GitLab.com is using 1028-bit DH groups. Due to incompatibilities with older Java-based clients we haven't enabled 2048-bit DH params yet as this would prevent some people from using GitLab.com. We are looking into ways to keep a good SSLlabs score and allowing users with older Java-base clients to use GitLab.com. We are examining the impact of this and we will update this blog post once we have more information. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","url":"http://www.ciandcd.com/security-advisory-for-logjam-vulnerability.html","tags":"scm","title":"Security Advisory for Logjam Vulnerability"},{"text":"from:https://www.gitlab.com/2015/05/20/gitlab-gitorious-free-software/ GitLab, Gitorious, and Free Software This is a guest post by Mike Gerwitz , a free software hacker and activist, and author of GNU ease.js . In early March of this year, it was announced that GitLab would acquire Gitorious and shut down gitorious.org by 1 June, 2015. Reactions from the community were mixed, and understandably so: while GitLab itself is a formidable alternative to wholly proprietary services, its acquisition of Gitorious strikes a chord with the free software community that gathered around Gitorious in the name of software freedom . After hearing that announcement, as a free software hacker and activist myself , I was naturally uneasy. Discussions of alternatives to Gitorious and GitLab ensued on the libreplanet-discuss mailing list. Sytse Sijbrandij (GitLab B.V. CEO) happened to be present on that list; I approached him very sternly with a number of concerns, just as I would with anyone that I feel does not understand certain aspects of the free software philosophy . To my surprise, this was not the case at all. Sytse has spent a lot of time accepting and considering community input for both the Gitorious acquisition and GitLab itself. He has also worked with me to address some of the issues that I had raised. And while these issues won't address everyone's concerns, they do strengthen GitLab's commitment to software freedom , and are commendable. I wish to share some of these details here; but to do so, I first have to provide some background to explain what the issues are, and why they are important. Free Software Ideology Gitorious was (and still is) one of the most popular Git repository hosts, and largely dominated until the introduction of GitHub. But even as users flocked to GitHub's proprietary services , users who value freedom continued to support Gitorious, both on gitorious.org and by installing their own instances on their own servers. Since Gitorious is free software , users are free to study, modify, and share it with others. But software freedom does not apply to Services as a Software Substitute (SaaSS) or remote services—you cannot apply the four freedoms to something that you do not yourself possess—so why do users still insist on using gitorious.org despite this? The matter boils down to supporting a philosophy: The GNU General Public License (GPL) is a license that turns copyright on its head: rather than using copyright to restrict what users can do with a program, the GPL instead ensures users' freedoms to study, modify, and share it. But that isn't itself enough: to ensure that the software always remains free (as in freedom), the GPL ensures that all derivatives are also licensed under similar terms. This is known as copyleft , and it is vital to the free software movement. Gitorious is licensed under the GNU Affero General Public License Version 3 (AGPLv3) —this takes the GPL and adds an additional requirement: if a modified version of the program is run on a sever, users communicating with the program on that server must have access to the modified program's source code. This ensures that modifications to the program are available to all users ; they would otherwise be hidden in private behind the server, with others unable to incorporate, study, or share them. The AGPLv3 is an ideal license for Gitorious, since most of its users will only ever interact with it over a network. GitLab is also free software: its Expat license (commonly referred to ambiguously as the \"MIT license\") permits all of the same freedoms that are granted under the the GNU GPL. But it does so in a way that is highly permissive: it permits relicensing under any terms, free or not. In other words, one can fork GitLab and derive a proprietary version from it, making changes that deny users their freedoms and cannot be incorporated back into the original work. This is the issue that the free software community surrounding Gitorious has a problem with: any changes contributed to GitLab could in turn benefit a proprietary derivative. This situation isn't unique to GitLab: it applies to all non-copyleft (\"permissive\") free software licenses . And this issue is realized by GitLab itself in the form of its GitLab Enterprise Edition (GitLab EE): a proprietary derivative that adds additional features atop of GitLab's free Community Edition (CE). For this reason, many free software advocates are uncomfortable contributing to GitLab, and feel that they should instead support other projects; this, in turn, means not supporting GitLab by using and drawing attention to their hosting services. The copyleft vs. permissive licensing debate is one of the free software movement's most heated. I do not wish to get into such a debate here. One thing is clear: GitLab Community Edition (GitLab CE) is free software. Richard Stallman (RMS) responded directly to the thread on libreplanet-discuss , stating plainly: We have a simple way of looking at these two versions. The free version is free software, so it is ethical. The nonfree version is nonfree software, so it is not ethical. Does GitLab CE deserve attention from the free software community? I believe so. Importantly, there is another strong consideration: displacing proprietary services like GitHub and Bitbucket, which host a large number of projects and users. GitLab has a strong foothold, which is an excellent place for a free software project to be in. If we are to work together as a community, we need to respect GitLab's free licensing choices just as we expect GitLab to respect ours. Providing respect does not mean that you are conceding: I will never personally use a non-copyleft license for my software; I'm firmly rooted in my dedication to the free software philosophy , and I'm sure that many other readers are too. But using a non-copyleft license, although many of us consider it to be a weaker alternative, is not wrong . Free JavaScript As I mentioned above, software freedom and network services are separate issues —the four freedoms do not apply to interacting with gitlab.com purely over a network connection, for example, because you are not running its software on your computer. However, there is an overlap: JavaScript code downloaded to be executed in your web browser. Non-free JavaScript is a particularly nasty concern: it is software that is downloaded automatically from a server—often without prompting you—and then immediately executed. Software is now being executed on your machine, and your four freedoms are once again at risk. This, then, is the primary concern for any users visiting gitlab.com : not only would this affect users that use gitlab.com as a host, but it would also affect any user that visits the website. That would be a problem, since hosting your project there would be inviting users to run proprietary JavaScript. As I was considering migrating my projects to GitLab, this was the first concern I brought up to Sytse . This problem arises because gitlab.com uses a GitLab EE instance: if it had used only its Community Edition (GitLab CE)—which is free software—then all served JavaScript would have been free. But any scripts served by GitLab EE that are not identical to those served by GitLab CE are proprietary, and therefore unethical. This same concern applies to GitHub, Bitbucket, and other proprietary hosts that serve JavaScript. Sytse surprised me by stating that he would be willing to freely license all JavaScript in GitLab EE , and by offering to give anyone access to the GitLab EE source code who wants to help out. I took him up on that offer. Initially, I had submitted a patch to merge all GitLab EE JavaScript into GitLab CE, but Sytse came up with another, superior suggestion, that ultimately provided even greater reach. I'm pleased to announce that Sytse and I were able to agree on a license change (with absolutely no friction or hesitation on his part) that liberates all JavaScript served to the client from GitLab EE instances. There are two concerns that I had wanted to address: JavaScript code directly written for the client, and any code that produced JavaScript as output. In the former case, this includes JavaScript derived from other sources: for example, GitLab uses CoffeeScript, which compiles into JavaScript. The latter case is important: if there is any code that generates fragments of JavaScript—e.g. dynamically at runtime—then that code must also be free, or users would not be able to modify and share the resulting JavaScript that is actually being run on the client. Sytse accepted my change verbatim, while adding his own sentence after mine to disambiguate. At the time of writing this post, GitLab EE's source code isn't yet publicly visible, so here is the relevant snippet from its LICENSE file: The above copyright notices applies only to the part of this Software that is not distributed as part of GitLab Community Edition (CE), and that is not a file that produces client-side JavaScript, in whole or in part. Any part of this Software distributed as part of GitLab CE or that is a file that produces client-side JavaScript, in whole or in part, is copyrighted under the MIT Expat license. Further Discussion My discussions with Sytse did not end there: there are other topics that have not been able to be addressed before my writing of this post that would do well to demonstrate commitment toward software freedom . The license change liberating client-side JavaScript was an excellent move. To expand upon it, I wish to submit a patch that would make GitLab LibreJS compliant ; this provides even greater guarantees, since it would allow for users to continue to block other non-free JavaScript that may be served by the GitLab instance, but not produced by it. For example: a website/host that uses GitLab may embed proprietary JavaScript, or modify it without releasing the source code. Another common issue is the user of analytics software; gitlab.com uses Google Analytics. If you would like to help with LibreJS compliance, please contact me . I was brought into another discussion between Sytse and RMS that is unrelated to the GitLab software itself, but still a positive demonstration of a commitment to software freedom —the replacement of Disqus on the gitlab.com blog with a free alternative. Sytse ended up making a suggestion, saying he'd be \"happy to switch to\" Juvia if I'd help with the migration. I'm looking forward to this, as it is an important discussion area (that I honestly didn't know existed until Sytse told me about it, because I don't permit proprietary JavaScript!). He was even kind enough to compile a PDF of comments for one of our discussions, since he was cognizant ahead of time that I would not want to use Disqus. (Indeed, I will be unable to read and participate in the comments to this guest post unless I take the time to freely read and reply without running Disqus' proprietary JavaScript.) Considering the genuine interest and concern expressed by Sytse in working with myself and the free software community, I can only expect that GitLab will continue to accept and apply community input. Actions Speak Louder Than Words It is not possible to address the copyleft issue without a change in license, which GitLab is not interested in doing. So the best way to re-assure the community is through action. To quote Sytse : I think the only way to prove we're serious about open source is in our actions, licenses or statements don't help. There are fundamental disagreements that will not be able to be resolved between GitLab and the free software community—like their \"open core\" business model . But after working with Sytse and seeing his interactions with myself, RMS, and many others in the free software community, I find his actions to be very encouraging. Are you interested in helping other websites liberate their JavaScript? Consider joining the FSF's campaign , and please liberate your own ! This post is licensed under the Creative Commons Attribution-ShareAlike 3.0 Unported License . Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","url":"http://www.ciandcd.com/gitlab-gitorious-and-free-software.html","tags":"scm","title":"GitLab, Gitorious, and Free Software"},{"text":"from:http://blog.devopsguys.com/2015/05/20/steve-thairs-qcon-talk-now-available-online/ DevOps and the Need for Speed, the talk from our very own Steve Thair is now available online. You can check it out here . Steve's just spoken at Krakow's Atmosphere Conference . Stay tuned for more, coming soon.","url":"http://www.ciandcd.com/steve-thairs-qcon-talk-now-available-online.html","tags":"devops","title":"Steve Thair's QCon Talk – now available online"},{"text":"from:http://www.cnblogs.com/itech/p/4512113.html Deleting a HUGE file I had a huge log file 200GB I need to delete on a production web server. My rm and ls command was crashed and I was afraid that the system to a crawl with huge disk I/O load. To remove a HUGE file, enter: > /path/to/file.log # or use the following syntax : > /path/to/file.log # finally delete it rm /path/to/file.log Want to cache console output? Try the script command line utility to create a typescript of everything printed on your terminal. script my.terminal.sessio Type commands: ls date sudo service foo stop To exit (to end script session) type exit or logout or press control-D exit To view type: more my.terminal.session less my.terminal.session cat my.terminal.session Restoring deleted /tmp folder As my journey continues with Linux and Unix shell, I made a few mistakes . I accidentally deleted /tmp folder. To restore it all you have to do is: mkdir /tmp chmod 1777 /tmp chown root:root /tmp ls -ld /tmp Locking a directory For privacy of my data I wanted to lock down /downloads on my file server. So I ran: chmod 0000 /downloads The root user can still has access and ls and cd commands will not work. To go back: chmod 0755 /downloads Password protecting file in vim text editor Afraid that root user or someone may snoop into your personal text files? Try password protection to a file in vim, type: vim +X filename Or, before quitting in vim use :X vim command to encrypt your file and vim will prompt for a password. Clear gibberish all over the screen Just type: reset Becoming human Pass the -h or -H (and other options) command line option to GNU or BSD utilities to get output of command commands like ls, df, du, in human-understandable formats: ls -lh # print sizes in human readable format (e.g., 1K 234M 2G) df -h df -k # show output in bytes, KB, MB, or GB free -b free -k free -m free -g # print sizes in human readable format (e.g., 1K 234M 2G) du -h # get file system perms in human readable format stat -c %A /boot # compare human readable numbers sort -h -a file # display the CPU information in human readable format on a Linux lscpu lscpu -e lscpu -e=cpu,node # Show the size of each file but in a more human readable way tree -h tree -h /boot Show information about known users in the Linux based system Just type: ## linux version ## lslogins ## BSD version ## logins Sample outputs: UID USER PWD-LOCK PWD-DENY LAST-LOGIN GECOS 0 root 0 0 22:37:59 root 1 bin 0 1 bin 2 daemon 0 1 daemon 3 adm 0 1 adm 4 lp 0 1 lp 5 sync 0 1 sync 6 shutdown 0 1 2014-Dec17 shutdown 7 halt 0 1 halt 8 mail 0 1 mail 10 uucp 0 1 uucp 11 operator 0 1 operator 12 games 0 1 games 13 gopher 0 1 gopher 14 ftp 0 1 FTP User 27 mysql 0 1 MySQL Server 38 ntp 0 1 48 apache 0 1 Apache 68 haldaemon 0 1 HAL daemon 69 vcsa 0 1 virtual console memory owner 72 tcpdump 0 1 74 sshd 0 1 Privilege-separated SSH 81 dbus 0 1 System message bus 89 postfix 0 1 99 nobody 0 1 Nobody 173 abrt 0 1 497 vnstat 0 1 vnStat user 498 nginx 0 1 nginx user 499 saslauth 0 1 \"Saslauthd user\" How do I fix mess created by accidentally untarred files in the current dir? So I accidentally untar a tarball in /var/www/html/ directory instead of /home/projects/www/current. It created mess in /var/www/html/. The easiest way to fix this mess: cd /var/www/html/ /bin/rm -f \"$(tar ztf /path/to/file.tar.gz)\" Confused on a top command output? Seriously, you need to try out htop instead of top: sudo htop Want to run the same command again? Just type !!. For example: /myhome/dir/script/name arg1 arg2 # To run the same command again !! ## To run the last command again as root user sudo !! The !! repeats the most recent command. To run the most recent command beginning with \"foo\": !foo # Run the most recent command beginning with \"service\" as root sudo !service The !$ use to run command with the last argument of the most recent command: # Edit nginx.conf sudo vi /etc/nginx/nginx.conf # Test nginx.conf for errors /sbin/nginx -t -c /etc/nginx/nginx.conf # After testing a file with \"/sbin/nginx -t -c /etc/nginx/nginx.conf\", you # can edit file again with vi sudo vi !$ Get a reminder you when you have to leave If you need a reminder to leave your terminal, type the following command: leave +hhmm Where, hhmm - The time of day is in the form hhmm where hh is a time in hours (on a 12 or 24 hour clock), and mm are minutes. All times are converted to a 12 hour clock, and assumed to be in the next 12 hours. Home sweet home Want to go the directory you were just in? Run: cd - Need to quickly return to your home directory? Enter: cd The variable CDPATH defines the search path for the directory containing directories: export CDPATH=/var/www:/nas10 Now, instead of typing cd /var/www/html/ I can simply type the following to cd into /var/www/html path: cd html Editing a file being viewed with less pager To edit a file being viewed with less pager, press v. You will have the file for edit under $EDITOR: less *.c less foo.html ## Press v to edit file ## ## Quit from editor and you would return to the less pager again ## List all files or directories on your system To see all of the directories on your system, run: find / -type d | less # List all directories in your $HOME find $HOME -type d -ls | less To see all of the files, run: find / -type f | less # List all files in your $HOME find $HOME -type f -ls | less Build directory trees in a single command You can create directory trees one at a time using mkdir command by passing the -p option: mkdir -p /jail/{dev,bin,sbin,etc,usr,lib,lib64} ls -l /jail/ Copy file into multiple directories Instead of running: cp /path/to/file /usr/dir1 cp /path/to/file /var/dir2 cp /path/to/file /nas/dir3 Run the following command to copy file into multiple dirs: echo /usr/dir1 /var/dir2 /nas/dir3 | xargs -n 1 cp -v /path/to/file Creating a shell function is left as an exercise for the reader Quickly find differences between two directories The diff command compare files line by line. It can also compare two directories: ls -l /tmp/r ls -l /tmp/s # Compare two folders using diff ## diff /tmp/r/ /tmp/s/ Text formatting You can reformat each paragraph with fmt command. In this example, I'm going to reformat file by wrapping overlong lines and filling short lines: fmt file.txt You can also split long lines, but do not refill i.e. wrap overlong lines, but do not fill short lines: fmt -s file.txt See the output and write it to a file Use the tee command as follows to see the output on screen and also write to a log file named my.log: mycoolapp arg1 arg2 input.file | tee my.log The tee command ensures that you will see mycoolapp output on on the screen and to a file same time.","url":"http://www.ciandcd.com/20ge-you-yong-de-linuxming-ling-xing-ji-qiao-itech.html","tags":"中文","title":"20个有用的linux命令行技巧 - iTech"},{"text":"from:https://www.gitlab.com/2015/05/18/a-new-gitlab-logo/ A new GitLab Logo We hear you: Gitlab seems like a cool service, but my god that logo is scary — Matt Bachmann (@MattBachmann) March 11, 2015 We have a scary, angry looking raccoon dog logo. this creepy human/racoon hybrid that is the @gitlab logo is starting to really freak me out pic.twitter.com/HJarlbRNOo — hatewell (@hatwell) January 16, 2015 We figured we could use a better representation of GitLab. Update May 20th: After careful consideration we have decided that this is our new logo: We like the way it looks in GitLab: And compared to the old logo: The options we didn't pick: If you have a better suggestion than one of the ones above, a certain preference or opinion, we'd love to hear it. The final choice of our new logo rests with Dmitriy . Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","url":"http://www.ciandcd.com/a-new-gitlab-logo.html","tags":"scm","title":"A New GitLab Logo"},{"text":"from:http://blog.devopsguys.com/2015/05/18/increase-your-elk-herd-with-consul-io/ Originally posted on DevOps Is Common Sense... : At work, I recently had a need to put in place a scalable logging solution based around the ELK stack. Issues with Multicast networking aside, Elasticsearch scales pretty well on its own without the need for any additional overheads, however discovering whether a node is online or not and connecting only to available nodes can be tricky. Scaling Logstash can be tricky, but it basically involves adding more Logstash servers to the mix and pointing them at your Elasticsearch cluster by defining multiple hosts in your Logstash configuration. Kibana (like most web applications) can only have one Elasticsearch host defined in the config, so scaling out Kibana is more difficult. The above raises the question – how do I know which Elasticsearch node to point my configuration at if I don't know whether they are there or not. The answer came in the form of consul.io . If you've not looked at…","url":"http://www.ciandcd.com/increase-your-elk-herd-with-consulio.html","tags":"devops","title":"Increase your ELK herd with Consul.io"},{"text":"from:https://www.gitlab.com/2015/05/18/simple-words-for-a-gitlab-newbie/ For most of us, when we work with a new tool, there's a process of learning the right vocabulary and the best steps to make things happen; this while we try to keep the best attitude. Not very long ago, I learned how to use Git and GitLab and it was a little bit painful. I read a lot about it, but it was mostly vocabulary that didn't make any sense to me. If you've been there or if you are there now, you'll know what I'm talking about (some people may have it naturally). So, to make this learning process easier for others, I took many of the basic Git vocabulary and wrote easy definitions for each word. I hope they are useful for you and please share them with your Git and Gitlab newbie friends! Cloud Based Services What is a cloud based service? It's a service or resource that is opposed to services that are hosted on the servers inside a company, which is the traditional way of doing it. It helps people and companies lower their costs and be more efficient while helping with different functions such as trannings, storage, etc. GitLab.com is a cloud based service because it can be hosted both in house and in the cloud. Source control or revision control software What is source control? It's a system that records and manages changes to projects, files and documents. It helps you recall specific versions later. It also makes it easier to collaborate, because it shows who has changed what and helps you combine contributions. Continuous Integration What is continuous integration? It's the system of continuously incorporating the work advances with a shared mainline in a project. Git and GitLab together make continuous integration happen. Continuous deployment What is continuous deployment? It means that whenever there is a change to the code, it is deployed or made live immediately. This is in contrast to continous integration, where code is continuously being merged in the mainline and is always ready to be deployed, rather than actually deployed. When people talk about CI and CD what they usually mean to say is that they are constantly and automatically testing their code against their tests using a tool such as GitLab CI and upon passing to a certain action. That action could be merging the code into a branch (master, production, etc), deploying it to a server or building a package / piece of software out of it. Non-continuous integration would be everyone working on something and only integrating all the work as the very last step. Obviously, that results in many conflicts and issues, which is why CI is adopted widely nowadays. Git What is Git? Git is a system where you can create projects of different sizes with speed and efficiency. It helps you manage code, communicate and collaborate on different software projects. Git will allow you to go back to a previous status on a project or to see its entire evolution since the project was created. You could think of it as a time machine which will allow you to go back in time to whenever you'd like in your project. With Git, 3 basic issues were solved when working on projects: 1. It became easier to manage large projects. 2. It helps you avoid overwriting the team's advances and work. 3. With git, you just pull the entire code and history to your machine, so you can calmly work in your own little space without interference or boundaries. It's much simpler and much more light-weight. Repository What is a repository? The place where the history of your work is stored. Remote repository What is a remote repository? It's a repository that is not-on-your-machine, so it's anything that is not your computer. Usually, it is online, GitLab.com for instance. The main remote repository is usually called \"Origin\". Commit What is a commit? It's the way you call the latest changes of source code that you made on a repository. When changes are tracked, commits mark the changes on a document. Master What is a master? It's how you call the main and definitive branch (the independent line of development of a project). Branch What is a branch? It's an independent line of development. They are a brand new working directory, staging area, and project history. New commits are recorded in the history for the current branch, which results in taking the source from someone's repository (the place where the history of your work is stored) at certain point in time, and apply your own changes to it in the history of the project. Fork What is a fork? It's a copy of an original repository (the place where the history of your work is stored) that you can put somewhere else or where you can experiment and apply changes that you can later decide if publishing or not, without affecting your original project. Git Clone What is a clone? It's to get a copy of a git project to look at or to use the code. Git Merge What is to merge? It's integrating separate changes that you made to a project, on different branches. md: markdown What is markdown? It's a plain text format that will make any document easy-to-write and easy-to-read. Push a repository What is to push a repository? It's to incorporate a local branch (the independent line of development of a project) to a remote repository (online version of your project). README.md What is a README.md? I't a file in a simple format which summarizes a repository. If there's also a README (without the .md), the README.md will have priority. SSH (secure shell protocol) What is SSH? It's how you call the commands that help communicate through a network and that are encrypted and secure. It's used for remote logins and it helps users connect to a server in a secure way. Stage Files What is to stage a file? It's how you call the act of preparing a file for a commit (the latest changes of source code in a repository). GitLab What is GitLab? GitLab is an online Git repository manager with a wiki, issue tracking, CI and CD. It is a great way to manage git repositories on a centralized server. GitLab gives you complete control over your repositories or projects and allows you to decide whether they are public or private for free. GitLab.com GitLab.com hosts your (private) software projects for free. It offers free public and private repositories, issue-tracking and wikis. It runs GitLab Enterprise Edition and GitLab CI. No installation required, you can just sign up for a free account. Support Package: Free subscribers can use the GitLab.com Support Forum if they have questions. GitLab.com Bronze Support will let you email support directly for timely, personal and private answers. This costs $9.99 per user per year for next-business-day response time and is available in packs of 20 users. GitLab Community Edition (CE) Free, self hosted application where you can get support from the Community Feature rich: Git repository management, code reviews, issue tracking, activity feeds and wikis. It comes with GitLab CI for continuous integration and delivery. Open Source: MIT licensed, community driven, 700+ contributors, inspect and modify the source, easy to integrate into your infrastructure. Scalable: support 25,000 users on one server or a highly available active/active cluster. Merge requests with line-by-line comments, CI and issue tracker integrations. GitLab Enterprise Edition (EE) Self hosted application that comes with additional support. Builds on top of the Community Edition and includes extra features mainly aimed at organizations with more than 100 users. It has LDAP group sync, audit logs and multiple roles. It includes deeper authentication and authorization integration, has fine-grained workflow management, has extra server management options and it integrates with your tool stack. GitLab EE runs on your servers. GitLab Continuous Integration (CI) Free, self hosted application that integrates with GitLab CE/EE. Also availble as SaaS at ci.gitlab.com. Easy to set up since it is included in Omnibus packages of GitLab or use it for free on ci.gitlab.com. Beautiful interface with a clear menu structure. Performant and stable, as tests run distributed on separate machines. Will help you receive test results faster with each commit running in parallel on multiple jobs. Free to use and completely open source. All CI code is MIT licensed. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","url":"http://www.ciandcd.com/simple-words-for-a-gitlab-newbie.html","tags":"scm","title":"Simple Words for a GitLab Newbie"},{"text":"from:http://blog.devopsguys.com/2015/05/11/devopsguys-partner-redgate/ More and more companies are now considering source control, continuous integration, and automated deployment for their database. To help them adopt each of these stages of Database Lifecycle Management (DLM), Redgate Software has launched a new partner program. Redgate Certified Consultants are now being trained in the USA, Europe and Australia – and many of them will be familiar to SQL Server professionals. The advantages of implementing any stage of DLM are many. Just as with Application Lifecycle Management (ALM), it speeds up the introduction of new features, and makes deployments reliable and error-free. But even though Redgate tools are designed to plug into the tools companies already use for their application d evelopment, questions can arise during the implementation process. As Dan Wood of Northwest Cadence says: \"How can we take a system designed to develop, build and deliver applications and make it work with databases as well? It is a question that has plagued a vast majority of the clients I have worked with over the past few years.\" To address this issue, Redgate's new partner program is training expert consultants like Dan Wood in DLM – and giving them the tools and support they need to help clients on-site, or in training sessions. The list of Certified Consultants is growing and already includes familiar faces like Ike Ellis and Northwest Cadence in the USA, The DevOpsGuys and Skelton Thatcher in the UK, and WARDY IT Solutions in Australia. As John Theron of Redgate points out, the advantages are clear. \"Redgate has spent a lot of time and effort joining the dots in DLM, and making it possible with a suite of dedicated tools, alongside learning materials and resources. The partner program complements this with a group of experts on the ground able to help companies on-site, and provide training in a series of public workshops.\" In places as far apart as Washington, London, San Diego, Philadelphia, Northern Ireland, and Baton Rouge, database professionals are how being trained in source control, continuous integration, and automated deployment for the database. A measure of the success the training is already achieving can be found in the reaction from database professionals like Jim Dorame. A Database Systems Manager for a large scale educational assessment corporation in the Greater Minneapolis area, he reviewed a continuous integration training day on his blog . \"This tool makes the job of the DBA easier as there will be little doubt that the database is in a consistent and correct state. This alone makes me smile, I cannot tell you how many times I've been executing a release and there was a piece missing that caused a failure.\" Further information about the training opportunities available can be found on the Redgate training pages .","url":"http://www.ciandcd.com/devopsguys-announce-redgate-partnership.html","tags":"devops","title":"DevOpsGuys announce RedGate partnership"},{"text":"from:https://www.gitlab.com/2015/05/11/gitlab-7-dot-10-dot-4-released/ GitLab 7.10.4 released Last week we had to pull our 7.10.2 release as in a small number of installations the migrations would fail because of a uniqueness constraint on tags. We did not release GitLab 7.10.3, as we improved a migration after creating the 7.10.3 version tag and wanted to include that in our patch release. Today we release GitLab 7.10.4 which solves the issues with the migrations and contains all fixes also present in 7.10.2. If you've already successfully upgraded to 7.10.2, you do not need to update at this time. The fixes in this patch: Fix migrations broken in 7.10.2 Add missing indices to tags for some installations Make tags for GitLab installations running on MySQL case sensitive And the following were fixed with 7.10.2, also included here: A bug when using the Gitorious importer A bug that prevented adding group members through the admin screen Broken links on the merge request page leading to CI services A 500 error when trying to search in the wiki A 500 error when trying to add new tags to a project A bug where commit data would not appear in some subdirectories due to escaped slashes A bug where branches with escaped characters in their names would not always work in the compare view Upgrade barometer There is a migration that loops through all tags. This can take a while for larger installations. The upgrade can be performed online. Theoretically, there is a small chance that if a tag is created during the migration of that specific tag, the tag counter gets a value that is slightly higher or lower than its actual value. We do not believe this is reason to schedule downtime and recommend performing the upgrade online. Updating To update, check out our update page . Enterprise Edition Omnibus packages for GitLab Enterprise Edition 7.10.4 are available for subscribers here . For installations from source, use this guide . Interested in GitLab Enterprise Edition? For an overview of feature exclusive to GitLab Enterprise Edition please have a look at the features exclusive to GitLab EE . Access to GitLab Enterprise Edition is included with a subscription . No time to upgrade GitLab yourself? A subscription also entitles to our upgrade and installation services. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","url":"http://www.ciandcd.com/gitlab-7104-released.html","tags":"scm","title":"GitLab 7.10.4 Released"},{"text":"from:http://blog.devopsguys.com/2015/05/08/sponsored-dog-walk/ The DevOpsGuys team are pulling up our hiking socks to raise cash for SSNAP and Countess Mountbatten Hospice this summer with a 13 mile walk across Wales. TrekFest it's no mean feat. We cross the highest peaks in the Beacons and South Wales including Pen y Fan (886m), Corn Du (873m), Cribyn (795m) and Fan y Big (719m). http://www.trekfest.org.uk/ Click here to donate now through our Just Giving Page Our first charity is special since last year, one of our brave team members was diagnosed with terminal cancer. Unfortunately, and with great sadness we know now they are losing their fight – even after enduring endless rounds of chemotherapy and surgery. The amazing staff at Countess Mountbatten Hospice are proving specialist palliative (end of life) care to many fighting a losing battles against advanced stage cancer. They also support their families and loved ones. We'd love to show our thanks and support by raising money on their behalf. It's with a tear in our eye and sadness in our hearts, that we chose our second charity to support. Last year, little Ollie lost his fight for his life after being born with a heart defect. He was 4 days old. His parents have displayed so much courage during an immensely difficult time during which the amazing doctors and nurses, support by the SSNAP team provided them with much needed support. It is their wish to continue to champion SSNAP, who provide support for the sick newborns and their parents at new born intensive care unit at The John Radcliffe Hospital, Oxford as so, we'll match whatever we raise through our JustGiving page as a donation to SSNAP. These amazing charities are truly special to us here DevOpsGuys. Please support us in raising funds for these brilliant charities who have done so much to support our close friends and employees. The team have signed up to complete the distance in six hours. The trek takes place in the Brecon Beacons and covers the highest peaks in South Wales: Pen y Fan, Corn Du and Fan y Big. The Beacons are a training ground for the SAS so, while the DOGs will have their work cut out for them, they're more than up for the challenge: \"I can't wait to get out there\" says office manager Rhian Owen. \"It's such a great opportunity to work together as a team to achieve personal goals and to raise money for good causes – it's going to be brilliant!\" We've set up a Just Giving page, so you can show your support here. It's a chance to donate to some good causes and get the DevOpsGuys and gals out from behind their screens and into the beautiful Welsh wilderness – come on y'all, dig deep! Click here to donate now through our Just Giving Page","url":"http://www.ciandcd.com/sponsored-dog-walk.html","tags":"devops","title":"Sponsored DOG Walk"},{"text":"from:https://www.gitlab.com/2015/05/07/gitlab-7-dot-10-dot-2-released/ GitLab 7.10.2 released Please note that we have discovered an issue in the migrations in this patch release. The tag migration will fail on certain duplicate tags in the database. Do not upgrade to 7.10.2 at this time. If you have already updated to 7.10.2 and everything is working fine, there is no need to worry. You can keep using GitLab normally. We've released GitLab 7.10.2 CE, EE and CI. It includes the following fixes, that apply to both CE and EE: A bug when using the Gitorious importer A bug that prevented adding group members through the admin screen Broken links on the merge request page leading to CI services A 500 error when trying to search in the wiki A 500 error when trying to add new tags to a project A bug where commit data would not appear in some subdirectories due to escaped slashes A bug where branches with escaped characters in their names would not always work in the compare view Upgrade barometer There is a migration that loops through all tags. This can take a while for larger installations. The upgrade can be performed online. Theoretically, there is a small chance that if a tag is created during the migration of that specific tag, the tag counter gets a value that is slightly higher or lower than its actual value. We do not believe this is reason to schedule downtime and recommend performing the upgrade online. Updating To update, check out our update page . Enterprise Edition Omnibus packages for GitLab Enterprise Edition 7.10.2 are available for subscribers here . For installations from source, use this guide . Interested in GitLab Enterprise Edition? For an overview of feature exclusive to GitLab Enterprise Edition please have a look at the features exclusive to GitLab EE . Access to GitLab Enterprise Edition is included with a subscription . No time to upgrade GitLab yourself? A subscription also entitles to our upgrade and installation services. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","url":"http://www.ciandcd.com/gitlab-7102-released.html","tags":"scm","title":"GitLab 7.10.2 Released"},{"text":"from:https://www.gitlab.com/2015/05/07/version-check/ Version Check Functionality We're working on a version check function for GitLab to reduce the problem of outdated servers. These servers are a security problem, provide a bad user experience and lead to issues being created with problems that have already been solved. By making outdated installations visible to its users we hope that people will upgrade sooner. How it'll work The version check will work in the following way. The /help page of GitLab will load an image from version.gitlab.com. This image will show green for an up to date version, yellow for an out of date version and red for a missing security update. The image requests parameters requests will contain the GitLab version and the server hostname. We'll store each request with a timestamp, the GitLab version and the server hostname. We will not store the user ip-address. We will send the server hostname to have more information about where and how GitLab is used. Loading external images is similar to how the gravatar images of users are used. Opt-out Just like the gravatar images you will be able to to turn off the functionality if you don't want your GitLab server to connect outside the firewall. The version check functionality can be disabled in the application settings. Trade-off Providing the new package server and the version check server requires constant maintenance and operational capacity. Getting better insight into where and how GitLab is used will help us improve GitLab for everyone. We realize that it sending the server hostname by default is not a trivial action and not everyone will be happy about this. We think that ensuring the sustainability of GitLab package server and version check services makes it a good trade-off. There will always be an option to turn this behavior off. Please let us know what you think about the above plan in the comments. Update We decided against sending the hostname in the url of the picture request. But the https picture request itself will have a HTTP referer header. We can use that to see where and how GitLab is used. We will still not store the ip-address of the requests. Install GitLab on your own server in 2 minutes Let's do it! Install GitLab on your own server in 2 minutes → Browse all posts @gitlab on Twitter. For the latest and most detailed news followon Twitter. Get every GitLab blog post and stay up to date. Please enable JavaScript to view the <a href=\"http://disqus.com/?ref_noscript\">comments powered by Disqus.</a>","url":"http://www.ciandcd.com/version-check-functionality.html","tags":"scm","title":"Version Check Functionality"},{"text":"from:http://blog.devopsguys.com/2015/05/06/devops-and-the-digital-supply-chain/ What is the \"Digital Supply Chain\" and why is it important to your organisation and to DevOps as a practice? The concept of the \"Digital Supply Chain\" is a different way of looking at the SDLC and the Continuous Delivery \"pipeline\" that we feel makes it easier for traditional organisations to understand the criticality of software delivery (and by extension DevOps) in the modern world. Any organisation that deals with physical goods understands the concept of the supply chain . They are intimately familiar with ideas like supply chain management , supply chain optimisation and, most importantly, they understand the economics of inventory in the supply chain e.g the carrying cost of inventory . So what is the \"Digital Supply Chain\"? The current definitions of the digital supply chain are anchored in the \"New Media\" sector and focus on digital assets like music, video etc \"The \" digital supply chain \" is a \" new media \" term which encompasses the process of the delivery of digital media, be it music or video, by electronic means, from the point of origin (content provider) to destination (consumer).\" – Wikipedia The Wikipedia article references above breaks it down into a number of discrete steps as shown in Figure 1 below. If we contrast this with our SDLC Continuous Delivery Pipeline (Figure 2) we can see that many of the steps are directly analogous – we are creating digital assets (code) which we then \"compress\" (i.e. the Build/Integrate process), which we then subject to Quality Control (Test), we store in a Digital Asset Management system (e.g. like Nexus or Artifactory), we tag it with metadata (e.g. what release/version we're deploying) and when then deploy it out to servers, CDN's, the AppStore or wherever. Once your customers grasp the idea that software is a digital asset and that carrying excess inventory and delays in moving these digital assets along the supply chain is costing them money it can be a lightbulb moment for many organisations. Software assets can depreciate over time. Indeed \" technical debt \" can be looked at as the \"cost of deprecation\" of your software assets over time. Code that is \"stuck\" in your Digital Supply Chain waiting for your next release (as source code in Git, or as artefacts in an artefact repository) represents a capital investment in \"digital assets\" held as \"digital inventory\" and having it sat on the digital shelf in your digital warehouse is costing you money is analogous to the carrying cost of inventory for physical inventory. Sure, the warehousing costs of a digital asset – your latest idea transformed into software code – is fairly trivial compared to the costs of physical warehousing BUT the \" opportunity cost \" is very real. Each digital software asset represents a significant investment in time & money by your designers, developers, testers, project managers etc and it doesn't start generating a return on that investment until it gets to the end of your digital supply chain and into the hands of your customers. DevOps then becomes a way to optimise your digital supply chain to ensure that we: only build the right things (reducing waste and optimising our digital inventory), Supplier management (by improving the relationships between Dev, Test, Ops etc we ensure that we are getting the best from all of the \"suppliers\" in our digital supply chain) improving our logistics to get our digital assets in the hand of our customers (by automating testing, release and deployment to accelerate the movement of the digital assets from left to right) Constantly seeking \"flow\" across the supply chain (the 1st way of DevOps!) Gathering metrics along the supply chain to give us insight into the bottlenecks (the M in the C.A.L.M.S model) So next time you're talking with people in the business try out the \"Digital Supply Chain\" analogy and see if it works for you – we'd love to hear your feedback! -TheOpsMgr","url":"http://www.ciandcd.com/devops-and-the-digital-supply-chain.html","tags":"devops","title":"DevOps and the Digital Supply Chain"},{"text":"from:http://blog.devopsguys.com/2015/05/05/cardiff-silicon-valley-comes-to-wales/ We've been set up in Cardiff, South Wales for nearly six months now. Every week it becomes more and more apparent that this city is fast becoming an exciting IT and technical hub; an attractive area for emerging and experienced tech talent alike. The term ‘Silicwm Valley' is being bandied about as more and more tech start-ups spring up in, or near, the city centre. Companies like DevOpsGuys, Cardiff Start, Indycube, Method 4, BBC Cymru's Roath Lock studios and a huge collection of digital and design agencies are choosing Cardiff as their base. It seems to be a logical step; the community is small enough to be interconnected, influential and supportive, but large enough to allow for the freedom to develop, expand and learn from the huge range of related industries in the immediate area. With several major universities in and around the city the wealth of talent is growing and Cardiff is taking the reins and nurturing Welsh talent and ability; a very different picture from several years ago where work in Wales was hard to come by and the majority of experienced IT professionals were obliged to seek work further afield, in London or Cambridge. Founder James Smith says: \"Cardiff has historically been built on industry, from the days of exporting coal. It's also frequently voted one of the top places to live and work in the UK, so it's no wonder that this tradition is developing and changing shape with the emergence of the tech industry – Cardiff is moving with the times. \"We've set up DevOpsGuys in Cardiff in order to be a part of this development. We wanted to provide opportunities for people in Wales – there's so much skill here. Plus we are working with international companies and forming partnerships with industry giants across the world; this is a great opportunity to share some of the home-grown Welsh talent, create unique, fulfilling career opportunities and forge connections all over the world. It's a really exciting time.\" The movement has been supported by the Welsh Government, with DevOpsGuys receiving funding to grow as a business and provide career opportunities in the Welsh capital. Meet-ups, tech events, talks and conferences all taking place in the city, give related, but wildly diverse businesses a chance to meet, mix, talk, share thoughts; ideas flow freely, business connections are forged easily and some new and interesting work is emerging. We're excited about Agile Cymru – the first event of its kind in Cardiff – this summer. There seems to be something new to see, do, read, visit, look at or enjoy every week! We're excited to see where Cardiff will take the DevOpsGuys and the future of the UK tech industry.","url":"http://www.ciandcd.com/cardiff-silicon-valley-comes-to-wales.html","tags":"devops","title":"Cardiff: Silicon Valley comes to Wales"},{"text":"from:http://blog.devopsguys.com/2015/05/01/devopsguys-at-redgate/ The DevOpsGuys headed off on a road trip this week to meet with the RedGate team at their amazing offices in Cambridge. As well as working on some workshop training opportunities and guest blog articles (stay tuned to the DevOpsGuys Blog for some RedGate articles coming soon) the teams got together to brainstorm ideas and share skills. We were able to look at some of their newest tools and we're excited to announce that we will be delivering workshops on RedGate DLM tools at various sessions across the country this summer. We've already implemented these tools for a many of our customers and we're delighted to be able to introduce their qualities, in detail, to a wide range of industry professionals as part of an effective, independent DevOps adoption process. The workshops will be running on: May 20 – Automated Database Deployment, London June 26 – Automated Database Deployment, Belfast July 8 – Database Source Control, London July 24 – Database Source Control, Manchester August 20 – Database Continuous Integration, Cardiff Spaces are limited, so register now to take part in a workshop or request a workshop near you.","url":"http://www.ciandcd.com/devopsguys-at-redgate.html","tags":"devops","title":"DevOpsGuys at RedGate"},{"text":"from:http://www.cnblogs.com/itech/p/3800590.html 如果是在外企工作的话，可以访问美国的机器，这样就可以在美国的机器上为自己装个proxy，然后本地就可以很容易的使用proxy来上网了。 主页： http://www.voidtrance.net/2010/01/simple-python-http-proxy/ 下载： http://www.voidtrance.net/downloads/tiny-proxy-0.3.1.tar.gz 1）很好用，下载然后在后台运行。只依赖于基本的python modules，运行的时候不需要root权限。 2） Chrome中的switchsharper插件的配置： #!/usr/bin/python __doc__ = \"\"\"Tiny HTTP Proxy. This module implements GET, HEAD, POST, PUT and DELETE methods on BaseHTTPServer, and behaves as an HTTP proxy. The CONNECT method is also implemented experimentally, but has not been tested yet. Any help will be greatly appreciated. SUZUKI Hisao 2009/11/23 - Modified by Mitko Haralanov * Added very simple FTP file retrieval * Added custom logging methods * Added code to make this a standalone application \"\"\" __version__ = \"0.3.1\" import BaseHTTPServer, select, socket, SocketServer, urlparse import logging import logging.handlers import getopt import sys import os import signal import threading from types import FrameType, CodeType from time import sleep import ftplib DEFAULT_LOG_FILENAME = \"proxy.log\" class ProxyHandler (BaseHTTPServer.BaseHTTPRequestHandler): __base = BaseHTTPServer.BaseHTTPRequestHandler __base_handle = __base.handle server_version = \"TinyHTTPProxy/\" + __version__ rbufsize = 0 # self.rfile Be unbuffered def handle(self): (ip, port) = self.client_address self.server.logger.log (logging.INFO, \"Request from '%s'\", ip) if hasattr(self, 'allowed_clients') and ip not in self.allowed_clients: self.raw_requestline = self.rfile.readline() if self.parse_request(): self.send_error(403) else: self.__base_handle() def _connect_to(self, netloc, soc): i = netloc.find(':') if i >= 0: host_port = netloc[:i], int(netloc[i+1:]) else: host_port = netloc, 80 self.server.logger.log (logging.INFO, \"connect to %s:%d\", host_port[0], host_port[1]) try: soc.connect(host_port) except socket.error, arg: try: msg = arg[1] except: msg = arg self.send_error(404, msg) return 0 return 1 def do_CONNECT(self): soc = socket.socket(socket.AF_INET, socket.SOCK_STREAM) try: if self._connect_to(self.path, soc): self.log_request(200) self.wfile.write(self.protocol_version + \" 200 Connection established\\r\\n\") self.wfile.write(\"Proxy-agent: %s\\r\\n\" % self.version_string()) self.wfile.write(\"\\r\\n\") self._read_write(soc, 300) finally: soc.close() self.connection.close() def do_GET(self): (scm, netloc, path, params, query, fragment) = urlparse.urlparse( self.path, 'http') if scm not in ('http', 'ftp') or fragment or not netloc: self.send_error(400, \"bad url %s\" % self.path) return soc = socket.socket(socket.AF_INET, socket.SOCK_STREAM) try: if scm == 'http': if self._connect_to(netloc, soc): self.log_request() soc.send(\"%s %s %s\\r\\n\" % (self.command, urlparse.urlunparse(('', '', path, params, query, '')), self.request_version)) self.headers['Connection'] = 'close' del self.headers['Proxy-Connection'] for key_val in self.headers.items(): soc.send(\"%s: %s\\r\\n\" % key_val) soc.send(\"\\r\\n\") self._read_write(soc) elif scm == 'ftp': # fish out user and password information i = netloc.find ('@') if i >= 0: login_info, netloc = netloc[:i], netloc[i+1:] try: user, passwd = login_info.split (':', 1) except ValueError: user, passwd = \"anonymous\", None else: user, passwd =\"anonymous\", None self.log_request () try: ftp = ftplib.FTP (netloc) ftp.login (user, passwd) if self.command == \"GET\": ftp.retrbinary (\"RETR %s\"%path, self.connection.send) ftp.quit () except Exception, e: self.server.logger.log (logging.WARNING, \"FTP Exception: %s\", e) finally: soc.close() self.connection.close() def _read_write(self, soc, max_idling=20, local=False): iw = [self.connection, soc] local_data = \"\" ow = [] count = 0 while 1: count += 1 (ins, _, exs) = select.select(iw, ow, iw, 1) if exs: break if ins: for i in ins: if i is soc: out = self.connection else: out = soc data = i.recv(8192) if data: if local: local_data += data else: out.send(data) count = 0 if count == max_idling: break if local: return local_data return None do_HEAD = do_GET do_POST = do_GET do_PUT = do_GET do_DELETE=do_GET def log_message (self, format, *args): self.server.logger.log (logging.INFO, \"%s %s\", self.address_string (), format % args) def log_error (self, format, *args): self.server.logger.log (logging.ERROR, \"%s %s\", self.address_string (), format % args) class ThreadingHTTPServer (SocketServer.ThreadingMixIn, BaseHTTPServer.HTTPServer): def __init__ (self, server_address, RequestHandlerClass, logger=None): BaseHTTPServer.HTTPServer.__init__ (self, server_address, RequestHandlerClass) self.logger = logger def logSetup (filename, log_size, daemon): logger = logging.getLogger (\"TinyHTTPProxy\") logger.setLevel (logging.INFO) if not filename: if not daemon: # display to the screen handler = logging.StreamHandler () else: handler = logging.handlers.RotatingFileHandler (DEFAULT_LOG_FILENAME, maxBytes=(log_size*(1<<20)), backupCount=5) else: handler = logging.handlers.RotatingFileHandler (filename, maxBytes=(log_size*(1<<20)), backupCount=5) fmt = logging.Formatter (\"[%(asctime)-12s.%(msecs)03d] \" \"%(levelname)-8s {%(name)s %(threadName)s}\" \" %(message)s\", \"%Y-%m-%d %H:%M:%S\") handler.setFormatter (fmt) logger.addHandler (handler) return logger def usage (msg=None): if msg: print msg print sys.argv[0], \"[-p port] [-l logfile] [-dh] [allowed_client_name ...]]\" print print \" -p - Port to bind to\" print \" -l - Path to logfile. If not specified, STDOUT is used\" print \" -d - Run in the background\" print def handler (signo, frame): while frame and isinstance (frame, FrameType): if frame.f_code and isinstance (frame.f_code, CodeType): if \"run_event\" in frame.f_code.co_varnames: frame.f_locals[\"run_event\"].set () return frame = frame.f_back def daemonize (logger): class DevNull (object): def __init__ (self): self.fd = os.open (\"/dev/null\", os.O_WRONLY) def write (self, *args, **kwargs): return 0 def read (self, *args, **kwargs): return 0 def fileno (self): return self.fd def close (self): os.close (self.fd) class ErrorLog: def __init__ (self, obj): self.obj = obj def write (self, string): self.obj.log (logging.ERROR, string) def read (self, *args, **kwargs): return 0 def close (self): pass if os.fork () != 0: ## allow the child pid to instanciate the server ## class sleep (1) sys.exit (0) os.setsid () fd = os.open ('/dev/null', os.O_RDONLY) if fd != 0: os.dup2 (fd, 0) os.close (fd) null = DevNull () log = ErrorLog (logger) sys.stdout = null sys.stderr = log sys.stdin = null fd = os.open ('/dev/null', os.O_WRONLY) #if fd != 1: os.dup2 (fd, 1) os.dup2 (sys.stdout.fileno (), 1) if fd != 2: os.dup2 (fd, 2) if fd not in (1, 2): os.close (fd) def main (): logfile = None daemon = False max_log_size = 20 port = 8000 allowed = [] run_event = threading.Event () local_hostname = socket.gethostname () try: opts, args = getopt.getopt (sys.argv[1:], \"l:dhp:\", []) except getopt.GetoptError, e: usage (str (e)) return 1 for opt, value in opts: if opt == \"-p\": port = int (value) if opt == \"-l\": logfile = value if opt == \"-d\": daemon = not daemon if opt == \"-h\": usage () return 0 # setup the log file logger = logSetup (logfile, max_log_size, daemon) if daemon: daemonize (logger) signal.signal (signal.SIGINT, handler) if args: allowed = [] for name in args: client = socket.gethostbyname(name) allowed.append(client) logger.log (logging.INFO, \"Accept: %s (%s)\" % (client, name)) ProxyHandler.allowed_clients = allowed else: logger.log (logging.INFO, \"Any clients will be served...\") server_address = (socket.gethostbyname (local_hostname), port) ProxyHandler.protocol = \"HTTP/1.0\" httpd = ThreadingHTTPServer (server_address, ProxyHandler, logger) sa = httpd.socket.getsockname () print \"Servering HTTP on\", sa[0], \"port\", sa[1] req_count = 0 while not run_event.isSet (): try: httpd.handle_request () req_count += 1 if req_count == 1000: logger.log (logging.INFO, \"Number of active threads: %s\", threading.activeCount ()) req_count = 0 except select.error, e: if e[0] == 4 and run_event.isSet (): pass else: logger.log (logging.CRITICAL, \"Errno: %d - %s\", e[0], e[1]) logger.log (logging.INFO, \"Server shutdown\") return 0 if __name__ == '__main__': sys.exit (main ())","url":"http://www.ciandcd.com/jian-dan-de-proxyzhi-tinyhttpproxypy-itech.html","tags":"中文","title":"简单的proxy之TinyHTTPProxy.py - iTech"},{"text":"from:http://continuousdelivery.com/2014/06/the-2014-state-of-devops-report-is-here/ The 2014 State of DevOps Report Is Here! DevOps, a movement of people who care about developing and operating reliable, secure, high performance systems at scale, has always — intentionally — lacked a definition or manifesto. However (and this is fascinating in its own right) that doesn't mean that we can't measure the impact of DevOps, or how good people are at doing it. The proof of this, and also of the startling impact of the DevOps movement, is now available in the form of the 2014 State of DevOps report (which you can download for free ). The report, a collaboration between Nicole Forsgren Velasquez , Gene Kim , Puppet Labs , and yours truly , surveyed over 9,200 people worldwide, covering a wide range of industries and types of organization. Our goal for the report was ambitious. We set out to measure IT performance, business performance, the impact of particular practices (such as continuous integration, test automation, and version control), and also culture, and then to discover to what extent they influenced each other. How, you might ask, do you measure these things like culture and organizational performance? Following Douglas Hubbard's definition of measurement as \"A quantitatively expressed reduction of uncertainty based on one or more observations,\" it turns out that you can measure anything if you put your mind to it. The report describes both our methodology and the way we measured these apparent intangibles. Indeed we not only measured these things: we have sound, statistically significant data that shows that culture and DevOps practices impact both IT performance and organizational performance. In direct contradiction to a popular narrative of the last ten years, IT matters — indeed, the results show it is a competitive advantage — and DevOps culture and practices are instrumental in achieving both high IT performance and organizational performance. Readers of this blog will be especially interested to learn that: Trunk-based development, continuous integration, and automated testing measurably improve both IT performance and organizational performance. Having a high-trust culture has a strong impact on both IT performance and organizational performance. Using external change approval processes such as a change advisory board, as opposed to peer-based code review techniques, significantly impacts throughput while doing almost nothing to improve stability. Job satisfaction is the biggest predictor of organizational performance, and using DevOps practices are good predictors of job satisfaction. I'm very excited by the report. We improved on last year's method for measuring IT performance. We showed how you can measure culture and organizational performance. Most important, the analysis of our enormous data set demonstrates definitively that the strategies championed by the DevOps movement work, and that they provide a competitive advantage to your business. Many thanks to my collaborators, the fabulous team at PuppetLabs, and to all of you who took the survey. You can download the 2014 State of Devops Report for free.","url":"http://www.ciandcd.com/the-2014-state-of-devops-report-is-here.html","tags":"ciandcd","title":"The 2014 State of DevOps Report Is Here!"},{"text":"from:http://www.cnblogs.com/itech/p/3696895.html #http://perlmaven.com/open-and-read-from-files #mode operand create truncate #read < #write > yes yes #append >> yes Case 1: Throw an exception if you cannot open the file: use strict; use warnings; my $filename = 'data.txt'; open(my $fh, '<:encoding(UTF-8)', $filename) or die \"Could not open file '$filename' with the error $!\"; while (my $row = <$fh>) { chomp $row; print \"$row\\n\"; } close($fh); Case 2: Give a warning if you cannot open the file, but keep running: use strict; use warnings; my $filename = 'data.txt'; if (open(my $fh, '<:encoding(UTF-8)', $filename)) { while (my $row = <$fh>) { chomp $row; print \"$row\\n\"; } close($fh); } else { warn \"Could not open file '$filename' $!\"; } Case 3: Read one file into array use strict; use warnings; my $filename = 'data.txt'; open (FILEIN, \"<\", $filename) or die \"Could not open file '$filename' with the error $!\"; my @FileContents = <FILEIN>; for my $l (@FileContents){ print \"$l\\n\"; } close FILEIN; end","url":"http://www.ciandcd.com/perl-du-xie-wen-jian-itech.html","tags":"中文","title":"perl 读写文件 - iTech"},{"text":"from:http://continuousdelivery.com/2014/02/visualizations-of-continuous-delivery/ Visualizations of Continuous Delivery Nhan Ngo , a QA engineer at Spotify , made four fabulous visualizations while reading Continuous Delivery . She has very kindly agreed to make them available under a Creative Commons license so feel free to share them, download them, and print them out (click to get a higher resolution version). Thank you Nhan!","url":"http://www.ciandcd.com/visualizations-of-continuous-delivery.html","tags":"ciandcd","title":"Visualizations of Continuous Delivery"},{"text":"from:http://continuousdelivery.com/2013/12/the-science-behind-the-2013-puppet-labs-devops-survey-of-practice/ The Science Behind the 2013 Puppet Labs DevOps Survey Of Practice By Gene Kim and Jez Humble Last year, we both had the privilege of working with Puppet Labs to develop the 2012 DevOps Survey Of Practice. It was especially exciting for Gene, because we were able to benchmark the performance of over 4000 IT organizations, and to gain an understanding what behaviors result in their incredible performance. This continues research that he has been doing of high performing IT organizations that started for him in 1999. In this blog post, Gene Kim and I will discuss the research hypotheses that we're setting out to test in the 2013 DevOps Survey Of Practice, explain the mechanics of how these types of cross-population studies actually work (so you help this research effort or even start your own), then describe the key findings that came out of the 2012 study. But first off, if you're even remotely interested in DevOps, go take the 2013 Puppet Labs DevOps Survey here ! The survey closes on January 15, 2014, so hurry! It only takes about ten minutes. 2013 DevOps Survey Research Goals Last year's study (which we'll describe in more detail below) found that high performing organizations that were employing DevOps practices were massively outperforming their peers: they were doing 30x more frequent code deploys, and had deployment lead times measured in minutes or hours (versus lower performers, who required weeks, months or quarters to complete their deployments). The high performers also had far better deployment outcomes: their changes and deployments had twice the change success rates, and when the changes failed, they could restore service 12x faster. The goal of the 2013 study is to gain a better understanding of exactly what practices are required to achieve this high performance. Our hypothesis is that the following are required, and we'll be looking to independently evaluate the effect of each of these practices on performance: small teams with high trust that span the entire value stream: Dev, QA, IT Operations and Infosec shared goals and shared pain that span the entire value stream small development batch sizes presence of continuous, automated integration and testing emphasis on creating a culture of learning, experimentation and innovation emphasis on creating resilient systems We are also testing two other hypotheses that one of us (Gene) is especially excited about, because it's something he's wanted to do ever since 1999! Lead time : In plant manufacturing, lead time is the time required to turn raw materials into finished goods. There is a deeply held belief in the Lean community that lead time is the single best predictor of quality, customer satisfaction and employee happiness. We are testing this hypothesis for the DevOps value stream in the 2013 survey instrument. Organizational performance : Last year, we confirmed that DevOps practices correlate with substantially improved IT performance (e.g., deploy frequencies, lead times, change success rates, MTTR). This year, we will be testing whether improved IT performance correlates with improved business performance. In this year's study, we've added inserted three questions that are known to correlate with organizational performance, which is known to correlate with business performance (e.g., competitiveness in the marketplace, return on assets, etc.). Our dream headline would be, \"high performing organizations not only do 30x more frequent code deployments than their peers, but they also outperform the S&P 500 by 3x as measured by shareholder return and return on assets.\" Obviously, there are many other variables that contribute to business performance besides Dev and Ops performance (e.g., profitability, market segment, market share, etc.). However, in our minds, the reliance upon IT performance is obvious: as Chris Little said, \"Every organization is an IT business, regardless of what business they think they're in.\" When IT does poorly, the business will do poorly. And when IT helps the organization win, those organizations will out-perform their competitors in the marketplace. (This hypothesis forms the basis of the hedge fund that Erik wants to create in the last chapter of \"The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win\" , where they would make long or short bets, based on the known operating characteristics of the IT organization.) The Theory Behind Cross-Population Studies and Survey Instruments Like last year, this year's DevOps survey is a cross-population study, designed to explore the link between organizational performance and organizational practices and cultural norms. What is a cross-population study? It's a statistical research technique designed to uncover what factors (e.g., practices, cultural norms, etc.) correlate with outcomes (e.g., IT performance). Cross-population studies are often used in medical research to answer questions like, \"is cigarette smoking a significant factor in early mortality?\" Properly designed cross-population studies are considered a much more rigorous approach of testing efficacy of what practices work than say, interviewing people about what they think worked, ROI stories from vendors, or collecting \"known, best practices.\" When doing survey design, we might state our hypotheses in the following form: \"we believe that IT organizations which have high trust have higher IT performance.\" In other words, the higher the trust levels in the IT organization, the higher the performance. We then put this question in the survey instrument, and then analyze the results. If we were to plot the results on a graph, we would put the dependent variable (i.e., performance) on the Y-axis, and the independent variable (i.e., presence of high trust) on the X-axis. We would then test to see if there is a correlation between the two. Shown below is an example of what it looks like when the two variables have low or no correlation, and one that has a significant positive correlation. If we were to find a significant correlation, such as displayed on the right, we could then assert that \"the higher your organization's trust levels, in general, the higher your IT performance.\" (Graph adapted from Wikipedia entry on Correlation and Dependence .) The 2012 DevOps Survey In this section, we will describe the the key findings that came out of the 2012 DevOps Survey, as well as a brief discussion of the research hypotheses that went into the survey design. In the DevOps community, we have long asserted that certain practices enables organizations simultaneously deliver fast flow of features to market, while providing world-class stability, reliability and security. We designed the survey to validate this, and tested a series of technical practices to determine which of them correlated with high performance. The survey ran for 30 days, and we had 4,039 completed respondents. (This is an astonishingly high number, by the way. When Kurt Milne and Gene Kim did similar studies in 2006, each study typically required $200K to do the survey design, gather responses from a couple hundred people, and then perform survey analysis.) You can find the slides that Gene Kim, Jez Humble and James Turnbull presented at the 2013 Velocity Conference here , and the full Puppet Labs infographics and results here . The first surprise was how much the high performing organizations were outperforming their non-high-performing peers: Agility metrics 30x more frequent code deployments 8,000x faster lead time than their peers Reliability metrics 2x the change success rate 12x faster MTTR In other words, they were more agile: they were deploying code 30x more frequently, and the lead time required to go from \"code committed\" to \"successfully running in production\" was completed 8,000x faster — high performers had lead times measured in minutes or hours, while lower performers had lead times measured in weeks, months or even quarters. Not only were the high performers doing more work, but they had far better outcomes: when the high performers deployed changes and code, they were twice as likely to be completed successfully (i.e., without causing a production outage or service impairment), and when the change failed and resulted in an incident, the time required to resolve the incident was 12x faster. We were astonished and delighted with this finding, as it showed not only that it was possible to break the core, chronic conflict, but that it seemed to confirm that just as in manufacturing, agility and reliability go hand in hand. In other words, lead time correlates with both both agility and reliability. (Gene will write more on his personal interpretations of the 2012 DevOps Survey Of Practice in a future post.) Conclusion We hope this gives you a good idea of why we've worked so hard on the 2012 and 2013 DevOps Survey, as well as how to conduct your own cross-population studies. Please let us know if you have any questions or if there's anything we can do for you. And of course, help us understand what in DevOps and Continuous Delivery work by taking 10 minutes to participate in the 2013 Puppet Labs DevOps Survey here by January 15, 2014 ! Thank you! –Gene Kim and Jez Humble","url":"http://www.ciandcd.com/the-science-behind-the-2013-puppet-labs-devops-survey-of-practice.html","tags":"ciandcd","title":"The Science Behind the 2013 Puppet Labs DevOps Survey Of Practice"},{"text":"from:http://continuousdelivery.com/2013/12/flowcon-2013-wrap-up/ FlowCon 2013 Wrap-Up, With Some Hard Data on Gender Diversity in Tech Conferences. Thanks to all of you who came along to FlowCon! If you weren't able to make it, you can watch the videos for free thanks to BMC and ThoughtWorks Studios . The slides are also available for downloading. Let me first express my thanks to our producers: Geeta Schmidt and Niley Barros of Trifork and Rebecca Phillips of ThoughtWorks Studios . I also want to thank my fellow PC members Lane Halley , Elisabeth Hendrickson , Gene Kim and John Esser ; our fabulous speakers ; our generous sponsors ; and everyone who came along. The Program The goal of the program committee was to create a conference that represents our industry as we want it to look, not as it is right now. That's an ambitious goal that involves changing the way we think about everything from leadership and governance through product development and design , to IT operations . Not only did our speakers cover all these topics; they also provided real examples of how these changes, along with the cultural changes necessary to support them, have been achieved at enterprise scale. Thus we attacked one of the main objections we hear time and time again — \"that sounds great, but it couldn't work here\". Part of our vision was to provide a platform for people to speak about gnarly, real-life examples that demonstrate that, with sufficient hard work and ingenuity, ideas like continuous delivery, devops, and lean product development can provide significant competitive advantage through higher quality, cost savings, and happier customers, even in traditionally slow-moving and highly-regulated industries with large, complex, heterogeneous systems. Two talks that I am particularly happy to have on record are Gary Gruver's talk on doing continuous delivery for printer firmware at HP, and John Kordyback's talk on doing continuous delivery with mainframes in the financial services industry. Alternatively, if you want a vision of the state of the art of continuous delivery, it would be hard to beat Adrian Cockcroft's opening keynote (the most highly rated talk of the conference) on how Netflix approach building and running systems. Overall, both the individual quality of the talks and the vision they present in concert was incredibly inspiring. Gene Kim comments, \"The FlowCon program was amazing. In my mind, what was presented at FlowCon is what every IT practitioner will be required to know in 10 years time.\" Thank you again to all of our speakers. Data on Gender Diversity Part of representing the industry as we want it to look is changing its composition. Thus another personal goal for me was to gather data to support my hypothesis that taking steps to increase diversity at conferences doesn't mean reducing quality. FlowCon, like the excellent GOTO conferences that Trifork produces, records feedback from participants. Everybody leaving a session can give feedback on whether they thought the talk was good, mediocre or poor by tapping a red, amber or green rectangle on an iPhone on their way out. We then calculate overall satisfaction as follows: satisfaction = (green votes) / (total votes). When we got back all the data, the first thing I did is look at the average (mean) satisfaction for male speakers versus female speakers. It turns out that in both cases the average is between 71% and 72%. First of all, this demonstrates that there was no statistically significant difference in satisfaction between male and female speakers. This is important because it means our steps to increase diversity — including reaching out to a wide network to ensure that 50% of our invited speakers were women — didn't \"lower the bar\". There is also a deeper implication: any claim that the all-white-male conference programs that are so depressingly common in the tech industry are the result of some meritocratic process is BS. They are, rather, the result of not putting in enough effort to seek out high quality speakers from historically discriminated against groups. If our industry were truly meritocratic, the speaker line-up and attendees would resemble the wider population, because we know that there is no biological explanation for the overwhelming proportion of white dudes in our industry. So let's not fool ourselves any more with claims that taking steps to improve diversity is \"reverse discrimination\". Any time we don't take concrete, systematic steps forward we are silently complicit in perpetuating the status quo — which is why it's not good enough when leaders in the tech community ignore the problem. If you ignore the problem, you're part of the problem. Finally, I want to emphasize that what the program committee achieved was not very hard, once we spent some time thinking the problem through, and also that it was insufficient. We had a reasonable level of gender diversity, but the speakers were still overwhelmingly white. I don't have data for the diversity of our audience, but based on observation, there were more white guys than I would see if I walked out of the door onto the streets (and this is in San Francisco, which is far from being representative of the wider population). If you want to educate yourself further on these issues, I suggest watching Ashe Dryden's talk on programming diversity. And if you'd like to become more effective at creating change, check out Linda Rising's closing keynote . Here's to taking small steps every day to make 2014 a marginally, incrementally, better year than 2013 .","url":"http://www.ciandcd.com/flowcon-2013-wrap-up-with-some-hard-data-on-gender-diversity-in-tech-conferences.html","tags":"ciandcd","title":"FlowCon 2013 Wrap-Up, With Some Hard Data on Gender Diversity in Tech Conferences."},{"text":"from:http://continuousdelivery.com/2013/09/how-we-got-40-female-speakers-at-flowcon/ How To Create A More Diverse Tech Conference I have been advised by people I trust that it's not a good idea to talk about how you got serious female representation at your conference until after it's over. However the shameful RubyConf \"binders full of men\" debacle and the Neanderthal level of discussion around it has wound me up enough to write this account somewhat prematurely. So here is how we achieved >40% female representation on our speaker roster at FlowCon . Step 0. Care About The Outcome. When John Esser approached me to put together a conference about continuous delivery, devops and lean product development, I thought carefully about it. I've helped put together a conference program before ( QCon SF 2012 ), and that was pretty hard work, so I wanted to be sure I had the correct motivation. One of the things that I have always disliked about tech conferences is being surrounded by a bunch of other straight white guys (nothing personal, some of my best friends are straight white guys). It's a constant reminder of the fact that, due to a number of socioeconomic factors, straight white guys have it easier than others . I wanted to put together a conference which reflects my community as I would like it to look, not as it actually looks. So one of the four values the FlowCon program committee came up with was this: \"Diversity: We believe the technology community – and thus the conference speakers and participants – should reflect the demographics of our customers and the wider world.\" There are two reasons for this. Firstly, we can't effectively change the world through technology without diversity. To find out why, come and see Ashe Dryden talk about how \"diverse communities and workplaces create better products\" . Second, one of the main reasons I like working at ThoughtWorks is that one of the three pillars of our mission is to \"advocate passionately for social and economic justice.\" The fact there are so few women in IT reflects social and economic injustice inherent in our world. Making sure you actually have a mission for your conference is something I learned from helping out with QCon SF . It is a constant reminder of why you're doing it and what's important about it. If you don't have a mission, you're at the mercy of the implicit biases of the organizers. As RubyConf shows, you can't just throw in the \"one weird trick\" of anonymous submissions and expect that it will somehow solve the problem. Everybody on the program committee actually has to care about the outcome, or they won't put in the right amount of work to make it happen. Once you do that, the rest of the steps aren't that hard. Step 1. Make Sure Your Program Committee Is Aligned With Your Mission Once I had an idea about the mission of the conference, I reached out to some people whom I thought would share it. I was lucky enough that Elizabeth Hendrickson , Lane Halley and Gene Kim agreed to join John Esser and me on the program committee. One of the main reasons I asked those particular people, apart from being extremely competent and well-respected in their field, was another conference goal: \"Spanning boundaries: We believe that the best products are created collaboratively by people with a range of skills and experiences.\" The program committee has representation from the UX, testing, operations, product development and programming communities. Step 2. Make Sure Your Invited Speakers Are Aligned With Your Mission. We made the decision to have about half the program be invited speakers. Part of that was about ensuring that we had a solid core program. But it was also a chance for us to put our mission into practice, so that when we put out the call for proposals we had a bunch of confirmed speakers who demonstrated we were serious about our mission. Thus we made sure that the invited speakers were respected boundary spanners, and that 50% of them were women. This involved more work than we would have had to put in had we just invited our friends (a popular strategy for program committees). It was also telling that we got more refusals from women than we got from men due to schedule conflicts. The main factor here was that female speakers are actually in greater demand than men because there are relatively fewer of them. Step 3. The Anonymous Call For Proposals If you jump straight to step 3, it's likely you will suffer the fate of RubyConf and fail. If you use this as your only strategy for increasing representation it won't work. This strategy has been thoroughly discussed by others who have used this approach as part of increasing diversity at their conference. We created a form in Google Docs for people to propose talks. They had to enter their email address, but we mentioned in the form that they should use one that didn't identify them if they wanted their proposal to be more anonymous. Of the 82 people who submitted a talk proposal, 18 (21%) were women as far as we can work out (once the program was confirmed I used Rapportive to reverse-engineer email addresses based on publicly available information). Ultimately, three of the eight people who made it into the final program based on submitted proposals were women. The low female representation through the CFP is the reason our program isn't 50% female. Even getting the 21% of submissions that we did involved reaching out through mailing lists, Twitter, and our networks to encourage women to submit. This step, along with making it clear that you actually care, is essential if you in fact expect women to submit through the anonymous CFP. Observations These four steps resulted in 10 of our 24 speakers being women . I have three main observations coming out of this process: First, unlike increasing the number of women who take programming classes in school or enter the IT industry and don't immediately quit in horror, creating a conference with reasonable female representation is not actually a hard problem. Yes, we put in more work to achieve this goal than we would have had we not cared. But it wasn't significantly more. Conference organizers who claim to care but fail to achieve good representation should quit whining and take real steps to achieve this goal. The community should hold them to higher standards. If the conference speakers are a bunch of straight white guys, the only reason is that the organizers didn't care enough. Second, in the wake of RubyConf, I have been angered but unsurprised to observe the usual chorus about how increasing representation somehow means lowering standards. Not only is this incredibly insulting to the many extraordinary women working in our industry, but it is just false. I dare anyone to look at the kick-ass program we have put together for FlowCon and try and claim that we have somehow lowered standards to achieve great a barely acceptable level of representation. Another thing you will hear is that it is harder to find female speakers on \"hard\" topics such as programming than for \"soft\" ones. I find this claim baffling because in my experience changing organizational culture (considered a \"soft\" topic) is, in my experience, way way harder than knocking out lines of code (even well-factored unit-tested ones). But you'll see on our program that women are covering the whole gamut from organizational change to refactoring to configuration management . Third, it's not all good news. In particular, we have only one non-white speaker. I'll hold my hand up on this – we didn't explicitly set non-white representation as a goal within the program committee, and by the time it became obvious it was a problem (Step 3) it was too late to do anything. This demonstrates why steps 0-2 are important. If we run FlowCon again, we will do better. Meanwhile check out the program , and follow this link to register with a 10% discount. If you need more than a one day conference to come to San Francisco, Balanced Team are running their conference the following two days. End notes Another popular silencing tactic in this discussion is that bringing attention to the level of diversity in a conference is in itself a form of sexism or racism. There's a cartoon on the left which expresses nicely why this is in fact horribly misguided (or you could check out one of the many excellent articles on \"colourblindness\" and racism ). Check out the Geek Feminism blog and wiki for tons of useful information and advice on making things better for women in tech. Also check out the @CallbackWomen and @DevChix Twitter accounts to spread the word for your CFP. Ashe Dryden also wrote an excellent post on creating more diverse conferences. Another important factor when designing a woman-friendly conference is to create an anti-harassment policy. Check out this account of a woman who actually needed to use the anti-harassment policy (trigger alert). UPDATE Of course, this entry is now starting to receive the attention of anonymous trolls. I've left the first one as an example of the idiocy that passes for dialogue in this debate (and from supposedly smart people at that). But forthwith I'll be deleting anonymous or otherwise uncivil posts.","url":"http://www.ciandcd.com/how-to-create-a-more-diverse-tech-conference.html","tags":"ciandcd","title":"How To Create A More Diverse Tech Conference"},{"text":"from:http://continuousdelivery.com/2013/08/risk-management-theatre/ How To Create A More Diverse Tech Conference Videos from the Continuous Delivery track at QCon SF 2012 » Risk Management Theatre: On Show At An Organization Near You Translations: 한국말 One of the concepts that will feature in the new book I am working on is \"risk management theatre\". This is the name I coined for the commonly-encountered control apparatus, imposed in a top-down way, which makes life painful for the innocent but can be circumvented by the guilty (the name comes by analogy with security theatre .) Risk management theatre is the outcome of optimizing processes for the case that somebody will do something stupid or bad, because (to quote Bjarte Bogsnes talking about management ), \"there might be someone who who cannot be trusted. The strategy seems to be preventative control on everybody instead of damage control on those few.\" Unfortunately risk management theatre is everywhere in large organizations, and reflects the continuing dominance of the Theory X management paradigm. The alternative to the top-down control approach is what I have called adaptive risk management, informed by human-centred management theories (for example the work of Ohno , Deming , Drucker, Denning and Dweck ) and the study of how complex systems behave, particularly when they drift into failure . Adaptive risk management is based on systems thinking, transparency, experimentation, and fast feedback loops. Here are some examples of the differences between the two approaches. Adaptive risk management (people work to detect problems through improving transparency and feedback, and solve them through improvisation and experimentation) Risk management theatre (management imposes controls and processes which make life painful for the innocent but can be circumvented by the guilty) Continuous code review in which engineers ask a colleague to look over their changes before check-in, technical leads review all check-ins made by their team, and code review tools allow people to comment on each others' work once it is in trunk. Mandatory code review enforced by check-in gates where a tool requires changes to be signed off by somebody else before they can be merged into trunk. This is inefficient and delays feedback on non-trivial regressions (including performance regressions). Fast, automated unit and acceptance tests which inform engineers within minutes (for unit tests) or tens of minutes (for acceptance tests) if they have introduced a known regression into trunk, and which can be run on workstations before commit. Manual testing as a precondition for integration, especially when performed by a different team or in a different location. Like mandatory code review, this delays feedback on the effect of the change on the system as a whole. A deployment pipeline which provides complete traceability of all changes from check-in to release, and which detects and rejects risky changes automatically through a combination of automated tests and manual validations. A comprehensive documentation trail so that in the event of a failure we can discover the human error that is the root cause of failures in the mechanistic, Cartesian paradigm that applies in the domain of systems that are not complex . Situational awareness created through tools which make it easy to monitor, analyze and correlate relevant data. This includes process, business and systems level metrics as well as the discussion threads around events. Segregation of duties which acts as a barrier to knowledge sharing, feedback and collaboration, and reduces the situational awareness which is essential to an effective response in the event of an incident. It's important to emphasize that there are circumstances in which the countermeasures on the right are appropriate. If your delivery and operational processes are chaotic and undisciplined, imposing controls can be an effective way to improve – so long as we understand they are a temporary countermeasure rather than an end in themselves, and provided they are applied with the consent of the people who must work within them. Here are some differences between the two approaches in the field of IT: Adaptive risk management (people work to detect problems through improving transparency and feedback, and solve them through improvisation and experimentation) Risk management theatre (management imposes controls and processes which make life painful for the innocent but can be circumvented by the guilty) Principle-based and dynamic: principles can be applied to situations that were not envisaged when the principles were created. Rule-based and static : when we encounter new technologies and processes (for example, cloud computing) we need to rewrite the rules. Uses transparency to prevent accidents and bad behaviour. When it's easy for anybody to see what anybody else is doing, people are more careful. As Louis Brandeis said, \"Publicity is justly commended as a remedy for social and industrial diseases. Sunlight is said to be the best of disinfectants; electric light the most efficient policeman.\" Uses controls to prevent accidents and bad behaviour. This approach is the default for legislators as a way to prove they have taken action in response to a disaster. But controls limit our ability to adapt quickly to unexpected problems. This introduces a new class of risks, for example over-reliance on emergency change processes because the standard change process is too slow and bureaucratic. Accepts that systems drift into failure. Our systems and the environment are constantly changing, and there will never be sufficient information to make globally rational decisions. Humans solve our problems and we must rely on them to make judgement calls. Assumes humans are the problem. If people always follow the processes correctly, nothing bad can happen. Controls are put in place to manage \"bad apples\". Ignores the fact that process specifications always require interpretation and adaptation in reality. Rewards people for collaboration, experimentation, and system-level improvements. People collaborate to improve system-level metrics such as lead time and time to restore service. No rewards for \"productivity\" on individual or function level. Accepts that locally rational decisions can lead to system level failures. Rewards people based on personal \"productivity\" and local optimization . For example operations people optimizing for stability at the expense of throughput, or developers optimizing for velocity at the expense of quality (even though these are false dichotomies.) Creates a culture of continuous learning and experimentation : People openly discuss mistakes to learn from them and conduct blameless post-mortems after outages or customer service problems with the goal of improving the system. People are encouraged to try things out and experiment (with the expectations that many hypotheses will be invalidated) in order to get better. Creates a culture of fear and mistrust . Encourages finger pointing and lack of ownership for errors, omissions and failure to get things done. As in: If I don't do anything unless someone tells me to, I won't be held responsible for any resulting failure. Failures are a learning opportunity . They occur in controlled circumstances, their effects are appropriately mitigated, and they are encouraged as an opportunity to learn how to improve. Failures are caused by human error (usually a failure to follow some process correctly), and the primary response is to find the person responsible and punish them, and then use further controls and processes as the main strategy to prevent future problems. Risk management theatre is not just painful and a barrier to the adoption of continuous delivery (and indeed to continuous improvement in general). It is actually dangerous, primarily because it creates a culture of fear and mistrust. As Bogsnes says, \"if the entire management model reeks of mistrust and control mechanisms against unwanted behavior, the result might actually be more, not less, of what we try to prevent. The more people are treated as criminals, the more we risk that they will behave as such.\" This kind of organizational culture is a major factor whenever we see people who are scared of losing their jobs, or engage in activities designed to protect themselves in the case that something goes wrong, or attempt to make themselves indispensable through hoarding information. I'm certainly not suggesting that controls, IT governance frameworks, and oversight are bad in and of themselves. Indeed, applied correctly, they are essential for effective risk management. ITIL for example allows for a lightweight change management process that is completely compatible with an adaptive approach to risk management. What's decisive is how these framework are implemented. The way such frameworks are used and applied is determined by—and perpetuates— organizational culture .","url":"http://www.ciandcd.com/risk-management-theatre-on-show-at-an-organization-near-you.html","tags":"ciandcd","title":"Risk Management Theatre: On Show At An Organization Near You"},{"text":"from:http://continuousdelivery.com/2013/05/videos-from-the-continuous-delivery-track-at-qcon-sf-2012/ Videos from the Continuous Delivery track at QCon SF 2012 At last year's QCon San Francisco I got to curate a track on continuous delivery. One of the goals of the QCon conferences is \"information Robin Hood\" – finding ways to get out into public the secret sauce of high performing organizations. So I set out to find talks that would answer the questions I frequently get asked: can continuous integration, automated testing, and trunk-based development scale? How does continuous delivery affect the way we do product management? What's the business case for continuous delivery? How do you grow a culture that enables it? You'll find the all these questions answered in the talks below, from the leaders who have been at the forefront of continuous delivery at Amazon, Facebook, Google and Etsy. They also discuss the tools they built and the and practices they use to enable continuous delivery. Finally, you get me talking about how you can adopt continuous delivery at your organization. Thanks so much to Jesse Robbins, Frank Harris, Nell Thomas, John Penix and Chuck Rossi for these great talks, and to the folks behind QCon SF for an awesome conference. Jesse Robbins ran ops at Amazon before quitting to co-found Opscode (creators of Chef ). He is also co-founder of Velocity . In his copious spare time, he's a volunteer firefighter. Basically, Jesse is an enormous over-achiever. This is a fabulous – and hilarious – talk that discusses the hardest part of implementing continuous delivery: cultural change. This talk features my favourite devops aphorism: One of the main goals of continuous delivery is to get fast feedback on your hypotheses so you can build the right thing. In this talk Frank Harris and Nell Thomas of Etsy show off a bunch of their tools, including the A/B testing framework they built for running experiments (which uses feature toggles under the hood). They give an example of an experiment they're running right now, and discuss how the ability to gather and analyze data on customer behaviour in real time (see screenshot below) affects the way they do product development. In this talk, John Penix of Google shows off the awesome product he and his team built for continuous integration and cloud-based testing at Google. Teams at Google are free to choose their own development practices and toolchain, but this one has a pretty high uptake. When people ask me if trunk-based development and continuous integration can scale, I like to show them the following slide: In addition to discussing the process he uses to release twice a day, Facebook's lead release engineer Chuck Rossi shows off the extensive toolchain they built to deploy at scale. Highlights include Gatekeeper (screenshot below), which manages who gets to see which features as part of their dark launching process, and their deploy tool which categorizes all proposed patches based on the size of the patch, the amount of discussion around it, and the \"push karma\" of the committers. Amazon, Etsy, Google and Facebook are all primarily software development shops which command enormous amounts of resources. They are, to use Christopher Little's metaphor, unicorns. How can the rest of us adopt continuous delivery? That's the subject of my talk, which describes four case studies of organizations that adopted continuous delivery, with varying degrees of success. One of my favourites – partly because it's embedded software, not a website – is the story of HP's LaserJet Firmware team, who re-architected their software around the principles of continuous delivery. People always want to know the business case for continuous delivery: the FutureSmart team provide one in the book they wrote that discusses how they did it:","url":"http://www.ciandcd.com/videos-from-the-continuous-delivery-track-at-qcon-sf-2012.html","tags":"ciandcd","title":"Videos from the Continuous Delivery track at QCon SF 2012"},{"text":"from:http://continuousdelivery.com/2013/05/announcing-flowcon/ Announcing FlowCon I spend quite a lot of time at conferences, and it consistently bothers me that they are so often focused on one particular function: development, testing, UX, systems administration. The point of continuous delivery is to accelerate the rate at which we can learn from each other – and from our customers. That requires everyone involved in the delivery process (including users, product owners and entrepreneurs) to collaborate throughout. So why isn't there a conference which focuses on flow – the emergent property of great teams? So I got together with a bunch of like-minded folks – Elisabeth Hendrickson , Gene Kim , John Esser and Lane Halley – and now there is a conference about creating flow: FlowCon . It's on Friday November 1 in San Francisco , and it's produced by ThoughtWorks and Trifork (creators of the GOTO conferences ). The conference is based around four values: Learning : Our goal is to provide the best possible conference forum for practitioners to learn from each other how to build great products and services. Open Information : We aim to uncover how great products and services are built in real life and make this information freely available to the widest audience possible. Diversity : We believe the technology community – and thus the conference speakers and participants – should reflect the demographics of our customers and the wider world. Spanning boundaries : We believe that the best products and services are created collaboratively by people with a range of skills and experiences. We have put together nearly half of the program , and we're delighted to announce that Adrian Cockcroft , Catherine Courage , Jeff Gothelf and Linda Rising will be giving keynotes. The program is still a work in process (a minimum viable product, if you will). In particular, the after lunch sessions are empty – for a good reason: we want you to speak in those slots . We're looking for people working to create flow in their organization – especially those who: Span multiple roles and work across organizational silos. Work in any of the following areas: a highly regulated environment; a large, traditional enterprise; in the pursuit of social and economic justice. Are willing to share obstacles encountered or mistakes made and how you overcame them – whether cultural or technological. Offer actionable advice \"the rest of us\" can apply today (even if we don't have the resources of Etsy / Amazon / Google). Your talk could be about culture, technology, design, process – the only really important criterion is that it draws on what you've learned about helping to create flow in your organization. If that sounds like you, please submit your proposal . If you know someone who would do a great job, please encourage them to submit. Our submission process is designed to be entirely merit-based, which means that the first round is anonymous. The deadline is midnight Pacific time, Sunday June 23, 2013. Tickets for the conference are now on sale – at $350 if you register before July 31, or $500 if you register afterwards. Whatever your role or domain, you're sure to find inspirational, disruptive thinking that will make you better at creating great products and services. I hope to see you there!","url":"http://www.ciandcd.com/announcing-flowcon.html","tags":"ciandcd","title":"Announcing FlowCon"},{"text":"from:http://continuousdelivery.com/2013/01/book-review-the-phoenix-project/ Book Review: The Phoenix Project I am not going to do a ton of book reviews on this blog (I have one more planned for next month). I'll only bother posting reviews of books that I believe are both excellent and relevant to Continuous Delivery . This book easily satisfies both criteria. Full disclosure: Gene gave me a draft of this book for free for reviewing purposes. You've probably heard of Gene Kim, Kevin Behr and George Spafford before. They are the three amigos responsible for The Visible Ops Handbook , which can be found in the book pile of every good IT operator. Their new book, The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win , follows the format of Eliyahu Goldratt's classic, The Goal . Told from the perspective of newly-minted VP of IT Operations Bill Palmer, it describes the turnaround of failing auto parts company Parts Unlimited. This is to be achieved through the delivery of the eponymous Phoenix Project, a classic \"too big to fail\" software project designed to build a system which will revive the fortunes of the company. To quote (p51): The plot is simple: First, you take an urgent date-driven project, where the shipment date cannot be delayed because of external commitments made to Wall Street or customers. Then you add a bunch of developers who use up all the time in the schedule, leaving no time for testing or operations deployment. And because no one is willing to slip the deployment date, everyone after Development has to take outrageous and unacceptable shortcuts to hit the date. The results are never pretty. Usually, the software product is so unstable and unusable that even the people who were screaming for it end up saying that it's not worth shipping. And it's always IT Operations who still has to stay up all night, rebooting servers hourly to compensate for crappy code, doing whatever heroics are required to hide from the rest of the world just how bad things really are. Part One of the book describes in loving detail the enormous clusterfuck pie that is baked from these ingredients. The pie is spiced with an internal Sarbanes-Oxley audit which reveals 952 control deficiencies, an outage of the payroll processing system, and various other problems that conspire to deepen the woe of the operations group, all of which are clearly drawn from the deep well of the authors' real-life experiences. Apart from the main characters – our hero Bill, his boss Steve, and the evil villain Sarah – The Phoenix Project features a delightful rogues' gallery which anyone working in an enterprise will recognize: Brent Geller, the boy wonder whose encyclopedic knowledge of the company's Byzantine IT systems means that his involvement is necessary to get anything done. Patty McKee, the Director of Support who runs a change management process so bureaucratic that everybody bypasses it. John Pesche, the black binder wielding Chief Information Security Officer whose constant meddling under the guise of improving security has turned him into a pariah. The second part of the book details how the IT group is reborn from the ashes of the Phoenix Project into a high-performing organization that is a strategic partner to the business. This is achieved through the application of a heavy dose of lean thinking (including continuous delivery ) administered by Erik, a mercurial IT and manufacturing guru Steve is courting to join the board. The book does an excellent job of showing – as well as telling – how to apply the concepts (and the effect of doing so) in an enterprise with plenty of technical debt. Perhaps the most eyebrow-raising part of this section is the way in which John has his soul mercilessly crushed to the point where he goes on a multi-day drinking spree before he is rehabilitated towards the end of the book (he is a phoenix too). John's narrative arc is just one example of how the book also succeeds as a novel. It's gripping, with moments of drama and high emotion, as well as some great one-liners. There was even one point when I teared up (bear in mind that I also cried during Forrest Gump – unlike the book's central characters, I did not serve in the armed forces). Nobody who has read The Goal will miss The Phoenix Project's similarity in terms of style and plot. Perhaps my favourite thing about the book's pedagogical style is the way Erik (like Jonah in The Goal) uses the Socratic Method to give Bill the tools to solve his problems by himself. Of course this learning process is fictional, but it means you get to see Bill struggling with the questions and trying things out. It remains to be seen whether readers of the book will be able to apply these techniques as successfully as Bill without a real Erik to guide them. But of course, this is a limitation of any book. If I had one criticism it's that unlike real life, there aren't many experiments in the book that end up making things worse, and it's this process of failing fast, learning from your failures, and coming up with new experiments that is instrumental to a real learning culture. One important point worth noting if you are working in an organization like Parts Unlimited is this: the IT department's rebirth is only possible because of the Titanic proportions of the disaster that unfolds in Part One. For management to truly embrace change, a compelling event or a teachable moment (i.e. a Charlie Foxtrot) is required. Unless your organization faces the same existential threat that Parts Unlimited does, you'll have a much harder time convincing people they should adopt the tools described in the book. Overall, The Phoenix Project is a fantastic read. It's entertaining, cathartic, inspirational and informative. If, like me, you have an enormous backlog of books (and more work in process than you'd like) I suggest giving yourself a break and putting this one to the top of your list. It'll only take you a day or two, and despite its conceptual density it will leave you feeling refreshed and energized with a bunch of new ideas to try out. The Phoenix Project deserves to be read by everyone who works in – or with – IT.","url":"http://www.ciandcd.com/book-review-the-phoenix-project.html","tags":"ciandcd","title":"Book Review: The Phoenix Project"},{"text":"from:http://continuousdelivery.com/2013/01/on-antifragility-in-systems-and-organizational-architecture/ On Antifragility in Systems and Organizational Architecture In his new book, Antifragile , Nassim Taleb discusses the behaviour of complex systems and distinguishes three kinds: those that are fragile, those that are robust or resilient, and those that are antifragile. These types of systems differ in how they respond to volatility: \"The fragile wants tranquility, the antifragile grows from disorder, and the robust doesn't care too much.\" (p20) Taleb argues that we want to create systems that are antifragile – that are designed to take advantage of volatility. I think this concept is incredibly powerful when applied to systems and organizational architecture. Why Continuous Delivery Works Taleb shows why the traditional approach of operations – making change hard, since change is risky – is flawed: \"the problem with artificially suppressed volatility is not just that the system tends to become extremely fragile; it is that, at the same time, it exhibits no visible risks… These artificially constrained systems become prone to Black Swans. Such environments eventually experience massive blowups… catching everyone off guard and undoing years of stability or, in almost all cases, ending up far worse than they were in their initial volatile state\" (p105) 1 . This a great explanation of how many attempts to manage risk actually result in risk management theatre – giving the appearance of effective risk management while actually making the system (and the organization) extremely fragile to unexpected events. It also explains why continuous delivery works. The most important heuristic we describe in the book is \"if it hurts, do it more often, and bring the pain forward.\" The effect of following this principle is to exert a constant stress on your delivery and deployment process to reduce its fragility so that releasing becomes a boring, low-risk activity. Antifragile Systems Another of Taleb's key claims is that it is impossible to predict \"Black Swan\" events: \"you cannot say with any reliability that a certain remote event or shock is more likely than another… but you can state with a lot more confidence that an object or a structure is more fragile than another should a certain event happen.\" (p8). Thus we need \"to switch the blame from the inability to see an event coming… to the failure to understand (anti)fragility, namely, ‘why did we build something so fragile to these types of events?'\" (p136). Unlike risk, fragility is actually measurable. How do we measure the fragility of the systems we build? We try to break them, using techniques such as game days and systems like chaos monkey . The systematic application of stress to your systems is essential – not just to ensure your systems are antifragile, but to develop the muscles of the people who create and maintain them through constant practice. After all, it's the combination of the system and the people who build and run it that has the quality of antifragility. In this context, an important quality of legacy systems is their fragility. Legacy systems that aren't touched for a long time will turn into fragile \"works of art\": changing them is considered risky, the number of people who understand the system decreases with time, and their knowledge atrophies from lack of exercise. How do we create antifragile systems? Apply stress to them continuously so we are forced to simplify, homogenise, and automate. Antifragile Organizations We can measure the fragility of an organization by how long it takes before it liquidates its assets. Deloitte's Shift Index shows that the average life expectancy of a Fortune 500 company has declined from around 75 years half a century ago to less than 15 years today. Start-ups are notoriously fragile. But the ones that survive and grow turn into something potentially more dangerous – robust organizations. The problem with robust organizations is that they resist change. They aren't quickly killed by changes to their environment, but they don't adapt to them either – they die slowly. We see this effect all the time – changing the culture of an established organization is incredibly hard. Antifragile organizations are those that have a culture that enables them to learn fast from their environment and adapt to it so they can take advantage of volatility. Here are some characteristics of antifragile organizations: Systems thinking. Everybody in the organization knows the goals of the organization and makes sure their work is directly contributing towards these goals. Theory Y Management. Management needs to assume employees are self-motivated and will be able to learn how to solve problems themselves. Organizations need to make sure they hire antifragile people who will thrive in this environment. As Daniel Pink's Drive points out, giving your employees autonomy, purpose, and the opportunity to learn and master new skills is what stops them from quitting, thus increasing the antifragility of your organization. Continuous experimentation. As described in Toyota Kata , good management knows that the best solutions come from the workers. They create an environment in which practitioners are able to run experiments to learn as rapidly as possible. The feedback loops in command and control organizations are too slow for them to adapt effectively. Disruptive product development. Antifragile organizations aren't content with stress generated by their environment. Like humans exercising, they also try and disrupt themselves (the organizational equivalent of a game day). For example, Amazon cannibalized its own business , creating the Amazon Marketplace and the Kindle. Apple is cannibalizing its Mac business with the iPad. Fragile organizations resist disrupting their own product lines, as Toshiba did at first with flash memory . If you do a good job at this you never need to worry about the competition – you'll always beat them to it. Fragility and Agility As Taleb points out, \"antifragility is desirable in general, but not always, as there are cases in which antifragility will be costly, extremely so. Further, it is hard to consider robustness as always desirable—to quote Nietzsche, one can die from being immortal.\" (p22) Of course working out where on the spectrum you want your systems and your organization to lie is an art, and the great artists are those that know how to build systems, organizations, and products simply, quickly and cheaply so that they are antifragile with respect to our biggest enemy: time. How do they do that? Using the same heuristics described in \"antifragile organizations\", above, which closely mirror the Three Ways of Devops . As I read Antifragile , it reminded me of something I read a number of years ago: Kent Beck and Cynthia Andres' Extreme Programming Explained . The subtitle? Embrace Change. It strikes me that the concept of antifragile is what we were aiming for with agile the whole time: building systems (including human systems – organizations) that benefit from volatility. Endnotes Thanks to Badrinath Janakiraman for feedback on an earlier draft of this post. 1 He is talking about financial markets, which are rather less fragile than IT systems, hence his rather generous \"years of stability\"","url":"http://www.ciandcd.com/on-antifragility-in-systems-and-organizational-architecture.html","tags":"ciandcd","title":"On Antifragility in Systems and Organizational Architecture"}]}